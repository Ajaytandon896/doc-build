import{S as oYr,i as rYr,s as tYr,e as a,k as l,w as f,t as o,L as aYr,c as n,d as t,m as i,a as s,x as m,h as r,b as d,J as e,g as v,y as h,q as g,o as u,B as p}from"../../chunks/vendor-b1433968.js";import{T as Upr}from"../../chunks/Tip-c3840994.js";import{D as y}from"../../chunks/Docstring-ff504c58.js";import{C as w}from"../../chunks/CodeBlock-a320dbd7.js";import{I as Z}from"../../chunks/IconCopyLink-7029626d.js";import"../../chunks/CopyButton-f65cb278.js";function nYr(ql){let re,Se,me,ue,ro,ge,Ce,$o,zl,Lc,Ot,Xl,Ql,vC,Bc,xe,ao,Vl,gn,bC,un,pn,TC,Wl,_n,FC,Hl,xc,_a;return{c(){re=a("p"),Se=o("If your "),me=a("code"),ue=o("NewModelConfig"),ro=o(" is a subclass of "),ge=a("code"),Ce=o("PretrainedConfig"),$o=o(`, make sure its
`),zl=a("code"),Lc=o("model_type"),Ot=o(" attribute is set to the same key you use when registering the config (here "),Xl=a("code"),Ql=o('"new-model"'),vC=o(")."),Bc=l(),xe=a("p"),ao=o("Likewise, if your "),Vl=a("code"),gn=o("NewModel"),bC=o(" is a subclass of "),un=a("a"),pn=o("PreTrainedModel"),TC=o(`, make sure its
`),Wl=a("code"),_n=o("config_class"),FC=o(` attribute is set to the same class you use when registering the model (here
`),Hl=a("code"),xc=o("NewModelConfig"),_a=o(")."),this.h()},l(no){re=n(no,"P",{});var pe=s(re);Se=r(pe,"If your "),me=n(pe,"CODE",{});var E0=s(me);ue=r(E0,"NewModelConfig"),E0.forEach(t),ro=r(pe," is a subclass of "),ge=n(pe,"CODE",{});var Ul=s(ge);Ce=r(Ul,"PretrainedConfig"),Ul.forEach(t),$o=r(pe,`, make sure its
`),zl=n(pe,"CODE",{});var C0=s(zl);Lc=r(C0,"model_type"),C0.forEach(t),Ot=r(pe," attribute is set to the same key you use when registering the config (here "),Xl=n(pe,"CODE",{});var M0=s(Xl);Ql=r(M0,'"new-model"'),M0.forEach(t),vC=r(pe,")."),pe.forEach(t),Bc=i(no),xe=n(no,"P",{});var Io=s(xe);ao=r(Io,"Likewise, if your "),Vl=n(Io,"CODE",{});var va=s(Vl);gn=r(va,"NewModel"),va.forEach(t),bC=r(Io," is a subclass of "),un=n(Io,"A",{href:!0});var y0=s(un);pn=r(y0,"PreTrainedModel"),y0.forEach(t),TC=r(Io,`, make sure its
`),Wl=n(Io,"CODE",{});var kc=s(Wl);_n=r(kc,"config_class"),kc.forEach(t),FC=r(Io,` attribute is set to the same class you use when registering the model (here
`),Hl=n(Io,"CODE",{});var w0=s(Hl);xc=r(w0,"NewModelConfig"),w0.forEach(t),_a=r(Io,")."),Io.forEach(t),this.h()},h(){d(un,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel")},m(no,pe){v(no,re,pe),e(re,Se),e(re,me),e(me,ue),e(re,ro),e(re,ge),e(ge,Ce),e(re,$o),e(re,zl),e(zl,Lc),e(re,Ot),e(re,Xl),e(Xl,Ql),e(re,vC),v(no,Bc,pe),v(no,xe,pe),e(xe,ao),e(xe,Vl),e(Vl,gn),e(xe,bC),e(xe,un),e(un,pn),e(xe,TC),e(xe,Wl),e(Wl,_n),e(xe,FC),e(xe,Hl),e(Hl,xc),e(xe,_a)},d(no){no&&t(re),no&&t(Bc),no&&t(xe)}}}function sYr(ql){let re,Se,me,ue,ro;return{c(){re=a("p"),Se=o("Passing "),me=a("code"),ue=o("use_auth_token=True"),ro=o(" is required when you want to use a private model.")},l(ge){re=n(ge,"P",{});var Ce=s(re);Se=r(Ce,"Passing "),me=n(Ce,"CODE",{});var $o=s(me);ue=r($o,"use_auth_token=True"),$o.forEach(t),ro=r(Ce," is required when you want to use a private model."),Ce.forEach(t)},m(ge,Ce){v(ge,re,Ce),e(re,Se),e(re,me),e(me,ue),e(re,ro)},d(ge){ge&&t(re)}}}function lYr(ql){let re,Se,me,ue,ro;return{c(){re=a("p"),Se=o("Passing "),me=a("code"),ue=o("use_auth_token=True"),ro=o(" is required when you want to use a private model.")},l(ge){re=n(ge,"P",{});var Ce=s(re);Se=r(Ce,"Passing "),me=n(Ce,"CODE",{});var $o=s(me);ue=r($o,"use_auth_token=True"),$o.forEach(t),ro=r(Ce," is required when you want to use a private model."),Ce.forEach(t)},m(ge,Ce){v(ge,re,Ce),e(re,Se),e(re,me),e(me,ue),e(re,ro)},d(ge){ge&&t(re)}}}function iYr(ql){let re,Se,me,ue,ro,ge,Ce,$o,zl,Lc,Ot,Xl,Ql,vC,Bc,xe,ao,Vl,gn,bC,un,pn,TC,Wl,_n,FC,Hl,xc,_a,no,pe,E0,Ul,C0,M0,Io,va,y0,kc,w0,i0e,m5e,Jl,Rc,PO,EC,d0e,SO,c0e,h5e,vn,f0e,$O,m0e,h0e,IO,g0e,u0e,g5e,CC,u5e,A0,p0e,p5e,Pc,_5e,Kl,Sc,DO,MC,_0e,NO,v0e,v5e,Do,yC,b0e,wC,T0e,L0,F0e,E0e,C0e,AC,M0e,jO,y0e,w0e,A0e,so,LC,L0e,OO,B0e,x0e,Yl,k0e,GO,R0e,P0e,qO,S0e,$0e,I0e,b,$c,zO,D0e,N0e,B0,j0e,O0e,G0e,Ic,XO,q0e,z0e,x0,X0e,Q0e,V0e,Dc,QO,W0e,H0e,k0,U0e,J0e,K0e,Nc,VO,Y0e,Z0e,R0,eAe,oAe,rAe,jc,WO,tAe,aAe,P0,nAe,sAe,lAe,Oc,HO,iAe,dAe,S0,cAe,fAe,mAe,Gc,UO,hAe,gAe,$0,uAe,pAe,_Ae,qc,JO,vAe,bAe,I0,TAe,FAe,EAe,zc,KO,CAe,MAe,D0,yAe,wAe,AAe,Xc,YO,LAe,BAe,N0,xAe,kAe,RAe,Qc,ZO,PAe,SAe,j0,$Ae,IAe,DAe,Vc,eG,NAe,jAe,O0,OAe,GAe,qAe,Wc,oG,zAe,XAe,G0,QAe,VAe,WAe,Hc,rG,HAe,UAe,q0,JAe,KAe,YAe,Uc,tG,ZAe,e6e,z0,o6e,r6e,t6e,Jc,aG,a6e,n6e,X0,s6e,l6e,i6e,Kc,nG,d6e,c6e,Q0,f6e,m6e,h6e,Yc,sG,g6e,u6e,V0,p6e,_6e,v6e,Zc,lG,b6e,T6e,W0,F6e,E6e,C6e,ef,iG,M6e,y6e,H0,w6e,A6e,L6e,of,dG,B6e,x6e,U0,k6e,R6e,P6e,rf,cG,S6e,$6e,J0,I6e,D6e,N6e,tf,fG,j6e,O6e,K0,G6e,q6e,z6e,af,mG,X6e,Q6e,Y0,V6e,W6e,H6e,nf,hG,U6e,J6e,Z0,K6e,Y6e,Z6e,sf,gG,eLe,oLe,eA,rLe,tLe,aLe,lf,uG,nLe,sLe,oA,lLe,iLe,dLe,df,pG,cLe,fLe,rA,mLe,hLe,gLe,cf,_G,uLe,pLe,tA,_Le,vLe,bLe,ff,vG,TLe,FLe,aA,ELe,CLe,MLe,mf,bG,yLe,wLe,nA,ALe,LLe,BLe,hf,TG,xLe,kLe,sA,RLe,PLe,SLe,gf,FG,$Le,ILe,lA,DLe,NLe,jLe,uf,EG,OLe,GLe,iA,qLe,zLe,XLe,pf,CG,QLe,VLe,dA,WLe,HLe,ULe,_f,MG,JLe,KLe,cA,YLe,ZLe,e8e,vf,yG,o8e,r8e,fA,t8e,a8e,n8e,bf,wG,s8e,l8e,mA,i8e,d8e,c8e,Tf,AG,f8e,m8e,hA,h8e,g8e,u8e,Ff,LG,p8e,_8e,gA,v8e,b8e,T8e,Ef,BG,F8e,E8e,uA,C8e,M8e,y8e,Cf,xG,w8e,A8e,pA,L8e,B8e,x8e,Mf,kG,k8e,R8e,_A,P8e,S8e,$8e,yf,RG,I8e,D8e,vA,N8e,j8e,O8e,wf,PG,G8e,q8e,bA,z8e,X8e,Q8e,Af,SG,V8e,W8e,TA,H8e,U8e,J8e,Lf,$G,K8e,Y8e,FA,Z8e,eBe,oBe,Bf,IG,rBe,tBe,EA,aBe,nBe,sBe,xf,DG,lBe,iBe,CA,dBe,cBe,fBe,kf,NG,mBe,hBe,MA,gBe,uBe,pBe,Rf,jG,_Be,vBe,yA,bBe,TBe,FBe,Pf,OG,EBe,CBe,wA,MBe,yBe,wBe,Sf,GG,ABe,LBe,AA,BBe,xBe,kBe,$f,qG,RBe,PBe,LA,SBe,$Be,IBe,If,zG,DBe,NBe,BA,jBe,OBe,GBe,Df,XG,qBe,zBe,xA,XBe,QBe,VBe,Nf,QG,WBe,HBe,kA,UBe,JBe,KBe,jf,VG,YBe,ZBe,RA,e9e,o9e,r9e,Of,WG,t9e,a9e,PA,n9e,s9e,l9e,Gf,HG,i9e,d9e,SA,c9e,f9e,m9e,qf,UG,h9e,g9e,$A,u9e,p9e,_9e,zf,JG,v9e,b9e,IA,T9e,F9e,E9e,Xf,KG,C9e,M9e,DA,y9e,w9e,A9e,Qf,YG,L9e,B9e,NA,x9e,k9e,R9e,Vf,ZG,P9e,S9e,jA,$9e,I9e,D9e,Wf,eq,N9e,j9e,OA,O9e,G9e,q9e,Hf,oq,z9e,X9e,GA,Q9e,V9e,W9e,Uf,rq,H9e,U9e,qA,J9e,K9e,Y9e,Jf,tq,Z9e,exe,zA,oxe,rxe,txe,Kf,aq,axe,nxe,XA,sxe,lxe,ixe,Yf,nq,dxe,cxe,QA,fxe,mxe,hxe,Zf,sq,gxe,uxe,VA,pxe,_xe,vxe,em,lq,bxe,Txe,WA,Fxe,Exe,Cxe,om,iq,Mxe,yxe,HA,wxe,Axe,Lxe,rm,dq,Bxe,xxe,UA,kxe,Rxe,Pxe,tm,cq,Sxe,$xe,JA,Ixe,Dxe,Nxe,am,fq,jxe,Oxe,KA,Gxe,qxe,zxe,nm,mq,Xxe,Qxe,YA,Vxe,Wxe,Hxe,sm,hq,Uxe,Jxe,ZA,Kxe,Yxe,Zxe,gq,eke,oke,BC,rke,lm,xC,tke,uq,ake,b5e,Zl,im,pq,kC,nke,_q,ske,T5e,No,RC,lke,PC,ike,e6,dke,cke,fke,SC,mke,vq,hke,gke,uke,ye,$C,pke,bq,_ke,vke,ba,bke,Tq,Tke,Fke,Fq,Eke,Cke,Eq,Mke,yke,wke,C,bn,Cq,Ake,Lke,o6,Bke,xke,r6,kke,Rke,Pke,Tn,Mq,Ske,$ke,t6,Ike,Dke,a6,Nke,jke,Oke,Fn,yq,Gke,qke,n6,zke,Xke,s6,Qke,Vke,Wke,dm,wq,Hke,Uke,l6,Jke,Kke,Yke,En,Aq,Zke,eRe,i6,oRe,rRe,d6,tRe,aRe,nRe,cm,Lq,sRe,lRe,c6,iRe,dRe,cRe,fm,Bq,fRe,mRe,f6,hRe,gRe,uRe,mm,xq,pRe,_Re,m6,vRe,bRe,TRe,Cn,kq,FRe,ERe,h6,CRe,MRe,g6,yRe,wRe,ARe,Mn,Rq,LRe,BRe,u6,xRe,kRe,p6,RRe,PRe,SRe,yn,Pq,$Re,IRe,_6,DRe,NRe,v6,jRe,ORe,GRe,hm,Sq,qRe,zRe,b6,XRe,QRe,VRe,gm,$q,WRe,HRe,T6,URe,JRe,KRe,wn,Iq,YRe,ZRe,F6,ePe,oPe,E6,rPe,tPe,aPe,um,Dq,nPe,sPe,C6,lPe,iPe,dPe,An,Nq,cPe,fPe,M6,mPe,hPe,y6,gPe,uPe,pPe,Ln,jq,_Pe,vPe,w6,bPe,TPe,A6,FPe,EPe,CPe,Bn,Oq,MPe,yPe,L6,wPe,APe,Gq,LPe,BPe,xPe,pm,qq,kPe,RPe,B6,PPe,SPe,$Pe,xn,zq,IPe,DPe,x6,NPe,jPe,k6,OPe,GPe,qPe,_m,Xq,zPe,XPe,R6,QPe,VPe,WPe,kn,Qq,HPe,UPe,P6,JPe,KPe,S6,YPe,ZPe,eSe,Rn,Vq,oSe,rSe,$6,tSe,aSe,I6,nSe,sSe,lSe,Pn,Wq,iSe,dSe,D6,cSe,fSe,N6,mSe,hSe,gSe,vm,Hq,uSe,pSe,j6,_Se,vSe,bSe,Sn,Uq,TSe,FSe,O6,ESe,CSe,G6,MSe,ySe,wSe,bm,Jq,ASe,LSe,q6,BSe,xSe,kSe,$n,Kq,RSe,PSe,z6,SSe,$Se,X6,ISe,DSe,NSe,In,Yq,jSe,OSe,Q6,GSe,qSe,V6,zSe,XSe,QSe,Dn,Zq,VSe,WSe,W6,HSe,USe,H6,JSe,KSe,YSe,Tm,ez,ZSe,e$e,U6,o$e,r$e,t$e,Nn,oz,a$e,n$e,J6,s$e,l$e,K6,i$e,d$e,c$e,jn,rz,f$e,m$e,Y6,h$e,g$e,Z6,u$e,p$e,_$e,On,tz,v$e,b$e,eL,T$e,F$e,oL,E$e,C$e,M$e,Gn,az,y$e,w$e,rL,A$e,L$e,tL,B$e,x$e,k$e,qn,nz,R$e,P$e,aL,S$e,$$e,nL,I$e,D$e,N$e,Fm,sz,j$e,O$e,sL,G$e,q$e,z$e,zn,lz,X$e,Q$e,lL,V$e,W$e,iL,H$e,U$e,J$e,Em,iz,K$e,Y$e,dL,Z$e,eIe,oIe,Cm,dz,rIe,tIe,cL,aIe,nIe,sIe,Xn,cz,lIe,iIe,fL,dIe,cIe,mL,fIe,mIe,hIe,Qn,fz,gIe,uIe,hL,pIe,_Ie,gL,vIe,bIe,TIe,Vn,mz,FIe,EIe,uL,CIe,MIe,pL,yIe,wIe,AIe,Wn,hz,LIe,BIe,_L,xIe,kIe,vL,RIe,PIe,SIe,Hn,gz,$Ie,IIe,bL,DIe,NIe,TL,jIe,OIe,GIe,Un,uz,qIe,zIe,FL,XIe,QIe,EL,VIe,WIe,HIe,Jn,pz,UIe,JIe,CL,KIe,YIe,ML,ZIe,eDe,oDe,Mm,_z,rDe,tDe,yL,aDe,nDe,sDe,ym,vz,lDe,iDe,wL,dDe,cDe,fDe,wm,bz,mDe,hDe,AL,gDe,uDe,pDe,Kn,Tz,_De,vDe,LL,bDe,TDe,BL,FDe,EDe,CDe,Am,Fz,MDe,yDe,xL,wDe,ADe,LDe,Yn,Ez,BDe,xDe,kL,kDe,RDe,RL,PDe,SDe,$De,Zn,Cz,IDe,DDe,PL,NDe,jDe,SL,ODe,GDe,qDe,es,Mz,zDe,XDe,$L,QDe,VDe,IL,WDe,HDe,UDe,os,yz,JDe,KDe,DL,YDe,ZDe,NL,eNe,oNe,rNe,rs,wz,tNe,aNe,jL,nNe,sNe,OL,lNe,iNe,dNe,Lm,Az,cNe,fNe,GL,mNe,hNe,gNe,Bm,Lz,uNe,pNe,qL,_Ne,vNe,bNe,ts,Bz,TNe,FNe,zL,ENe,CNe,XL,MNe,yNe,wNe,as,xz,ANe,LNe,QL,BNe,xNe,VL,kNe,RNe,PNe,ns,kz,SNe,$Ne,WL,INe,DNe,HL,NNe,jNe,ONe,xm,Rz,GNe,qNe,UL,zNe,XNe,QNe,km,Pz,VNe,WNe,JL,HNe,UNe,JNe,Rm,Sz,KNe,YNe,KL,ZNe,eje,oje,Pm,$z,rje,tje,YL,aje,nje,sje,Sm,Iz,lje,ije,ZL,dje,cje,fje,ss,Dz,mje,hje,e8,gje,uje,o8,pje,_je,vje,ls,Nz,bje,Tje,r8,Fje,Eje,t8,Cje,Mje,yje,ei,wje,jz,Aje,Lje,Oz,Bje,xje,kje,oi,Ta,Rje,Gz,Pje,Sje,qz,$je,Ije,zz,Dje,Nje,jje,Fa,Oje,Xz,Gje,qje,a8,zje,Xje,Qz,Qje,Vje,Wje,k,Hje,Vz,Uje,Jje,Wz,Kje,Yje,Hz,Zje,eOe,n8,oOe,rOe,Uz,tOe,aOe,ri,nOe,Jz,sOe,lOe,Kz,iOe,dOe,cOe,IC,fOe,Yz,mOe,hOe,gOe,Zz,uOe,pOe,DC,_Oe,eX,vOe,bOe,TOe,oX,FOe,EOe,rX,COe,MOe,tX,yOe,wOe,aX,AOe,LOe,nX,BOe,xOe,sX,kOe,ROe,lX,POe,SOe,iX,$Oe,IOe,dX,DOe,NOe,cX,jOe,OOe,NC,GOe,fX,qOe,zOe,XOe,mX,QOe,VOe,jC,WOe,hX,HOe,UOe,JOe,OC,KOe,gX,YOe,ZOe,eGe,uX,oGe,rGe,pX,tGe,aGe,_X,nGe,sGe,vX,lGe,iGe,bX,dGe,cGe,TX,fGe,mGe,FX,hGe,gGe,EX,uGe,pGe,CX,_Ge,vGe,MX,bGe,TGe,yX,FGe,EGe,wX,CGe,MGe,AX,yGe,wGe,AGe,LX,LGe,BGe,GC,xGe,$m,qC,kGe,BX,RGe,F5e,ti,Im,xX,zC,PGe,kX,SGe,E5e,Dt,XC,$Ge,QC,IGe,s8,DGe,NGe,jGe,VC,OGe,RX,GGe,qGe,zGe,Fe,WC,XGe,PX,QGe,VGe,Ea,WGe,SX,HGe,UGe,$X,JGe,KGe,IX,YGe,ZGe,eqe,_e,Dm,DX,oqe,rqe,l8,tqe,aqe,nqe,Nm,NX,sqe,lqe,i8,iqe,dqe,cqe,jm,jX,fqe,mqe,d8,hqe,gqe,uqe,Om,OX,pqe,_qe,c8,vqe,bqe,Tqe,Gm,GX,Fqe,Eqe,f8,Cqe,Mqe,yqe,qm,qX,wqe,Aqe,m8,Lqe,Bqe,xqe,zm,zX,kqe,Rqe,h8,Pqe,Sqe,$qe,Xm,XX,Iqe,Dqe,g8,Nqe,jqe,Oqe,Qm,QX,Gqe,qqe,u8,zqe,Xqe,Qqe,Vm,VX,Vqe,Wqe,p8,Hqe,Uqe,Jqe,ai,Kqe,WX,Yqe,Zqe,HX,eze,oze,rze,ni,Ca,tze,UX,aze,nze,JX,sze,lze,KX,ize,dze,cze,Ma,fze,YX,mze,hze,_8,gze,uze,ZX,pze,_ze,vze,N,bze,eQ,Tze,Fze,oQ,Eze,Cze,si,Mze,rQ,yze,wze,tQ,Aze,Lze,Bze,HC,xze,aQ,kze,Rze,Pze,nQ,Sze,$ze,UC,Ize,sQ,Dze,Nze,jze,lQ,Oze,Gze,iQ,qze,zze,dQ,Xze,Qze,cQ,Vze,Wze,JC,Hze,fQ,Uze,Jze,Kze,mQ,Yze,Zze,hQ,eXe,oXe,gQ,rXe,tXe,uQ,aXe,nXe,pQ,sXe,lXe,_Q,iXe,dXe,vQ,cXe,fXe,bQ,mXe,hXe,KC,gXe,TQ,uXe,pXe,_Xe,FQ,vXe,bXe,EQ,TXe,FXe,CQ,EXe,CXe,MQ,MXe,yXe,yQ,wXe,AXe,wQ,LXe,BXe,AQ,xXe,kXe,LQ,RXe,PXe,BQ,SXe,$Xe,xQ,IXe,DXe,kQ,NXe,jXe,OXe,Wm,GXe,RQ,qXe,zXe,YC,C5e,li,Hm,PQ,ZC,XXe,SQ,QXe,M5e,Nt,e3,VXe,o3,WXe,v8,HXe,UXe,JXe,r3,KXe,$Q,YXe,ZXe,eQe,Ee,t3,oQe,IQ,rQe,tQe,ii,aQe,DQ,nQe,sQe,NQ,lQe,iQe,dQe,to,Um,jQ,cQe,fQe,b8,mQe,hQe,gQe,Jm,OQ,uQe,pQe,T8,_Qe,vQe,bQe,Km,GQ,TQe,FQe,F8,EQe,CQe,MQe,Ym,qQ,yQe,wQe,E8,AQe,LQe,BQe,Zm,zQ,xQe,kQe,C8,RQe,PQe,SQe,eh,XQ,$Qe,IQe,M8,DQe,NQe,jQe,oh,QQ,OQe,GQe,y8,qQe,zQe,XQe,di,QQe,VQ,VQe,WQe,WQ,HQe,UQe,JQe,a3,ya,KQe,HQ,YQe,ZQe,UQ,eVe,oVe,JQ,rVe,tVe,aVe,D,nVe,KQ,sVe,lVe,YQ,iVe,dVe,ZQ,cVe,fVe,ci,mVe,eV,hVe,gVe,oV,uVe,pVe,_Ve,n3,vVe,rV,bVe,TVe,FVe,tV,EVe,CVe,s3,MVe,aV,yVe,wVe,AVe,nV,LVe,BVe,sV,xVe,kVe,lV,RVe,PVe,iV,SVe,$Ve,l3,IVe,dV,DVe,NVe,jVe,cV,OVe,GVe,fV,qVe,zVe,mV,XVe,QVe,hV,VVe,WVe,gV,HVe,UVe,uV,JVe,KVe,pV,YVe,ZVe,_V,eWe,oWe,i3,rWe,vV,tWe,aWe,nWe,bV,sWe,lWe,TV,iWe,dWe,FV,cWe,fWe,EV,mWe,hWe,CV,gWe,uWe,MV,pWe,_We,yV,vWe,bWe,wV,TWe,FWe,AV,EWe,CWe,LV,MWe,yWe,BV,wWe,AWe,LWe,rh,BWe,xV,xWe,kWe,d3,y5e,fi,th,kV,c3,RWe,RV,PWe,w5e,jo,f3,SWe,mi,$We,PV,IWe,DWe,SV,NWe,jWe,OWe,m3,GWe,$V,qWe,zWe,XWe,kr,h3,QWe,IV,VWe,WWe,hi,HWe,DV,UWe,JWe,NV,KWe,YWe,ZWe,jV,eHe,oHe,g3,rHe,$e,u3,tHe,OV,aHe,nHe,wa,sHe,GV,lHe,iHe,qV,dHe,cHe,zV,fHe,mHe,hHe,F,ah,XV,gHe,uHe,w8,pHe,_He,vHe,nh,QV,bHe,THe,A8,FHe,EHe,CHe,sh,VV,MHe,yHe,L8,wHe,AHe,LHe,lh,WV,BHe,xHe,B8,kHe,RHe,PHe,ih,HV,SHe,$He,x8,IHe,DHe,NHe,dh,UV,jHe,OHe,k8,GHe,qHe,zHe,ch,JV,XHe,QHe,R8,VHe,WHe,HHe,fh,KV,UHe,JHe,P8,KHe,YHe,ZHe,mh,YV,eUe,oUe,S8,rUe,tUe,aUe,hh,ZV,nUe,sUe,$8,lUe,iUe,dUe,gh,eW,cUe,fUe,I8,mUe,hUe,gUe,uh,oW,uUe,pUe,D8,_Ue,vUe,bUe,ph,rW,TUe,FUe,N8,EUe,CUe,MUe,_h,tW,yUe,wUe,j8,AUe,LUe,BUe,vh,aW,xUe,kUe,O8,RUe,PUe,SUe,bh,nW,$Ue,IUe,G8,DUe,NUe,jUe,Th,sW,OUe,GUe,q8,qUe,zUe,XUe,Fh,lW,QUe,VUe,z8,WUe,HUe,UUe,Eh,iW,JUe,KUe,X8,YUe,ZUe,eJe,Ch,dW,oJe,rJe,Q8,tJe,aJe,nJe,Mh,cW,sJe,lJe,V8,iJe,dJe,cJe,yh,fW,fJe,mJe,W8,hJe,gJe,uJe,wh,mW,pJe,_Je,H8,vJe,bJe,TJe,Ah,hW,FJe,EJe,U8,CJe,MJe,yJe,is,gW,wJe,AJe,J8,LJe,BJe,K8,xJe,kJe,RJe,Lh,uW,PJe,SJe,Y8,$Je,IJe,DJe,Bh,pW,NJe,jJe,Z8,OJe,GJe,qJe,xh,_W,zJe,XJe,eB,QJe,VJe,WJe,kh,vW,HJe,UJe,oB,JJe,KJe,YJe,Rh,bW,ZJe,eKe,rB,oKe,rKe,tKe,Ph,TW,aKe,nKe,tB,sKe,lKe,iKe,Sh,FW,dKe,cKe,aB,fKe,mKe,hKe,$h,EW,gKe,uKe,nB,pKe,_Ke,vKe,Ih,CW,bKe,TKe,sB,FKe,EKe,CKe,Dh,MW,MKe,yKe,lB,wKe,AKe,LKe,Nh,yW,BKe,xKe,iB,kKe,RKe,PKe,jh,wW,SKe,$Ke,dB,IKe,DKe,NKe,Oh,AW,jKe,OKe,cB,GKe,qKe,zKe,Gh,LW,XKe,QKe,fB,VKe,WKe,HKe,qh,BW,UKe,JKe,mB,KKe,YKe,ZKe,zh,xW,eYe,oYe,hB,rYe,tYe,aYe,Xh,kW,nYe,sYe,gB,lYe,iYe,dYe,Qh,RW,cYe,fYe,uB,mYe,hYe,gYe,Vh,PW,uYe,pYe,pB,_Ye,vYe,bYe,Wh,SW,TYe,FYe,_B,EYe,CYe,MYe,Hh,$W,yYe,wYe,vB,AYe,LYe,BYe,Uh,IW,xYe,kYe,bB,RYe,PYe,SYe,Jh,DW,$Ye,IYe,TB,DYe,NYe,jYe,Kh,NW,OYe,GYe,FB,qYe,zYe,XYe,Yh,jW,QYe,VYe,EB,WYe,HYe,UYe,Zh,OW,JYe,KYe,CB,YYe,ZYe,eZe,eg,GW,oZe,rZe,MB,tZe,aZe,nZe,og,qW,sZe,lZe,yB,iZe,dZe,cZe,rg,zW,fZe,mZe,wB,hZe,gZe,uZe,tg,XW,pZe,_Ze,AB,vZe,bZe,TZe,ag,QW,FZe,EZe,LB,CZe,MZe,yZe,ng,VW,wZe,AZe,BB,LZe,BZe,xZe,sg,WW,kZe,RZe,xB,PZe,SZe,$Ze,lg,HW,IZe,DZe,kB,NZe,jZe,OZe,ig,UW,GZe,qZe,RB,zZe,XZe,QZe,dg,JW,VZe,WZe,PB,HZe,UZe,JZe,cg,KW,KZe,YZe,SB,ZZe,eeo,oeo,fg,YW,reo,teo,$B,aeo,neo,seo,mg,ZW,leo,ieo,IB,deo,ceo,feo,hg,eH,meo,heo,DB,geo,ueo,peo,gg,oH,_eo,veo,NB,beo,Teo,Feo,ug,rH,Eeo,Ceo,jB,Meo,yeo,weo,pg,tH,Aeo,Leo,OB,Beo,xeo,keo,_g,aH,Reo,Peo,GB,Seo,$eo,Ieo,vg,nH,Deo,Neo,qB,jeo,Oeo,Geo,bg,sH,qeo,zeo,zB,Xeo,Qeo,Veo,Tg,lH,Weo,Heo,XB,Ueo,Jeo,Keo,Fg,iH,Yeo,Zeo,QB,eoo,ooo,roo,Eg,too,dH,aoo,noo,cH,soo,loo,fH,ioo,doo,p3,A5e,gi,Cg,mH,_3,coo,hH,foo,L5e,Oo,v3,moo,ui,hoo,gH,goo,uoo,uH,poo,_oo,voo,b3,boo,pH,Too,Foo,Eoo,Rr,T3,Coo,_H,Moo,yoo,pi,woo,vH,Aoo,Loo,bH,Boo,xoo,koo,TH,Roo,Poo,F3,Soo,Ie,E3,$oo,FH,Ioo,Doo,Aa,Noo,EH,joo,Ooo,CH,Goo,qoo,MH,zoo,Xoo,Qoo,R,Mg,yH,Voo,Woo,VB,Hoo,Uoo,Joo,yg,wH,Koo,Yoo,WB,Zoo,ero,oro,wg,AH,rro,tro,HB,aro,nro,sro,Ag,LH,lro,iro,UB,dro,cro,fro,Lg,BH,mro,hro,JB,gro,uro,pro,Bg,xH,_ro,vro,KB,bro,Tro,Fro,xg,kH,Ero,Cro,YB,Mro,yro,wro,kg,RH,Aro,Lro,ZB,Bro,xro,kro,Rg,PH,Rro,Pro,e9,Sro,$ro,Iro,Pg,SH,Dro,Nro,o9,jro,Oro,Gro,Sg,$H,qro,zro,r9,Xro,Qro,Vro,$g,IH,Wro,Hro,t9,Uro,Jro,Kro,Ig,DH,Yro,Zro,a9,eto,oto,rto,Dg,NH,tto,ato,n9,nto,sto,lto,Ng,jH,ito,dto,s9,cto,fto,mto,jg,OH,hto,gto,l9,uto,pto,_to,Og,GH,vto,bto,i9,Tto,Fto,Eto,Gg,qH,Cto,Mto,d9,yto,wto,Ato,qg,zH,Lto,Bto,c9,xto,kto,Rto,zg,XH,Pto,Sto,f9,$to,Ito,Dto,Xg,QH,Nto,jto,m9,Oto,Gto,qto,Qg,VH,zto,Xto,h9,Qto,Vto,Wto,Vg,WH,Hto,Uto,g9,Jto,Kto,Yto,Wg,HH,Zto,eao,u9,oao,rao,tao,Hg,UH,aao,nao,p9,sao,lao,iao,Ug,JH,dao,cao,_9,fao,mao,hao,Jg,KH,gao,uao,v9,pao,_ao,vao,Kg,YH,bao,Tao,b9,Fao,Eao,Cao,Yg,ZH,Mao,yao,T9,wao,Aao,Lao,Zg,eU,Bao,xao,F9,kao,Rao,Pao,eu,oU,Sao,$ao,E9,Iao,Dao,Nao,ou,rU,jao,Oao,C9,Gao,qao,zao,ru,tU,Xao,Qao,M9,Vao,Wao,Hao,tu,aU,Uao,Jao,y9,Kao,Yao,Zao,au,nU,eno,ono,w9,rno,tno,ano,nu,sU,nno,sno,A9,lno,ino,dno,su,cno,lU,fno,mno,iU,hno,gno,dU,uno,pno,C3,B5e,_i,lu,cU,M3,_no,fU,vno,x5e,Go,y3,bno,vi,Tno,mU,Fno,Eno,hU,Cno,Mno,yno,w3,wno,gU,Ano,Lno,Bno,Pr,A3,xno,uU,kno,Rno,bi,Pno,pU,Sno,$no,_U,Ino,Dno,Nno,vU,jno,Ono,L3,Gno,De,B3,qno,bU,zno,Xno,La,Qno,TU,Vno,Wno,FU,Hno,Uno,EU,Jno,Kno,Yno,q,iu,CU,Zno,eso,L9,oso,rso,tso,du,MU,aso,nso,B9,sso,lso,iso,cu,yU,dso,cso,x9,fso,mso,hso,fu,wU,gso,uso,k9,pso,_so,vso,mu,AU,bso,Tso,R9,Fso,Eso,Cso,hu,LU,Mso,yso,P9,wso,Aso,Lso,gu,BU,Bso,xso,S9,kso,Rso,Pso,uu,xU,Sso,$so,$9,Iso,Dso,Nso,pu,kU,jso,Oso,I9,Gso,qso,zso,_u,RU,Xso,Qso,D9,Vso,Wso,Hso,vu,PU,Uso,Jso,N9,Kso,Yso,Zso,bu,SU,elo,olo,j9,rlo,tlo,alo,Tu,$U,nlo,slo,O9,llo,ilo,dlo,Fu,IU,clo,flo,G9,mlo,hlo,glo,Eu,DU,ulo,plo,q9,_lo,vlo,blo,Cu,NU,Tlo,Flo,z9,Elo,Clo,Mlo,Mu,jU,ylo,wlo,X9,Alo,Llo,Blo,yu,OU,xlo,klo,Q9,Rlo,Plo,Slo,wu,GU,$lo,Ilo,V9,Dlo,Nlo,jlo,Au,qU,Olo,Glo,W9,qlo,zlo,Xlo,Lu,zU,Qlo,Vlo,H9,Wlo,Hlo,Ulo,Bu,XU,Jlo,Klo,U9,Ylo,Zlo,eio,xu,QU,oio,rio,J9,tio,aio,nio,ku,VU,sio,lio,K9,iio,dio,cio,Ru,WU,fio,mio,Y9,hio,gio,uio,Pu,HU,pio,_io,Z9,vio,bio,Tio,Su,UU,Fio,Eio,ex,Cio,Mio,yio,$u,JU,wio,Aio,ox,Lio,Bio,xio,Iu,KU,kio,Rio,rx,Pio,Sio,$io,Du,YU,Iio,Dio,tx,Nio,jio,Oio,Nu,Gio,ZU,qio,zio,eJ,Xio,Qio,oJ,Vio,Wio,x3,k5e,Ti,ju,rJ,k3,Hio,tJ,Uio,R5e,qo,R3,Jio,Fi,Kio,aJ,Yio,Zio,nJ,edo,odo,rdo,P3,tdo,sJ,ado,ndo,sdo,Sr,S3,ldo,lJ,ido,ddo,Ei,cdo,iJ,fdo,mdo,dJ,hdo,gdo,udo,cJ,pdo,_do,$3,vdo,Ne,I3,bdo,fJ,Tdo,Fdo,Ba,Edo,mJ,Cdo,Mdo,hJ,ydo,wdo,gJ,Ado,Ldo,Bdo,O,Ou,uJ,xdo,kdo,ax,Rdo,Pdo,Sdo,Gu,pJ,$do,Ido,nx,Ddo,Ndo,jdo,qu,_J,Odo,Gdo,sx,qdo,zdo,Xdo,zu,vJ,Qdo,Vdo,lx,Wdo,Hdo,Udo,Xu,bJ,Jdo,Kdo,ix,Ydo,Zdo,eco,Qu,TJ,oco,rco,dx,tco,aco,nco,Vu,FJ,sco,lco,cx,ico,dco,cco,Wu,EJ,fco,mco,fx,hco,gco,uco,Hu,CJ,pco,_co,mx,vco,bco,Tco,Uu,MJ,Fco,Eco,hx,Cco,Mco,yco,Ju,yJ,wco,Aco,gx,Lco,Bco,xco,Ku,wJ,kco,Rco,ux,Pco,Sco,$co,Yu,AJ,Ico,Dco,px,Nco,jco,Oco,Zu,LJ,Gco,qco,_x,zco,Xco,Qco,ep,BJ,Vco,Wco,vx,Hco,Uco,Jco,op,xJ,Kco,Yco,bx,Zco,efo,ofo,rp,kJ,rfo,tfo,Tx,afo,nfo,sfo,tp,RJ,lfo,ifo,Fx,dfo,cfo,ffo,ap,PJ,mfo,hfo,Ex,gfo,ufo,pfo,np,SJ,_fo,vfo,Cx,bfo,Tfo,Ffo,sp,$J,Efo,Cfo,Mx,Mfo,yfo,wfo,lp,IJ,Afo,Lfo,yx,Bfo,xfo,kfo,ip,DJ,Rfo,Pfo,wx,Sfo,$fo,Ifo,dp,NJ,Dfo,Nfo,Ax,jfo,Ofo,Gfo,cp,jJ,qfo,zfo,Lx,Xfo,Qfo,Vfo,fp,OJ,Wfo,Hfo,Bx,Ufo,Jfo,Kfo,mp,GJ,Yfo,Zfo,xx,emo,omo,rmo,hp,qJ,tmo,amo,kx,nmo,smo,lmo,gp,zJ,imo,dmo,XJ,cmo,fmo,mmo,up,QJ,hmo,gmo,Rx,umo,pmo,_mo,pp,VJ,vmo,bmo,Px,Tmo,Fmo,Emo,_p,Cmo,WJ,Mmo,ymo,HJ,wmo,Amo,UJ,Lmo,Bmo,D3,P5e,Ci,vp,JJ,N3,xmo,KJ,kmo,S5e,zo,j3,Rmo,Mi,Pmo,YJ,Smo,$mo,ZJ,Imo,Dmo,Nmo,O3,jmo,eK,Omo,Gmo,qmo,$r,G3,zmo,oK,Xmo,Qmo,yi,Vmo,rK,Wmo,Hmo,tK,Umo,Jmo,Kmo,aK,Ymo,Zmo,q3,eho,je,z3,oho,nK,rho,tho,xa,aho,sK,nho,sho,lK,lho,iho,iK,dho,cho,fho,fe,bp,dK,mho,hho,Sx,gho,uho,pho,Tp,cK,_ho,vho,$x,bho,Tho,Fho,Fp,fK,Eho,Cho,Ix,Mho,yho,who,Ep,mK,Aho,Lho,Dx,Bho,xho,kho,Cp,hK,Rho,Pho,Nx,Sho,$ho,Iho,Mp,gK,Dho,Nho,jx,jho,Oho,Gho,yp,uK,qho,zho,Ox,Xho,Qho,Vho,wp,pK,Who,Hho,Gx,Uho,Jho,Kho,Ap,_K,Yho,Zho,qx,ego,ogo,rgo,Lp,vK,tgo,ago,zx,ngo,sgo,lgo,Bp,bK,igo,dgo,Xx,cgo,fgo,mgo,xp,TK,hgo,ggo,Qx,ugo,pgo,_go,kp,FK,vgo,bgo,Vx,Tgo,Fgo,Ego,Rp,EK,Cgo,Mgo,Wx,ygo,wgo,Ago,Pp,CK,Lgo,Bgo,Hx,xgo,kgo,Rgo,Sp,Pgo,MK,Sgo,$go,yK,Igo,Dgo,wK,Ngo,jgo,X3,$5e,wi,$p,AK,Q3,Ogo,LK,Ggo,I5e,Xo,V3,qgo,Ai,zgo,BK,Xgo,Qgo,xK,Vgo,Wgo,Hgo,W3,Ugo,kK,Jgo,Kgo,Ygo,Ir,H3,Zgo,RK,euo,ouo,Li,ruo,PK,tuo,auo,SK,nuo,suo,luo,$K,iuo,duo,U3,cuo,Oe,J3,fuo,IK,muo,huo,ka,guo,DK,uuo,puo,NK,_uo,vuo,jK,buo,Tuo,Fuo,A,Ip,OK,Euo,Cuo,Ux,Muo,yuo,wuo,Dp,GK,Auo,Luo,Jx,Buo,xuo,kuo,Np,qK,Ruo,Puo,Kx,Suo,$uo,Iuo,jp,zK,Duo,Nuo,Yx,juo,Ouo,Guo,Op,XK,quo,zuo,Zx,Xuo,Quo,Vuo,Gp,QK,Wuo,Huo,ek,Uuo,Juo,Kuo,qp,VK,Yuo,Zuo,ok,epo,opo,rpo,zp,WK,tpo,apo,rk,npo,spo,lpo,Xp,HK,ipo,dpo,tk,cpo,fpo,mpo,Qp,UK,hpo,gpo,ak,upo,ppo,_po,Vp,JK,vpo,bpo,nk,Tpo,Fpo,Epo,Wp,KK,Cpo,Mpo,sk,ypo,wpo,Apo,Hp,YK,Lpo,Bpo,lk,xpo,kpo,Rpo,Up,ZK,Ppo,Spo,ik,$po,Ipo,Dpo,Jp,eY,Npo,jpo,dk,Opo,Gpo,qpo,Kp,oY,zpo,Xpo,ck,Qpo,Vpo,Wpo,Yp,rY,Hpo,Upo,fk,Jpo,Kpo,Ypo,Zp,tY,Zpo,e_o,mk,o_o,r_o,t_o,e_,aY,a_o,n_o,hk,s_o,l_o,i_o,o_,nY,d_o,c_o,gk,f_o,m_o,h_o,r_,sY,g_o,u_o,uk,p_o,__o,v_o,t_,lY,b_o,T_o,pk,F_o,E_o,C_o,a_,iY,M_o,y_o,_k,w_o,A_o,L_o,n_,dY,B_o,x_o,vk,k_o,R_o,P_o,s_,cY,S_o,$_o,bk,I_o,D_o,N_o,l_,fY,j_o,O_o,Tk,G_o,q_o,z_o,i_,mY,X_o,Q_o,Fk,V_o,W_o,H_o,d_,hY,U_o,J_o,Ek,K_o,Y_o,Z_o,c_,gY,e1o,o1o,Ck,r1o,t1o,a1o,f_,uY,n1o,s1o,Mk,l1o,i1o,d1o,m_,pY,c1o,f1o,yk,m1o,h1o,g1o,h_,_Y,u1o,p1o,wk,_1o,v1o,b1o,g_,vY,T1o,F1o,Ak,E1o,C1o,M1o,u_,bY,y1o,w1o,Lk,A1o,L1o,B1o,p_,TY,x1o,k1o,Bk,R1o,P1o,S1o,__,FY,$1o,I1o,xk,D1o,N1o,j1o,v_,EY,O1o,G1o,kk,q1o,z1o,X1o,b_,CY,Q1o,V1o,Rk,W1o,H1o,U1o,T_,MY,J1o,K1o,Pk,Y1o,Z1o,e4o,F_,yY,o4o,r4o,Sk,t4o,a4o,n4o,E_,wY,s4o,l4o,$k,i4o,d4o,c4o,C_,f4o,AY,m4o,h4o,LY,g4o,u4o,BY,p4o,_4o,K3,D5e,Bi,M_,xY,Y3,v4o,kY,b4o,N5e,Qo,Z3,T4o,xi,F4o,RY,E4o,C4o,PY,M4o,y4o,w4o,eM,A4o,SY,L4o,B4o,x4o,Dr,oM,k4o,$Y,R4o,P4o,ki,S4o,IY,$4o,I4o,DY,D4o,N4o,j4o,NY,O4o,G4o,rM,q4o,Ge,tM,z4o,jY,X4o,Q4o,Ra,V4o,OY,W4o,H4o,GY,U4o,J4o,qY,K4o,Y4o,Z4o,H,y_,zY,evo,ovo,Ik,rvo,tvo,avo,w_,XY,nvo,svo,Dk,lvo,ivo,dvo,A_,QY,cvo,fvo,Nk,mvo,hvo,gvo,L_,VY,uvo,pvo,jk,_vo,vvo,bvo,B_,WY,Tvo,Fvo,Ok,Evo,Cvo,Mvo,x_,HY,yvo,wvo,Gk,Avo,Lvo,Bvo,k_,UY,xvo,kvo,qk,Rvo,Pvo,Svo,R_,JY,$vo,Ivo,zk,Dvo,Nvo,jvo,P_,KY,Ovo,Gvo,Xk,qvo,zvo,Xvo,S_,YY,Qvo,Vvo,Qk,Wvo,Hvo,Uvo,$_,ZY,Jvo,Kvo,Vk,Yvo,Zvo,ebo,I_,eZ,obo,rbo,Wk,tbo,abo,nbo,D_,oZ,sbo,lbo,Hk,ibo,dbo,cbo,N_,rZ,fbo,mbo,Uk,hbo,gbo,ubo,j_,tZ,pbo,_bo,Jk,vbo,bbo,Tbo,O_,aZ,Fbo,Ebo,Kk,Cbo,Mbo,ybo,G_,nZ,wbo,Abo,Yk,Lbo,Bbo,xbo,q_,sZ,kbo,Rbo,Zk,Pbo,Sbo,$bo,z_,lZ,Ibo,Dbo,eR,Nbo,jbo,Obo,X_,iZ,Gbo,qbo,oR,zbo,Xbo,Qbo,Q_,dZ,Vbo,Wbo,rR,Hbo,Ubo,Jbo,V_,cZ,Kbo,Ybo,tR,Zbo,e2o,o2o,W_,fZ,r2o,t2o,aR,a2o,n2o,s2o,H_,mZ,l2o,i2o,nR,d2o,c2o,f2o,U_,m2o,hZ,h2o,g2o,gZ,u2o,p2o,uZ,_2o,v2o,aM,j5e,Ri,J_,pZ,nM,b2o,_Z,T2o,O5e,Vo,sM,F2o,Pi,E2o,vZ,C2o,M2o,bZ,y2o,w2o,A2o,lM,L2o,TZ,B2o,x2o,k2o,Nr,iM,R2o,FZ,P2o,S2o,Si,$2o,EZ,I2o,D2o,CZ,N2o,j2o,O2o,MZ,G2o,q2o,dM,z2o,qe,cM,X2o,yZ,Q2o,V2o,Pa,W2o,wZ,H2o,U2o,AZ,J2o,K2o,LZ,Y2o,Z2o,eTo,jt,K_,BZ,oTo,rTo,sR,tTo,aTo,nTo,Y_,xZ,sTo,lTo,lR,iTo,dTo,cTo,Z_,kZ,fTo,mTo,iR,hTo,gTo,uTo,e1,RZ,pTo,_To,dR,vTo,bTo,TTo,o1,PZ,FTo,ETo,cR,CTo,MTo,yTo,r1,wTo,SZ,ATo,LTo,$Z,BTo,xTo,IZ,kTo,RTo,fM,G5e,$i,t1,DZ,mM,PTo,NZ,STo,q5e,Wo,hM,$To,Ii,ITo,jZ,DTo,NTo,OZ,jTo,OTo,GTo,gM,qTo,GZ,zTo,XTo,QTo,jr,uM,VTo,qZ,WTo,HTo,Di,UTo,zZ,JTo,KTo,XZ,YTo,ZTo,eFo,QZ,oFo,rFo,pM,tFo,ze,_M,aFo,VZ,nFo,sFo,Sa,lFo,WZ,iFo,dFo,HZ,cFo,fFo,UZ,mFo,hFo,gFo,X,a1,JZ,uFo,pFo,fR,_Fo,vFo,bFo,n1,KZ,TFo,FFo,mR,EFo,CFo,MFo,s1,YZ,yFo,wFo,hR,AFo,LFo,BFo,l1,ZZ,xFo,kFo,gR,RFo,PFo,SFo,i1,eee,$Fo,IFo,uR,DFo,NFo,jFo,d1,oee,OFo,GFo,pR,qFo,zFo,XFo,c1,ree,QFo,VFo,_R,WFo,HFo,UFo,f1,tee,JFo,KFo,vR,YFo,ZFo,eEo,m1,aee,oEo,rEo,bR,tEo,aEo,nEo,h1,nee,sEo,lEo,TR,iEo,dEo,cEo,g1,see,fEo,mEo,FR,hEo,gEo,uEo,u1,lee,pEo,_Eo,ER,vEo,bEo,TEo,p1,iee,FEo,EEo,CR,CEo,MEo,yEo,_1,dee,wEo,AEo,MR,LEo,BEo,xEo,v1,cee,kEo,REo,yR,PEo,SEo,$Eo,b1,fee,IEo,DEo,wR,NEo,jEo,OEo,T1,mee,GEo,qEo,AR,zEo,XEo,QEo,F1,hee,VEo,WEo,LR,HEo,UEo,JEo,E1,gee,KEo,YEo,BR,ZEo,eCo,oCo,C1,uee,rCo,tCo,xR,aCo,nCo,sCo,M1,pee,lCo,iCo,kR,dCo,cCo,fCo,y1,_ee,mCo,hCo,RR,gCo,uCo,pCo,w1,vee,_Co,vCo,PR,bCo,TCo,FCo,A1,bee,ECo,CCo,SR,MCo,yCo,wCo,L1,Tee,ACo,LCo,$R,BCo,xCo,kCo,B1,Fee,RCo,PCo,IR,SCo,$Co,ICo,x1,Eee,DCo,NCo,DR,jCo,OCo,GCo,k1,Cee,qCo,zCo,NR,XCo,QCo,VCo,R1,Mee,WCo,HCo,jR,UCo,JCo,KCo,P1,YCo,yee,ZCo,e3o,wee,o3o,r3o,Aee,t3o,a3o,vM,z5e,Ni,S1,Lee,bM,n3o,Bee,s3o,X5e,Ho,TM,l3o,ji,i3o,xee,d3o,c3o,kee,f3o,m3o,h3o,FM,g3o,Ree,u3o,p3o,_3o,Or,EM,v3o,Pee,b3o,T3o,Oi,F3o,See,E3o,C3o,$ee,M3o,y3o,w3o,Iee,A3o,L3o,CM,B3o,Xe,MM,x3o,Dee,k3o,R3o,$a,P3o,Nee,S3o,$3o,jee,I3o,D3o,Oee,N3o,j3o,O3o,S,$1,Gee,G3o,q3o,OR,z3o,X3o,Q3o,I1,qee,V3o,W3o,GR,H3o,U3o,J3o,D1,zee,K3o,Y3o,qR,Z3o,eMo,oMo,N1,Xee,rMo,tMo,zR,aMo,nMo,sMo,j1,Qee,lMo,iMo,XR,dMo,cMo,fMo,O1,Vee,mMo,hMo,QR,gMo,uMo,pMo,G1,Wee,_Mo,vMo,VR,bMo,TMo,FMo,q1,Hee,EMo,CMo,WR,MMo,yMo,wMo,z1,Uee,AMo,LMo,HR,BMo,xMo,kMo,X1,Jee,RMo,PMo,UR,SMo,$Mo,IMo,Q1,Kee,DMo,NMo,JR,jMo,OMo,GMo,V1,Yee,qMo,zMo,KR,XMo,QMo,VMo,W1,Zee,WMo,HMo,YR,UMo,JMo,KMo,H1,eoe,YMo,ZMo,ZR,e5o,o5o,r5o,U1,ooe,t5o,a5o,eP,n5o,s5o,l5o,J1,roe,i5o,d5o,oP,c5o,f5o,m5o,K1,toe,h5o,g5o,rP,u5o,p5o,_5o,Y1,aoe,v5o,b5o,tP,T5o,F5o,E5o,Z1,noe,C5o,M5o,aP,y5o,w5o,A5o,e4,soe,L5o,B5o,nP,x5o,k5o,R5o,o4,loe,P5o,S5o,sP,$5o,I5o,D5o,r4,ioe,N5o,j5o,lP,O5o,G5o,q5o,t4,doe,z5o,X5o,iP,Q5o,V5o,W5o,a4,coe,H5o,U5o,dP,J5o,K5o,Y5o,n4,foe,Z5o,eyo,cP,oyo,ryo,tyo,s4,moe,ayo,nyo,fP,syo,lyo,iyo,l4,hoe,dyo,cyo,mP,fyo,myo,hyo,i4,goe,gyo,uyo,hP,pyo,_yo,vyo,d4,uoe,byo,Tyo,gP,Fyo,Eyo,Cyo,c4,poe,Myo,yyo,uP,wyo,Ayo,Lyo,f4,_oe,Byo,xyo,pP,kyo,Ryo,Pyo,m4,voe,Syo,$yo,_P,Iyo,Dyo,Nyo,h4,boe,jyo,Oyo,vP,Gyo,qyo,zyo,g4,Toe,Xyo,Qyo,bP,Vyo,Wyo,Hyo,u4,Foe,Uyo,Jyo,TP,Kyo,Yyo,Zyo,p4,ewo,Eoe,owo,rwo,Coe,two,awo,Moe,nwo,swo,yM,Q5e,Gi,_4,yoe,wM,lwo,woe,iwo,V5e,Uo,AM,dwo,qi,cwo,Aoe,fwo,mwo,Loe,hwo,gwo,uwo,LM,pwo,Boe,_wo,vwo,bwo,Gr,BM,Two,xoe,Fwo,Ewo,zi,Cwo,koe,Mwo,ywo,Roe,wwo,Awo,Lwo,Poe,Bwo,xwo,xM,kwo,Qe,kM,Rwo,Soe,Pwo,Swo,Ia,$wo,$oe,Iwo,Dwo,Ioe,Nwo,jwo,Doe,Owo,Gwo,qwo,Noe,v4,joe,zwo,Xwo,FP,Qwo,Vwo,Wwo,b4,Hwo,Ooe,Uwo,Jwo,Goe,Kwo,Ywo,qoe,Zwo,e7o,RM,W5e,Xi,T4,zoe,PM,o7o,Xoe,r7o,H5e,Jo,SM,t7o,Qi,a7o,Qoe,n7o,s7o,Voe,l7o,i7o,d7o,$M,c7o,Woe,f7o,m7o,h7o,qr,IM,g7o,Hoe,u7o,p7o,Vi,_7o,Uoe,v7o,b7o,Joe,T7o,F7o,E7o,Koe,C7o,M7o,DM,y7o,Ve,NM,w7o,Yoe,A7o,L7o,Da,B7o,Zoe,x7o,k7o,ere,R7o,P7o,ore,S7o,$7o,I7o,Ko,F4,rre,D7o,N7o,EP,j7o,O7o,G7o,ds,tre,q7o,z7o,CP,X7o,Q7o,MP,V7o,W7o,H7o,E4,are,U7o,J7o,yP,K7o,Y7o,Z7o,Gt,nre,e0o,o0o,wP,r0o,t0o,AP,a0o,n0o,LP,s0o,l0o,i0o,C4,sre,d0o,c0o,BP,f0o,m0o,h0o,M4,lre,g0o,u0o,xP,p0o,_0o,v0o,y4,b0o,ire,T0o,F0o,dre,E0o,C0o,cre,M0o,y0o,jM,U5e,Wi,w4,fre,OM,w0o,mre,A0o,J5e,Yo,GM,L0o,Hi,B0o,hre,x0o,k0o,gre,R0o,P0o,S0o,qM,$0o,ure,I0o,D0o,N0o,zr,zM,j0o,pre,O0o,G0o,Ui,q0o,_re,z0o,X0o,vre,Q0o,V0o,W0o,bre,H0o,U0o,XM,J0o,We,QM,K0o,Tre,Y0o,Z0o,Na,eAo,Fre,oAo,rAo,Ere,tAo,aAo,Cre,nAo,sAo,lAo,Mre,A4,yre,iAo,dAo,kP,cAo,fAo,mAo,L4,hAo,wre,gAo,uAo,Are,pAo,_Ao,Lre,vAo,bAo,VM,K5e,Ji,B4,Bre,WM,TAo,xre,FAo,Y5e,Zo,HM,EAo,Ki,CAo,kre,MAo,yAo,Rre,wAo,AAo,LAo,UM,BAo,Pre,xAo,kAo,RAo,Xr,JM,PAo,Sre,SAo,$Ao,Yi,IAo,$re,DAo,NAo,Ire,jAo,OAo,GAo,Dre,qAo,zAo,KM,XAo,He,YM,QAo,Nre,VAo,WAo,ja,HAo,jre,UAo,JAo,Ore,KAo,YAo,Gre,ZAo,e6o,o6o,er,x4,qre,r6o,t6o,RP,a6o,n6o,s6o,k4,zre,l6o,i6o,PP,d6o,c6o,f6o,R4,Xre,m6o,h6o,SP,g6o,u6o,p6o,P4,Qre,_6o,v6o,$P,b6o,T6o,F6o,S4,Vre,E6o,C6o,IP,M6o,y6o,w6o,$4,Wre,A6o,L6o,DP,B6o,x6o,k6o,I4,R6o,Hre,P6o,S6o,Ure,$6o,I6o,Jre,D6o,N6o,ZM,Z5e,Zi,D4,Kre,e5,j6o,Yre,O6o,eye,or,o5,G6o,ed,q6o,Zre,z6o,X6o,ete,Q6o,V6o,W6o,r5,H6o,ote,U6o,J6o,K6o,Qr,t5,Y6o,rte,Z6o,eLo,od,oLo,tte,rLo,tLo,ate,aLo,nLo,sLo,nte,lLo,iLo,a5,dLo,Ue,n5,cLo,ste,fLo,mLo,Oa,hLo,lte,gLo,uLo,ite,pLo,_Lo,dte,vLo,bLo,TLo,rr,N4,cte,FLo,ELo,NP,CLo,MLo,yLo,j4,fte,wLo,ALo,jP,LLo,BLo,xLo,O4,mte,kLo,RLo,OP,PLo,SLo,$Lo,G4,hte,ILo,DLo,GP,NLo,jLo,OLo,q4,gte,GLo,qLo,qP,zLo,XLo,QLo,z4,ute,VLo,WLo,zP,HLo,ULo,JLo,X4,KLo,pte,YLo,ZLo,_te,e8o,o8o,vte,r8o,t8o,s5,oye,rd,Q4,bte,l5,a8o,Tte,n8o,rye,tr,i5,s8o,td,l8o,Fte,i8o,d8o,Ete,c8o,f8o,m8o,d5,h8o,Cte,g8o,u8o,p8o,Vr,c5,_8o,Mte,v8o,b8o,ad,T8o,yte,F8o,E8o,wte,C8o,M8o,y8o,Ate,w8o,A8o,f5,L8o,Je,m5,B8o,Lte,x8o,k8o,Ga,R8o,Bte,P8o,S8o,xte,$8o,I8o,kte,D8o,N8o,j8o,h5,V4,Rte,O8o,G8o,XP,q8o,z8o,X8o,W4,Pte,Q8o,V8o,QP,W8o,H8o,U8o,H4,J8o,Ste,K8o,Y8o,$te,Z8o,eBo,Ite,oBo,rBo,g5,tye,nd,U4,Dte,u5,tBo,Nte,aBo,aye,ar,p5,nBo,sd,sBo,jte,lBo,iBo,Ote,dBo,cBo,fBo,_5,mBo,Gte,hBo,gBo,uBo,Wr,v5,pBo,qte,_Bo,vBo,ld,bBo,zte,TBo,FBo,Xte,EBo,CBo,MBo,Qte,yBo,wBo,b5,ABo,Ke,T5,LBo,Vte,BBo,xBo,qa,kBo,Wte,RBo,PBo,Hte,SBo,$Bo,Ute,IBo,DBo,NBo,Jte,J4,Kte,jBo,OBo,VP,GBo,qBo,zBo,K4,XBo,Yte,QBo,VBo,Zte,WBo,HBo,eae,UBo,JBo,F5,nye,id,Y4,oae,E5,KBo,rae,YBo,sye,nr,C5,ZBo,dd,e9o,tae,o9o,r9o,aae,t9o,a9o,n9o,M5,s9o,nae,l9o,i9o,d9o,Hr,y5,c9o,sae,f9o,m9o,cd,h9o,lae,g9o,u9o,iae,p9o,_9o,v9o,dae,b9o,T9o,w5,F9o,Ye,A5,E9o,cae,C9o,M9o,za,y9o,fae,w9o,A9o,mae,L9o,B9o,hae,x9o,k9o,R9o,gae,Z4,uae,P9o,S9o,WP,$9o,I9o,D9o,ev,N9o,pae,j9o,O9o,_ae,G9o,q9o,vae,z9o,X9o,L5,lye,fd,ov,bae,B5,Q9o,Tae,V9o,iye,sr,x5,W9o,md,H9o,Fae,U9o,J9o,Eae,K9o,Y9o,Z9o,k5,exo,Cae,oxo,rxo,txo,Ur,R5,axo,Mae,nxo,sxo,hd,lxo,yae,ixo,dxo,wae,cxo,fxo,mxo,Aae,hxo,gxo,P5,uxo,lo,S5,pxo,Lae,_xo,vxo,Xa,bxo,Bae,Txo,Fxo,xae,Exo,Cxo,kae,Mxo,yxo,wxo,B,rv,Rae,Axo,Lxo,HP,Bxo,xxo,kxo,tv,Pae,Rxo,Pxo,UP,Sxo,$xo,Ixo,av,Sae,Dxo,Nxo,JP,jxo,Oxo,Gxo,nv,$ae,qxo,zxo,KP,Xxo,Qxo,Vxo,sv,Iae,Wxo,Hxo,YP,Uxo,Jxo,Kxo,lv,Dae,Yxo,Zxo,ZP,eko,oko,rko,iv,Nae,tko,ako,eS,nko,sko,lko,dv,jae,iko,dko,oS,cko,fko,mko,cv,Oae,hko,gko,rS,uko,pko,_ko,fv,Gae,vko,bko,tS,Tko,Fko,Eko,mv,qae,Cko,Mko,aS,yko,wko,Ako,hv,zae,Lko,Bko,nS,xko,kko,Rko,gv,Xae,Pko,Sko,sS,$ko,Iko,Dko,uv,Qae,Nko,jko,lS,Oko,Gko,qko,cs,Vae,zko,Xko,iS,Qko,Vko,dS,Wko,Hko,Uko,pv,Wae,Jko,Kko,cS,Yko,Zko,eRo,_v,Hae,oRo,rRo,fS,tRo,aRo,nRo,vv,Uae,sRo,lRo,mS,iRo,dRo,cRo,bv,Jae,fRo,mRo,hS,hRo,gRo,uRo,Tv,Kae,pRo,_Ro,gS,vRo,bRo,TRo,Fv,Yae,FRo,ERo,uS,CRo,MRo,yRo,Ev,Zae,wRo,ARo,pS,LRo,BRo,xRo,Cv,ene,kRo,RRo,_S,PRo,SRo,$Ro,Mv,one,IRo,DRo,vS,NRo,jRo,ORo,yv,rne,GRo,qRo,bS,zRo,XRo,QRo,wv,tne,VRo,WRo,TS,HRo,URo,JRo,Av,ane,KRo,YRo,FS,ZRo,ePo,oPo,Lv,nne,rPo,tPo,ES,aPo,nPo,sPo,Bv,sne,lPo,iPo,CS,dPo,cPo,fPo,xv,lne,mPo,hPo,MS,gPo,uPo,pPo,kv,ine,_Po,vPo,yS,bPo,TPo,FPo,Rv,dne,EPo,CPo,wS,MPo,yPo,wPo,Pv,cne,APo,LPo,AS,BPo,xPo,kPo,Sv,fne,RPo,PPo,LS,SPo,$Po,IPo,$v,mne,DPo,NPo,BS,jPo,OPo,GPo,Iv,hne,qPo,zPo,xS,XPo,QPo,VPo,Dv,gne,WPo,HPo,kS,UPo,JPo,KPo,Nv,une,YPo,ZPo,RS,eSo,oSo,rSo,jv,pne,tSo,aSo,PS,nSo,sSo,lSo,_ne,iSo,dSo,$5,dye,gd,Ov,vne,I5,cSo,bne,fSo,cye,lr,D5,mSo,ud,hSo,Tne,gSo,uSo,Fne,pSo,_So,vSo,N5,bSo,Ene,TSo,FSo,ESo,Jr,j5,CSo,Cne,MSo,ySo,pd,wSo,Mne,ASo,LSo,yne,BSo,xSo,kSo,wne,RSo,PSo,O5,SSo,io,G5,$So,Ane,ISo,DSo,Qa,NSo,Lne,jSo,OSo,Bne,GSo,qSo,xne,zSo,XSo,QSo,K,Gv,kne,VSo,WSo,SS,HSo,USo,JSo,qv,Rne,KSo,YSo,$S,ZSo,e$o,o$o,zv,Pne,r$o,t$o,IS,a$o,n$o,s$o,Xv,Sne,l$o,i$o,DS,d$o,c$o,f$o,Qv,$ne,m$o,h$o,NS,g$o,u$o,p$o,Vv,Ine,_$o,v$o,jS,b$o,T$o,F$o,Wv,Dne,E$o,C$o,OS,M$o,y$o,w$o,Hv,Nne,A$o,L$o,GS,B$o,x$o,k$o,Uv,jne,R$o,P$o,qS,S$o,$$o,I$o,Jv,One,D$o,N$o,zS,j$o,O$o,G$o,Kv,Gne,q$o,z$o,XS,X$o,Q$o,V$o,Yv,qne,W$o,H$o,QS,U$o,J$o,K$o,Zv,zne,Y$o,Z$o,VS,eIo,oIo,rIo,eb,Xne,tIo,aIo,WS,nIo,sIo,lIo,ob,Qne,iIo,dIo,HS,cIo,fIo,mIo,rb,Vne,hIo,gIo,US,uIo,pIo,_Io,tb,Wne,vIo,bIo,JS,TIo,FIo,EIo,ab,Hne,CIo,MIo,KS,yIo,wIo,AIo,nb,Une,LIo,BIo,YS,xIo,kIo,RIo,sb,Jne,PIo,SIo,ZS,$Io,IIo,DIo,lb,Kne,NIo,jIo,e$,OIo,GIo,qIo,ib,Yne,zIo,XIo,o$,QIo,VIo,WIo,Zne,HIo,UIo,q5,fye,_d,db,ese,z5,JIo,ose,KIo,mye,ir,X5,YIo,vd,ZIo,rse,eDo,oDo,tse,rDo,tDo,aDo,Q5,nDo,ase,sDo,lDo,iDo,Kr,V5,dDo,nse,cDo,fDo,bd,mDo,sse,hDo,gDo,lse,uDo,pDo,_Do,ise,vDo,bDo,W5,TDo,co,H5,FDo,dse,EDo,CDo,Va,MDo,cse,yDo,wDo,fse,ADo,LDo,mse,BDo,xDo,kDo,ve,cb,hse,RDo,PDo,r$,SDo,$Do,IDo,fb,gse,DDo,NDo,t$,jDo,ODo,GDo,mb,use,qDo,zDo,a$,XDo,QDo,VDo,hb,pse,WDo,HDo,n$,UDo,JDo,KDo,gb,_se,YDo,ZDo,s$,eNo,oNo,rNo,ub,vse,tNo,aNo,l$,nNo,sNo,lNo,pb,bse,iNo,dNo,i$,cNo,fNo,mNo,_b,Tse,hNo,gNo,d$,uNo,pNo,_No,vb,Fse,vNo,bNo,c$,TNo,FNo,ENo,bb,Ese,CNo,MNo,f$,yNo,wNo,ANo,Cse,LNo,BNo,U5,hye,Td,Tb,Mse,J5,xNo,yse,kNo,gye,dr,K5,RNo,Fd,PNo,wse,SNo,$No,Ase,INo,DNo,NNo,Y5,jNo,Lse,ONo,GNo,qNo,Yr,Z5,zNo,Bse,XNo,QNo,Ed,VNo,xse,WNo,HNo,kse,UNo,JNo,KNo,Rse,YNo,ZNo,ey,ejo,fo,oy,ojo,Pse,rjo,tjo,Wa,ajo,Sse,njo,sjo,$se,ljo,ijo,Ise,djo,cjo,fjo,Dse,Fb,Nse,mjo,hjo,m$,gjo,ujo,pjo,jse,_jo,vjo,ry,uye,Cd,Eb,Ose,ty,bjo,Gse,Tjo,pye,cr,ay,Fjo,Md,Ejo,qse,Cjo,Mjo,zse,yjo,wjo,Ajo,ny,Ljo,Xse,Bjo,xjo,kjo,Zr,sy,Rjo,Qse,Pjo,Sjo,yd,$jo,Vse,Ijo,Djo,Wse,Njo,jjo,Ojo,Hse,Gjo,qjo,ly,zjo,mo,iy,Xjo,Use,Qjo,Vjo,Ha,Wjo,Jse,Hjo,Ujo,Kse,Jjo,Kjo,Yse,Yjo,Zjo,eOo,te,Cb,Zse,oOo,rOo,h$,tOo,aOo,nOo,Mb,ele,sOo,lOo,g$,iOo,dOo,cOo,yb,ole,fOo,mOo,u$,hOo,gOo,uOo,wb,rle,pOo,_Oo,p$,vOo,bOo,TOo,Ab,tle,FOo,EOo,_$,COo,MOo,yOo,Lb,ale,wOo,AOo,v$,LOo,BOo,xOo,Bb,nle,kOo,ROo,b$,POo,SOo,$Oo,xb,sle,IOo,DOo,T$,NOo,jOo,OOo,kb,lle,GOo,qOo,F$,zOo,XOo,QOo,Rb,ile,VOo,WOo,E$,HOo,UOo,JOo,Pb,dle,KOo,YOo,C$,ZOo,eGo,oGo,Sb,cle,rGo,tGo,M$,aGo,nGo,sGo,$b,fle,lGo,iGo,y$,dGo,cGo,fGo,Ib,mle,mGo,hGo,w$,gGo,uGo,pGo,Db,hle,_Go,vGo,A$,bGo,TGo,FGo,Nb,gle,EGo,CGo,L$,MGo,yGo,wGo,jb,ule,AGo,LGo,B$,BGo,xGo,kGo,Ob,ple,RGo,PGo,x$,SGo,$Go,IGo,Gb,_le,DGo,NGo,k$,jGo,OGo,GGo,qb,vle,qGo,zGo,R$,XGo,QGo,VGo,ble,WGo,HGo,dy,_ye,wd,zb,Tle,cy,UGo,Fle,JGo,vye,fr,fy,KGo,Ad,YGo,Ele,ZGo,eqo,Cle,oqo,rqo,tqo,my,aqo,Mle,nqo,sqo,lqo,et,hy,iqo,yle,dqo,cqo,Ld,fqo,wle,mqo,hqo,Ale,gqo,uqo,pqo,Lle,_qo,vqo,gy,bqo,ho,uy,Tqo,Ble,Fqo,Eqo,Ua,Cqo,xle,Mqo,yqo,kle,wqo,Aqo,Rle,Lqo,Bqo,xqo,be,Xb,Ple,kqo,Rqo,P$,Pqo,Sqo,$qo,Qb,Sle,Iqo,Dqo,S$,Nqo,jqo,Oqo,Vb,$le,Gqo,qqo,$$,zqo,Xqo,Qqo,Wb,Ile,Vqo,Wqo,I$,Hqo,Uqo,Jqo,Hb,Dle,Kqo,Yqo,D$,Zqo,ezo,ozo,Ub,Nle,rzo,tzo,N$,azo,nzo,szo,Jb,jle,lzo,izo,j$,dzo,czo,fzo,Kb,Ole,mzo,hzo,O$,gzo,uzo,pzo,Yb,Gle,_zo,vzo,G$,bzo,Tzo,Fzo,Zb,qle,Ezo,Czo,q$,Mzo,yzo,wzo,zle,Azo,Lzo,py,bye,Bd,e2,Xle,_y,Bzo,Qle,xzo,Tye,mr,vy,kzo,xd,Rzo,Vle,Pzo,Szo,Wle,$zo,Izo,Dzo,by,Nzo,Hle,jzo,Ozo,Gzo,ot,Ty,qzo,Ule,zzo,Xzo,kd,Qzo,Jle,Vzo,Wzo,Kle,Hzo,Uzo,Jzo,Yle,Kzo,Yzo,Fy,Zzo,go,Ey,eXo,Zle,oXo,rXo,Ja,tXo,eie,aXo,nXo,oie,sXo,lXo,rie,iXo,dXo,cXo,W,o2,tie,fXo,mXo,z$,hXo,gXo,uXo,r2,aie,pXo,_Xo,X$,vXo,bXo,TXo,t2,nie,FXo,EXo,Q$,CXo,MXo,yXo,a2,sie,wXo,AXo,V$,LXo,BXo,xXo,n2,lie,kXo,RXo,W$,PXo,SXo,$Xo,s2,iie,IXo,DXo,H$,NXo,jXo,OXo,l2,die,GXo,qXo,U$,zXo,XXo,QXo,i2,cie,VXo,WXo,J$,HXo,UXo,JXo,d2,fie,KXo,YXo,K$,ZXo,eQo,oQo,c2,mie,rQo,tQo,Y$,aQo,nQo,sQo,f2,hie,lQo,iQo,Z$,dQo,cQo,fQo,m2,gie,mQo,hQo,eI,gQo,uQo,pQo,h2,uie,_Qo,vQo,oI,bQo,TQo,FQo,g2,pie,EQo,CQo,rI,MQo,yQo,wQo,u2,_ie,AQo,LQo,tI,BQo,xQo,kQo,p2,vie,RQo,PQo,aI,SQo,$Qo,IQo,_2,bie,DQo,NQo,nI,jQo,OQo,GQo,v2,Tie,qQo,zQo,sI,XQo,QQo,VQo,b2,Fie,WQo,HQo,lI,UQo,JQo,KQo,T2,Eie,YQo,ZQo,iI,eVo,oVo,rVo,F2,Cie,tVo,aVo,dI,nVo,sVo,lVo,E2,Mie,iVo,dVo,cI,cVo,fVo,mVo,C2,yie,hVo,gVo,fI,uVo,pVo,_Vo,M2,wie,vVo,bVo,mI,TVo,FVo,EVo,y2,Aie,CVo,MVo,hI,yVo,wVo,AVo,Lie,LVo,BVo,Cy,Fye,Rd,w2,Bie,My,xVo,xie,kVo,Eye,hr,yy,RVo,Pd,PVo,kie,SVo,$Vo,Rie,IVo,DVo,NVo,wy,jVo,Pie,OVo,GVo,qVo,rt,Ay,zVo,Sie,XVo,QVo,Sd,VVo,$ie,WVo,HVo,Iie,UVo,JVo,KVo,Die,YVo,ZVo,Ly,eWo,uo,By,oWo,Nie,rWo,tWo,Ka,aWo,jie,nWo,sWo,Oie,lWo,iWo,Gie,dWo,cWo,fWo,de,A2,qie,mWo,hWo,gI,gWo,uWo,pWo,L2,zie,_Wo,vWo,uI,bWo,TWo,FWo,B2,Xie,EWo,CWo,pI,MWo,yWo,wWo,x2,Qie,AWo,LWo,_I,BWo,xWo,kWo,k2,Vie,RWo,PWo,vI,SWo,$Wo,IWo,R2,Wie,DWo,NWo,bI,jWo,OWo,GWo,P2,Hie,qWo,zWo,TI,XWo,QWo,VWo,S2,Uie,WWo,HWo,FI,UWo,JWo,KWo,$2,Jie,YWo,ZWo,EI,eHo,oHo,rHo,I2,Kie,tHo,aHo,CI,nHo,sHo,lHo,D2,Yie,iHo,dHo,MI,cHo,fHo,mHo,N2,Zie,hHo,gHo,yI,uHo,pHo,_Ho,j2,ede,vHo,bHo,wI,THo,FHo,EHo,O2,ode,CHo,MHo,AI,yHo,wHo,AHo,G2,rde,LHo,BHo,LI,xHo,kHo,RHo,q2,tde,PHo,SHo,BI,$Ho,IHo,DHo,z2,ade,NHo,jHo,xI,OHo,GHo,qHo,nde,zHo,XHo,xy,Cye,$d,X2,sde,ky,QHo,lde,VHo,Mye,gr,Ry,WHo,Id,HHo,ide,UHo,JHo,dde,KHo,YHo,ZHo,Py,eUo,cde,oUo,rUo,tUo,tt,Sy,aUo,fde,nUo,sUo,Dd,lUo,mde,iUo,dUo,hde,cUo,fUo,mUo,gde,hUo,gUo,$y,uUo,po,Iy,pUo,ude,_Uo,vUo,Ya,bUo,pde,TUo,FUo,_de,EUo,CUo,vde,MUo,yUo,wUo,bde,Q2,Tde,AUo,LUo,kI,BUo,xUo,kUo,Fde,RUo,PUo,Dy,yye,Nd,V2,Ede,Ny,SUo,Cde,$Uo,wye,ur,jy,IUo,jd,DUo,Mde,NUo,jUo,yde,OUo,GUo,qUo,Oy,zUo,wde,XUo,QUo,VUo,at,Gy,WUo,Ade,HUo,UUo,Od,JUo,Lde,KUo,YUo,Bde,ZUo,eJo,oJo,xde,rJo,tJo,qy,aJo,_o,zy,nJo,kde,sJo,lJo,Za,iJo,Rde,dJo,cJo,Pde,fJo,mJo,Sde,hJo,gJo,uJo,ae,W2,$de,pJo,_Jo,RI,vJo,bJo,TJo,H2,Ide,FJo,EJo,PI,CJo,MJo,yJo,U2,Dde,wJo,AJo,SI,LJo,BJo,xJo,J2,Nde,kJo,RJo,$I,PJo,SJo,$Jo,K2,jde,IJo,DJo,II,NJo,jJo,OJo,Y2,Ode,GJo,qJo,DI,zJo,XJo,QJo,Z2,Gde,VJo,WJo,NI,HJo,UJo,JJo,eT,qde,KJo,YJo,jI,ZJo,eKo,oKo,oT,zde,rKo,tKo,OI,aKo,nKo,sKo,rT,Xde,lKo,iKo,GI,dKo,cKo,fKo,tT,Qde,mKo,hKo,qI,gKo,uKo,pKo,aT,Vde,_Ko,vKo,zI,bKo,TKo,FKo,nT,Wde,EKo,CKo,XI,MKo,yKo,wKo,sT,Hde,AKo,LKo,QI,BKo,xKo,kKo,lT,Ude,RKo,PKo,VI,SKo,$Ko,IKo,iT,Jde,DKo,NKo,WI,jKo,OKo,GKo,dT,Kde,qKo,zKo,HI,XKo,QKo,VKo,cT,Yde,WKo,HKo,UI,UKo,JKo,KKo,fT,Zde,YKo,ZKo,JI,eYo,oYo,rYo,mT,ece,tYo,aYo,KI,nYo,sYo,lYo,oce,iYo,dYo,Xy,Aye,Gd,hT,rce,Qy,cYo,tce,fYo,Lye,pr,Vy,mYo,qd,hYo,ace,gYo,uYo,nce,pYo,_Yo,vYo,Wy,bYo,sce,TYo,FYo,EYo,nt,Hy,CYo,lce,MYo,yYo,zd,wYo,ice,AYo,LYo,dce,BYo,xYo,kYo,cce,RYo,PYo,Uy,SYo,vo,Jy,$Yo,fce,IYo,DYo,en,NYo,mce,jYo,OYo,hce,GYo,qYo,gce,zYo,XYo,QYo,ne,gT,uce,VYo,WYo,YI,HYo,UYo,JYo,uT,pce,KYo,YYo,ZI,ZYo,eZo,oZo,pT,_ce,rZo,tZo,eD,aZo,nZo,sZo,_T,vce,lZo,iZo,oD,dZo,cZo,fZo,vT,bce,mZo,hZo,rD,gZo,uZo,pZo,bT,Tce,_Zo,vZo,tD,bZo,TZo,FZo,TT,Fce,EZo,CZo,aD,MZo,yZo,wZo,FT,Ece,AZo,LZo,nD,BZo,xZo,kZo,ET,Cce,RZo,PZo,sD,SZo,$Zo,IZo,CT,Mce,DZo,NZo,lD,jZo,OZo,GZo,MT,yce,qZo,zZo,iD,XZo,QZo,VZo,yT,wce,WZo,HZo,dD,UZo,JZo,KZo,wT,Ace,YZo,ZZo,cD,eer,oer,rer,AT,Lce,ter,aer,fD,ner,ser,ler,LT,Bce,ier,der,mD,cer,fer,mer,BT,xce,her,ger,hD,uer,per,_er,xT,kce,ver,ber,gD,Ter,Fer,Eer,kT,Rce,Cer,Mer,uD,yer,wer,Aer,RT,Pce,Ler,Ber,pD,xer,ker,Rer,Sce,Per,Ser,Ky,Bye,Xd,PT,$ce,Yy,$er,Ice,Ier,xye,_r,Zy,Der,Qd,Ner,Dce,jer,Oer,Nce,Ger,qer,zer,ew,Xer,jce,Qer,Ver,Wer,st,ow,Her,Oce,Uer,Jer,Vd,Ker,Gce,Yer,Zer,qce,eor,oor,ror,zce,tor,aor,rw,nor,bo,tw,sor,Xce,lor,ior,on,dor,Qce,cor,mor,Vce,hor,gor,Wce,uor,por,_or,Y,ST,Hce,vor,bor,_D,Tor,For,Eor,$T,Uce,Cor,Mor,vD,yor,wor,Aor,IT,Jce,Lor,Bor,bD,xor,kor,Ror,DT,Kce,Por,Sor,TD,$or,Ior,Dor,NT,Yce,Nor,jor,FD,Oor,Gor,qor,jT,Zce,zor,Xor,ED,Qor,Vor,Wor,OT,efe,Hor,Uor,CD,Jor,Kor,Yor,GT,ofe,Zor,err,MD,orr,rrr,trr,qT,rfe,arr,nrr,yD,srr,lrr,irr,zT,tfe,drr,crr,wD,frr,mrr,hrr,XT,afe,grr,urr,AD,prr,_rr,vrr,QT,nfe,brr,Trr,LD,Frr,Err,Crr,VT,sfe,Mrr,yrr,BD,wrr,Arr,Lrr,WT,lfe,Brr,xrr,xD,krr,Rrr,Prr,HT,ife,Srr,$rr,kD,Irr,Drr,Nrr,UT,dfe,jrr,Orr,RD,Grr,qrr,zrr,JT,cfe,Xrr,Qrr,PD,Vrr,Wrr,Hrr,KT,ffe,Urr,Jrr,SD,Krr,Yrr,Zrr,YT,mfe,etr,otr,$D,rtr,ttr,atr,ZT,hfe,ntr,str,ID,ltr,itr,dtr,eF,gfe,ctr,ftr,DD,mtr,htr,gtr,oF,ufe,utr,ptr,ND,_tr,vtr,btr,pfe,Ttr,Ftr,aw,kye,Wd,rF,_fe,nw,Etr,vfe,Ctr,Rye,vr,sw,Mtr,Hd,ytr,bfe,wtr,Atr,Tfe,Ltr,Btr,xtr,lw,ktr,Ffe,Rtr,Ptr,Str,lt,iw,$tr,Efe,Itr,Dtr,Ud,Ntr,Cfe,jtr,Otr,Mfe,Gtr,qtr,ztr,yfe,Xtr,Qtr,dw,Vtr,To,cw,Wtr,wfe,Htr,Utr,rn,Jtr,Afe,Ktr,Ytr,Lfe,Ztr,ear,Bfe,oar,rar,tar,Jd,tF,xfe,aar,nar,jD,sar,lar,iar,aF,kfe,dar,car,OD,far,mar,har,nF,Rfe,gar,uar,GD,par,_ar,bar,Pfe,Tar,Far,fw,Pye,Kd,sF,Sfe,mw,Ear,$fe,Car,Sye,br,hw,Mar,Yd,yar,Ife,war,Aar,Dfe,Lar,Bar,xar,gw,kar,Nfe,Rar,Par,Sar,it,uw,$ar,jfe,Iar,Dar,Zd,Nar,Ofe,jar,Oar,Gfe,Gar,qar,zar,qfe,Xar,Qar,pw,Var,Fo,_w,War,zfe,Har,Uar,tn,Jar,Xfe,Kar,Yar,Qfe,Zar,enr,Vfe,onr,rnr,tnr,Te,lF,Wfe,anr,nnr,qD,snr,lnr,inr,iF,Hfe,dnr,cnr,zD,fnr,mnr,hnr,dF,Ufe,gnr,unr,XD,pnr,_nr,vnr,cF,Jfe,bnr,Tnr,QD,Fnr,Enr,Cnr,fF,Kfe,Mnr,ynr,VD,wnr,Anr,Lnr,mF,Yfe,Bnr,xnr,WD,knr,Rnr,Pnr,hF,Zfe,Snr,$nr,HD,Inr,Dnr,Nnr,gF,eme,jnr,Onr,UD,Gnr,qnr,znr,uF,ome,Xnr,Qnr,JD,Vnr,Wnr,Hnr,pF,rme,Unr,Jnr,KD,Knr,Ynr,Znr,tme,esr,osr,vw,$ye,ec,_F,ame,bw,rsr,nme,tsr,Iye,Tr,Tw,asr,oc,nsr,sme,ssr,lsr,lme,isr,dsr,csr,Fw,fsr,ime,msr,hsr,gsr,dt,Ew,usr,dme,psr,_sr,rc,vsr,cme,bsr,Tsr,fme,Fsr,Esr,Csr,mme,Msr,ysr,Cw,wsr,Eo,Mw,Asr,hme,Lsr,Bsr,an,xsr,gme,ksr,Rsr,ume,Psr,Ssr,pme,$sr,Isr,Dsr,ke,vF,_me,Nsr,jsr,YD,Osr,Gsr,qsr,bF,vme,zsr,Xsr,ZD,Qsr,Vsr,Wsr,TF,bme,Hsr,Usr,eN,Jsr,Ksr,Ysr,FF,Tme,Zsr,elr,oN,olr,rlr,tlr,EF,Fme,alr,nlr,rN,slr,llr,ilr,CF,Eme,dlr,clr,tN,flr,mlr,hlr,MF,Cme,glr,ulr,aN,plr,_lr,vlr,yF,Mme,blr,Tlr,nN,Flr,Elr,Clr,yme,Mlr,ylr,yw,Dye,tc,wF,wme,ww,wlr,Ame,Alr,Nye,Fr,Aw,Llr,ac,Blr,Lme,xlr,klr,Bme,Rlr,Plr,Slr,Lw,$lr,xme,Ilr,Dlr,Nlr,ct,Bw,jlr,kme,Olr,Glr,nc,qlr,Rme,zlr,Xlr,Pme,Qlr,Vlr,Wlr,Sme,Hlr,Ulr,xw,Jlr,Co,kw,Klr,$me,Ylr,Zlr,nn,eir,Ime,oir,rir,Dme,tir,air,Nme,nir,sir,lir,Me,AF,jme,iir,dir,sN,cir,fir,mir,LF,Ome,hir,gir,lN,uir,pir,_ir,BF,Gme,vir,bir,iN,Tir,Fir,Eir,xF,qme,Cir,Mir,dN,yir,wir,Air,kF,zme,Lir,Bir,cN,xir,kir,Rir,RF,Xme,Pir,Sir,fN,$ir,Iir,Dir,PF,Qme,Nir,jir,mN,Oir,Gir,qir,SF,Vme,zir,Xir,hN,Qir,Vir,Wir,$F,Wme,Hir,Uir,gN,Jir,Kir,Yir,Hme,Zir,edr,Rw,jye,sc,IF,Ume,Pw,odr,Jme,rdr,Oye,Er,Sw,tdr,lc,adr,Kme,ndr,sdr,Yme,ldr,idr,ddr,$w,cdr,Zme,fdr,mdr,hdr,ft,Iw,gdr,ehe,udr,pdr,ic,_dr,ohe,vdr,bdr,rhe,Tdr,Fdr,Edr,the,Cdr,Mdr,Dw,ydr,Mo,Nw,wdr,ahe,Adr,Ldr,sn,Bdr,nhe,xdr,kdr,she,Rdr,Pdr,lhe,Sdr,$dr,Idr,Re,DF,ihe,Ddr,Ndr,uN,jdr,Odr,Gdr,NF,dhe,qdr,zdr,pN,Xdr,Qdr,Vdr,jF,che,Wdr,Hdr,_N,Udr,Jdr,Kdr,OF,fhe,Ydr,Zdr,vN,ecr,ocr,rcr,GF,mhe,tcr,acr,bN,ncr,scr,lcr,qF,hhe,icr,dcr,TN,ccr,fcr,mcr,zF,ghe,hcr,gcr,FN,ucr,pcr,_cr,XF,uhe,vcr,bcr,EN,Tcr,Fcr,Ecr,phe,Ccr,Mcr,jw,Gye,dc,QF,_he,Ow,ycr,vhe,wcr,qye,Cr,Gw,Acr,cc,Lcr,bhe,Bcr,xcr,The,kcr,Rcr,Pcr,qw,Scr,Fhe,$cr,Icr,Dcr,mt,zw,Ncr,Ehe,jcr,Ocr,fc,Gcr,Che,qcr,zcr,Mhe,Xcr,Qcr,Vcr,yhe,Wcr,Hcr,Xw,Ucr,yo,Qw,Jcr,whe,Kcr,Ycr,ln,Zcr,Ahe,efr,ofr,Lhe,rfr,tfr,Bhe,afr,nfr,sfr,Pe,VF,xhe,lfr,ifr,CN,dfr,cfr,ffr,WF,khe,mfr,hfr,MN,gfr,ufr,pfr,HF,Rhe,_fr,vfr,yN,bfr,Tfr,Ffr,UF,Phe,Efr,Cfr,wN,Mfr,yfr,wfr,JF,She,Afr,Lfr,AN,Bfr,xfr,kfr,KF,$he,Rfr,Pfr,LN,Sfr,$fr,Ifr,YF,Ihe,Dfr,Nfr,BN,jfr,Ofr,Gfr,ZF,Dhe,qfr,zfr,xN,Xfr,Qfr,Vfr,Nhe,Wfr,Hfr,Vw,zye,mc,eE,jhe,Ww,Ufr,Ohe,Jfr,Xye,Mr,Hw,Kfr,hc,Yfr,Ghe,Zfr,emr,qhe,omr,rmr,tmr,Uw,amr,zhe,nmr,smr,lmr,ht,Jw,imr,Xhe,dmr,cmr,gc,fmr,Qhe,mmr,hmr,Vhe,gmr,umr,pmr,Whe,_mr,vmr,Kw,bmr,wo,Yw,Tmr,Hhe,Fmr,Emr,dn,Cmr,Uhe,Mmr,ymr,Jhe,wmr,Amr,Khe,Lmr,Bmr,xmr,yr,oE,Yhe,kmr,Rmr,kN,Pmr,Smr,$mr,rE,Zhe,Imr,Dmr,RN,Nmr,jmr,Omr,tE,ege,Gmr,qmr,PN,zmr,Xmr,Qmr,aE,oge,Vmr,Wmr,SN,Hmr,Umr,Jmr,nE,rge,Kmr,Ymr,$N,Zmr,ehr,ohr,sE,tge,rhr,thr,IN,ahr,nhr,shr,age,lhr,ihr,Zw,Qye,uc,lE,nge,e7,dhr,sge,chr,Vye,wr,o7,fhr,pc,mhr,lge,hhr,ghr,ige,uhr,phr,_hr,r7,vhr,dge,bhr,Thr,Fhr,gt,t7,Ehr,cge,Chr,Mhr,_c,yhr,fge,whr,Ahr,mge,Lhr,Bhr,xhr,hge,khr,Rhr,a7,Phr,Ao,n7,Shr,gge,$hr,Ihr,cn,Dhr,uge,Nhr,jhr,pge,Ohr,Ghr,_ge,qhr,zhr,Xhr,Ar,iE,vge,Qhr,Vhr,DN,Whr,Hhr,Uhr,dE,bge,Jhr,Khr,NN,Yhr,Zhr,egr,cE,Tge,ogr,rgr,jN,tgr,agr,ngr,fE,Fge,sgr,lgr,ON,igr,dgr,cgr,mE,Ege,fgr,mgr,GN,hgr,ggr,ugr,hE,Cge,pgr,_gr,qN,vgr,bgr,Tgr,Mge,Fgr,Egr,s7,Wye,vc,gE,yge,l7,Cgr,wge,Mgr,Hye,Lr,i7,ygr,bc,wgr,Age,Agr,Lgr,Lge,Bgr,xgr,kgr,d7,Rgr,Bge,Pgr,Sgr,$gr,ut,c7,Igr,xge,Dgr,Ngr,Tc,jgr,kge,Ogr,Ggr,Rge,qgr,zgr,Xgr,Pge,Qgr,Vgr,f7,Wgr,Lo,m7,Hgr,Sge,Ugr,Jgr,fn,Kgr,$ge,Ygr,Zgr,Ige,eur,our,Dge,rur,tur,aur,Nge,uE,jge,nur,sur,zN,lur,iur,dur,Oge,cur,fur,h7,Uye,Fc,pE,Gge,g7,mur,qge,hur,Jye,Br,u7,gur,Ec,uur,zge,pur,_ur,Xge,vur,bur,Tur,p7,Fur,Qge,Eur,Cur,Mur,pt,_7,yur,Vge,wur,Aur,Cc,Lur,Wge,Bur,xur,Hge,kur,Rur,Pur,Uge,Sur,$ur,v7,Iur,Bo,b7,Dur,Jge,Nur,jur,mn,Our,Kge,Gur,qur,Yge,zur,Xur,Zge,Qur,Vur,Wur,T7,_E,eue,Hur,Uur,XN,Jur,Kur,Yur,vE,oue,Zur,epr,QN,opr,rpr,tpr,rue,apr,npr,F7,Kye,Mc,bE,tue,E7,spr,aue,lpr,Yye,xr,C7,ipr,yc,dpr,nue,cpr,fpr,sue,mpr,hpr,gpr,M7,upr,lue,ppr,_pr,vpr,_t,y7,bpr,iue,Tpr,Fpr,wc,Epr,due,Cpr,Mpr,cue,ypr,wpr,Apr,fue,Lpr,Bpr,w7,xpr,xo,A7,kpr,mue,Rpr,Ppr,hn,Spr,hue,$pr,Ipr,gue,Dpr,Npr,uue,jpr,Opr,Gpr,pue,TE,_ue,qpr,zpr,VN,Xpr,Qpr,Vpr,vue,Wpr,Hpr,L7,Zye;return ge=new Z({}),_a=new w({props:{code:"model = AutoModel.from_pretrained('bert-base-cased'),",highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)'}}),EC=new Z({}),CC=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Pc=new Upr({props:{warning:!0,$$slots:{default:[nYr]},$$scope:{ctx:ql}}}),MC=new Z({}),yC=new y({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/configuration_auto.py#L473"}}),LC=new y({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/configuration_auto.py#L496",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em>
is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,
the part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),BC=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-uncased')

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained('dbmdz/bert-base-german-cased')

# If configuration file is in a directory (e.g., was saved using _save_pretrained('./test/saved_model/')_).
config = AutoConfig.from_pretrained('./test/bert_saved_model/')

# Load a specific configuration file.
config = AutoConfig.from_pretrained('./test/bert_saved_model/my_configuration.json')

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False)
config.output_attentions
config, unused_kwargs = AutoConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)
config.output_attentions
config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;dbmdz/bert-base-german-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/my_configuration.json&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),xC=new y({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/configuration_auto.py#L614",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),kC=new Z({}),RC=new y({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/tokenization_auto.py#L359"}}),$C=new y({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/tokenization_auto.py#L373"}}),GC=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-german-cased')

# If vocabulary files are in a directory (e.g. tokenizer was saved using _save_pretrained('./test/saved_model/')_)
tokenizer = AutoTokenizer.from_pretrained('./test/bert_saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;dbmdz/bert-base-german-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/&#x27;</span>)`}}),qC=new y({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/tokenization_auto.py#L564",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),zC=new Z({}),XC=new y({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/feature_extraction_auto.py#L65"}}),WC=new y({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/feature_extraction_auto.py#L79"}}),Wm=new Upr({props:{$$slots:{default:[sYr]},$$scope:{ctx:ql}}}),YC=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')

# If feature extractor files are in a directory (e.g. feature extractor was saved using _save_pretrained('./test/saved_model/')_)
feature_extractor = AutoFeatureExtractor.from_pretrained('./test/saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;facebook/wav2vec2-base-960h&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;./test/saved_model/&#x27;</span>)`}}),ZC=new Z({}),e3=new y({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/processing_auto.py#L62"}}),t3=new y({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/processing_auto.py#L76"}}),rh=new Upr({props:{$$slots:{default:[lYr]},$$scope:{ctx:ql}}}),d3=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained('facebook/wav2vec2-base-960h')

# If processor files are in a directory (e.g. processor was saved using _save_pretrained('./test/saved_model/')_)
processor = AutoProcessor.from_pretrained('./test/saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/wav2vec2-base-960h&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&#x27;./test/saved_model/&#x27;</span>)`}}),c3=new Z({}),f3=new y({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L583"}}),h3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),g3=new w({props:{code:`from transformers import AutoConfig, AutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),u3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p3=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModel.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),_3=new Z({}),v3=new y({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L590"}}),T3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),F3=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),E3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C3=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForPreTraining.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),M3=new Z({}),y3=new y({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L605"}}),A3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),L3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),B3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),x3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForCausalLM.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),k3=new Z({}),R3=new y({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L612"}}),S3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),$3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),I3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),D3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForMaskedLM.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),N3=new Z({}),j3=new y({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L619"}}),G3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),z3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/t5_tf_model_config.json')
model = AutoModelForSeq2SeqLM.from_pretrained('./tf_model/t5_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/t5_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/t5_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),Q3=new Z({}),V3=new y({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L628"}}),H3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),U3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),J3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),K3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForSequenceClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),Y3=new Z({}),Z3=new y({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L662"}}),oM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),rM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),tM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForMultipleChoice.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),nM=new Z({}),sM=new y({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L669"}}),iM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),dM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),cM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForNextSentencePrediction.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),mM=new Z({}),hM=new y({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L655"}}),uM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),pM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),_M=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForTokenClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),bM=new Z({}),TM=new y({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L637"}}),EM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),CM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),MM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForQuestionAnswering.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),wM=new Z({}),AM=new y({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L644"}}),BM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),xM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('google/tapas-base-finetuned-wtq')
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),kM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),RM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/tapas_tf_model_config.json')
model = AutoModelForTableQuestionAnswering.from_pretrained('./tf_model/tapas_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/tapas_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./tf_model/tapas_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),PM=new Z({}),SM=new y({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L678"}}),IM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),DM=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),NM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jM=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForImageClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),OM=new Z({}),GM=new y({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L699"}}),zM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),QM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForVision2Seq.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),WM=new Z({}),HM=new y({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L706"}}),JM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),KM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),YM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ZM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForAudioClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),e5=new Z({}),o5=new y({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L713"}}),t5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),a5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),n5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForCTC.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForCTC.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),l5=new Z({}),i5=new y({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L720"}}),c5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),f5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),m5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForSpeechSeq2Seq.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),u5=new Z({}),p5=new y({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L692"}}),v5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),b5=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),T5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F5=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForObjectDetection.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),E5=new Z({}),C5=new y({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_auto.py#L685"}}),y5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),w5=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),A5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L5=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForImageSegmentation.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),B5=new Z({}),x5=new y({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L353"}}),R5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),P5=new w({props:{code:`from transformers import AutoConfig, TFAutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),S5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$5=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModel.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),I5=new Z({}),D5=new y({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L360"}}),j5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),O5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),G5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),q5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForPreTraining.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),z5=new Z({}),X5=new y({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L375"}}),V5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),W5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),H5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),U5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForCausalLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),J5=new Z({}),K5=new y({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L382"}}),Z5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ey=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),oy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ry=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForImageClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),ty=new Z({}),ay=new y({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L389"}}),sy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),iy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForMaskedLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),cy=new Z({}),fy=new y({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L396"}}),hy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),uy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),py=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/t5_pt_model_config.json')
model = TFAutoModelForSeq2SeqLM.from_pretrained('./pt_model/t5_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),_y=new Z({}),vy=new y({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L405"}}),Ty=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Ey=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForSequenceClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),My=new Z({}),yy=new y({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),Ay=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),By=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForMultipleChoice.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),ky=new Z({}),Ry=new y({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),Sy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('google/tapas-base-finetuned-wtq')
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Iy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/tapas_pt_model_config.json')
model = TFAutoModelForTableQuestionAnswering.from_pretrained('./pt_model/tapas_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/tapas_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/tapas_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Ny=new Z({}),jy=new y({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),Gy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),qy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),zy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForTokenClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Qy=new Z({}),Vy=new y({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),Hy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Jy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ky=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForQuestionAnswering.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Yy=new Z({}),Zy=new y({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L211"}}),ow=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),tw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModel.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),nw=new Z({}),sw=new y({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L225"}}),iw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
</ul>`,name:"config"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),cw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForCausalLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),mw=new Z({}),hw=new y({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L218"}}),uw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),pw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),_w=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForPreTraining.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),bw=new Z({}),Tw=new y({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L232"}}),Ew=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),Mw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForMaskedLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),ww=new Z({}),Aw=new y({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L239"}}),Bw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),xw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),kw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/t5_pt_model_config.json')
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('./pt_model/t5_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Pw=new Z({}),Sw=new y({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),Iw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),Nw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForSequenceClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Ow=new Z({}),Gw=new y({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),zw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),Qw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Vw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForQuestionAnswering.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Ww=new Z({}),Hw=new y({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L264"}}),Jw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Kw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),Yw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForTokenClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),e7=new Z({}),o7=new y({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),t7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),a7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),n7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForMultipleChoice.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),l7=new Z({}),i7=new y({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L280"}}),c7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),f7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),m7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),g7=new Z({}),u7=new y({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),_7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),v7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),b7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForImageClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),E7=new Z({}),C7=new y({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),y7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),w7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),A7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForVision2Seq.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),{c(){re=a("meta"),Se=l(),me=a("h1"),ue=a("a"),ro=a("span"),f(ge.$$.fragment),Ce=l(),$o=a("span"),zl=o("Auto Classes"),Lc=l(),Ot=a("p"),Xl=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Ql=a("code"),vC=o("from_pretrained()"),Bc=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),xe=l(),ao=a("p"),Vl=o("Instantiating one of "),gn=a("a"),bC=o("AutoConfig"),un=o(", "),pn=a("a"),TC=o("AutoModel"),Wl=o(`, and
`),_n=a("a"),FC=o("AutoTokenizer"),Hl=o(" will directly create a class of the relevant architecture. For instance"),xc=l(),f(_a.$$.fragment),no=l(),pe=a("p"),E0=o("will create a model that is an instance of "),Ul=a("a"),C0=o("BertModel"),M0=o("."),Io=l(),va=a("p"),y0=o("There is one class of "),kc=a("code"),w0=o("AutoModel"),i0e=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),m5e=l(),Jl=a("h2"),Rc=a("a"),PO=a("span"),f(EC.$$.fragment),d0e=l(),SO=a("span"),c0e=o("Extending the Auto Classes"),h5e=l(),vn=a("p"),f0e=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),$O=a("code"),m0e=o("NewModel"),h0e=o(", make sure you have a "),IO=a("code"),g0e=o("NewModelConfig"),u0e=o(` then you can add those to the auto
classes like this:`),g5e=l(),f(CC.$$.fragment),u5e=l(),A0=a("p"),p0e=o("You will then be able to use the auto classes like you would usually do!"),p5e=l(),f(Pc.$$.fragment),_5e=l(),Kl=a("h2"),Sc=a("a"),DO=a("span"),f(MC.$$.fragment),_0e=l(),NO=a("span"),v0e=o("AutoConfig"),v5e=l(),Do=a("div"),f(yC.$$.fragment),b0e=l(),wC=a("p"),T0e=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),L0=a("a"),F0e=o("from_pretrained()"),E0e=o(" class method."),C0e=l(),AC=a("p"),M0e=o("This class cannot be instantiated directly using "),jO=a("code"),y0e=o("__init__()"),w0e=o(" (throws an error)."),A0e=l(),so=a("div"),f(LC.$$.fragment),L0e=l(),OO=a("p"),B0e=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),x0e=l(),Yl=a("p"),k0e=o("The configuration class to instantiate is selected based on the "),GO=a("code"),R0e=o("model_type"),P0e=o(` property of the config object
that is loaded, or when it\u2019s missing, by falling back to using pattern matching on
`),qO=a("code"),S0e=o("pretrained_model_name_or_path"),$0e=o(":"),I0e=l(),b=a("ul"),$c=a("li"),zO=a("strong"),D0e=o("albert"),N0e=o(" \u2014 "),B0=a("a"),j0e=o("AlbertConfig"),O0e=o(" (ALBERT model)"),G0e=l(),Ic=a("li"),XO=a("strong"),q0e=o("bart"),z0e=o(" \u2014 "),x0=a("a"),X0e=o("BartConfig"),Q0e=o(" (BART model)"),V0e=l(),Dc=a("li"),QO=a("strong"),W0e=o("beit"),H0e=o(" \u2014 "),k0=a("a"),U0e=o("BeitConfig"),J0e=o(" (BEiT model)"),K0e=l(),Nc=a("li"),VO=a("strong"),Y0e=o("bert"),Z0e=o(" \u2014 "),R0=a("a"),eAe=o("BertConfig"),oAe=o(" (BERT model)"),rAe=l(),jc=a("li"),WO=a("strong"),tAe=o("bert-generation"),aAe=o(" \u2014 "),P0=a("a"),nAe=o("BertGenerationConfig"),sAe=o(" (Bert Generation model)"),lAe=l(),Oc=a("li"),HO=a("strong"),iAe=o("big_bird"),dAe=o(" \u2014 "),S0=a("a"),cAe=o("BigBirdConfig"),fAe=o(" (BigBird model)"),mAe=l(),Gc=a("li"),UO=a("strong"),hAe=o("bigbird_pegasus"),gAe=o(" \u2014 "),$0=a("a"),uAe=o("BigBirdPegasusConfig"),pAe=o(" (BigBirdPegasus model)"),_Ae=l(),qc=a("li"),JO=a("strong"),vAe=o("blenderbot"),bAe=o(" \u2014 "),I0=a("a"),TAe=o("BlenderbotConfig"),FAe=o(" (Blenderbot model)"),EAe=l(),zc=a("li"),KO=a("strong"),CAe=o("blenderbot-small"),MAe=o(" \u2014 "),D0=a("a"),yAe=o("BlenderbotSmallConfig"),wAe=o(" (BlenderbotSmall model)"),AAe=l(),Xc=a("li"),YO=a("strong"),LAe=o("camembert"),BAe=o(" \u2014 "),N0=a("a"),xAe=o("CamembertConfig"),kAe=o(" (CamemBERT model)"),RAe=l(),Qc=a("li"),ZO=a("strong"),PAe=o("canine"),SAe=o(" \u2014 "),j0=a("a"),$Ae=o("CanineConfig"),IAe=o(" (Canine model)"),DAe=l(),Vc=a("li"),eG=a("strong"),NAe=o("clip"),jAe=o(" \u2014 "),O0=a("a"),OAe=o("CLIPConfig"),GAe=o(" (CLIP model)"),qAe=l(),Wc=a("li"),oG=a("strong"),zAe=o("convbert"),XAe=o(" \u2014 "),G0=a("a"),QAe=o("ConvBertConfig"),VAe=o(" (ConvBERT model)"),WAe=l(),Hc=a("li"),rG=a("strong"),HAe=o("ctrl"),UAe=o(" \u2014 "),q0=a("a"),JAe=o("CTRLConfig"),KAe=o(" (CTRL model)"),YAe=l(),Uc=a("li"),tG=a("strong"),ZAe=o("deberta"),e6e=o(" \u2014 "),z0=a("a"),o6e=o("DebertaConfig"),r6e=o(" (DeBERTa model)"),t6e=l(),Jc=a("li"),aG=a("strong"),a6e=o("deberta-v2"),n6e=o(" \u2014 "),X0=a("a"),s6e=o("DebertaV2Config"),l6e=o(" (DeBERTa-v2 model)"),i6e=l(),Kc=a("li"),nG=a("strong"),d6e=o("deit"),c6e=o(" \u2014 "),Q0=a("a"),f6e=o("DeiTConfig"),m6e=o(" (DeiT model)"),h6e=l(),Yc=a("li"),sG=a("strong"),g6e=o("detr"),u6e=o(" \u2014 "),V0=a("a"),p6e=o("DetrConfig"),_6e=o(" (DETR model)"),v6e=l(),Zc=a("li"),lG=a("strong"),b6e=o("distilbert"),T6e=o(" \u2014 "),W0=a("a"),F6e=o("DistilBertConfig"),E6e=o(" (DistilBERT model)"),C6e=l(),ef=a("li"),iG=a("strong"),M6e=o("dpr"),y6e=o(" \u2014 "),H0=a("a"),w6e=o("DPRConfig"),A6e=o(" (DPR model)"),L6e=l(),of=a("li"),dG=a("strong"),B6e=o("electra"),x6e=o(" \u2014 "),U0=a("a"),k6e=o("ElectraConfig"),R6e=o(" (ELECTRA model)"),P6e=l(),rf=a("li"),cG=a("strong"),S6e=o("encoder-decoder"),$6e=o(" \u2014 "),J0=a("a"),I6e=o("EncoderDecoderConfig"),D6e=o(" (Encoder decoder model)"),N6e=l(),tf=a("li"),fG=a("strong"),j6e=o("flaubert"),O6e=o(" \u2014 "),K0=a("a"),G6e=o("FlaubertConfig"),q6e=o(" (FlauBERT model)"),z6e=l(),af=a("li"),mG=a("strong"),X6e=o("fnet"),Q6e=o(" \u2014 "),Y0=a("a"),V6e=o("FNetConfig"),W6e=o(" (FNet model)"),H6e=l(),nf=a("li"),hG=a("strong"),U6e=o("fsmt"),J6e=o(" \u2014 "),Z0=a("a"),K6e=o("FSMTConfig"),Y6e=o(" (FairSeq Machine-Translation model)"),Z6e=l(),sf=a("li"),gG=a("strong"),eLe=o("funnel"),oLe=o(" \u2014 "),eA=a("a"),rLe=o("FunnelConfig"),tLe=o(" (Funnel Transformer model)"),aLe=l(),lf=a("li"),uG=a("strong"),nLe=o("gpt2"),sLe=o(" \u2014 "),oA=a("a"),lLe=o("GPT2Config"),iLe=o(" (OpenAI GPT-2 model)"),dLe=l(),df=a("li"),pG=a("strong"),cLe=o("gpt_neo"),fLe=o(" \u2014 "),rA=a("a"),mLe=o("GPTNeoConfig"),hLe=o(" (GPT Neo model)"),gLe=l(),cf=a("li"),_G=a("strong"),uLe=o("gptj"),pLe=o(" \u2014 "),tA=a("a"),_Le=o("GPTJConfig"),vLe=o(" (GPT-J model)"),bLe=l(),ff=a("li"),vG=a("strong"),TLe=o("hubert"),FLe=o(" \u2014 "),aA=a("a"),ELe=o("HubertConfig"),CLe=o(" (Hubert model)"),MLe=l(),mf=a("li"),bG=a("strong"),yLe=o("ibert"),wLe=o(" \u2014 "),nA=a("a"),ALe=o("IBertConfig"),LLe=o(" (I-BERT model)"),BLe=l(),hf=a("li"),TG=a("strong"),xLe=o("imagegpt"),kLe=o(" \u2014 "),sA=a("a"),RLe=o("ImageGPTConfig"),PLe=o(" (ImageGPT model)"),SLe=l(),gf=a("li"),FG=a("strong"),$Le=o("layoutlm"),ILe=o(" \u2014 "),lA=a("a"),DLe=o("LayoutLMConfig"),NLe=o(" (LayoutLM model)"),jLe=l(),uf=a("li"),EG=a("strong"),OLe=o("layoutlmv2"),GLe=o(" \u2014 "),iA=a("a"),qLe=o("LayoutLMv2Config"),zLe=o(" (LayoutLMv2 model)"),XLe=l(),pf=a("li"),CG=a("strong"),QLe=o("led"),VLe=o(" \u2014 "),dA=a("a"),WLe=o("LEDConfig"),HLe=o(" (LED model)"),ULe=l(),_f=a("li"),MG=a("strong"),JLe=o("longformer"),KLe=o(" \u2014 "),cA=a("a"),YLe=o("LongformerConfig"),ZLe=o(" (Longformer model)"),e8e=l(),vf=a("li"),yG=a("strong"),o8e=o("luke"),r8e=o(" \u2014 "),fA=a("a"),t8e=o("LukeConfig"),a8e=o(" (LUKE model)"),n8e=l(),bf=a("li"),wG=a("strong"),s8e=o("lxmert"),l8e=o(" \u2014 "),mA=a("a"),i8e=o("LxmertConfig"),d8e=o(" (LXMERT model)"),c8e=l(),Tf=a("li"),AG=a("strong"),f8e=o("m2m_100"),m8e=o(" \u2014 "),hA=a("a"),h8e=o("M2M100Config"),g8e=o(" (M2M100 model)"),u8e=l(),Ff=a("li"),LG=a("strong"),p8e=o("marian"),_8e=o(" \u2014 "),gA=a("a"),v8e=o("MarianConfig"),b8e=o(" (Marian model)"),T8e=l(),Ef=a("li"),BG=a("strong"),F8e=o("mbart"),E8e=o(" \u2014 "),uA=a("a"),C8e=o("MBartConfig"),M8e=o(" (mBART model)"),y8e=l(),Cf=a("li"),xG=a("strong"),w8e=o("megatron-bert"),A8e=o(" \u2014 "),pA=a("a"),L8e=o("MegatronBertConfig"),B8e=o(" (MegatronBert model)"),x8e=l(),Mf=a("li"),kG=a("strong"),k8e=o("mobilebert"),R8e=o(" \u2014 "),_A=a("a"),P8e=o("MobileBertConfig"),S8e=o(" (MobileBERT model)"),$8e=l(),yf=a("li"),RG=a("strong"),I8e=o("mpnet"),D8e=o(" \u2014 "),vA=a("a"),N8e=o("MPNetConfig"),j8e=o(" (MPNet model)"),O8e=l(),wf=a("li"),PG=a("strong"),G8e=o("mt5"),q8e=o(" \u2014 "),bA=a("a"),z8e=o("MT5Config"),X8e=o(" (mT5 model)"),Q8e=l(),Af=a("li"),SG=a("strong"),V8e=o("openai-gpt"),W8e=o(" \u2014 "),TA=a("a"),H8e=o("OpenAIGPTConfig"),U8e=o(" (OpenAI GPT model)"),J8e=l(),Lf=a("li"),$G=a("strong"),K8e=o("pegasus"),Y8e=o(" \u2014 "),FA=a("a"),Z8e=o("PegasusConfig"),eBe=o(" (Pegasus model)"),oBe=l(),Bf=a("li"),IG=a("strong"),rBe=o("perceiver"),tBe=o(" \u2014 "),EA=a("a"),aBe=o("PerceiverConfig"),nBe=o(" (Perceiver model)"),sBe=l(),xf=a("li"),DG=a("strong"),lBe=o("prophetnet"),iBe=o(" \u2014 "),CA=a("a"),dBe=o("ProphetNetConfig"),cBe=o(" (ProphetNet model)"),fBe=l(),kf=a("li"),NG=a("strong"),mBe=o("qdqbert"),hBe=o(" \u2014 "),MA=a("a"),gBe=o("QDQBertConfig"),uBe=o(" (QDQBert model)"),pBe=l(),Rf=a("li"),jG=a("strong"),_Be=o("rag"),vBe=o(" \u2014 "),yA=a("a"),bBe=o("RagConfig"),TBe=o(" (RAG model)"),FBe=l(),Pf=a("li"),OG=a("strong"),EBe=o("reformer"),CBe=o(" \u2014 "),wA=a("a"),MBe=o("ReformerConfig"),yBe=o(" (Reformer model)"),wBe=l(),Sf=a("li"),GG=a("strong"),ABe=o("rembert"),LBe=o(" \u2014 "),AA=a("a"),BBe=o("RemBertConfig"),xBe=o(" (RemBERT model)"),kBe=l(),$f=a("li"),qG=a("strong"),RBe=o("retribert"),PBe=o(" \u2014 "),LA=a("a"),SBe=o("RetriBertConfig"),$Be=o(" (RetriBERT model)"),IBe=l(),If=a("li"),zG=a("strong"),DBe=o("roberta"),NBe=o(" \u2014 "),BA=a("a"),jBe=o("RobertaConfig"),OBe=o(" (RoBERTa model)"),GBe=l(),Df=a("li"),XG=a("strong"),qBe=o("roformer"),zBe=o(" \u2014 "),xA=a("a"),XBe=o("RoFormerConfig"),QBe=o(" (RoFormer model)"),VBe=l(),Nf=a("li"),QG=a("strong"),WBe=o("segformer"),HBe=o(" \u2014 "),kA=a("a"),UBe=o("SegformerConfig"),JBe=o(" (SegFormer model)"),KBe=l(),jf=a("li"),VG=a("strong"),YBe=o("sew"),ZBe=o(" \u2014 "),RA=a("a"),e9e=o("SEWConfig"),o9e=o(" (SEW model)"),r9e=l(),Of=a("li"),WG=a("strong"),t9e=o("sew-d"),a9e=o(" \u2014 "),PA=a("a"),n9e=o("SEWDConfig"),s9e=o(" (SEW-D model)"),l9e=l(),Gf=a("li"),HG=a("strong"),i9e=o("speech-encoder-decoder"),d9e=o(" \u2014 "),SA=a("a"),c9e=o("SpeechEncoderDecoderConfig"),f9e=o(" (Speech Encoder decoder model)"),m9e=l(),qf=a("li"),UG=a("strong"),h9e=o("speech_to_text"),g9e=o(" \u2014 "),$A=a("a"),u9e=o("Speech2TextConfig"),p9e=o(" (Speech2Text model)"),_9e=l(),zf=a("li"),JG=a("strong"),v9e=o("speech_to_text_2"),b9e=o(" \u2014 "),IA=a("a"),T9e=o("Speech2Text2Config"),F9e=o(" (Speech2Text2 model)"),E9e=l(),Xf=a("li"),KG=a("strong"),C9e=o("splinter"),M9e=o(" \u2014 "),DA=a("a"),y9e=o("SplinterConfig"),w9e=o(" (Splinter model)"),A9e=l(),Qf=a("li"),YG=a("strong"),L9e=o("squeezebert"),B9e=o(" \u2014 "),NA=a("a"),x9e=o("SqueezeBertConfig"),k9e=o(" (SqueezeBERT model)"),R9e=l(),Vf=a("li"),ZG=a("strong"),P9e=o("t5"),S9e=o(" \u2014 "),jA=a("a"),$9e=o("T5Config"),I9e=o(" (T5 model)"),D9e=l(),Wf=a("li"),eq=a("strong"),N9e=o("tapas"),j9e=o(" \u2014 "),OA=a("a"),O9e=o("TapasConfig"),G9e=o(" (TAPAS model)"),q9e=l(),Hf=a("li"),oq=a("strong"),z9e=o("transfo-xl"),X9e=o(" \u2014 "),GA=a("a"),Q9e=o("TransfoXLConfig"),V9e=o(" (Transformer-XL model)"),W9e=l(),Uf=a("li"),rq=a("strong"),H9e=o("trocr"),U9e=o(" \u2014 "),qA=a("a"),J9e=o("TrOCRConfig"),K9e=o(" (TrOCR model)"),Y9e=l(),Jf=a("li"),tq=a("strong"),Z9e=o("unispeech"),exe=o(" \u2014 "),zA=a("a"),oxe=o("UniSpeechConfig"),rxe=o(" (UniSpeech model)"),txe=l(),Kf=a("li"),aq=a("strong"),axe=o("unispeech-sat"),nxe=o(" \u2014 "),XA=a("a"),sxe=o("UniSpeechSatConfig"),lxe=o(" (UniSpeechSat model)"),ixe=l(),Yf=a("li"),nq=a("strong"),dxe=o("vision-encoder-decoder"),cxe=o(" \u2014 "),QA=a("a"),fxe=o("VisionEncoderDecoderConfig"),mxe=o(" (Vision Encoder decoder model)"),hxe=l(),Zf=a("li"),sq=a("strong"),gxe=o("vision-text-dual-encoder"),uxe=o(" \u2014 "),VA=a("a"),pxe=o("VisionTextDualEncoderConfig"),_xe=o(" (VisionTextDualEncoder model)"),vxe=l(),em=a("li"),lq=a("strong"),bxe=o("visual_bert"),Txe=o(" \u2014 "),WA=a("a"),Fxe=o("VisualBertConfig"),Exe=o(" (VisualBert model)"),Cxe=l(),om=a("li"),iq=a("strong"),Mxe=o("vit"),yxe=o(" \u2014 "),HA=a("a"),wxe=o("ViTConfig"),Axe=o(" (ViT model)"),Lxe=l(),rm=a("li"),dq=a("strong"),Bxe=o("wav2vec2"),xxe=o(" \u2014 "),UA=a("a"),kxe=o("Wav2Vec2Config"),Rxe=o(" (Wav2Vec2 model)"),Pxe=l(),tm=a("li"),cq=a("strong"),Sxe=o("xlm"),$xe=o(" \u2014 "),JA=a("a"),Ixe=o("XLMConfig"),Dxe=o(" (XLM model)"),Nxe=l(),am=a("li"),fq=a("strong"),jxe=o("xlm-prophetnet"),Oxe=o(" \u2014 "),KA=a("a"),Gxe=o("XLMProphetNetConfig"),qxe=o(" (XLMProphetNet model)"),zxe=l(),nm=a("li"),mq=a("strong"),Xxe=o("xlm-roberta"),Qxe=o(" \u2014 "),YA=a("a"),Vxe=o("XLMRobertaConfig"),Wxe=o(" (XLM-RoBERTa model)"),Hxe=l(),sm=a("li"),hq=a("strong"),Uxe=o("xlnet"),Jxe=o(" \u2014 "),ZA=a("a"),Kxe=o("XLNetConfig"),Yxe=o(" (XLNet model)"),Zxe=l(),gq=a("p"),eke=o("Examples:"),oke=l(),f(BC.$$.fragment),rke=l(),lm=a("div"),f(xC.$$.fragment),tke=l(),uq=a("p"),ake=o("Register a new configuration for this class."),b5e=l(),Zl=a("h2"),im=a("a"),pq=a("span"),f(kC.$$.fragment),nke=l(),_q=a("span"),ske=o("AutoTokenizer"),T5e=l(),No=a("div"),f(RC.$$.fragment),lke=l(),PC=a("p"),ike=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),e6=a("a"),dke=o("AutoTokenizer.from_pretrained()"),cke=o(" class method."),fke=l(),SC=a("p"),mke=o("This class cannot be instantiated directly using "),vq=a("code"),hke=o("__init__()"),gke=o(" (throws an error)."),uke=l(),ye=a("div"),f($C.$$.fragment),pke=l(),bq=a("p"),_ke=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),vke=l(),ba=a("p"),bke=o("The tokenizer class to instantiate is selected based on the "),Tq=a("code"),Tke=o("model_type"),Fke=o(` property of the config object
(either passed as an argument or loaded from `),Fq=a("code"),Eke=o("pretrained_model_name_or_path"),Cke=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),Eq=a("code"),Mke=o("pretrained_model_name_or_path"),yke=o(":"),wke=l(),C=a("ul"),bn=a("li"),Cq=a("strong"),Ake=o("albert"),Lke=o(" \u2014 "),o6=a("a"),Bke=o("AlbertTokenizer"),xke=o(" or "),r6=a("a"),kke=o("AlbertTokenizerFast"),Rke=o(" (ALBERT model)"),Pke=l(),Tn=a("li"),Mq=a("strong"),Ske=o("bart"),$ke=o(" \u2014 "),t6=a("a"),Ike=o("BartTokenizer"),Dke=o(" or "),a6=a("a"),Nke=o("BartTokenizerFast"),jke=o(" (BART model)"),Oke=l(),Fn=a("li"),yq=a("strong"),Gke=o("barthez"),qke=o(" \u2014 "),n6=a("a"),zke=o("BarthezTokenizer"),Xke=o(" or "),s6=a("a"),Qke=o("BarthezTokenizerFast"),Vke=o(" (BARThez model)"),Wke=l(),dm=a("li"),wq=a("strong"),Hke=o("bartpho"),Uke=o(" \u2014 "),l6=a("a"),Jke=o("BartphoTokenizer"),Kke=o(" (BARTpho model)"),Yke=l(),En=a("li"),Aq=a("strong"),Zke=o("bert"),eRe=o(" \u2014 "),i6=a("a"),oRe=o("BertTokenizer"),rRe=o(" or "),d6=a("a"),tRe=o("BertTokenizerFast"),aRe=o(" (BERT model)"),nRe=l(),cm=a("li"),Lq=a("strong"),sRe=o("bert-generation"),lRe=o(" \u2014 "),c6=a("a"),iRe=o("BertGenerationTokenizer"),dRe=o(" (Bert Generation model)"),cRe=l(),fm=a("li"),Bq=a("strong"),fRe=o("bert-japanese"),mRe=o(" \u2014 "),f6=a("a"),hRe=o("BertJapaneseTokenizer"),gRe=o(" (BertJapanese model)"),uRe=l(),mm=a("li"),xq=a("strong"),pRe=o("bertweet"),_Re=o(" \u2014 "),m6=a("a"),vRe=o("BertweetTokenizer"),bRe=o(" (Bertweet model)"),TRe=l(),Cn=a("li"),kq=a("strong"),FRe=o("big_bird"),ERe=o(" \u2014 "),h6=a("a"),CRe=o("BigBirdTokenizer"),MRe=o(" or "),g6=a("a"),yRe=o("BigBirdTokenizerFast"),wRe=o(" (BigBird model)"),ARe=l(),Mn=a("li"),Rq=a("strong"),LRe=o("bigbird_pegasus"),BRe=o(" \u2014 "),u6=a("a"),xRe=o("PegasusTokenizer"),kRe=o(" or "),p6=a("a"),RRe=o("PegasusTokenizerFast"),PRe=o(" (BigBirdPegasus model)"),SRe=l(),yn=a("li"),Pq=a("strong"),$Re=o("blenderbot"),IRe=o(" \u2014 "),_6=a("a"),DRe=o("BlenderbotTokenizer"),NRe=o(" or "),v6=a("a"),jRe=o("BlenderbotTokenizerFast"),ORe=o(" (Blenderbot model)"),GRe=l(),hm=a("li"),Sq=a("strong"),qRe=o("blenderbot-small"),zRe=o(" \u2014 "),b6=a("a"),XRe=o("BlenderbotSmallTokenizer"),QRe=o(" (BlenderbotSmall model)"),VRe=l(),gm=a("li"),$q=a("strong"),WRe=o("byt5"),HRe=o(" \u2014 "),T6=a("a"),URe=o("ByT5Tokenizer"),JRe=o(" (ByT5 model)"),KRe=l(),wn=a("li"),Iq=a("strong"),YRe=o("camembert"),ZRe=o(" \u2014 "),F6=a("a"),ePe=o("CamembertTokenizer"),oPe=o(" or "),E6=a("a"),rPe=o("CamembertTokenizerFast"),tPe=o(" (CamemBERT model)"),aPe=l(),um=a("li"),Dq=a("strong"),nPe=o("canine"),sPe=o(" \u2014 "),C6=a("a"),lPe=o("CanineTokenizer"),iPe=o(" (Canine model)"),dPe=l(),An=a("li"),Nq=a("strong"),cPe=o("clip"),fPe=o(" \u2014 "),M6=a("a"),mPe=o("CLIPTokenizer"),hPe=o(" or "),y6=a("a"),gPe=o("CLIPTokenizerFast"),uPe=o(" (CLIP model)"),pPe=l(),Ln=a("li"),jq=a("strong"),_Pe=o("convbert"),vPe=o(" \u2014 "),w6=a("a"),bPe=o("ConvBertTokenizer"),TPe=o(" or "),A6=a("a"),FPe=o("ConvBertTokenizerFast"),EPe=o(" (ConvBERT model)"),CPe=l(),Bn=a("li"),Oq=a("strong"),MPe=o("cpm"),yPe=o(" \u2014 "),L6=a("a"),wPe=o("CpmTokenizer"),APe=o(" or "),Gq=a("code"),LPe=o("CpmTokenizerFast"),BPe=o(" (CPM model)"),xPe=l(),pm=a("li"),qq=a("strong"),kPe=o("ctrl"),RPe=o(" \u2014 "),B6=a("a"),PPe=o("CTRLTokenizer"),SPe=o(" (CTRL model)"),$Pe=l(),xn=a("li"),zq=a("strong"),IPe=o("deberta"),DPe=o(" \u2014 "),x6=a("a"),NPe=o("DebertaTokenizer"),jPe=o(" or "),k6=a("a"),OPe=o("DebertaTokenizerFast"),GPe=o(" (DeBERTa model)"),qPe=l(),_m=a("li"),Xq=a("strong"),zPe=o("deberta-v2"),XPe=o(" \u2014 "),R6=a("a"),QPe=o("DebertaV2Tokenizer"),VPe=o(" (DeBERTa-v2 model)"),WPe=l(),kn=a("li"),Qq=a("strong"),HPe=o("distilbert"),UPe=o(" \u2014 "),P6=a("a"),JPe=o("DistilBertTokenizer"),KPe=o(" or "),S6=a("a"),YPe=o("DistilBertTokenizerFast"),ZPe=o(" (DistilBERT model)"),eSe=l(),Rn=a("li"),Vq=a("strong"),oSe=o("dpr"),rSe=o(" \u2014 "),$6=a("a"),tSe=o("DPRQuestionEncoderTokenizer"),aSe=o(" or "),I6=a("a"),nSe=o("DPRQuestionEncoderTokenizerFast"),sSe=o(" (DPR model)"),lSe=l(),Pn=a("li"),Wq=a("strong"),iSe=o("electra"),dSe=o(" \u2014 "),D6=a("a"),cSe=o("ElectraTokenizer"),fSe=o(" or "),N6=a("a"),mSe=o("ElectraTokenizerFast"),hSe=o(" (ELECTRA model)"),gSe=l(),vm=a("li"),Hq=a("strong"),uSe=o("flaubert"),pSe=o(" \u2014 "),j6=a("a"),_Se=o("FlaubertTokenizer"),vSe=o(" (FlauBERT model)"),bSe=l(),Sn=a("li"),Uq=a("strong"),TSe=o("fnet"),FSe=o(" \u2014 "),O6=a("a"),ESe=o("FNetTokenizer"),CSe=o(" or "),G6=a("a"),MSe=o("FNetTokenizerFast"),ySe=o(" (FNet model)"),wSe=l(),bm=a("li"),Jq=a("strong"),ASe=o("fsmt"),LSe=o(" \u2014 "),q6=a("a"),BSe=o("FSMTTokenizer"),xSe=o(" (FairSeq Machine-Translation model)"),kSe=l(),$n=a("li"),Kq=a("strong"),RSe=o("funnel"),PSe=o(" \u2014 "),z6=a("a"),SSe=o("FunnelTokenizer"),$Se=o(" or "),X6=a("a"),ISe=o("FunnelTokenizerFast"),DSe=o(" (Funnel Transformer model)"),NSe=l(),In=a("li"),Yq=a("strong"),jSe=o("gpt2"),OSe=o(" \u2014 "),Q6=a("a"),GSe=o("GPT2Tokenizer"),qSe=o(" or "),V6=a("a"),zSe=o("GPT2TokenizerFast"),XSe=o(" (OpenAI GPT-2 model)"),QSe=l(),Dn=a("li"),Zq=a("strong"),VSe=o("gpt_neo"),WSe=o(" \u2014 "),W6=a("a"),HSe=o("GPT2Tokenizer"),USe=o(" or "),H6=a("a"),JSe=o("GPT2TokenizerFast"),KSe=o(" (GPT Neo model)"),YSe=l(),Tm=a("li"),ez=a("strong"),ZSe=o("hubert"),e$e=o(" \u2014 "),U6=a("a"),o$e=o("Wav2Vec2CTCTokenizer"),r$e=o(" (Hubert model)"),t$e=l(),Nn=a("li"),oz=a("strong"),a$e=o("ibert"),n$e=o(" \u2014 "),J6=a("a"),s$e=o("RobertaTokenizer"),l$e=o(" or "),K6=a("a"),i$e=o("RobertaTokenizerFast"),d$e=o(" (I-BERT model)"),c$e=l(),jn=a("li"),rz=a("strong"),f$e=o("layoutlm"),m$e=o(" \u2014 "),Y6=a("a"),h$e=o("LayoutLMTokenizer"),g$e=o(" or "),Z6=a("a"),u$e=o("LayoutLMTokenizerFast"),p$e=o(" (LayoutLM model)"),_$e=l(),On=a("li"),tz=a("strong"),v$e=o("layoutlmv2"),b$e=o(" \u2014 "),eL=a("a"),T$e=o("LayoutLMv2Tokenizer"),F$e=o(" or "),oL=a("a"),E$e=o("LayoutLMv2TokenizerFast"),C$e=o(" (LayoutLMv2 model)"),M$e=l(),Gn=a("li"),az=a("strong"),y$e=o("led"),w$e=o(" \u2014 "),rL=a("a"),A$e=o("LEDTokenizer"),L$e=o(" or "),tL=a("a"),B$e=o("LEDTokenizerFast"),x$e=o(" (LED model)"),k$e=l(),qn=a("li"),nz=a("strong"),R$e=o("longformer"),P$e=o(" \u2014 "),aL=a("a"),S$e=o("LongformerTokenizer"),$$e=o(" or "),nL=a("a"),I$e=o("LongformerTokenizerFast"),D$e=o(" (Longformer model)"),N$e=l(),Fm=a("li"),sz=a("strong"),j$e=o("luke"),O$e=o(" \u2014 "),sL=a("a"),G$e=o("LukeTokenizer"),q$e=o(" (LUKE model)"),z$e=l(),zn=a("li"),lz=a("strong"),X$e=o("lxmert"),Q$e=o(" \u2014 "),lL=a("a"),V$e=o("LxmertTokenizer"),W$e=o(" or "),iL=a("a"),H$e=o("LxmertTokenizerFast"),U$e=o(" (LXMERT model)"),J$e=l(),Em=a("li"),iz=a("strong"),K$e=o("m2m_100"),Y$e=o(" \u2014 "),dL=a("a"),Z$e=o("M2M100Tokenizer"),eIe=o(" (M2M100 model)"),oIe=l(),Cm=a("li"),dz=a("strong"),rIe=o("marian"),tIe=o(" \u2014 "),cL=a("a"),aIe=o("MarianTokenizer"),nIe=o(" (Marian model)"),sIe=l(),Xn=a("li"),cz=a("strong"),lIe=o("mbart"),iIe=o(" \u2014 "),fL=a("a"),dIe=o("MBartTokenizer"),cIe=o(" or "),mL=a("a"),fIe=o("MBartTokenizerFast"),mIe=o(" (mBART model)"),hIe=l(),Qn=a("li"),fz=a("strong"),gIe=o("mbart50"),uIe=o(" \u2014 "),hL=a("a"),pIe=o("MBart50Tokenizer"),_Ie=o(" or "),gL=a("a"),vIe=o("MBart50TokenizerFast"),bIe=o(" (mBART-50 model)"),TIe=l(),Vn=a("li"),mz=a("strong"),FIe=o("mobilebert"),EIe=o(" \u2014 "),uL=a("a"),CIe=o("MobileBertTokenizer"),MIe=o(" or "),pL=a("a"),yIe=o("MobileBertTokenizerFast"),wIe=o(" (MobileBERT model)"),AIe=l(),Wn=a("li"),hz=a("strong"),LIe=o("mpnet"),BIe=o(" \u2014 "),_L=a("a"),xIe=o("MPNetTokenizer"),kIe=o(" or "),vL=a("a"),RIe=o("MPNetTokenizerFast"),PIe=o(" (MPNet model)"),SIe=l(),Hn=a("li"),gz=a("strong"),$Ie=o("mt5"),IIe=o(" \u2014 "),bL=a("a"),DIe=o("MT5Tokenizer"),NIe=o(" or "),TL=a("a"),jIe=o("MT5TokenizerFast"),OIe=o(" (mT5 model)"),GIe=l(),Un=a("li"),uz=a("strong"),qIe=o("openai-gpt"),zIe=o(" \u2014 "),FL=a("a"),XIe=o("OpenAIGPTTokenizer"),QIe=o(" or "),EL=a("a"),VIe=o("OpenAIGPTTokenizerFast"),WIe=o(" (OpenAI GPT model)"),HIe=l(),Jn=a("li"),pz=a("strong"),UIe=o("pegasus"),JIe=o(" \u2014 "),CL=a("a"),KIe=o("PegasusTokenizer"),YIe=o(" or "),ML=a("a"),ZIe=o("PegasusTokenizerFast"),eDe=o(" (Pegasus model)"),oDe=l(),Mm=a("li"),_z=a("strong"),rDe=o("perceiver"),tDe=o(" \u2014 "),yL=a("a"),aDe=o("PerceiverTokenizer"),nDe=o(" (Perceiver model)"),sDe=l(),ym=a("li"),vz=a("strong"),lDe=o("phobert"),iDe=o(" \u2014 "),wL=a("a"),dDe=o("PhobertTokenizer"),cDe=o(" (PhoBERT model)"),fDe=l(),wm=a("li"),bz=a("strong"),mDe=o("prophetnet"),hDe=o(" \u2014 "),AL=a("a"),gDe=o("ProphetNetTokenizer"),uDe=o(" (ProphetNet model)"),pDe=l(),Kn=a("li"),Tz=a("strong"),_De=o("qdqbert"),vDe=o(" \u2014 "),LL=a("a"),bDe=o("BertTokenizer"),TDe=o(" or "),BL=a("a"),FDe=o("BertTokenizerFast"),EDe=o(" (QDQBert model)"),CDe=l(),Am=a("li"),Fz=a("strong"),MDe=o("rag"),yDe=o(" \u2014 "),xL=a("a"),wDe=o("RagTokenizer"),ADe=o(" (RAG model)"),LDe=l(),Yn=a("li"),Ez=a("strong"),BDe=o("reformer"),xDe=o(" \u2014 "),kL=a("a"),kDe=o("ReformerTokenizer"),RDe=o(" or "),RL=a("a"),PDe=o("ReformerTokenizerFast"),SDe=o(" (Reformer model)"),$De=l(),Zn=a("li"),Cz=a("strong"),IDe=o("rembert"),DDe=o(" \u2014 "),PL=a("a"),NDe=o("RemBertTokenizer"),jDe=o(" or "),SL=a("a"),ODe=o("RemBertTokenizerFast"),GDe=o(" (RemBERT model)"),qDe=l(),es=a("li"),Mz=a("strong"),zDe=o("retribert"),XDe=o(" \u2014 "),$L=a("a"),QDe=o("RetriBertTokenizer"),VDe=o(" or "),IL=a("a"),WDe=o("RetriBertTokenizerFast"),HDe=o(" (RetriBERT model)"),UDe=l(),os=a("li"),yz=a("strong"),JDe=o("roberta"),KDe=o(" \u2014 "),DL=a("a"),YDe=o("RobertaTokenizer"),ZDe=o(" or "),NL=a("a"),eNe=o("RobertaTokenizerFast"),oNe=o(" (RoBERTa model)"),rNe=l(),rs=a("li"),wz=a("strong"),tNe=o("roformer"),aNe=o(" \u2014 "),jL=a("a"),nNe=o("RoFormerTokenizer"),sNe=o(" or "),OL=a("a"),lNe=o("RoFormerTokenizerFast"),iNe=o(" (RoFormer model)"),dNe=l(),Lm=a("li"),Az=a("strong"),cNe=o("speech_to_text"),fNe=o(" \u2014 "),GL=a("a"),mNe=o("Speech2TextTokenizer"),hNe=o(" (Speech2Text model)"),gNe=l(),Bm=a("li"),Lz=a("strong"),uNe=o("speech_to_text_2"),pNe=o(" \u2014 "),qL=a("a"),_Ne=o("Speech2Text2Tokenizer"),vNe=o(" (Speech2Text2 model)"),bNe=l(),ts=a("li"),Bz=a("strong"),TNe=o("splinter"),FNe=o(" \u2014 "),zL=a("a"),ENe=o("SplinterTokenizer"),CNe=o(" or "),XL=a("a"),MNe=o("SplinterTokenizerFast"),yNe=o(" (Splinter model)"),wNe=l(),as=a("li"),xz=a("strong"),ANe=o("squeezebert"),LNe=o(" \u2014 "),QL=a("a"),BNe=o("SqueezeBertTokenizer"),xNe=o(" or "),VL=a("a"),kNe=o("SqueezeBertTokenizerFast"),RNe=o(" (SqueezeBERT model)"),PNe=l(),ns=a("li"),kz=a("strong"),SNe=o("t5"),$Ne=o(" \u2014 "),WL=a("a"),INe=o("T5Tokenizer"),DNe=o(" or "),HL=a("a"),NNe=o("T5TokenizerFast"),jNe=o(" (T5 model)"),ONe=l(),xm=a("li"),Rz=a("strong"),GNe=o("tapas"),qNe=o(" \u2014 "),UL=a("a"),zNe=o("TapasTokenizer"),XNe=o(" (TAPAS model)"),QNe=l(),km=a("li"),Pz=a("strong"),VNe=o("transfo-xl"),WNe=o(" \u2014 "),JL=a("a"),HNe=o("TransfoXLTokenizer"),UNe=o(" (Transformer-XL model)"),JNe=l(),Rm=a("li"),Sz=a("strong"),KNe=o("wav2vec2"),YNe=o(" \u2014 "),KL=a("a"),ZNe=o("Wav2Vec2CTCTokenizer"),eje=o(" (Wav2Vec2 model)"),oje=l(),Pm=a("li"),$z=a("strong"),rje=o("xlm"),tje=o(" \u2014 "),YL=a("a"),aje=o("XLMTokenizer"),nje=o(" (XLM model)"),sje=l(),Sm=a("li"),Iz=a("strong"),lje=o("xlm-prophetnet"),ije=o(" \u2014 "),ZL=a("a"),dje=o("XLMProphetNetTokenizer"),cje=o(" (XLMProphetNet model)"),fje=l(),ss=a("li"),Dz=a("strong"),mje=o("xlm-roberta"),hje=o(" \u2014 "),e8=a("a"),gje=o("XLMRobertaTokenizer"),uje=o(" or "),o8=a("a"),pje=o("XLMRobertaTokenizerFast"),_je=o(" (XLM-RoBERTa model)"),vje=l(),ls=a("li"),Nz=a("strong"),bje=o("xlnet"),Tje=o(" \u2014 "),r8=a("a"),Fje=o("XLNetTokenizer"),Eje=o(" or "),t8=a("a"),Cje=o("XLNetTokenizerFast"),Mje=o(" (XLNet model)"),yje=l(),ei=a("p"),wje=o(`Params:
pretrained_model_name_or_path (`),jz=a("code"),Aje=o("str"),Lje=o(" or "),Oz=a("code"),Bje=o("os.PathLike"),xje=o(`):
Can be either:`),kje=l(),oi=a("ul"),Ta=a("li"),Rje=o("A string, the "),Gz=a("em"),Pje=o("model id"),Sje=o(` of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),qz=a("code"),$je=o("bert-base-uncased"),Ije=o(`, or namespaced under
a user or organization name, like `),zz=a("code"),Dje=o("dbmdz/bert-base-german-cased"),Nje=o("."),jje=l(),Fa=a("li"),Oje=o("A path to a "),Xz=a("em"),Gje=o("directory"),qje=o(` containing vocabulary files required by the tokenizer, for instance saved
using the `),a8=a("a"),zje=o("save_pretrained()"),Xje=o(` method, e.g.,
`),Qz=a("code"),Qje=o("./my_model_directory/"),Vje=o("."),Wje=l(),k=a("li"),Hje=o(`A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: `),Vz=a("code"),Uje=o("./my_model_directory/vocab.txt"),Jje=o(`. (Not
applicable to all derived classes)
inputs (additional positional arguments, `),Wz=a("em"),Kje=o("optional"),Yje=o(`):
Will be passed along to the Tokenizer `),Hz=a("code"),Zje=o("__init__()"),eOe=o(` method.
config (`),n8=a("a"),oOe=o("PretrainedConfig"),rOe=o(", "),Uz=a("em"),tOe=o("optional"),aOe=o(`)
The configuration object used to dertermine the tokenizer class to instantiate.
cache`),ri=a("em"),nOe=o("dir ("),Jz=a("code"),sOe=o("str"),lOe=o(" or "),Kz=a("code"),iOe=o("os.PathLike"),dOe=o(", _optional"),cOe=o(`):
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.
force`),IC=a("em"),fOe=o("download ("),Yz=a("code"),mOe=o("bool"),hOe=o(", _optional"),gOe=o(", defaults to "),Zz=a("code"),uOe=o("False"),pOe=o(`):
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.
resume`),DC=a("em"),_Oe=o("download ("),eX=a("code"),vOe=o("bool"),bOe=o(", _optional"),TOe=o(", defaults to "),oX=a("code"),FOe=o("False"),EOe=o(`):
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.
proxies (`),rX=a("code"),COe=o("Dict[str, str]"),MOe=o(", "),tX=a("em"),yOe=o("optional"),wOe=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),aX=a("code"),AOe=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}"),LOe=o(`. The proxies are used on each request.
revision(`),nX=a("code"),BOe=o("str"),xOe=o(", "),sX=a("em"),kOe=o("optional"),ROe=o(", defaults to "),lX=a("code"),POe=o('"main"'),SOe=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),iX=a("code"),$Oe=o("revision"),IOe=o(` can be any
identifier allowed by git.
subfolder (`),dX=a("code"),DOe=o("str"),NOe=o(", "),cX=a("em"),jOe=o("optional"),OOe=o(`):
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.
use`),NC=a("em"),GOe=o("fast ("),fX=a("code"),qOe=o("bool"),zOe=o(", _optional"),XOe=o(", defaults to "),mX=a("code"),QOe=o("True"),VOe=o(`):
Whether or not to try to load the fast version of the tokenizer.
tokenizer`),jC=a("em"),WOe=o("type ("),hX=a("code"),HOe=o("str"),UOe=o(", _optional"),JOe=o(`):
Tokenizer type to be loaded.
trust`),OC=a("em"),KOe=o("remote_code ("),gX=a("code"),YOe=o("bool"),ZOe=o(", _optional"),eGe=o(", defaults to "),uX=a("code"),oGe=o("False"),rGe=o(`):
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to `),pX=a("code"),tGe=o("True"),aGe=o(` for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.
kwargs (additional keyword arguments, `),_X=a("em"),nGe=o("optional"),sGe=o(`):
Will be passed to the Tokenizer `),vX=a("code"),lGe=o("__init__()"),iGe=o(` method. Can be used to set special tokens like
`),bX=a("code"),dGe=o("bos_token"),cGe=o(", "),TX=a("code"),fGe=o("eos_token"),mGe=o(", "),FX=a("code"),hGe=o("unk_token"),gGe=o(", "),EX=a("code"),uGe=o("sep_token"),pGe=o(", "),CX=a("code"),_Ge=o("pad_token"),vGe=o(", "),MX=a("code"),bGe=o("cls_token"),TGe=o(`,
`),yX=a("code"),FGe=o("mask_token"),EGe=o(", "),wX=a("code"),CGe=o("additional_special_tokens"),MGe=o(". See parameters in the "),AX=a("code"),yGe=o("__init__()"),wGe=o(" for more details."),AGe=l(),LX=a("p"),LGe=o("Examples:"),BGe=l(),f(GC.$$.fragment),xGe=l(),$m=a("div"),f(qC.$$.fragment),kGe=l(),BX=a("p"),RGe=o("Register a new tokenizer in this mapping."),F5e=l(),ti=a("h2"),Im=a("a"),xX=a("span"),f(zC.$$.fragment),PGe=l(),kX=a("span"),SGe=o("AutoFeatureExtractor"),E5e=l(),Dt=a("div"),f(XC.$$.fragment),$Ge=l(),QC=a("p"),IGe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),s8=a("a"),DGe=o("AutoFeatureExtractor.from_pretrained()"),NGe=o(" class method."),jGe=l(),VC=a("p"),OGe=o("This class cannot be instantiated directly using "),RX=a("code"),GGe=o("__init__()"),qGe=o(" (throws an error)."),zGe=l(),Fe=a("div"),f(WC.$$.fragment),XGe=l(),PX=a("p"),QGe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),VGe=l(),Ea=a("p"),WGe=o("The feature extractor class to instantiate is selected based on the "),SX=a("code"),HGe=o("model_type"),UGe=o(` property of the config
object (either passed as an argument or loaded from `),$X=a("code"),JGe=o("pretrained_model_name_or_path"),KGe=o(` if possible), or when
it\u2019s missing, by falling back to using pattern matching on `),IX=a("code"),YGe=o("pretrained_model_name_or_path"),ZGe=o(":"),eqe=l(),_e=a("ul"),Dm=a("li"),DX=a("strong"),oqe=o("beit"),rqe=o(" \u2014 "),l8=a("a"),tqe=o("BeitFeatureExtractor"),aqe=o(" (BEiT model)"),nqe=l(),Nm=a("li"),NX=a("strong"),sqe=o("clip"),lqe=o(" \u2014 "),i8=a("a"),iqe=o("CLIPFeatureExtractor"),dqe=o(" (CLIP model)"),cqe=l(),jm=a("li"),jX=a("strong"),fqe=o("deit"),mqe=o(" \u2014 "),d8=a("a"),hqe=o("DeiTFeatureExtractor"),gqe=o(" (DeiT model)"),uqe=l(),Om=a("li"),OX=a("strong"),pqe=o("detr"),_qe=o(" \u2014 "),c8=a("a"),vqe=o("DetrFeatureExtractor"),bqe=o(" (DETR model)"),Tqe=l(),Gm=a("li"),GX=a("strong"),Fqe=o("hubert"),Eqe=o(" \u2014 "),f8=a("a"),Cqe=o("Wav2Vec2FeatureExtractor"),Mqe=o(" (Hubert model)"),yqe=l(),qm=a("li"),qX=a("strong"),wqe=o("layoutlmv2"),Aqe=o(" \u2014 "),m8=a("a"),Lqe=o("LayoutLMv2FeatureExtractor"),Bqe=o(" (LayoutLMv2 model)"),xqe=l(),zm=a("li"),zX=a("strong"),kqe=o("perceiver"),Rqe=o(" \u2014 "),h8=a("a"),Pqe=o("PerceiverFeatureExtractor"),Sqe=o(" (Perceiver model)"),$qe=l(),Xm=a("li"),XX=a("strong"),Iqe=o("speech_to_text"),Dqe=o(" \u2014 "),g8=a("a"),Nqe=o("Speech2TextFeatureExtractor"),jqe=o(" (Speech2Text model)"),Oqe=l(),Qm=a("li"),QX=a("strong"),Gqe=o("vit"),qqe=o(" \u2014 "),u8=a("a"),zqe=o("ViTFeatureExtractor"),Xqe=o(" (ViT model)"),Qqe=l(),Vm=a("li"),VX=a("strong"),Vqe=o("wav2vec2"),Wqe=o(" \u2014 "),p8=a("a"),Hqe=o("Wav2Vec2FeatureExtractor"),Uqe=o(" (Wav2Vec2 model)"),Jqe=l(),ai=a("p"),Kqe=o(`Params:
pretrained_model_name_or_path (`),WX=a("code"),Yqe=o("str"),Zqe=o(" or "),HX=a("code"),eze=o("os.PathLike"),oze=o(`):
This can be either:`),rze=l(),ni=a("ul"),Ca=a("li"),tze=o("a string, the "),UX=a("em"),aze=o("model id"),nze=o(` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),JX=a("code"),sze=o("bert-base-uncased"),lze=o(`, or
namespaced under a user or organization name, like `),KX=a("code"),ize=o("dbmdz/bert-base-german-cased"),dze=o("."),cze=l(),Ma=a("li"),fze=o("a path to a "),YX=a("em"),mze=o("directory"),hze=o(` containing a feature extractor file saved using the
`),_8=a("a"),gze=o("save_pretrained()"),uze=o(` method, e.g.,
`),ZX=a("code"),pze=o("./my_model_directory/"),_ze=o("."),vze=l(),N=a("li"),bze=o("a path or url to a saved feature extractor JSON "),eQ=a("em"),Tze=o("file"),Fze=o(`, e.g.,
`),oQ=a("code"),Eze=o("./my_model_directory/preprocessor_config.json"),Cze=o(`.
cache`),si=a("em"),Mze=o("dir ("),rQ=a("code"),yze=o("str"),wze=o(" or "),tQ=a("code"),Aze=o("os.PathLike"),Lze=o(", _optional"),Bze=o(`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),HC=a("em"),xze=o("download ("),aQ=a("code"),kze=o("bool"),Rze=o(", _optional"),Pze=o(", defaults to "),nQ=a("code"),Sze=o("False"),$ze=o(`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),UC=a("em"),Ize=o("download ("),sQ=a("code"),Dze=o("bool"),Nze=o(", _optional"),jze=o(", defaults to "),lQ=a("code"),Oze=o("False"),Gze=o(`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),iQ=a("code"),qze=o("Dict[str, str]"),zze=o(", "),dQ=a("em"),Xze=o("optional"),Qze=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),cQ=a("code"),Vze=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),Wze=o(` The proxies are used on each request.
use`),JC=a("em"),Hze=o("auth_token ("),fQ=a("code"),Uze=o("str"),Jze=o(" or _bool"),Kze=o(", "),mQ=a("em"),Yze=o("optional"),Zze=o(`):
The token to use as HTTP bearer authorization for remote files. If `),hQ=a("code"),eXe=o("True"),oXe=o(`, will use the token
generated when running `),gQ=a("code"),rXe=o("transformers-cli login"),tXe=o(" (stored in "),uQ=a("code"),aXe=o("~/.huggingface"),nXe=o(`).
revision(`),pQ=a("code"),sXe=o("str"),lXe=o(", "),_Q=a("em"),iXe=o("optional"),dXe=o(", defaults to "),vQ=a("code"),cXe=o('"main"'),fXe=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),bQ=a("code"),mXe=o("revision"),hXe=o(` can be any
identifier allowed by git.
return`),KC=a("em"),gXe=o("unused_kwargs ("),TQ=a("code"),uXe=o("bool"),pXe=o(", _optional"),_Xe=o(", defaults to "),FQ=a("code"),vXe=o("False"),bXe=o(`):
If `),EQ=a("code"),TXe=o("False"),FXe=o(", then this function returns just the final feature extractor object. If "),CQ=a("code"),EXe=o("True"),CXe=o(`,
then this functions returns a `),MQ=a("code"),MXe=o("Tuple(feature_extractor, unused_kwargs)"),yXe=o(" where "),yQ=a("em"),wXe=o("unused_kwargs"),AXe=o(` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),wQ=a("code"),LXe=o("kwargs"),BXe=o(" which has not been used to update "),AQ=a("code"),xXe=o("feature_extractor"),kXe=o(` and is otherwise ignored.
kwargs (`),LQ=a("code"),RXe=o("Dict[str, Any]"),PXe=o(", "),BQ=a("em"),SXe=o("optional"),$Xe=o(`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),xQ=a("em"),IXe=o("not"),DXe=o(` feature extractor attributes is
controlled by the `),kQ=a("code"),NXe=o("return_unused_kwargs"),jXe=o(" keyword parameter."),OXe=l(),f(Wm.$$.fragment),GXe=l(),RQ=a("p"),qXe=o("Examples:"),zXe=l(),f(YC.$$.fragment),C5e=l(),li=a("h2"),Hm=a("a"),PQ=a("span"),f(ZC.$$.fragment),XXe=l(),SQ=a("span"),QXe=o("AutoProcessor"),M5e=l(),Nt=a("div"),f(e3.$$.fragment),VXe=l(),o3=a("p"),WXe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),v8=a("a"),HXe=o("AutoProcessor.from_pretrained()"),UXe=o(" class method."),JXe=l(),r3=a("p"),KXe=o("This class cannot be instantiated directly using "),$Q=a("code"),YXe=o("__init__()"),ZXe=o(" (throws an error)."),eQe=l(),Ee=a("div"),f(t3.$$.fragment),oQe=l(),IQ=a("p"),rQe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),tQe=l(),ii=a("p"),aQe=o("The processor class to instantiate is selected based on the "),DQ=a("code"),nQe=o("model_type"),sQe=o(` property of the config object
(either passed as an argument or loaded from `),NQ=a("code"),lQe=o("pretrained_model_name_or_path"),iQe=o(" if possible):"),dQe=l(),to=a("ul"),Um=a("li"),jQ=a("strong"),cQe=o("clip"),fQe=o(" \u2014 "),b8=a("a"),mQe=o("CLIPProcessor"),hQe=o(" (CLIP model)"),gQe=l(),Jm=a("li"),OQ=a("strong"),uQe=o("layoutlmv2"),pQe=o(" \u2014 "),T8=a("a"),_Qe=o("LayoutLMv2Processor"),vQe=o(" (LayoutLMv2 model)"),bQe=l(),Km=a("li"),GQ=a("strong"),TQe=o("speech_to_text"),FQe=o(" \u2014 "),F8=a("a"),EQe=o("Speech2TextProcessor"),CQe=o(" (Speech2Text model)"),MQe=l(),Ym=a("li"),qQ=a("strong"),yQe=o("speech_to_text_2"),wQe=o(" \u2014 "),E8=a("a"),AQe=o("Speech2Text2Processor"),LQe=o(" (Speech2Text2 model)"),BQe=l(),Zm=a("li"),zQ=a("strong"),xQe=o("trocr"),kQe=o(" \u2014 "),C8=a("a"),RQe=o("TrOCRProcessor"),PQe=o(" (TrOCR model)"),SQe=l(),eh=a("li"),XQ=a("strong"),$Qe=o("vision-text-dual-encoder"),IQe=o(" \u2014 "),M8=a("a"),DQe=o("VisionTextDualEncoderProcessor"),NQe=o(" (VisionTextDualEncoder model)"),jQe=l(),oh=a("li"),QQ=a("strong"),OQe=o("wav2vec2"),GQe=o(" \u2014 "),y8=a("a"),qQe=o("Wav2Vec2Processor"),zQe=o(" (Wav2Vec2 model)"),XQe=l(),di=a("p"),QQe=o(`Params:
pretrained_model_name_or_path (`),VQ=a("code"),VQe=o("str"),WQe=o(" or "),WQ=a("code"),HQe=o("os.PathLike"),UQe=o(`):
This can be either:`),JQe=l(),a3=a("ul"),ya=a("li"),KQe=o("a string, the "),HQ=a("em"),YQe=o("model id"),ZQe=o(` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),UQ=a("code"),eVe=o("bert-base-uncased"),oVe=o(`, or
namespaced under a user or organization name, like `),JQ=a("code"),rVe=o("dbmdz/bert-base-german-cased"),tVe=o("."),aVe=l(),D=a("li"),nVe=o("a path to a "),KQ=a("em"),sVe=o("directory"),lVe=o(" containing a processor files saved using the "),YQ=a("code"),iVe=o("save_pretrained()"),dVe=o(` method,
e.g., `),ZQ=a("code"),cVe=o("./my_model_directory/"),fVe=o(`.
cache`),ci=a("em"),mVe=o("dir ("),eV=a("code"),hVe=o("str"),gVe=o(" or "),oV=a("code"),uVe=o("os.PathLike"),pVe=o(", _optional"),_Ve=o(`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),n3=a("em"),vVe=o("download ("),rV=a("code"),bVe=o("bool"),TVe=o(", _optional"),FVe=o(", defaults to "),tV=a("code"),EVe=o("False"),CVe=o(`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),s3=a("em"),MVe=o("download ("),aV=a("code"),yVe=o("bool"),wVe=o(", _optional"),AVe=o(", defaults to "),nV=a("code"),LVe=o("False"),BVe=o(`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),sV=a("code"),xVe=o("Dict[str, str]"),kVe=o(", "),lV=a("em"),RVe=o("optional"),PVe=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),iV=a("code"),SVe=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),$Ve=o(` The proxies are used on each request.
use`),l3=a("em"),IVe=o("auth_token ("),dV=a("code"),DVe=o("str"),NVe=o(" or _bool"),jVe=o(", "),cV=a("em"),OVe=o("optional"),GVe=o(`):
The token to use as HTTP bearer authorization for remote files. If `),fV=a("code"),qVe=o("True"),zVe=o(`, will use the token
generated when running `),mV=a("code"),XVe=o("transformers-cli login"),QVe=o(" (stored in "),hV=a("code"),VVe=o("~/.huggingface"),WVe=o(`).
revision (`),gV=a("code"),HVe=o("str"),UVe=o(", "),uV=a("em"),JVe=o("optional"),KVe=o(", defaults to "),pV=a("code"),YVe=o('"main"'),ZVe=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),_V=a("code"),eWe=o("revision"),oWe=o(` can be any
identifier allowed by git.
return`),i3=a("em"),rWe=o("unused_kwargs ("),vV=a("code"),tWe=o("bool"),aWe=o(", _optional"),nWe=o(", defaults to "),bV=a("code"),sWe=o("False"),lWe=o(`):
If `),TV=a("code"),iWe=o("False"),dWe=o(", then this function returns just the final feature extractor object. If "),FV=a("code"),cWe=o("True"),fWe=o(`,
then this functions returns a `),EV=a("code"),mWe=o("Tuple(feature_extractor, unused_kwargs)"),hWe=o(" where "),CV=a("em"),gWe=o("unused_kwargs"),uWe=o(` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),MV=a("code"),pWe=o("kwargs"),_We=o(" which has not been used to update "),yV=a("code"),vWe=o("feature_extractor"),bWe=o(` and is otherwise ignored.
kwargs (`),wV=a("code"),TWe=o("Dict[str, Any]"),FWe=o(", "),AV=a("em"),EWe=o("optional"),CWe=o(`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),LV=a("em"),MWe=o("not"),yWe=o(` feature extractor attributes is
controlled by the `),BV=a("code"),wWe=o("return_unused_kwargs"),AWe=o(" keyword parameter."),LWe=l(),f(rh.$$.fragment),BWe=l(),xV=a("p"),xWe=o("Examples:"),kWe=l(),f(d3.$$.fragment),y5e=l(),fi=a("h2"),th=a("a"),kV=a("span"),f(c3.$$.fragment),RWe=l(),RV=a("span"),PWe=o("AutoModel"),w5e=l(),jo=a("div"),f(f3.$$.fragment),SWe=l(),mi=a("p"),$We=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),PV=a("code"),IWe=o("from_pretrained()"),DWe=o(` class method or the
`),SV=a("code"),NWe=o("from_config()"),jWe=o(" class method."),OWe=l(),m3=a("p"),GWe=o("This class cannot be instantiated directly using "),$V=a("code"),qWe=o("__init__()"),zWe=o(" (throws an error)."),XWe=l(),kr=a("div"),f(h3.$$.fragment),QWe=l(),IV=a("p"),VWe=o("Instantiates one of the base model classes of the library from a configuration."),WWe=l(),hi=a("p"),HWe=o(`Note:
Loading a model from its configuration file does `),DV=a("strong"),UWe=o("not"),JWe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),NV=a("code"),KWe=o("from_pretrained()"),YWe=o(` to load the model
weights.`),ZWe=l(),jV=a("p"),eHe=o("Examples:"),oHe=l(),f(g3.$$.fragment),rHe=l(),$e=a("div"),f(u3.$$.fragment),tHe=l(),OV=a("p"),aHe=o("Instantiate one of the base model classes of the library from a pretrained model."),nHe=l(),wa=a("p"),sHe=o("The model class to instantiate is selected based on the "),GV=a("code"),lHe=o("model_type"),iHe=o(` property of the config object (either
passed as an argument or loaded from `),qV=a("code"),dHe=o("pretrained_model_name_or_path"),cHe=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zV=a("code"),fHe=o("pretrained_model_name_or_path"),mHe=o(":"),hHe=l(),F=a("ul"),ah=a("li"),XV=a("strong"),gHe=o("albert"),uHe=o(" \u2014 "),w8=a("a"),pHe=o("AlbertModel"),_He=o(" (ALBERT model)"),vHe=l(),nh=a("li"),QV=a("strong"),bHe=o("bart"),THe=o(" \u2014 "),A8=a("a"),FHe=o("BartModel"),EHe=o(" (BART model)"),CHe=l(),sh=a("li"),VV=a("strong"),MHe=o("beit"),yHe=o(" \u2014 "),L8=a("a"),wHe=o("BeitModel"),AHe=o(" (BEiT model)"),LHe=l(),lh=a("li"),WV=a("strong"),BHe=o("bert"),xHe=o(" \u2014 "),B8=a("a"),kHe=o("BertModel"),RHe=o(" (BERT model)"),PHe=l(),ih=a("li"),HV=a("strong"),SHe=o("bert-generation"),$He=o(" \u2014 "),x8=a("a"),IHe=o("BertGenerationEncoder"),DHe=o(" (Bert Generation model)"),NHe=l(),dh=a("li"),UV=a("strong"),jHe=o("big_bird"),OHe=o(" \u2014 "),k8=a("a"),GHe=o("BigBirdModel"),qHe=o(" (BigBird model)"),zHe=l(),ch=a("li"),JV=a("strong"),XHe=o("bigbird_pegasus"),QHe=o(" \u2014 "),R8=a("a"),VHe=o("BigBirdPegasusModel"),WHe=o(" (BigBirdPegasus model)"),HHe=l(),fh=a("li"),KV=a("strong"),UHe=o("blenderbot"),JHe=o(" \u2014 "),P8=a("a"),KHe=o("BlenderbotModel"),YHe=o(" (Blenderbot model)"),ZHe=l(),mh=a("li"),YV=a("strong"),eUe=o("blenderbot-small"),oUe=o(" \u2014 "),S8=a("a"),rUe=o("BlenderbotSmallModel"),tUe=o(" (BlenderbotSmall model)"),aUe=l(),hh=a("li"),ZV=a("strong"),nUe=o("camembert"),sUe=o(" \u2014 "),$8=a("a"),lUe=o("CamembertModel"),iUe=o(" (CamemBERT model)"),dUe=l(),gh=a("li"),eW=a("strong"),cUe=o("canine"),fUe=o(" \u2014 "),I8=a("a"),mUe=o("CanineModel"),hUe=o(" (Canine model)"),gUe=l(),uh=a("li"),oW=a("strong"),uUe=o("clip"),pUe=o(" \u2014 "),D8=a("a"),_Ue=o("CLIPModel"),vUe=o(" (CLIP model)"),bUe=l(),ph=a("li"),rW=a("strong"),TUe=o("convbert"),FUe=o(" \u2014 "),N8=a("a"),EUe=o("ConvBertModel"),CUe=o(" (ConvBERT model)"),MUe=l(),_h=a("li"),tW=a("strong"),yUe=o("ctrl"),wUe=o(" \u2014 "),j8=a("a"),AUe=o("CTRLModel"),LUe=o(" (CTRL model)"),BUe=l(),vh=a("li"),aW=a("strong"),xUe=o("deberta"),kUe=o(" \u2014 "),O8=a("a"),RUe=o("DebertaModel"),PUe=o(" (DeBERTa model)"),SUe=l(),bh=a("li"),nW=a("strong"),$Ue=o("deberta-v2"),IUe=o(" \u2014 "),G8=a("a"),DUe=o("DebertaV2Model"),NUe=o(" (DeBERTa-v2 model)"),jUe=l(),Th=a("li"),sW=a("strong"),OUe=o("deit"),GUe=o(" \u2014 "),q8=a("a"),qUe=o("DeiTModel"),zUe=o(" (DeiT model)"),XUe=l(),Fh=a("li"),lW=a("strong"),QUe=o("detr"),VUe=o(" \u2014 "),z8=a("a"),WUe=o("DetrModel"),HUe=o(" (DETR model)"),UUe=l(),Eh=a("li"),iW=a("strong"),JUe=o("distilbert"),KUe=o(" \u2014 "),X8=a("a"),YUe=o("DistilBertModel"),ZUe=o(" (DistilBERT model)"),eJe=l(),Ch=a("li"),dW=a("strong"),oJe=o("dpr"),rJe=o(" \u2014 "),Q8=a("a"),tJe=o("DPRQuestionEncoder"),aJe=o(" (DPR model)"),nJe=l(),Mh=a("li"),cW=a("strong"),sJe=o("electra"),lJe=o(" \u2014 "),V8=a("a"),iJe=o("ElectraModel"),dJe=o(" (ELECTRA model)"),cJe=l(),yh=a("li"),fW=a("strong"),fJe=o("flaubert"),mJe=o(" \u2014 "),W8=a("a"),hJe=o("FlaubertModel"),gJe=o(" (FlauBERT model)"),uJe=l(),wh=a("li"),mW=a("strong"),pJe=o("fnet"),_Je=o(" \u2014 "),H8=a("a"),vJe=o("FNetModel"),bJe=o(" (FNet model)"),TJe=l(),Ah=a("li"),hW=a("strong"),FJe=o("fsmt"),EJe=o(" \u2014 "),U8=a("a"),CJe=o("FSMTModel"),MJe=o(" (FairSeq Machine-Translation model)"),yJe=l(),is=a("li"),gW=a("strong"),wJe=o("funnel"),AJe=o(" \u2014 "),J8=a("a"),LJe=o("FunnelModel"),BJe=o(" or "),K8=a("a"),xJe=o("FunnelBaseModel"),kJe=o(" (Funnel Transformer model)"),RJe=l(),Lh=a("li"),uW=a("strong"),PJe=o("gpt2"),SJe=o(" \u2014 "),Y8=a("a"),$Je=o("GPT2Model"),IJe=o(" (OpenAI GPT-2 model)"),DJe=l(),Bh=a("li"),pW=a("strong"),NJe=o("gpt_neo"),jJe=o(" \u2014 "),Z8=a("a"),OJe=o("GPTNeoModel"),GJe=o(" (GPT Neo model)"),qJe=l(),xh=a("li"),_W=a("strong"),zJe=o("gptj"),XJe=o(" \u2014 "),eB=a("a"),QJe=o("GPTJModel"),VJe=o(" (GPT-J model)"),WJe=l(),kh=a("li"),vW=a("strong"),HJe=o("hubert"),UJe=o(" \u2014 "),oB=a("a"),JJe=o("HubertModel"),KJe=o(" (Hubert model)"),YJe=l(),Rh=a("li"),bW=a("strong"),ZJe=o("ibert"),eKe=o(" \u2014 "),rB=a("a"),oKe=o("IBertModel"),rKe=o(" (I-BERT model)"),tKe=l(),Ph=a("li"),TW=a("strong"),aKe=o("imagegpt"),nKe=o(" \u2014 "),tB=a("a"),sKe=o("ImageGPTModel"),lKe=o(" (ImageGPT model)"),iKe=l(),Sh=a("li"),FW=a("strong"),dKe=o("layoutlm"),cKe=o(" \u2014 "),aB=a("a"),fKe=o("LayoutLMModel"),mKe=o(" (LayoutLM model)"),hKe=l(),$h=a("li"),EW=a("strong"),gKe=o("layoutlmv2"),uKe=o(" \u2014 "),nB=a("a"),pKe=o("LayoutLMv2Model"),_Ke=o(" (LayoutLMv2 model)"),vKe=l(),Ih=a("li"),CW=a("strong"),bKe=o("led"),TKe=o(" \u2014 "),sB=a("a"),FKe=o("LEDModel"),EKe=o(" (LED model)"),CKe=l(),Dh=a("li"),MW=a("strong"),MKe=o("longformer"),yKe=o(" \u2014 "),lB=a("a"),wKe=o("LongformerModel"),AKe=o(" (Longformer model)"),LKe=l(),Nh=a("li"),yW=a("strong"),BKe=o("luke"),xKe=o(" \u2014 "),iB=a("a"),kKe=o("LukeModel"),RKe=o(" (LUKE model)"),PKe=l(),jh=a("li"),wW=a("strong"),SKe=o("lxmert"),$Ke=o(" \u2014 "),dB=a("a"),IKe=o("LxmertModel"),DKe=o(" (LXMERT model)"),NKe=l(),Oh=a("li"),AW=a("strong"),jKe=o("m2m_100"),OKe=o(" \u2014 "),cB=a("a"),GKe=o("M2M100Model"),qKe=o(" (M2M100 model)"),zKe=l(),Gh=a("li"),LW=a("strong"),XKe=o("marian"),QKe=o(" \u2014 "),fB=a("a"),VKe=o("MarianModel"),WKe=o(" (Marian model)"),HKe=l(),qh=a("li"),BW=a("strong"),UKe=o("mbart"),JKe=o(" \u2014 "),mB=a("a"),KKe=o("MBartModel"),YKe=o(" (mBART model)"),ZKe=l(),zh=a("li"),xW=a("strong"),eYe=o("megatron-bert"),oYe=o(" \u2014 "),hB=a("a"),rYe=o("MegatronBertModel"),tYe=o(" (MegatronBert model)"),aYe=l(),Xh=a("li"),kW=a("strong"),nYe=o("mobilebert"),sYe=o(" \u2014 "),gB=a("a"),lYe=o("MobileBertModel"),iYe=o(" (MobileBERT model)"),dYe=l(),Qh=a("li"),RW=a("strong"),cYe=o("mpnet"),fYe=o(" \u2014 "),uB=a("a"),mYe=o("MPNetModel"),hYe=o(" (MPNet model)"),gYe=l(),Vh=a("li"),PW=a("strong"),uYe=o("mt5"),pYe=o(" \u2014 "),pB=a("a"),_Ye=o("MT5Model"),vYe=o(" (mT5 model)"),bYe=l(),Wh=a("li"),SW=a("strong"),TYe=o("openai-gpt"),FYe=o(" \u2014 "),_B=a("a"),EYe=o("OpenAIGPTModel"),CYe=o(" (OpenAI GPT model)"),MYe=l(),Hh=a("li"),$W=a("strong"),yYe=o("pegasus"),wYe=o(" \u2014 "),vB=a("a"),AYe=o("PegasusModel"),LYe=o(" (Pegasus model)"),BYe=l(),Uh=a("li"),IW=a("strong"),xYe=o("perceiver"),kYe=o(" \u2014 "),bB=a("a"),RYe=o("PerceiverModel"),PYe=o(" (Perceiver model)"),SYe=l(),Jh=a("li"),DW=a("strong"),$Ye=o("prophetnet"),IYe=o(" \u2014 "),TB=a("a"),DYe=o("ProphetNetModel"),NYe=o(" (ProphetNet model)"),jYe=l(),Kh=a("li"),NW=a("strong"),OYe=o("qdqbert"),GYe=o(" \u2014 "),FB=a("a"),qYe=o("QDQBertModel"),zYe=o(" (QDQBert model)"),XYe=l(),Yh=a("li"),jW=a("strong"),QYe=o("reformer"),VYe=o(" \u2014 "),EB=a("a"),WYe=o("ReformerModel"),HYe=o(" (Reformer model)"),UYe=l(),Zh=a("li"),OW=a("strong"),JYe=o("rembert"),KYe=o(" \u2014 "),CB=a("a"),YYe=o("RemBertModel"),ZYe=o(" (RemBERT model)"),eZe=l(),eg=a("li"),GW=a("strong"),oZe=o("retribert"),rZe=o(" \u2014 "),MB=a("a"),tZe=o("RetriBertModel"),aZe=o(" (RetriBERT model)"),nZe=l(),og=a("li"),qW=a("strong"),sZe=o("roberta"),lZe=o(" \u2014 "),yB=a("a"),iZe=o("RobertaModel"),dZe=o(" (RoBERTa model)"),cZe=l(),rg=a("li"),zW=a("strong"),fZe=o("roformer"),mZe=o(" \u2014 "),wB=a("a"),hZe=o("RoFormerModel"),gZe=o(" (RoFormer model)"),uZe=l(),tg=a("li"),XW=a("strong"),pZe=o("segformer"),_Ze=o(" \u2014 "),AB=a("a"),vZe=o("SegformerModel"),bZe=o(" (SegFormer model)"),TZe=l(),ag=a("li"),QW=a("strong"),FZe=o("sew"),EZe=o(" \u2014 "),LB=a("a"),CZe=o("SEWModel"),MZe=o(" (SEW model)"),yZe=l(),ng=a("li"),VW=a("strong"),wZe=o("sew-d"),AZe=o(" \u2014 "),BB=a("a"),LZe=o("SEWDModel"),BZe=o(" (SEW-D model)"),xZe=l(),sg=a("li"),WW=a("strong"),kZe=o("speech_to_text"),RZe=o(" \u2014 "),xB=a("a"),PZe=o("Speech2TextModel"),SZe=o(" (Speech2Text model)"),$Ze=l(),lg=a("li"),HW=a("strong"),IZe=o("splinter"),DZe=o(" \u2014 "),kB=a("a"),NZe=o("SplinterModel"),jZe=o(" (Splinter model)"),OZe=l(),ig=a("li"),UW=a("strong"),GZe=o("squeezebert"),qZe=o(" \u2014 "),RB=a("a"),zZe=o("SqueezeBertModel"),XZe=o(" (SqueezeBERT model)"),QZe=l(),dg=a("li"),JW=a("strong"),VZe=o("t5"),WZe=o(" \u2014 "),PB=a("a"),HZe=o("T5Model"),UZe=o(" (T5 model)"),JZe=l(),cg=a("li"),KW=a("strong"),KZe=o("tapas"),YZe=o(" \u2014 "),SB=a("a"),ZZe=o("TapasModel"),eeo=o(" (TAPAS model)"),oeo=l(),fg=a("li"),YW=a("strong"),reo=o("transfo-xl"),teo=o(" \u2014 "),$B=a("a"),aeo=o("TransfoXLModel"),neo=o(" (Transformer-XL model)"),seo=l(),mg=a("li"),ZW=a("strong"),leo=o("unispeech"),ieo=o(" \u2014 "),IB=a("a"),deo=o("UniSpeechModel"),ceo=o(" (UniSpeech model)"),feo=l(),hg=a("li"),eH=a("strong"),meo=o("unispeech-sat"),heo=o(" \u2014 "),DB=a("a"),geo=o("UniSpeechSatModel"),ueo=o(" (UniSpeechSat model)"),peo=l(),gg=a("li"),oH=a("strong"),_eo=o("vision-text-dual-encoder"),veo=o(" \u2014 "),NB=a("a"),beo=o("VisionTextDualEncoderModel"),Teo=o(" (VisionTextDualEncoder model)"),Feo=l(),ug=a("li"),rH=a("strong"),Eeo=o("visual_bert"),Ceo=o(" \u2014 "),jB=a("a"),Meo=o("VisualBertModel"),yeo=o(" (VisualBert model)"),weo=l(),pg=a("li"),tH=a("strong"),Aeo=o("vit"),Leo=o(" \u2014 "),OB=a("a"),Beo=o("ViTModel"),xeo=o(" (ViT model)"),keo=l(),_g=a("li"),aH=a("strong"),Reo=o("wav2vec2"),Peo=o(" \u2014 "),GB=a("a"),Seo=o("Wav2Vec2Model"),$eo=o(" (Wav2Vec2 model)"),Ieo=l(),vg=a("li"),nH=a("strong"),Deo=o("xlm"),Neo=o(" \u2014 "),qB=a("a"),jeo=o("XLMModel"),Oeo=o(" (XLM model)"),Geo=l(),bg=a("li"),sH=a("strong"),qeo=o("xlm-prophetnet"),zeo=o(" \u2014 "),zB=a("a"),Xeo=o("XLMProphetNetModel"),Qeo=o(" (XLMProphetNet model)"),Veo=l(),Tg=a("li"),lH=a("strong"),Weo=o("xlm-roberta"),Heo=o(" \u2014 "),XB=a("a"),Ueo=o("XLMRobertaModel"),Jeo=o(" (XLM-RoBERTa model)"),Keo=l(),Fg=a("li"),iH=a("strong"),Yeo=o("xlnet"),Zeo=o(" \u2014 "),QB=a("a"),eoo=o("XLNetModel"),ooo=o(" (XLNet model)"),roo=l(),Eg=a("p"),too=o("The model is set in evaluation mode by default using "),dH=a("code"),aoo=o("model.eval()"),noo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cH=a("code"),soo=o("model.train()"),loo=l(),fH=a("p"),ioo=o("Examples:"),doo=l(),f(p3.$$.fragment),A5e=l(),gi=a("h2"),Cg=a("a"),mH=a("span"),f(_3.$$.fragment),coo=l(),hH=a("span"),foo=o("AutoModelForPreTraining"),L5e=l(),Oo=a("div"),f(v3.$$.fragment),moo=l(),ui=a("p"),hoo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gH=a("code"),goo=o("from_pretrained()"),uoo=o(` class method or the
`),uH=a("code"),poo=o("from_config()"),_oo=o(" class method."),voo=l(),b3=a("p"),boo=o("This class cannot be instantiated directly using "),pH=a("code"),Too=o("__init__()"),Foo=o(" (throws an error)."),Eoo=l(),Rr=a("div"),f(T3.$$.fragment),Coo=l(),_H=a("p"),Moo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),yoo=l(),pi=a("p"),woo=o(`Note:
Loading a model from its configuration file does `),vH=a("strong"),Aoo=o("not"),Loo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bH=a("code"),Boo=o("from_pretrained()"),xoo=o(` to load the model
weights.`),koo=l(),TH=a("p"),Roo=o("Examples:"),Poo=l(),f(F3.$$.fragment),Soo=l(),Ie=a("div"),f(E3.$$.fragment),$oo=l(),FH=a("p"),Ioo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Doo=l(),Aa=a("p"),Noo=o("The model class to instantiate is selected based on the "),EH=a("code"),joo=o("model_type"),Ooo=o(` property of the config object (either
passed as an argument or loaded from `),CH=a("code"),Goo=o("pretrained_model_name_or_path"),qoo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),MH=a("code"),zoo=o("pretrained_model_name_or_path"),Xoo=o(":"),Qoo=l(),R=a("ul"),Mg=a("li"),yH=a("strong"),Voo=o("albert"),Woo=o(" \u2014 "),VB=a("a"),Hoo=o("AlbertForPreTraining"),Uoo=o(" (ALBERT model)"),Joo=l(),yg=a("li"),wH=a("strong"),Koo=o("bart"),Yoo=o(" \u2014 "),WB=a("a"),Zoo=o("BartForConditionalGeneration"),ero=o(" (BART model)"),oro=l(),wg=a("li"),AH=a("strong"),rro=o("bert"),tro=o(" \u2014 "),HB=a("a"),aro=o("BertForPreTraining"),nro=o(" (BERT model)"),sro=l(),Ag=a("li"),LH=a("strong"),lro=o("big_bird"),iro=o(" \u2014 "),UB=a("a"),dro=o("BigBirdForPreTraining"),cro=o(" (BigBird model)"),fro=l(),Lg=a("li"),BH=a("strong"),mro=o("camembert"),hro=o(" \u2014 "),JB=a("a"),gro=o("CamembertForMaskedLM"),uro=o(" (CamemBERT model)"),pro=l(),Bg=a("li"),xH=a("strong"),_ro=o("ctrl"),vro=o(" \u2014 "),KB=a("a"),bro=o("CTRLLMHeadModel"),Tro=o(" (CTRL model)"),Fro=l(),xg=a("li"),kH=a("strong"),Ero=o("deberta"),Cro=o(" \u2014 "),YB=a("a"),Mro=o("DebertaForMaskedLM"),yro=o(" (DeBERTa model)"),wro=l(),kg=a("li"),RH=a("strong"),Aro=o("deberta-v2"),Lro=o(" \u2014 "),ZB=a("a"),Bro=o("DebertaV2ForMaskedLM"),xro=o(" (DeBERTa-v2 model)"),kro=l(),Rg=a("li"),PH=a("strong"),Rro=o("distilbert"),Pro=o(" \u2014 "),e9=a("a"),Sro=o("DistilBertForMaskedLM"),$ro=o(" (DistilBERT model)"),Iro=l(),Pg=a("li"),SH=a("strong"),Dro=o("electra"),Nro=o(" \u2014 "),o9=a("a"),jro=o("ElectraForPreTraining"),Oro=o(" (ELECTRA model)"),Gro=l(),Sg=a("li"),$H=a("strong"),qro=o("flaubert"),zro=o(" \u2014 "),r9=a("a"),Xro=o("FlaubertWithLMHeadModel"),Qro=o(" (FlauBERT model)"),Vro=l(),$g=a("li"),IH=a("strong"),Wro=o("fnet"),Hro=o(" \u2014 "),t9=a("a"),Uro=o("FNetForPreTraining"),Jro=o(" (FNet model)"),Kro=l(),Ig=a("li"),DH=a("strong"),Yro=o("fsmt"),Zro=o(" \u2014 "),a9=a("a"),eto=o("FSMTForConditionalGeneration"),oto=o(" (FairSeq Machine-Translation model)"),rto=l(),Dg=a("li"),NH=a("strong"),tto=o("funnel"),ato=o(" \u2014 "),n9=a("a"),nto=o("FunnelForPreTraining"),sto=o(" (Funnel Transformer model)"),lto=l(),Ng=a("li"),jH=a("strong"),ito=o("gpt2"),dto=o(" \u2014 "),s9=a("a"),cto=o("GPT2LMHeadModel"),fto=o(" (OpenAI GPT-2 model)"),mto=l(),jg=a("li"),OH=a("strong"),hto=o("ibert"),gto=o(" \u2014 "),l9=a("a"),uto=o("IBertForMaskedLM"),pto=o(" (I-BERT model)"),_to=l(),Og=a("li"),GH=a("strong"),vto=o("layoutlm"),bto=o(" \u2014 "),i9=a("a"),Tto=o("LayoutLMForMaskedLM"),Fto=o(" (LayoutLM model)"),Eto=l(),Gg=a("li"),qH=a("strong"),Cto=o("longformer"),Mto=o(" \u2014 "),d9=a("a"),yto=o("LongformerForMaskedLM"),wto=o(" (Longformer model)"),Ato=l(),qg=a("li"),zH=a("strong"),Lto=o("lxmert"),Bto=o(" \u2014 "),c9=a("a"),xto=o("LxmertForPreTraining"),kto=o(" (LXMERT model)"),Rto=l(),zg=a("li"),XH=a("strong"),Pto=o("megatron-bert"),Sto=o(" \u2014 "),f9=a("a"),$to=o("MegatronBertForPreTraining"),Ito=o(" (MegatronBert model)"),Dto=l(),Xg=a("li"),QH=a("strong"),Nto=o("mobilebert"),jto=o(" \u2014 "),m9=a("a"),Oto=o("MobileBertForPreTraining"),Gto=o(" (MobileBERT model)"),qto=l(),Qg=a("li"),VH=a("strong"),zto=o("mpnet"),Xto=o(" \u2014 "),h9=a("a"),Qto=o("MPNetForMaskedLM"),Vto=o(" (MPNet model)"),Wto=l(),Vg=a("li"),WH=a("strong"),Hto=o("openai-gpt"),Uto=o(" \u2014 "),g9=a("a"),Jto=o("OpenAIGPTLMHeadModel"),Kto=o(" (OpenAI GPT model)"),Yto=l(),Wg=a("li"),HH=a("strong"),Zto=o("retribert"),eao=o(" \u2014 "),u9=a("a"),oao=o("RetriBertModel"),rao=o(" (RetriBERT model)"),tao=l(),Hg=a("li"),UH=a("strong"),aao=o("roberta"),nao=o(" \u2014 "),p9=a("a"),sao=o("RobertaForMaskedLM"),lao=o(" (RoBERTa model)"),iao=l(),Ug=a("li"),JH=a("strong"),dao=o("squeezebert"),cao=o(" \u2014 "),_9=a("a"),fao=o("SqueezeBertForMaskedLM"),mao=o(" (SqueezeBERT model)"),hao=l(),Jg=a("li"),KH=a("strong"),gao=o("t5"),uao=o(" \u2014 "),v9=a("a"),pao=o("T5ForConditionalGeneration"),_ao=o(" (T5 model)"),vao=l(),Kg=a("li"),YH=a("strong"),bao=o("tapas"),Tao=o(" \u2014 "),b9=a("a"),Fao=o("TapasForMaskedLM"),Eao=o(" (TAPAS model)"),Cao=l(),Yg=a("li"),ZH=a("strong"),Mao=o("transfo-xl"),yao=o(" \u2014 "),T9=a("a"),wao=o("TransfoXLLMHeadModel"),Aao=o(" (Transformer-XL model)"),Lao=l(),Zg=a("li"),eU=a("strong"),Bao=o("unispeech"),xao=o(" \u2014 "),F9=a("a"),kao=o("UniSpeechForPreTraining"),Rao=o(" (UniSpeech model)"),Pao=l(),eu=a("li"),oU=a("strong"),Sao=o("unispeech-sat"),$ao=o(" \u2014 "),E9=a("a"),Iao=o("UniSpeechSatForPreTraining"),Dao=o(" (UniSpeechSat model)"),Nao=l(),ou=a("li"),rU=a("strong"),jao=o("visual_bert"),Oao=o(" \u2014 "),C9=a("a"),Gao=o("VisualBertForPreTraining"),qao=o(" (VisualBert model)"),zao=l(),ru=a("li"),tU=a("strong"),Xao=o("wav2vec2"),Qao=o(" \u2014 "),M9=a("a"),Vao=o("Wav2Vec2ForPreTraining"),Wao=o(" (Wav2Vec2 model)"),Hao=l(),tu=a("li"),aU=a("strong"),Uao=o("xlm"),Jao=o(" \u2014 "),y9=a("a"),Kao=o("XLMWithLMHeadModel"),Yao=o(" (XLM model)"),Zao=l(),au=a("li"),nU=a("strong"),eno=o("xlm-roberta"),ono=o(" \u2014 "),w9=a("a"),rno=o("XLMRobertaForMaskedLM"),tno=o(" (XLM-RoBERTa model)"),ano=l(),nu=a("li"),sU=a("strong"),nno=o("xlnet"),sno=o(" \u2014 "),A9=a("a"),lno=o("XLNetLMHeadModel"),ino=o(" (XLNet model)"),dno=l(),su=a("p"),cno=o("The model is set in evaluation mode by default using "),lU=a("code"),fno=o("model.eval()"),mno=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iU=a("code"),hno=o("model.train()"),gno=l(),dU=a("p"),uno=o("Examples:"),pno=l(),f(C3.$$.fragment),B5e=l(),_i=a("h2"),lu=a("a"),cU=a("span"),f(M3.$$.fragment),_no=l(),fU=a("span"),vno=o("AutoModelForCausalLM"),x5e=l(),Go=a("div"),f(y3.$$.fragment),bno=l(),vi=a("p"),Tno=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),mU=a("code"),Fno=o("from_pretrained()"),Eno=o(` class method or the
`),hU=a("code"),Cno=o("from_config()"),Mno=o(" class method."),yno=l(),w3=a("p"),wno=o("This class cannot be instantiated directly using "),gU=a("code"),Ano=o("__init__()"),Lno=o(" (throws an error)."),Bno=l(),Pr=a("div"),f(A3.$$.fragment),xno=l(),uU=a("p"),kno=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Rno=l(),bi=a("p"),Pno=o(`Note:
Loading a model from its configuration file does `),pU=a("strong"),Sno=o("not"),$no=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_U=a("code"),Ino=o("from_pretrained()"),Dno=o(` to load the model
weights.`),Nno=l(),vU=a("p"),jno=o("Examples:"),Ono=l(),f(L3.$$.fragment),Gno=l(),De=a("div"),f(B3.$$.fragment),qno=l(),bU=a("p"),zno=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Xno=l(),La=a("p"),Qno=o("The model class to instantiate is selected based on the "),TU=a("code"),Vno=o("model_type"),Wno=o(` property of the config object (either
passed as an argument or loaded from `),FU=a("code"),Hno=o("pretrained_model_name_or_path"),Uno=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),EU=a("code"),Jno=o("pretrained_model_name_or_path"),Kno=o(":"),Yno=l(),q=a("ul"),iu=a("li"),CU=a("strong"),Zno=o("bart"),eso=o(" \u2014 "),L9=a("a"),oso=o("BartForCausalLM"),rso=o(" (BART model)"),tso=l(),du=a("li"),MU=a("strong"),aso=o("bert"),nso=o(" \u2014 "),B9=a("a"),sso=o("BertLMHeadModel"),lso=o(" (BERT model)"),iso=l(),cu=a("li"),yU=a("strong"),dso=o("bert-generation"),cso=o(" \u2014 "),x9=a("a"),fso=o("BertGenerationDecoder"),mso=o(" (Bert Generation model)"),hso=l(),fu=a("li"),wU=a("strong"),gso=o("big_bird"),uso=o(" \u2014 "),k9=a("a"),pso=o("BigBirdForCausalLM"),_so=o(" (BigBird model)"),vso=l(),mu=a("li"),AU=a("strong"),bso=o("bigbird_pegasus"),Tso=o(" \u2014 "),R9=a("a"),Fso=o("BigBirdPegasusForCausalLM"),Eso=o(" (BigBirdPegasus model)"),Cso=l(),hu=a("li"),LU=a("strong"),Mso=o("blenderbot"),yso=o(" \u2014 "),P9=a("a"),wso=o("BlenderbotForCausalLM"),Aso=o(" (Blenderbot model)"),Lso=l(),gu=a("li"),BU=a("strong"),Bso=o("blenderbot-small"),xso=o(" \u2014 "),S9=a("a"),kso=o("BlenderbotSmallForCausalLM"),Rso=o(" (BlenderbotSmall model)"),Pso=l(),uu=a("li"),xU=a("strong"),Sso=o("camembert"),$so=o(" \u2014 "),$9=a("a"),Iso=o("CamembertForCausalLM"),Dso=o(" (CamemBERT model)"),Nso=l(),pu=a("li"),kU=a("strong"),jso=o("ctrl"),Oso=o(" \u2014 "),I9=a("a"),Gso=o("CTRLLMHeadModel"),qso=o(" (CTRL model)"),zso=l(),_u=a("li"),RU=a("strong"),Xso=o("gpt2"),Qso=o(" \u2014 "),D9=a("a"),Vso=o("GPT2LMHeadModel"),Wso=o(" (OpenAI GPT-2 model)"),Hso=l(),vu=a("li"),PU=a("strong"),Uso=o("gpt_neo"),Jso=o(" \u2014 "),N9=a("a"),Kso=o("GPTNeoForCausalLM"),Yso=o(" (GPT Neo model)"),Zso=l(),bu=a("li"),SU=a("strong"),elo=o("gptj"),olo=o(" \u2014 "),j9=a("a"),rlo=o("GPTJForCausalLM"),tlo=o(" (GPT-J model)"),alo=l(),Tu=a("li"),$U=a("strong"),nlo=o("marian"),slo=o(" \u2014 "),O9=a("a"),llo=o("MarianForCausalLM"),ilo=o(" (Marian model)"),dlo=l(),Fu=a("li"),IU=a("strong"),clo=o("mbart"),flo=o(" \u2014 "),G9=a("a"),mlo=o("MBartForCausalLM"),hlo=o(" (mBART model)"),glo=l(),Eu=a("li"),DU=a("strong"),ulo=o("megatron-bert"),plo=o(" \u2014 "),q9=a("a"),_lo=o("MegatronBertForCausalLM"),vlo=o(" (MegatronBert model)"),blo=l(),Cu=a("li"),NU=a("strong"),Tlo=o("openai-gpt"),Flo=o(" \u2014 "),z9=a("a"),Elo=o("OpenAIGPTLMHeadModel"),Clo=o(" (OpenAI GPT model)"),Mlo=l(),Mu=a("li"),jU=a("strong"),ylo=o("pegasus"),wlo=o(" \u2014 "),X9=a("a"),Alo=o("PegasusForCausalLM"),Llo=o(" (Pegasus model)"),Blo=l(),yu=a("li"),OU=a("strong"),xlo=o("prophetnet"),klo=o(" \u2014 "),Q9=a("a"),Rlo=o("ProphetNetForCausalLM"),Plo=o(" (ProphetNet model)"),Slo=l(),wu=a("li"),GU=a("strong"),$lo=o("qdqbert"),Ilo=o(" \u2014 "),V9=a("a"),Dlo=o("QDQBertLMHeadModel"),Nlo=o(" (QDQBert model)"),jlo=l(),Au=a("li"),qU=a("strong"),Olo=o("reformer"),Glo=o(" \u2014 "),W9=a("a"),qlo=o("ReformerModelWithLMHead"),zlo=o(" (Reformer model)"),Xlo=l(),Lu=a("li"),zU=a("strong"),Qlo=o("rembert"),Vlo=o(" \u2014 "),H9=a("a"),Wlo=o("RemBertForCausalLM"),Hlo=o(" (RemBERT model)"),Ulo=l(),Bu=a("li"),XU=a("strong"),Jlo=o("roberta"),Klo=o(" \u2014 "),U9=a("a"),Ylo=o("RobertaForCausalLM"),Zlo=o(" (RoBERTa model)"),eio=l(),xu=a("li"),QU=a("strong"),oio=o("roformer"),rio=o(" \u2014 "),J9=a("a"),tio=o("RoFormerForCausalLM"),aio=o(" (RoFormer model)"),nio=l(),ku=a("li"),VU=a("strong"),sio=o("speech_to_text_2"),lio=o(" \u2014 "),K9=a("a"),iio=o("Speech2Text2ForCausalLM"),dio=o(" (Speech2Text2 model)"),cio=l(),Ru=a("li"),WU=a("strong"),fio=o("transfo-xl"),mio=o(" \u2014 "),Y9=a("a"),hio=o("TransfoXLLMHeadModel"),gio=o(" (Transformer-XL model)"),uio=l(),Pu=a("li"),HU=a("strong"),pio=o("trocr"),_io=o(" \u2014 "),Z9=a("a"),vio=o("TrOCRForCausalLM"),bio=o(" (TrOCR model)"),Tio=l(),Su=a("li"),UU=a("strong"),Fio=o("xlm"),Eio=o(" \u2014 "),ex=a("a"),Cio=o("XLMWithLMHeadModel"),Mio=o(" (XLM model)"),yio=l(),$u=a("li"),JU=a("strong"),wio=o("xlm-prophetnet"),Aio=o(" \u2014 "),ox=a("a"),Lio=o("XLMProphetNetForCausalLM"),Bio=o(" (XLMProphetNet model)"),xio=l(),Iu=a("li"),KU=a("strong"),kio=o("xlm-roberta"),Rio=o(" \u2014 "),rx=a("a"),Pio=o("XLMRobertaForCausalLM"),Sio=o(" (XLM-RoBERTa model)"),$io=l(),Du=a("li"),YU=a("strong"),Iio=o("xlnet"),Dio=o(" \u2014 "),tx=a("a"),Nio=o("XLNetLMHeadModel"),jio=o(" (XLNet model)"),Oio=l(),Nu=a("p"),Gio=o("The model is set in evaluation mode by default using "),ZU=a("code"),qio=o("model.eval()"),zio=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),eJ=a("code"),Xio=o("model.train()"),Qio=l(),oJ=a("p"),Vio=o("Examples:"),Wio=l(),f(x3.$$.fragment),k5e=l(),Ti=a("h2"),ju=a("a"),rJ=a("span"),f(k3.$$.fragment),Hio=l(),tJ=a("span"),Uio=o("AutoModelForMaskedLM"),R5e=l(),qo=a("div"),f(R3.$$.fragment),Jio=l(),Fi=a("p"),Kio=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),aJ=a("code"),Yio=o("from_pretrained()"),Zio=o(` class method or the
`),nJ=a("code"),edo=o("from_config()"),odo=o(" class method."),rdo=l(),P3=a("p"),tdo=o("This class cannot be instantiated directly using "),sJ=a("code"),ado=o("__init__()"),ndo=o(" (throws an error)."),sdo=l(),Sr=a("div"),f(S3.$$.fragment),ldo=l(),lJ=a("p"),ido=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ddo=l(),Ei=a("p"),cdo=o(`Note:
Loading a model from its configuration file does `),iJ=a("strong"),fdo=o("not"),mdo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dJ=a("code"),hdo=o("from_pretrained()"),gdo=o(` to load the model
weights.`),udo=l(),cJ=a("p"),pdo=o("Examples:"),_do=l(),f($3.$$.fragment),vdo=l(),Ne=a("div"),f(I3.$$.fragment),bdo=l(),fJ=a("p"),Tdo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Fdo=l(),Ba=a("p"),Edo=o("The model class to instantiate is selected based on the "),mJ=a("code"),Cdo=o("model_type"),Mdo=o(` property of the config object (either
passed as an argument or loaded from `),hJ=a("code"),ydo=o("pretrained_model_name_or_path"),wdo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),gJ=a("code"),Ado=o("pretrained_model_name_or_path"),Ldo=o(":"),Bdo=l(),O=a("ul"),Ou=a("li"),uJ=a("strong"),xdo=o("albert"),kdo=o(" \u2014 "),ax=a("a"),Rdo=o("AlbertForMaskedLM"),Pdo=o(" (ALBERT model)"),Sdo=l(),Gu=a("li"),pJ=a("strong"),$do=o("bart"),Ido=o(" \u2014 "),nx=a("a"),Ddo=o("BartForConditionalGeneration"),Ndo=o(" (BART model)"),jdo=l(),qu=a("li"),_J=a("strong"),Odo=o("bert"),Gdo=o(" \u2014 "),sx=a("a"),qdo=o("BertForMaskedLM"),zdo=o(" (BERT model)"),Xdo=l(),zu=a("li"),vJ=a("strong"),Qdo=o("big_bird"),Vdo=o(" \u2014 "),lx=a("a"),Wdo=o("BigBirdForMaskedLM"),Hdo=o(" (BigBird model)"),Udo=l(),Xu=a("li"),bJ=a("strong"),Jdo=o("camembert"),Kdo=o(" \u2014 "),ix=a("a"),Ydo=o("CamembertForMaskedLM"),Zdo=o(" (CamemBERT model)"),eco=l(),Qu=a("li"),TJ=a("strong"),oco=o("convbert"),rco=o(" \u2014 "),dx=a("a"),tco=o("ConvBertForMaskedLM"),aco=o(" (ConvBERT model)"),nco=l(),Vu=a("li"),FJ=a("strong"),sco=o("deberta"),lco=o(" \u2014 "),cx=a("a"),ico=o("DebertaForMaskedLM"),dco=o(" (DeBERTa model)"),cco=l(),Wu=a("li"),EJ=a("strong"),fco=o("deberta-v2"),mco=o(" \u2014 "),fx=a("a"),hco=o("DebertaV2ForMaskedLM"),gco=o(" (DeBERTa-v2 model)"),uco=l(),Hu=a("li"),CJ=a("strong"),pco=o("distilbert"),_co=o(" \u2014 "),mx=a("a"),vco=o("DistilBertForMaskedLM"),bco=o(" (DistilBERT model)"),Tco=l(),Uu=a("li"),MJ=a("strong"),Fco=o("electra"),Eco=o(" \u2014 "),hx=a("a"),Cco=o("ElectraForMaskedLM"),Mco=o(" (ELECTRA model)"),yco=l(),Ju=a("li"),yJ=a("strong"),wco=o("flaubert"),Aco=o(" \u2014 "),gx=a("a"),Lco=o("FlaubertWithLMHeadModel"),Bco=o(" (FlauBERT model)"),xco=l(),Ku=a("li"),wJ=a("strong"),kco=o("fnet"),Rco=o(" \u2014 "),ux=a("a"),Pco=o("FNetForMaskedLM"),Sco=o(" (FNet model)"),$co=l(),Yu=a("li"),AJ=a("strong"),Ico=o("funnel"),Dco=o(" \u2014 "),px=a("a"),Nco=o("FunnelForMaskedLM"),jco=o(" (Funnel Transformer model)"),Oco=l(),Zu=a("li"),LJ=a("strong"),Gco=o("ibert"),qco=o(" \u2014 "),_x=a("a"),zco=o("IBertForMaskedLM"),Xco=o(" (I-BERT model)"),Qco=l(),ep=a("li"),BJ=a("strong"),Vco=o("layoutlm"),Wco=o(" \u2014 "),vx=a("a"),Hco=o("LayoutLMForMaskedLM"),Uco=o(" (LayoutLM model)"),Jco=l(),op=a("li"),xJ=a("strong"),Kco=o("longformer"),Yco=o(" \u2014 "),bx=a("a"),Zco=o("LongformerForMaskedLM"),efo=o(" (Longformer model)"),ofo=l(),rp=a("li"),kJ=a("strong"),rfo=o("mbart"),tfo=o(" \u2014 "),Tx=a("a"),afo=o("MBartForConditionalGeneration"),nfo=o(" (mBART model)"),sfo=l(),tp=a("li"),RJ=a("strong"),lfo=o("megatron-bert"),ifo=o(" \u2014 "),Fx=a("a"),dfo=o("MegatronBertForMaskedLM"),cfo=o(" (MegatronBert model)"),ffo=l(),ap=a("li"),PJ=a("strong"),mfo=o("mobilebert"),hfo=o(" \u2014 "),Ex=a("a"),gfo=o("MobileBertForMaskedLM"),ufo=o(" (MobileBERT model)"),pfo=l(),np=a("li"),SJ=a("strong"),_fo=o("mpnet"),vfo=o(" \u2014 "),Cx=a("a"),bfo=o("MPNetForMaskedLM"),Tfo=o(" (MPNet model)"),Ffo=l(),sp=a("li"),$J=a("strong"),Efo=o("perceiver"),Cfo=o(" \u2014 "),Mx=a("a"),Mfo=o("PerceiverForMaskedLM"),yfo=o(" (Perceiver model)"),wfo=l(),lp=a("li"),IJ=a("strong"),Afo=o("qdqbert"),Lfo=o(" \u2014 "),yx=a("a"),Bfo=o("QDQBertForMaskedLM"),xfo=o(" (QDQBert model)"),kfo=l(),ip=a("li"),DJ=a("strong"),Rfo=o("reformer"),Pfo=o(" \u2014 "),wx=a("a"),Sfo=o("ReformerForMaskedLM"),$fo=o(" (Reformer model)"),Ifo=l(),dp=a("li"),NJ=a("strong"),Dfo=o("rembert"),Nfo=o(" \u2014 "),Ax=a("a"),jfo=o("RemBertForMaskedLM"),Ofo=o(" (RemBERT model)"),Gfo=l(),cp=a("li"),jJ=a("strong"),qfo=o("roberta"),zfo=o(" \u2014 "),Lx=a("a"),Xfo=o("RobertaForMaskedLM"),Qfo=o(" (RoBERTa model)"),Vfo=l(),fp=a("li"),OJ=a("strong"),Wfo=o("roformer"),Hfo=o(" \u2014 "),Bx=a("a"),Ufo=o("RoFormerForMaskedLM"),Jfo=o(" (RoFormer model)"),Kfo=l(),mp=a("li"),GJ=a("strong"),Yfo=o("squeezebert"),Zfo=o(" \u2014 "),xx=a("a"),emo=o("SqueezeBertForMaskedLM"),omo=o(" (SqueezeBERT model)"),rmo=l(),hp=a("li"),qJ=a("strong"),tmo=o("tapas"),amo=o(" \u2014 "),kx=a("a"),nmo=o("TapasForMaskedLM"),smo=o(" (TAPAS model)"),lmo=l(),gp=a("li"),zJ=a("strong"),imo=o("wav2vec2"),dmo=o(" \u2014 "),XJ=a("code"),cmo=o("Wav2Vec2ForMaskedLM"),fmo=o(" (Wav2Vec2 model)"),mmo=l(),up=a("li"),QJ=a("strong"),hmo=o("xlm"),gmo=o(" \u2014 "),Rx=a("a"),umo=o("XLMWithLMHeadModel"),pmo=o(" (XLM model)"),_mo=l(),pp=a("li"),VJ=a("strong"),vmo=o("xlm-roberta"),bmo=o(" \u2014 "),Px=a("a"),Tmo=o("XLMRobertaForMaskedLM"),Fmo=o(" (XLM-RoBERTa model)"),Emo=l(),_p=a("p"),Cmo=o("The model is set in evaluation mode by default using "),WJ=a("code"),Mmo=o("model.eval()"),ymo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HJ=a("code"),wmo=o("model.train()"),Amo=l(),UJ=a("p"),Lmo=o("Examples:"),Bmo=l(),f(D3.$$.fragment),P5e=l(),Ci=a("h2"),vp=a("a"),JJ=a("span"),f(N3.$$.fragment),xmo=l(),KJ=a("span"),kmo=o("AutoModelForSeq2SeqLM"),S5e=l(),zo=a("div"),f(j3.$$.fragment),Rmo=l(),Mi=a("p"),Pmo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),YJ=a("code"),Smo=o("from_pretrained()"),$mo=o(` class method or the
`),ZJ=a("code"),Imo=o("from_config()"),Dmo=o(" class method."),Nmo=l(),O3=a("p"),jmo=o("This class cannot be instantiated directly using "),eK=a("code"),Omo=o("__init__()"),Gmo=o(" (throws an error)."),qmo=l(),$r=a("div"),f(G3.$$.fragment),zmo=l(),oK=a("p"),Xmo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Qmo=l(),yi=a("p"),Vmo=o(`Note:
Loading a model from its configuration file does `),rK=a("strong"),Wmo=o("not"),Hmo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=a("code"),Umo=o("from_pretrained()"),Jmo=o(` to load the model
weights.`),Kmo=l(),aK=a("p"),Ymo=o("Examples:"),Zmo=l(),f(q3.$$.fragment),eho=l(),je=a("div"),f(z3.$$.fragment),oho=l(),nK=a("p"),rho=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),tho=l(),xa=a("p"),aho=o("The model class to instantiate is selected based on the "),sK=a("code"),nho=o("model_type"),sho=o(` property of the config object (either
passed as an argument or loaded from `),lK=a("code"),lho=o("pretrained_model_name_or_path"),iho=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),iK=a("code"),dho=o("pretrained_model_name_or_path"),cho=o(":"),fho=l(),fe=a("ul"),bp=a("li"),dK=a("strong"),mho=o("bart"),hho=o(" \u2014 "),Sx=a("a"),gho=o("BartForConditionalGeneration"),uho=o(" (BART model)"),pho=l(),Tp=a("li"),cK=a("strong"),_ho=o("bigbird_pegasus"),vho=o(" \u2014 "),$x=a("a"),bho=o("BigBirdPegasusForConditionalGeneration"),Tho=o(" (BigBirdPegasus model)"),Fho=l(),Fp=a("li"),fK=a("strong"),Eho=o("blenderbot"),Cho=o(" \u2014 "),Ix=a("a"),Mho=o("BlenderbotForConditionalGeneration"),yho=o(" (Blenderbot model)"),who=l(),Ep=a("li"),mK=a("strong"),Aho=o("blenderbot-small"),Lho=o(" \u2014 "),Dx=a("a"),Bho=o("BlenderbotSmallForConditionalGeneration"),xho=o(" (BlenderbotSmall model)"),kho=l(),Cp=a("li"),hK=a("strong"),Rho=o("encoder-decoder"),Pho=o(" \u2014 "),Nx=a("a"),Sho=o("EncoderDecoderModel"),$ho=o(" (Encoder decoder model)"),Iho=l(),Mp=a("li"),gK=a("strong"),Dho=o("fsmt"),Nho=o(" \u2014 "),jx=a("a"),jho=o("FSMTForConditionalGeneration"),Oho=o(" (FairSeq Machine-Translation model)"),Gho=l(),yp=a("li"),uK=a("strong"),qho=o("led"),zho=o(" \u2014 "),Ox=a("a"),Xho=o("LEDForConditionalGeneration"),Qho=o(" (LED model)"),Vho=l(),wp=a("li"),pK=a("strong"),Who=o("m2m_100"),Hho=o(" \u2014 "),Gx=a("a"),Uho=o("M2M100ForConditionalGeneration"),Jho=o(" (M2M100 model)"),Kho=l(),Ap=a("li"),_K=a("strong"),Yho=o("marian"),Zho=o(" \u2014 "),qx=a("a"),ego=o("MarianMTModel"),ogo=o(" (Marian model)"),rgo=l(),Lp=a("li"),vK=a("strong"),tgo=o("mbart"),ago=o(" \u2014 "),zx=a("a"),ngo=o("MBartForConditionalGeneration"),sgo=o(" (mBART model)"),lgo=l(),Bp=a("li"),bK=a("strong"),igo=o("mt5"),dgo=o(" \u2014 "),Xx=a("a"),cgo=o("MT5ForConditionalGeneration"),fgo=o(" (mT5 model)"),mgo=l(),xp=a("li"),TK=a("strong"),hgo=o("pegasus"),ggo=o(" \u2014 "),Qx=a("a"),ugo=o("PegasusForConditionalGeneration"),pgo=o(" (Pegasus model)"),_go=l(),kp=a("li"),FK=a("strong"),vgo=o("prophetnet"),bgo=o(" \u2014 "),Vx=a("a"),Tgo=o("ProphetNetForConditionalGeneration"),Fgo=o(" (ProphetNet model)"),Ego=l(),Rp=a("li"),EK=a("strong"),Cgo=o("t5"),Mgo=o(" \u2014 "),Wx=a("a"),ygo=o("T5ForConditionalGeneration"),wgo=o(" (T5 model)"),Ago=l(),Pp=a("li"),CK=a("strong"),Lgo=o("xlm-prophetnet"),Bgo=o(" \u2014 "),Hx=a("a"),xgo=o("XLMProphetNetForConditionalGeneration"),kgo=o(" (XLMProphetNet model)"),Rgo=l(),Sp=a("p"),Pgo=o("The model is set in evaluation mode by default using "),MK=a("code"),Sgo=o("model.eval()"),$go=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yK=a("code"),Igo=o("model.train()"),Dgo=l(),wK=a("p"),Ngo=o("Examples:"),jgo=l(),f(X3.$$.fragment),$5e=l(),wi=a("h2"),$p=a("a"),AK=a("span"),f(Q3.$$.fragment),Ogo=l(),LK=a("span"),Ggo=o("AutoModelForSequenceClassification"),I5e=l(),Xo=a("div"),f(V3.$$.fragment),qgo=l(),Ai=a("p"),zgo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),BK=a("code"),Xgo=o("from_pretrained()"),Qgo=o(` class method or the
`),xK=a("code"),Vgo=o("from_config()"),Wgo=o(" class method."),Hgo=l(),W3=a("p"),Ugo=o("This class cannot be instantiated directly using "),kK=a("code"),Jgo=o("__init__()"),Kgo=o(" (throws an error)."),Ygo=l(),Ir=a("div"),f(H3.$$.fragment),Zgo=l(),RK=a("p"),euo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ouo=l(),Li=a("p"),ruo=o(`Note:
Loading a model from its configuration file does `),PK=a("strong"),tuo=o("not"),auo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),SK=a("code"),nuo=o("from_pretrained()"),suo=o(` to load the model
weights.`),luo=l(),$K=a("p"),iuo=o("Examples:"),duo=l(),f(U3.$$.fragment),cuo=l(),Oe=a("div"),f(J3.$$.fragment),fuo=l(),IK=a("p"),muo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),huo=l(),ka=a("p"),guo=o("The model class to instantiate is selected based on the "),DK=a("code"),uuo=o("model_type"),puo=o(` property of the config object (either
passed as an argument or loaded from `),NK=a("code"),_uo=o("pretrained_model_name_or_path"),vuo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),jK=a("code"),buo=o("pretrained_model_name_or_path"),Tuo=o(":"),Fuo=l(),A=a("ul"),Ip=a("li"),OK=a("strong"),Euo=o("albert"),Cuo=o(" \u2014 "),Ux=a("a"),Muo=o("AlbertForSequenceClassification"),yuo=o(" (ALBERT model)"),wuo=l(),Dp=a("li"),GK=a("strong"),Auo=o("bart"),Luo=o(" \u2014 "),Jx=a("a"),Buo=o("BartForSequenceClassification"),xuo=o(" (BART model)"),kuo=l(),Np=a("li"),qK=a("strong"),Ruo=o("bert"),Puo=o(" \u2014 "),Kx=a("a"),Suo=o("BertForSequenceClassification"),$uo=o(" (BERT model)"),Iuo=l(),jp=a("li"),zK=a("strong"),Duo=o("big_bird"),Nuo=o(" \u2014 "),Yx=a("a"),juo=o("BigBirdForSequenceClassification"),Ouo=o(" (BigBird model)"),Guo=l(),Op=a("li"),XK=a("strong"),quo=o("bigbird_pegasus"),zuo=o(" \u2014 "),Zx=a("a"),Xuo=o("BigBirdPegasusForSequenceClassification"),Quo=o(" (BigBirdPegasus model)"),Vuo=l(),Gp=a("li"),QK=a("strong"),Wuo=o("camembert"),Huo=o(" \u2014 "),ek=a("a"),Uuo=o("CamembertForSequenceClassification"),Juo=o(" (CamemBERT model)"),Kuo=l(),qp=a("li"),VK=a("strong"),Yuo=o("canine"),Zuo=o(" \u2014 "),ok=a("a"),epo=o("CanineForSequenceClassification"),opo=o(" (Canine model)"),rpo=l(),zp=a("li"),WK=a("strong"),tpo=o("convbert"),apo=o(" \u2014 "),rk=a("a"),npo=o("ConvBertForSequenceClassification"),spo=o(" (ConvBERT model)"),lpo=l(),Xp=a("li"),HK=a("strong"),ipo=o("ctrl"),dpo=o(" \u2014 "),tk=a("a"),cpo=o("CTRLForSequenceClassification"),fpo=o(" (CTRL model)"),mpo=l(),Qp=a("li"),UK=a("strong"),hpo=o("deberta"),gpo=o(" \u2014 "),ak=a("a"),upo=o("DebertaForSequenceClassification"),ppo=o(" (DeBERTa model)"),_po=l(),Vp=a("li"),JK=a("strong"),vpo=o("deberta-v2"),bpo=o(" \u2014 "),nk=a("a"),Tpo=o("DebertaV2ForSequenceClassification"),Fpo=o(" (DeBERTa-v2 model)"),Epo=l(),Wp=a("li"),KK=a("strong"),Cpo=o("distilbert"),Mpo=o(" \u2014 "),sk=a("a"),ypo=o("DistilBertForSequenceClassification"),wpo=o(" (DistilBERT model)"),Apo=l(),Hp=a("li"),YK=a("strong"),Lpo=o("electra"),Bpo=o(" \u2014 "),lk=a("a"),xpo=o("ElectraForSequenceClassification"),kpo=o(" (ELECTRA model)"),Rpo=l(),Up=a("li"),ZK=a("strong"),Ppo=o("flaubert"),Spo=o(" \u2014 "),ik=a("a"),$po=o("FlaubertForSequenceClassification"),Ipo=o(" (FlauBERT model)"),Dpo=l(),Jp=a("li"),eY=a("strong"),Npo=o("fnet"),jpo=o(" \u2014 "),dk=a("a"),Opo=o("FNetForSequenceClassification"),Gpo=o(" (FNet model)"),qpo=l(),Kp=a("li"),oY=a("strong"),zpo=o("funnel"),Xpo=o(" \u2014 "),ck=a("a"),Qpo=o("FunnelForSequenceClassification"),Vpo=o(" (Funnel Transformer model)"),Wpo=l(),Yp=a("li"),rY=a("strong"),Hpo=o("gpt2"),Upo=o(" \u2014 "),fk=a("a"),Jpo=o("GPT2ForSequenceClassification"),Kpo=o(" (OpenAI GPT-2 model)"),Ypo=l(),Zp=a("li"),tY=a("strong"),Zpo=o("gpt_neo"),e_o=o(" \u2014 "),mk=a("a"),o_o=o("GPTNeoForSequenceClassification"),r_o=o(" (GPT Neo model)"),t_o=l(),e_=a("li"),aY=a("strong"),a_o=o("gptj"),n_o=o(" \u2014 "),hk=a("a"),s_o=o("GPTJForSequenceClassification"),l_o=o(" (GPT-J model)"),i_o=l(),o_=a("li"),nY=a("strong"),d_o=o("ibert"),c_o=o(" \u2014 "),gk=a("a"),f_o=o("IBertForSequenceClassification"),m_o=o(" (I-BERT model)"),h_o=l(),r_=a("li"),sY=a("strong"),g_o=o("layoutlm"),u_o=o(" \u2014 "),uk=a("a"),p_o=o("LayoutLMForSequenceClassification"),__o=o(" (LayoutLM model)"),v_o=l(),t_=a("li"),lY=a("strong"),b_o=o("layoutlmv2"),T_o=o(" \u2014 "),pk=a("a"),F_o=o("LayoutLMv2ForSequenceClassification"),E_o=o(" (LayoutLMv2 model)"),C_o=l(),a_=a("li"),iY=a("strong"),M_o=o("led"),y_o=o(" \u2014 "),_k=a("a"),w_o=o("LEDForSequenceClassification"),A_o=o(" (LED model)"),L_o=l(),n_=a("li"),dY=a("strong"),B_o=o("longformer"),x_o=o(" \u2014 "),vk=a("a"),k_o=o("LongformerForSequenceClassification"),R_o=o(" (Longformer model)"),P_o=l(),s_=a("li"),cY=a("strong"),S_o=o("mbart"),$_o=o(" \u2014 "),bk=a("a"),I_o=o("MBartForSequenceClassification"),D_o=o(" (mBART model)"),N_o=l(),l_=a("li"),fY=a("strong"),j_o=o("megatron-bert"),O_o=o(" \u2014 "),Tk=a("a"),G_o=o("MegatronBertForSequenceClassification"),q_o=o(" (MegatronBert model)"),z_o=l(),i_=a("li"),mY=a("strong"),X_o=o("mobilebert"),Q_o=o(" \u2014 "),Fk=a("a"),V_o=o("MobileBertForSequenceClassification"),W_o=o(" (MobileBERT model)"),H_o=l(),d_=a("li"),hY=a("strong"),U_o=o("mpnet"),J_o=o(" \u2014 "),Ek=a("a"),K_o=o("MPNetForSequenceClassification"),Y_o=o(" (MPNet model)"),Z_o=l(),c_=a("li"),gY=a("strong"),e1o=o("openai-gpt"),o1o=o(" \u2014 "),Ck=a("a"),r1o=o("OpenAIGPTForSequenceClassification"),t1o=o(" (OpenAI GPT model)"),a1o=l(),f_=a("li"),uY=a("strong"),n1o=o("perceiver"),s1o=o(" \u2014 "),Mk=a("a"),l1o=o("PerceiverForSequenceClassification"),i1o=o(" (Perceiver model)"),d1o=l(),m_=a("li"),pY=a("strong"),c1o=o("qdqbert"),f1o=o(" \u2014 "),yk=a("a"),m1o=o("QDQBertForSequenceClassification"),h1o=o(" (QDQBert model)"),g1o=l(),h_=a("li"),_Y=a("strong"),u1o=o("reformer"),p1o=o(" \u2014 "),wk=a("a"),_1o=o("ReformerForSequenceClassification"),v1o=o(" (Reformer model)"),b1o=l(),g_=a("li"),vY=a("strong"),T1o=o("rembert"),F1o=o(" \u2014 "),Ak=a("a"),E1o=o("RemBertForSequenceClassification"),C1o=o(" (RemBERT model)"),M1o=l(),u_=a("li"),bY=a("strong"),y1o=o("roberta"),w1o=o(" \u2014 "),Lk=a("a"),A1o=o("RobertaForSequenceClassification"),L1o=o(" (RoBERTa model)"),B1o=l(),p_=a("li"),TY=a("strong"),x1o=o("roformer"),k1o=o(" \u2014 "),Bk=a("a"),R1o=o("RoFormerForSequenceClassification"),P1o=o(" (RoFormer model)"),S1o=l(),__=a("li"),FY=a("strong"),$1o=o("squeezebert"),I1o=o(" \u2014 "),xk=a("a"),D1o=o("SqueezeBertForSequenceClassification"),N1o=o(" (SqueezeBERT model)"),j1o=l(),v_=a("li"),EY=a("strong"),O1o=o("tapas"),G1o=o(" \u2014 "),kk=a("a"),q1o=o("TapasForSequenceClassification"),z1o=o(" (TAPAS model)"),X1o=l(),b_=a("li"),CY=a("strong"),Q1o=o("transfo-xl"),V1o=o(" \u2014 "),Rk=a("a"),W1o=o("TransfoXLForSequenceClassification"),H1o=o(" (Transformer-XL model)"),U1o=l(),T_=a("li"),MY=a("strong"),J1o=o("xlm"),K1o=o(" \u2014 "),Pk=a("a"),Y1o=o("XLMForSequenceClassification"),Z1o=o(" (XLM model)"),e4o=l(),F_=a("li"),yY=a("strong"),o4o=o("xlm-roberta"),r4o=o(" \u2014 "),Sk=a("a"),t4o=o("XLMRobertaForSequenceClassification"),a4o=o(" (XLM-RoBERTa model)"),n4o=l(),E_=a("li"),wY=a("strong"),s4o=o("xlnet"),l4o=o(" \u2014 "),$k=a("a"),i4o=o("XLNetForSequenceClassification"),d4o=o(" (XLNet model)"),c4o=l(),C_=a("p"),f4o=o("The model is set in evaluation mode by default using "),AY=a("code"),m4o=o("model.eval()"),h4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),LY=a("code"),g4o=o("model.train()"),u4o=l(),BY=a("p"),p4o=o("Examples:"),_4o=l(),f(K3.$$.fragment),D5e=l(),Bi=a("h2"),M_=a("a"),xY=a("span"),f(Y3.$$.fragment),v4o=l(),kY=a("span"),b4o=o("AutoModelForMultipleChoice"),N5e=l(),Qo=a("div"),f(Z3.$$.fragment),T4o=l(),xi=a("p"),F4o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),RY=a("code"),E4o=o("from_pretrained()"),C4o=o(` class method or the
`),PY=a("code"),M4o=o("from_config()"),y4o=o(" class method."),w4o=l(),eM=a("p"),A4o=o("This class cannot be instantiated directly using "),SY=a("code"),L4o=o("__init__()"),B4o=o(" (throws an error)."),x4o=l(),Dr=a("div"),f(oM.$$.fragment),k4o=l(),$Y=a("p"),R4o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),P4o=l(),ki=a("p"),S4o=o(`Note:
Loading a model from its configuration file does `),IY=a("strong"),$4o=o("not"),I4o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),DY=a("code"),D4o=o("from_pretrained()"),N4o=o(` to load the model
weights.`),j4o=l(),NY=a("p"),O4o=o("Examples:"),G4o=l(),f(rM.$$.fragment),q4o=l(),Ge=a("div"),f(tM.$$.fragment),z4o=l(),jY=a("p"),X4o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Q4o=l(),Ra=a("p"),V4o=o("The model class to instantiate is selected based on the "),OY=a("code"),W4o=o("model_type"),H4o=o(` property of the config object (either
passed as an argument or loaded from `),GY=a("code"),U4o=o("pretrained_model_name_or_path"),J4o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),qY=a("code"),K4o=o("pretrained_model_name_or_path"),Y4o=o(":"),Z4o=l(),H=a("ul"),y_=a("li"),zY=a("strong"),evo=o("albert"),ovo=o(" \u2014 "),Ik=a("a"),rvo=o("AlbertForMultipleChoice"),tvo=o(" (ALBERT model)"),avo=l(),w_=a("li"),XY=a("strong"),nvo=o("bert"),svo=o(" \u2014 "),Dk=a("a"),lvo=o("BertForMultipleChoice"),ivo=o(" (BERT model)"),dvo=l(),A_=a("li"),QY=a("strong"),cvo=o("big_bird"),fvo=o(" \u2014 "),Nk=a("a"),mvo=o("BigBirdForMultipleChoice"),hvo=o(" (BigBird model)"),gvo=l(),L_=a("li"),VY=a("strong"),uvo=o("camembert"),pvo=o(" \u2014 "),jk=a("a"),_vo=o("CamembertForMultipleChoice"),vvo=o(" (CamemBERT model)"),bvo=l(),B_=a("li"),WY=a("strong"),Tvo=o("canine"),Fvo=o(" \u2014 "),Ok=a("a"),Evo=o("CanineForMultipleChoice"),Cvo=o(" (Canine model)"),Mvo=l(),x_=a("li"),HY=a("strong"),yvo=o("convbert"),wvo=o(" \u2014 "),Gk=a("a"),Avo=o("ConvBertForMultipleChoice"),Lvo=o(" (ConvBERT model)"),Bvo=l(),k_=a("li"),UY=a("strong"),xvo=o("distilbert"),kvo=o(" \u2014 "),qk=a("a"),Rvo=o("DistilBertForMultipleChoice"),Pvo=o(" (DistilBERT model)"),Svo=l(),R_=a("li"),JY=a("strong"),$vo=o("electra"),Ivo=o(" \u2014 "),zk=a("a"),Dvo=o("ElectraForMultipleChoice"),Nvo=o(" (ELECTRA model)"),jvo=l(),P_=a("li"),KY=a("strong"),Ovo=o("flaubert"),Gvo=o(" \u2014 "),Xk=a("a"),qvo=o("FlaubertForMultipleChoice"),zvo=o(" (FlauBERT model)"),Xvo=l(),S_=a("li"),YY=a("strong"),Qvo=o("fnet"),Vvo=o(" \u2014 "),Qk=a("a"),Wvo=o("FNetForMultipleChoice"),Hvo=o(" (FNet model)"),Uvo=l(),$_=a("li"),ZY=a("strong"),Jvo=o("funnel"),Kvo=o(" \u2014 "),Vk=a("a"),Yvo=o("FunnelForMultipleChoice"),Zvo=o(" (Funnel Transformer model)"),ebo=l(),I_=a("li"),eZ=a("strong"),obo=o("ibert"),rbo=o(" \u2014 "),Wk=a("a"),tbo=o("IBertForMultipleChoice"),abo=o(" (I-BERT model)"),nbo=l(),D_=a("li"),oZ=a("strong"),sbo=o("longformer"),lbo=o(" \u2014 "),Hk=a("a"),ibo=o("LongformerForMultipleChoice"),dbo=o(" (Longformer model)"),cbo=l(),N_=a("li"),rZ=a("strong"),fbo=o("megatron-bert"),mbo=o(" \u2014 "),Uk=a("a"),hbo=o("MegatronBertForMultipleChoice"),gbo=o(" (MegatronBert model)"),ubo=l(),j_=a("li"),tZ=a("strong"),pbo=o("mobilebert"),_bo=o(" \u2014 "),Jk=a("a"),vbo=o("MobileBertForMultipleChoice"),bbo=o(" (MobileBERT model)"),Tbo=l(),O_=a("li"),aZ=a("strong"),Fbo=o("mpnet"),Ebo=o(" \u2014 "),Kk=a("a"),Cbo=o("MPNetForMultipleChoice"),Mbo=o(" (MPNet model)"),ybo=l(),G_=a("li"),nZ=a("strong"),wbo=o("qdqbert"),Abo=o(" \u2014 "),Yk=a("a"),Lbo=o("QDQBertForMultipleChoice"),Bbo=o(" (QDQBert model)"),xbo=l(),q_=a("li"),sZ=a("strong"),kbo=o("rembert"),Rbo=o(" \u2014 "),Zk=a("a"),Pbo=o("RemBertForMultipleChoice"),Sbo=o(" (RemBERT model)"),$bo=l(),z_=a("li"),lZ=a("strong"),Ibo=o("roberta"),Dbo=o(" \u2014 "),eR=a("a"),Nbo=o("RobertaForMultipleChoice"),jbo=o(" (RoBERTa model)"),Obo=l(),X_=a("li"),iZ=a("strong"),Gbo=o("roformer"),qbo=o(" \u2014 "),oR=a("a"),zbo=o("RoFormerForMultipleChoice"),Xbo=o(" (RoFormer model)"),Qbo=l(),Q_=a("li"),dZ=a("strong"),Vbo=o("squeezebert"),Wbo=o(" \u2014 "),rR=a("a"),Hbo=o("SqueezeBertForMultipleChoice"),Ubo=o(" (SqueezeBERT model)"),Jbo=l(),V_=a("li"),cZ=a("strong"),Kbo=o("xlm"),Ybo=o(" \u2014 "),tR=a("a"),Zbo=o("XLMForMultipleChoice"),e2o=o(" (XLM model)"),o2o=l(),W_=a("li"),fZ=a("strong"),r2o=o("xlm-roberta"),t2o=o(" \u2014 "),aR=a("a"),a2o=o("XLMRobertaForMultipleChoice"),n2o=o(" (XLM-RoBERTa model)"),s2o=l(),H_=a("li"),mZ=a("strong"),l2o=o("xlnet"),i2o=o(" \u2014 "),nR=a("a"),d2o=o("XLNetForMultipleChoice"),c2o=o(" (XLNet model)"),f2o=l(),U_=a("p"),m2o=o("The model is set in evaluation mode by default using "),hZ=a("code"),h2o=o("model.eval()"),g2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gZ=a("code"),u2o=o("model.train()"),p2o=l(),uZ=a("p"),_2o=o("Examples:"),v2o=l(),f(aM.$$.fragment),j5e=l(),Ri=a("h2"),J_=a("a"),pZ=a("span"),f(nM.$$.fragment),b2o=l(),_Z=a("span"),T2o=o("AutoModelForNextSentencePrediction"),O5e=l(),Vo=a("div"),f(sM.$$.fragment),F2o=l(),Pi=a("p"),E2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vZ=a("code"),C2o=o("from_pretrained()"),M2o=o(` class method or the
`),bZ=a("code"),y2o=o("from_config()"),w2o=o(" class method."),A2o=l(),lM=a("p"),L2o=o("This class cannot be instantiated directly using "),TZ=a("code"),B2o=o("__init__()"),x2o=o(" (throws an error)."),k2o=l(),Nr=a("div"),f(iM.$$.fragment),R2o=l(),FZ=a("p"),P2o=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),S2o=l(),Si=a("p"),$2o=o(`Note:
Loading a model from its configuration file does `),EZ=a("strong"),I2o=o("not"),D2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),CZ=a("code"),N2o=o("from_pretrained()"),j2o=o(` to load the model
weights.`),O2o=l(),MZ=a("p"),G2o=o("Examples:"),q2o=l(),f(dM.$$.fragment),z2o=l(),qe=a("div"),f(cM.$$.fragment),X2o=l(),yZ=a("p"),Q2o=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),V2o=l(),Pa=a("p"),W2o=o("The model class to instantiate is selected based on the "),wZ=a("code"),H2o=o("model_type"),U2o=o(` property of the config object (either
passed as an argument or loaded from `),AZ=a("code"),J2o=o("pretrained_model_name_or_path"),K2o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),LZ=a("code"),Y2o=o("pretrained_model_name_or_path"),Z2o=o(":"),eTo=l(),jt=a("ul"),K_=a("li"),BZ=a("strong"),oTo=o("bert"),rTo=o(" \u2014 "),sR=a("a"),tTo=o("BertForNextSentencePrediction"),aTo=o(" (BERT model)"),nTo=l(),Y_=a("li"),xZ=a("strong"),sTo=o("fnet"),lTo=o(" \u2014 "),lR=a("a"),iTo=o("FNetForNextSentencePrediction"),dTo=o(" (FNet model)"),cTo=l(),Z_=a("li"),kZ=a("strong"),fTo=o("megatron-bert"),mTo=o(" \u2014 "),iR=a("a"),hTo=o("MegatronBertForNextSentencePrediction"),gTo=o(" (MegatronBert model)"),uTo=l(),e1=a("li"),RZ=a("strong"),pTo=o("mobilebert"),_To=o(" \u2014 "),dR=a("a"),vTo=o("MobileBertForNextSentencePrediction"),bTo=o(" (MobileBERT model)"),TTo=l(),o1=a("li"),PZ=a("strong"),FTo=o("qdqbert"),ETo=o(" \u2014 "),cR=a("a"),CTo=o("QDQBertForNextSentencePrediction"),MTo=o(" (QDQBert model)"),yTo=l(),r1=a("p"),wTo=o("The model is set in evaluation mode by default using "),SZ=a("code"),ATo=o("model.eval()"),LTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$Z=a("code"),BTo=o("model.train()"),xTo=l(),IZ=a("p"),kTo=o("Examples:"),RTo=l(),f(fM.$$.fragment),G5e=l(),$i=a("h2"),t1=a("a"),DZ=a("span"),f(mM.$$.fragment),PTo=l(),NZ=a("span"),STo=o("AutoModelForTokenClassification"),q5e=l(),Wo=a("div"),f(hM.$$.fragment),$To=l(),Ii=a("p"),ITo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),jZ=a("code"),DTo=o("from_pretrained()"),NTo=o(` class method or the
`),OZ=a("code"),jTo=o("from_config()"),OTo=o(" class method."),GTo=l(),gM=a("p"),qTo=o("This class cannot be instantiated directly using "),GZ=a("code"),zTo=o("__init__()"),XTo=o(" (throws an error)."),QTo=l(),jr=a("div"),f(uM.$$.fragment),VTo=l(),qZ=a("p"),WTo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),HTo=l(),Di=a("p"),UTo=o(`Note:
Loading a model from its configuration file does `),zZ=a("strong"),JTo=o("not"),KTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),XZ=a("code"),YTo=o("from_pretrained()"),ZTo=o(` to load the model
weights.`),eFo=l(),QZ=a("p"),oFo=o("Examples:"),rFo=l(),f(pM.$$.fragment),tFo=l(),ze=a("div"),f(_M.$$.fragment),aFo=l(),VZ=a("p"),nFo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),sFo=l(),Sa=a("p"),lFo=o("The model class to instantiate is selected based on the "),WZ=a("code"),iFo=o("model_type"),dFo=o(` property of the config object (either
passed as an argument or loaded from `),HZ=a("code"),cFo=o("pretrained_model_name_or_path"),fFo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),UZ=a("code"),mFo=o("pretrained_model_name_or_path"),hFo=o(":"),gFo=l(),X=a("ul"),a1=a("li"),JZ=a("strong"),uFo=o("albert"),pFo=o(" \u2014 "),fR=a("a"),_Fo=o("AlbertForTokenClassification"),vFo=o(" (ALBERT model)"),bFo=l(),n1=a("li"),KZ=a("strong"),TFo=o("bert"),FFo=o(" \u2014 "),mR=a("a"),EFo=o("BertForTokenClassification"),CFo=o(" (BERT model)"),MFo=l(),s1=a("li"),YZ=a("strong"),yFo=o("big_bird"),wFo=o(" \u2014 "),hR=a("a"),AFo=o("BigBirdForTokenClassification"),LFo=o(" (BigBird model)"),BFo=l(),l1=a("li"),ZZ=a("strong"),xFo=o("camembert"),kFo=o(" \u2014 "),gR=a("a"),RFo=o("CamembertForTokenClassification"),PFo=o(" (CamemBERT model)"),SFo=l(),i1=a("li"),eee=a("strong"),$Fo=o("canine"),IFo=o(" \u2014 "),uR=a("a"),DFo=o("CanineForTokenClassification"),NFo=o(" (Canine model)"),jFo=l(),d1=a("li"),oee=a("strong"),OFo=o("convbert"),GFo=o(" \u2014 "),pR=a("a"),qFo=o("ConvBertForTokenClassification"),zFo=o(" (ConvBERT model)"),XFo=l(),c1=a("li"),ree=a("strong"),QFo=o("deberta"),VFo=o(" \u2014 "),_R=a("a"),WFo=o("DebertaForTokenClassification"),HFo=o(" (DeBERTa model)"),UFo=l(),f1=a("li"),tee=a("strong"),JFo=o("deberta-v2"),KFo=o(" \u2014 "),vR=a("a"),YFo=o("DebertaV2ForTokenClassification"),ZFo=o(" (DeBERTa-v2 model)"),eEo=l(),m1=a("li"),aee=a("strong"),oEo=o("distilbert"),rEo=o(" \u2014 "),bR=a("a"),tEo=o("DistilBertForTokenClassification"),aEo=o(" (DistilBERT model)"),nEo=l(),h1=a("li"),nee=a("strong"),sEo=o("electra"),lEo=o(" \u2014 "),TR=a("a"),iEo=o("ElectraForTokenClassification"),dEo=o(" (ELECTRA model)"),cEo=l(),g1=a("li"),see=a("strong"),fEo=o("flaubert"),mEo=o(" \u2014 "),FR=a("a"),hEo=o("FlaubertForTokenClassification"),gEo=o(" (FlauBERT model)"),uEo=l(),u1=a("li"),lee=a("strong"),pEo=o("fnet"),_Eo=o(" \u2014 "),ER=a("a"),vEo=o("FNetForTokenClassification"),bEo=o(" (FNet model)"),TEo=l(),p1=a("li"),iee=a("strong"),FEo=o("funnel"),EEo=o(" \u2014 "),CR=a("a"),CEo=o("FunnelForTokenClassification"),MEo=o(" (Funnel Transformer model)"),yEo=l(),_1=a("li"),dee=a("strong"),wEo=o("gpt2"),AEo=o(" \u2014 "),MR=a("a"),LEo=o("GPT2ForTokenClassification"),BEo=o(" (OpenAI GPT-2 model)"),xEo=l(),v1=a("li"),cee=a("strong"),kEo=o("ibert"),REo=o(" \u2014 "),yR=a("a"),PEo=o("IBertForTokenClassification"),SEo=o(" (I-BERT model)"),$Eo=l(),b1=a("li"),fee=a("strong"),IEo=o("layoutlm"),DEo=o(" \u2014 "),wR=a("a"),NEo=o("LayoutLMForTokenClassification"),jEo=o(" (LayoutLM model)"),OEo=l(),T1=a("li"),mee=a("strong"),GEo=o("layoutlmv2"),qEo=o(" \u2014 "),AR=a("a"),zEo=o("LayoutLMv2ForTokenClassification"),XEo=o(" (LayoutLMv2 model)"),QEo=l(),F1=a("li"),hee=a("strong"),VEo=o("longformer"),WEo=o(" \u2014 "),LR=a("a"),HEo=o("LongformerForTokenClassification"),UEo=o(" (Longformer model)"),JEo=l(),E1=a("li"),gee=a("strong"),KEo=o("megatron-bert"),YEo=o(" \u2014 "),BR=a("a"),ZEo=o("MegatronBertForTokenClassification"),eCo=o(" (MegatronBert model)"),oCo=l(),C1=a("li"),uee=a("strong"),rCo=o("mobilebert"),tCo=o(" \u2014 "),xR=a("a"),aCo=o("MobileBertForTokenClassification"),nCo=o(" (MobileBERT model)"),sCo=l(),M1=a("li"),pee=a("strong"),lCo=o("mpnet"),iCo=o(" \u2014 "),kR=a("a"),dCo=o("MPNetForTokenClassification"),cCo=o(" (MPNet model)"),fCo=l(),y1=a("li"),_ee=a("strong"),mCo=o("qdqbert"),hCo=o(" \u2014 "),RR=a("a"),gCo=o("QDQBertForTokenClassification"),uCo=o(" (QDQBert model)"),pCo=l(),w1=a("li"),vee=a("strong"),_Co=o("rembert"),vCo=o(" \u2014 "),PR=a("a"),bCo=o("RemBertForTokenClassification"),TCo=o(" (RemBERT model)"),FCo=l(),A1=a("li"),bee=a("strong"),ECo=o("roberta"),CCo=o(" \u2014 "),SR=a("a"),MCo=o("RobertaForTokenClassification"),yCo=o(" (RoBERTa model)"),wCo=l(),L1=a("li"),Tee=a("strong"),ACo=o("roformer"),LCo=o(" \u2014 "),$R=a("a"),BCo=o("RoFormerForTokenClassification"),xCo=o(" (RoFormer model)"),kCo=l(),B1=a("li"),Fee=a("strong"),RCo=o("squeezebert"),PCo=o(" \u2014 "),IR=a("a"),SCo=o("SqueezeBertForTokenClassification"),$Co=o(" (SqueezeBERT model)"),ICo=l(),x1=a("li"),Eee=a("strong"),DCo=o("xlm"),NCo=o(" \u2014 "),DR=a("a"),jCo=o("XLMForTokenClassification"),OCo=o(" (XLM model)"),GCo=l(),k1=a("li"),Cee=a("strong"),qCo=o("xlm-roberta"),zCo=o(" \u2014 "),NR=a("a"),XCo=o("XLMRobertaForTokenClassification"),QCo=o(" (XLM-RoBERTa model)"),VCo=l(),R1=a("li"),Mee=a("strong"),WCo=o("xlnet"),HCo=o(" \u2014 "),jR=a("a"),UCo=o("XLNetForTokenClassification"),JCo=o(" (XLNet model)"),KCo=l(),P1=a("p"),YCo=o("The model is set in evaluation mode by default using "),yee=a("code"),ZCo=o("model.eval()"),e3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wee=a("code"),o3o=o("model.train()"),r3o=l(),Aee=a("p"),t3o=o("Examples:"),a3o=l(),f(vM.$$.fragment),z5e=l(),Ni=a("h2"),S1=a("a"),Lee=a("span"),f(bM.$$.fragment),n3o=l(),Bee=a("span"),s3o=o("AutoModelForQuestionAnswering"),X5e=l(),Ho=a("div"),f(TM.$$.fragment),l3o=l(),ji=a("p"),i3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),xee=a("code"),d3o=o("from_pretrained()"),c3o=o(` class method or the
`),kee=a("code"),f3o=o("from_config()"),m3o=o(" class method."),h3o=l(),FM=a("p"),g3o=o("This class cannot be instantiated directly using "),Ree=a("code"),u3o=o("__init__()"),p3o=o(" (throws an error)."),_3o=l(),Or=a("div"),f(EM.$$.fragment),v3o=l(),Pee=a("p"),b3o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),T3o=l(),Oi=a("p"),F3o=o(`Note:
Loading a model from its configuration file does `),See=a("strong"),E3o=o("not"),C3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ee=a("code"),M3o=o("from_pretrained()"),y3o=o(` to load the model
weights.`),w3o=l(),Iee=a("p"),A3o=o("Examples:"),L3o=l(),f(CM.$$.fragment),B3o=l(),Xe=a("div"),f(MM.$$.fragment),x3o=l(),Dee=a("p"),k3o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),R3o=l(),$a=a("p"),P3o=o("The model class to instantiate is selected based on the "),Nee=a("code"),S3o=o("model_type"),$3o=o(` property of the config object (either
passed as an argument or loaded from `),jee=a("code"),I3o=o("pretrained_model_name_or_path"),D3o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Oee=a("code"),N3o=o("pretrained_model_name_or_path"),j3o=o(":"),O3o=l(),S=a("ul"),$1=a("li"),Gee=a("strong"),G3o=o("albert"),q3o=o(" \u2014 "),OR=a("a"),z3o=o("AlbertForQuestionAnswering"),X3o=o(" (ALBERT model)"),Q3o=l(),I1=a("li"),qee=a("strong"),V3o=o("bart"),W3o=o(" \u2014 "),GR=a("a"),H3o=o("BartForQuestionAnswering"),U3o=o(" (BART model)"),J3o=l(),D1=a("li"),zee=a("strong"),K3o=o("bert"),Y3o=o(" \u2014 "),qR=a("a"),Z3o=o("BertForQuestionAnswering"),eMo=o(" (BERT model)"),oMo=l(),N1=a("li"),Xee=a("strong"),rMo=o("big_bird"),tMo=o(" \u2014 "),zR=a("a"),aMo=o("BigBirdForQuestionAnswering"),nMo=o(" (BigBird model)"),sMo=l(),j1=a("li"),Qee=a("strong"),lMo=o("bigbird_pegasus"),iMo=o(" \u2014 "),XR=a("a"),dMo=o("BigBirdPegasusForQuestionAnswering"),cMo=o(" (BigBirdPegasus model)"),fMo=l(),O1=a("li"),Vee=a("strong"),mMo=o("camembert"),hMo=o(" \u2014 "),QR=a("a"),gMo=o("CamembertForQuestionAnswering"),uMo=o(" (CamemBERT model)"),pMo=l(),G1=a("li"),Wee=a("strong"),_Mo=o("canine"),vMo=o(" \u2014 "),VR=a("a"),bMo=o("CanineForQuestionAnswering"),TMo=o(" (Canine model)"),FMo=l(),q1=a("li"),Hee=a("strong"),EMo=o("convbert"),CMo=o(" \u2014 "),WR=a("a"),MMo=o("ConvBertForQuestionAnswering"),yMo=o(" (ConvBERT model)"),wMo=l(),z1=a("li"),Uee=a("strong"),AMo=o("deberta"),LMo=o(" \u2014 "),HR=a("a"),BMo=o("DebertaForQuestionAnswering"),xMo=o(" (DeBERTa model)"),kMo=l(),X1=a("li"),Jee=a("strong"),RMo=o("deberta-v2"),PMo=o(" \u2014 "),UR=a("a"),SMo=o("DebertaV2ForQuestionAnswering"),$Mo=o(" (DeBERTa-v2 model)"),IMo=l(),Q1=a("li"),Kee=a("strong"),DMo=o("distilbert"),NMo=o(" \u2014 "),JR=a("a"),jMo=o("DistilBertForQuestionAnswering"),OMo=o(" (DistilBERT model)"),GMo=l(),V1=a("li"),Yee=a("strong"),qMo=o("electra"),zMo=o(" \u2014 "),KR=a("a"),XMo=o("ElectraForQuestionAnswering"),QMo=o(" (ELECTRA model)"),VMo=l(),W1=a("li"),Zee=a("strong"),WMo=o("flaubert"),HMo=o(" \u2014 "),YR=a("a"),UMo=o("FlaubertForQuestionAnsweringSimple"),JMo=o(" (FlauBERT model)"),KMo=l(),H1=a("li"),eoe=a("strong"),YMo=o("fnet"),ZMo=o(" \u2014 "),ZR=a("a"),e5o=o("FNetForQuestionAnswering"),o5o=o(" (FNet model)"),r5o=l(),U1=a("li"),ooe=a("strong"),t5o=o("funnel"),a5o=o(" \u2014 "),eP=a("a"),n5o=o("FunnelForQuestionAnswering"),s5o=o(" (Funnel Transformer model)"),l5o=l(),J1=a("li"),roe=a("strong"),i5o=o("gptj"),d5o=o(" \u2014 "),oP=a("a"),c5o=o("GPTJForQuestionAnswering"),f5o=o(" (GPT-J model)"),m5o=l(),K1=a("li"),toe=a("strong"),h5o=o("ibert"),g5o=o(" \u2014 "),rP=a("a"),u5o=o("IBertForQuestionAnswering"),p5o=o(" (I-BERT model)"),_5o=l(),Y1=a("li"),aoe=a("strong"),v5o=o("layoutlmv2"),b5o=o(" \u2014 "),tP=a("a"),T5o=o("LayoutLMv2ForQuestionAnswering"),F5o=o(" (LayoutLMv2 model)"),E5o=l(),Z1=a("li"),noe=a("strong"),C5o=o("led"),M5o=o(" \u2014 "),aP=a("a"),y5o=o("LEDForQuestionAnswering"),w5o=o(" (LED model)"),A5o=l(),e4=a("li"),soe=a("strong"),L5o=o("longformer"),B5o=o(" \u2014 "),nP=a("a"),x5o=o("LongformerForQuestionAnswering"),k5o=o(" (Longformer model)"),R5o=l(),o4=a("li"),loe=a("strong"),P5o=o("lxmert"),S5o=o(" \u2014 "),sP=a("a"),$5o=o("LxmertForQuestionAnswering"),I5o=o(" (LXMERT model)"),D5o=l(),r4=a("li"),ioe=a("strong"),N5o=o("mbart"),j5o=o(" \u2014 "),lP=a("a"),O5o=o("MBartForQuestionAnswering"),G5o=o(" (mBART model)"),q5o=l(),t4=a("li"),doe=a("strong"),z5o=o("megatron-bert"),X5o=o(" \u2014 "),iP=a("a"),Q5o=o("MegatronBertForQuestionAnswering"),V5o=o(" (MegatronBert model)"),W5o=l(),a4=a("li"),coe=a("strong"),H5o=o("mobilebert"),U5o=o(" \u2014 "),dP=a("a"),J5o=o("MobileBertForQuestionAnswering"),K5o=o(" (MobileBERT model)"),Y5o=l(),n4=a("li"),foe=a("strong"),Z5o=o("mpnet"),eyo=o(" \u2014 "),cP=a("a"),oyo=o("MPNetForQuestionAnswering"),ryo=o(" (MPNet model)"),tyo=l(),s4=a("li"),moe=a("strong"),ayo=o("qdqbert"),nyo=o(" \u2014 "),fP=a("a"),syo=o("QDQBertForQuestionAnswering"),lyo=o(" (QDQBert model)"),iyo=l(),l4=a("li"),hoe=a("strong"),dyo=o("reformer"),cyo=o(" \u2014 "),mP=a("a"),fyo=o("ReformerForQuestionAnswering"),myo=o(" (Reformer model)"),hyo=l(),i4=a("li"),goe=a("strong"),gyo=o("rembert"),uyo=o(" \u2014 "),hP=a("a"),pyo=o("RemBertForQuestionAnswering"),_yo=o(" (RemBERT model)"),vyo=l(),d4=a("li"),uoe=a("strong"),byo=o("roberta"),Tyo=o(" \u2014 "),gP=a("a"),Fyo=o("RobertaForQuestionAnswering"),Eyo=o(" (RoBERTa model)"),Cyo=l(),c4=a("li"),poe=a("strong"),Myo=o("roformer"),yyo=o(" \u2014 "),uP=a("a"),wyo=o("RoFormerForQuestionAnswering"),Ayo=o(" (RoFormer model)"),Lyo=l(),f4=a("li"),_oe=a("strong"),Byo=o("splinter"),xyo=o(" \u2014 "),pP=a("a"),kyo=o("SplinterForQuestionAnswering"),Ryo=o(" (Splinter model)"),Pyo=l(),m4=a("li"),voe=a("strong"),Syo=o("squeezebert"),$yo=o(" \u2014 "),_P=a("a"),Iyo=o("SqueezeBertForQuestionAnswering"),Dyo=o(" (SqueezeBERT model)"),Nyo=l(),h4=a("li"),boe=a("strong"),jyo=o("xlm"),Oyo=o(" \u2014 "),vP=a("a"),Gyo=o("XLMForQuestionAnsweringSimple"),qyo=o(" (XLM model)"),zyo=l(),g4=a("li"),Toe=a("strong"),Xyo=o("xlm-roberta"),Qyo=o(" \u2014 "),bP=a("a"),Vyo=o("XLMRobertaForQuestionAnswering"),Wyo=o(" (XLM-RoBERTa model)"),Hyo=l(),u4=a("li"),Foe=a("strong"),Uyo=o("xlnet"),Jyo=o(" \u2014 "),TP=a("a"),Kyo=o("XLNetForQuestionAnsweringSimple"),Yyo=o(" (XLNet model)"),Zyo=l(),p4=a("p"),ewo=o("The model is set in evaluation mode by default using "),Eoe=a("code"),owo=o("model.eval()"),rwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Coe=a("code"),two=o("model.train()"),awo=l(),Moe=a("p"),nwo=o("Examples:"),swo=l(),f(yM.$$.fragment),Q5e=l(),Gi=a("h2"),_4=a("a"),yoe=a("span"),f(wM.$$.fragment),lwo=l(),woe=a("span"),iwo=o("AutoModelForTableQuestionAnswering"),V5e=l(),Uo=a("div"),f(AM.$$.fragment),dwo=l(),qi=a("p"),cwo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Aoe=a("code"),fwo=o("from_pretrained()"),mwo=o(` class method or the
`),Loe=a("code"),hwo=o("from_config()"),gwo=o(" class method."),uwo=l(),LM=a("p"),pwo=o("This class cannot be instantiated directly using "),Boe=a("code"),_wo=o("__init__()"),vwo=o(" (throws an error)."),bwo=l(),Gr=a("div"),f(BM.$$.fragment),Two=l(),xoe=a("p"),Fwo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Ewo=l(),zi=a("p"),Cwo=o(`Note:
Loading a model from its configuration file does `),koe=a("strong"),Mwo=o("not"),ywo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Roe=a("code"),wwo=o("from_pretrained()"),Awo=o(` to load the model
weights.`),Lwo=l(),Poe=a("p"),Bwo=o("Examples:"),xwo=l(),f(xM.$$.fragment),kwo=l(),Qe=a("div"),f(kM.$$.fragment),Rwo=l(),Soe=a("p"),Pwo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Swo=l(),Ia=a("p"),$wo=o("The model class to instantiate is selected based on the "),$oe=a("code"),Iwo=o("model_type"),Dwo=o(` property of the config object (either
passed as an argument or loaded from `),Ioe=a("code"),Nwo=o("pretrained_model_name_or_path"),jwo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Doe=a("code"),Owo=o("pretrained_model_name_or_path"),Gwo=o(":"),qwo=l(),Noe=a("ul"),v4=a("li"),joe=a("strong"),zwo=o("tapas"),Xwo=o(" \u2014 "),FP=a("a"),Qwo=o("TapasForQuestionAnswering"),Vwo=o(" (TAPAS model)"),Wwo=l(),b4=a("p"),Hwo=o("The model is set in evaluation mode by default using "),Ooe=a("code"),Uwo=o("model.eval()"),Jwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Goe=a("code"),Kwo=o("model.train()"),Ywo=l(),qoe=a("p"),Zwo=o("Examples:"),e7o=l(),f(RM.$$.fragment),W5e=l(),Xi=a("h2"),T4=a("a"),zoe=a("span"),f(PM.$$.fragment),o7o=l(),Xoe=a("span"),r7o=o("AutoModelForImageClassification"),H5e=l(),Jo=a("div"),f(SM.$$.fragment),t7o=l(),Qi=a("p"),a7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qoe=a("code"),n7o=o("from_pretrained()"),s7o=o(` class method or the
`),Voe=a("code"),l7o=o("from_config()"),i7o=o(" class method."),d7o=l(),$M=a("p"),c7o=o("This class cannot be instantiated directly using "),Woe=a("code"),f7o=o("__init__()"),m7o=o(" (throws an error)."),h7o=l(),qr=a("div"),f(IM.$$.fragment),g7o=l(),Hoe=a("p"),u7o=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),p7o=l(),Vi=a("p"),_7o=o(`Note:
Loading a model from its configuration file does `),Uoe=a("strong"),v7o=o("not"),b7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Joe=a("code"),T7o=o("from_pretrained()"),F7o=o(` to load the model
weights.`),E7o=l(),Koe=a("p"),C7o=o("Examples:"),M7o=l(),f(DM.$$.fragment),y7o=l(),Ve=a("div"),f(NM.$$.fragment),w7o=l(),Yoe=a("p"),A7o=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),L7o=l(),Da=a("p"),B7o=o("The model class to instantiate is selected based on the "),Zoe=a("code"),x7o=o("model_type"),k7o=o(` property of the config object (either
passed as an argument or loaded from `),ere=a("code"),R7o=o("pretrained_model_name_or_path"),P7o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ore=a("code"),S7o=o("pretrained_model_name_or_path"),$7o=o(":"),I7o=l(),Ko=a("ul"),F4=a("li"),rre=a("strong"),D7o=o("beit"),N7o=o(" \u2014 "),EP=a("a"),j7o=o("BeitForImageClassification"),O7o=o(" (BEiT model)"),G7o=l(),ds=a("li"),tre=a("strong"),q7o=o("deit"),z7o=o(" \u2014 "),CP=a("a"),X7o=o("DeiTForImageClassification"),Q7o=o(" or "),MP=a("a"),V7o=o("DeiTForImageClassificationWithTeacher"),W7o=o(" (DeiT model)"),H7o=l(),E4=a("li"),are=a("strong"),U7o=o("imagegpt"),J7o=o(" \u2014 "),yP=a("a"),K7o=o("ImageGPTForImageClassification"),Y7o=o(" (ImageGPT model)"),Z7o=l(),Gt=a("li"),nre=a("strong"),e0o=o("perceiver"),o0o=o(" \u2014 "),wP=a("a"),r0o=o("PerceiverForImageClassificationLearned"),t0o=o(" or "),AP=a("a"),a0o=o("PerceiverForImageClassificationFourier"),n0o=o(" or "),LP=a("a"),s0o=o("PerceiverForImageClassificationConvProcessing"),l0o=o(" (Perceiver model)"),i0o=l(),C4=a("li"),sre=a("strong"),d0o=o("segformer"),c0o=o(" \u2014 "),BP=a("a"),f0o=o("SegformerForImageClassification"),m0o=o(" (SegFormer model)"),h0o=l(),M4=a("li"),lre=a("strong"),g0o=o("vit"),u0o=o(" \u2014 "),xP=a("a"),p0o=o("ViTForImageClassification"),_0o=o(" (ViT model)"),v0o=l(),y4=a("p"),b0o=o("The model is set in evaluation mode by default using "),ire=a("code"),T0o=o("model.eval()"),F0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dre=a("code"),E0o=o("model.train()"),C0o=l(),cre=a("p"),M0o=o("Examples:"),y0o=l(),f(jM.$$.fragment),U5e=l(),Wi=a("h2"),w4=a("a"),fre=a("span"),f(OM.$$.fragment),w0o=l(),mre=a("span"),A0o=o("AutoModelForVision2Seq"),J5e=l(),Yo=a("div"),f(GM.$$.fragment),L0o=l(),Hi=a("p"),B0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),hre=a("code"),x0o=o("from_pretrained()"),k0o=o(` class method or the
`),gre=a("code"),R0o=o("from_config()"),P0o=o(" class method."),S0o=l(),qM=a("p"),$0o=o("This class cannot be instantiated directly using "),ure=a("code"),I0o=o("__init__()"),D0o=o(" (throws an error)."),N0o=l(),zr=a("div"),f(zM.$$.fragment),j0o=l(),pre=a("p"),O0o=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),G0o=l(),Ui=a("p"),q0o=o(`Note:
Loading a model from its configuration file does `),_re=a("strong"),z0o=o("not"),X0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vre=a("code"),Q0o=o("from_pretrained()"),V0o=o(` to load the model
weights.`),W0o=l(),bre=a("p"),H0o=o("Examples:"),U0o=l(),f(XM.$$.fragment),J0o=l(),We=a("div"),f(QM.$$.fragment),K0o=l(),Tre=a("p"),Y0o=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Z0o=l(),Na=a("p"),eAo=o("The model class to instantiate is selected based on the "),Fre=a("code"),oAo=o("model_type"),rAo=o(` property of the config object (either
passed as an argument or loaded from `),Ere=a("code"),tAo=o("pretrained_model_name_or_path"),aAo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Cre=a("code"),nAo=o("pretrained_model_name_or_path"),sAo=o(":"),lAo=l(),Mre=a("ul"),A4=a("li"),yre=a("strong"),iAo=o("vision-encoder-decoder"),dAo=o(" \u2014 "),kP=a("a"),cAo=o("VisionEncoderDecoderModel"),fAo=o(" (Vision Encoder decoder model)"),mAo=l(),L4=a("p"),hAo=o("The model is set in evaluation mode by default using "),wre=a("code"),gAo=o("model.eval()"),uAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Are=a("code"),pAo=o("model.train()"),_Ao=l(),Lre=a("p"),vAo=o("Examples:"),bAo=l(),f(VM.$$.fragment),K5e=l(),Ji=a("h2"),B4=a("a"),Bre=a("span"),f(WM.$$.fragment),TAo=l(),xre=a("span"),FAo=o("AutoModelForAudioClassification"),Y5e=l(),Zo=a("div"),f(HM.$$.fragment),EAo=l(),Ki=a("p"),CAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kre=a("code"),MAo=o("from_pretrained()"),yAo=o(` class method or the
`),Rre=a("code"),wAo=o("from_config()"),AAo=o(" class method."),LAo=l(),UM=a("p"),BAo=o("This class cannot be instantiated directly using "),Pre=a("code"),xAo=o("__init__()"),kAo=o(" (throws an error)."),RAo=l(),Xr=a("div"),f(JM.$$.fragment),PAo=l(),Sre=a("p"),SAo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),$Ao=l(),Yi=a("p"),IAo=o(`Note:
Loading a model from its configuration file does `),$re=a("strong"),DAo=o("not"),NAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ire=a("code"),jAo=o("from_pretrained()"),OAo=o(` to load the model
weights.`),GAo=l(),Dre=a("p"),qAo=o("Examples:"),zAo=l(),f(KM.$$.fragment),XAo=l(),He=a("div"),f(YM.$$.fragment),QAo=l(),Nre=a("p"),VAo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),WAo=l(),ja=a("p"),HAo=o("The model class to instantiate is selected based on the "),jre=a("code"),UAo=o("model_type"),JAo=o(` property of the config object (either
passed as an argument or loaded from `),Ore=a("code"),KAo=o("pretrained_model_name_or_path"),YAo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gre=a("code"),ZAo=o("pretrained_model_name_or_path"),e6o=o(":"),o6o=l(),er=a("ul"),x4=a("li"),qre=a("strong"),r6o=o("hubert"),t6o=o(" \u2014 "),RP=a("a"),a6o=o("HubertForSequenceClassification"),n6o=o(" (Hubert model)"),s6o=l(),k4=a("li"),zre=a("strong"),l6o=o("sew"),i6o=o(" \u2014 "),PP=a("a"),d6o=o("SEWForSequenceClassification"),c6o=o(" (SEW model)"),f6o=l(),R4=a("li"),Xre=a("strong"),m6o=o("sew-d"),h6o=o(" \u2014 "),SP=a("a"),g6o=o("SEWDForSequenceClassification"),u6o=o(" (SEW-D model)"),p6o=l(),P4=a("li"),Qre=a("strong"),_6o=o("unispeech"),v6o=o(" \u2014 "),$P=a("a"),b6o=o("UniSpeechForSequenceClassification"),T6o=o(" (UniSpeech model)"),F6o=l(),S4=a("li"),Vre=a("strong"),E6o=o("unispeech-sat"),C6o=o(" \u2014 "),IP=a("a"),M6o=o("UniSpeechSatForSequenceClassification"),y6o=o(" (UniSpeechSat model)"),w6o=l(),$4=a("li"),Wre=a("strong"),A6o=o("wav2vec2"),L6o=o(" \u2014 "),DP=a("a"),B6o=o("Wav2Vec2ForSequenceClassification"),x6o=o(" (Wav2Vec2 model)"),k6o=l(),I4=a("p"),R6o=o("The model is set in evaluation mode by default using "),Hre=a("code"),P6o=o("model.eval()"),S6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ure=a("code"),$6o=o("model.train()"),I6o=l(),Jre=a("p"),D6o=o("Examples:"),N6o=l(),f(ZM.$$.fragment),Z5e=l(),Zi=a("h2"),D4=a("a"),Kre=a("span"),f(e5.$$.fragment),j6o=l(),Yre=a("span"),O6o=o("AutoModelForCTC"),eye=l(),or=a("div"),f(o5.$$.fragment),G6o=l(),ed=a("p"),q6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Zre=a("code"),z6o=o("from_pretrained()"),X6o=o(` class method or the
`),ete=a("code"),Q6o=o("from_config()"),V6o=o(" class method."),W6o=l(),r5=a("p"),H6o=o("This class cannot be instantiated directly using "),ote=a("code"),U6o=o("__init__()"),J6o=o(" (throws an error)."),K6o=l(),Qr=a("div"),f(t5.$$.fragment),Y6o=l(),rte=a("p"),Z6o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),eLo=l(),od=a("p"),oLo=o(`Note:
Loading a model from its configuration file does `),tte=a("strong"),rLo=o("not"),tLo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ate=a("code"),aLo=o("from_pretrained()"),nLo=o(` to load the model
weights.`),sLo=l(),nte=a("p"),lLo=o("Examples:"),iLo=l(),f(a5.$$.fragment),dLo=l(),Ue=a("div"),f(n5.$$.fragment),cLo=l(),ste=a("p"),fLo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),mLo=l(),Oa=a("p"),hLo=o("The model class to instantiate is selected based on the "),lte=a("code"),gLo=o("model_type"),uLo=o(` property of the config object (either
passed as an argument or loaded from `),ite=a("code"),pLo=o("pretrained_model_name_or_path"),_Lo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),dte=a("code"),vLo=o("pretrained_model_name_or_path"),bLo=o(":"),TLo=l(),rr=a("ul"),N4=a("li"),cte=a("strong"),FLo=o("hubert"),ELo=o(" \u2014 "),NP=a("a"),CLo=o("HubertForCTC"),MLo=o(" (Hubert model)"),yLo=l(),j4=a("li"),fte=a("strong"),wLo=o("sew"),ALo=o(" \u2014 "),jP=a("a"),LLo=o("SEWForCTC"),BLo=o(" (SEW model)"),xLo=l(),O4=a("li"),mte=a("strong"),kLo=o("sew-d"),RLo=o(" \u2014 "),OP=a("a"),PLo=o("SEWDForCTC"),SLo=o(" (SEW-D model)"),$Lo=l(),G4=a("li"),hte=a("strong"),ILo=o("unispeech"),DLo=o(" \u2014 "),GP=a("a"),NLo=o("UniSpeechForCTC"),jLo=o(" (UniSpeech model)"),OLo=l(),q4=a("li"),gte=a("strong"),GLo=o("unispeech-sat"),qLo=o(" \u2014 "),qP=a("a"),zLo=o("UniSpeechSatForCTC"),XLo=o(" (UniSpeechSat model)"),QLo=l(),z4=a("li"),ute=a("strong"),VLo=o("wav2vec2"),WLo=o(" \u2014 "),zP=a("a"),HLo=o("Wav2Vec2ForCTC"),ULo=o(" (Wav2Vec2 model)"),JLo=l(),X4=a("p"),KLo=o("The model is set in evaluation mode by default using "),pte=a("code"),YLo=o("model.eval()"),ZLo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_te=a("code"),e8o=o("model.train()"),o8o=l(),vte=a("p"),r8o=o("Examples:"),t8o=l(),f(s5.$$.fragment),oye=l(),rd=a("h2"),Q4=a("a"),bte=a("span"),f(l5.$$.fragment),a8o=l(),Tte=a("span"),n8o=o("AutoModelForSpeechSeq2Seq"),rye=l(),tr=a("div"),f(i5.$$.fragment),s8o=l(),td=a("p"),l8o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) when created
with the `),Fte=a("code"),i8o=o("from_pretrained()"),d8o=o(` class method or the
`),Ete=a("code"),c8o=o("from_config()"),f8o=o(" class method."),m8o=l(),d5=a("p"),h8o=o("This class cannot be instantiated directly using "),Cte=a("code"),g8o=o("__init__()"),u8o=o(" (throws an error)."),p8o=l(),Vr=a("div"),f(c5.$$.fragment),_8o=l(),Mte=a("p"),v8o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a configuration."),b8o=l(),ad=a("p"),T8o=o(`Note:
Loading a model from its configuration file does `),yte=a("strong"),F8o=o("not"),E8o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wte=a("code"),C8o=o("from_pretrained()"),M8o=o(` to load the model
weights.`),y8o=l(),Ate=a("p"),w8o=o("Examples:"),A8o=l(),f(f5.$$.fragment),L8o=l(),Je=a("div"),f(m5.$$.fragment),B8o=l(),Lte=a("p"),x8o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a pretrained model."),k8o=l(),Ga=a("p"),R8o=o("The model class to instantiate is selected based on the "),Bte=a("code"),P8o=o("model_type"),S8o=o(` property of the config object (either
passed as an argument or loaded from `),xte=a("code"),$8o=o("pretrained_model_name_or_path"),I8o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kte=a("code"),D8o=o("pretrained_model_name_or_path"),N8o=o(":"),j8o=l(),h5=a("ul"),V4=a("li"),Rte=a("strong"),O8o=o("speech-encoder-decoder"),G8o=o(" \u2014 "),XP=a("a"),q8o=o("SpeechEncoderDecoderModel"),z8o=o(" (Speech Encoder decoder model)"),X8o=l(),W4=a("li"),Pte=a("strong"),Q8o=o("speech_to_text"),V8o=o(" \u2014 "),QP=a("a"),W8o=o("Speech2TextForConditionalGeneration"),H8o=o(" (Speech2Text model)"),U8o=l(),H4=a("p"),J8o=o("The model is set in evaluation mode by default using "),Ste=a("code"),K8o=o("model.eval()"),Y8o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$te=a("code"),Z8o=o("model.train()"),eBo=l(),Ite=a("p"),oBo=o("Examples:"),rBo=l(),f(g5.$$.fragment),tye=l(),nd=a("h2"),U4=a("a"),Dte=a("span"),f(u5.$$.fragment),tBo=l(),Nte=a("span"),aBo=o("AutoModelForObjectDetection"),aye=l(),ar=a("div"),f(p5.$$.fragment),nBo=l(),sd=a("p"),sBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),jte=a("code"),lBo=o("from_pretrained()"),iBo=o(` class method or the
`),Ote=a("code"),dBo=o("from_config()"),cBo=o(" class method."),fBo=l(),_5=a("p"),mBo=o("This class cannot be instantiated directly using "),Gte=a("code"),hBo=o("__init__()"),gBo=o(" (throws an error)."),uBo=l(),Wr=a("div"),f(v5.$$.fragment),pBo=l(),qte=a("p"),_Bo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),vBo=l(),ld=a("p"),bBo=o(`Note:
Loading a model from its configuration file does `),zte=a("strong"),TBo=o("not"),FBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xte=a("code"),EBo=o("from_pretrained()"),CBo=o(` to load the model
weights.`),MBo=l(),Qte=a("p"),yBo=o("Examples:"),wBo=l(),f(b5.$$.fragment),ABo=l(),Ke=a("div"),f(T5.$$.fragment),LBo=l(),Vte=a("p"),BBo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),xBo=l(),qa=a("p"),kBo=o("The model class to instantiate is selected based on the "),Wte=a("code"),RBo=o("model_type"),PBo=o(` property of the config object (either
passed as an argument or loaded from `),Hte=a("code"),SBo=o("pretrained_model_name_or_path"),$Bo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ute=a("code"),IBo=o("pretrained_model_name_or_path"),DBo=o(":"),NBo=l(),Jte=a("ul"),J4=a("li"),Kte=a("strong"),jBo=o("detr"),OBo=o(" \u2014 "),VP=a("a"),GBo=o("DetrForObjectDetection"),qBo=o(" (DETR model)"),zBo=l(),K4=a("p"),XBo=o("The model is set in evaluation mode by default using "),Yte=a("code"),QBo=o("model.eval()"),VBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zte=a("code"),WBo=o("model.train()"),HBo=l(),eae=a("p"),UBo=o("Examples:"),JBo=l(),f(F5.$$.fragment),nye=l(),id=a("h2"),Y4=a("a"),oae=a("span"),f(E5.$$.fragment),KBo=l(),rae=a("span"),YBo=o("AutoModelForImageSegmentation"),sye=l(),nr=a("div"),f(C5.$$.fragment),ZBo=l(),dd=a("p"),e9o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),tae=a("code"),o9o=o("from_pretrained()"),r9o=o(` class method or the
`),aae=a("code"),t9o=o("from_config()"),a9o=o(" class method."),n9o=l(),M5=a("p"),s9o=o("This class cannot be instantiated directly using "),nae=a("code"),l9o=o("__init__()"),i9o=o(" (throws an error)."),d9o=l(),Hr=a("div"),f(y5.$$.fragment),c9o=l(),sae=a("p"),f9o=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),m9o=l(),cd=a("p"),h9o=o(`Note:
Loading a model from its configuration file does `),lae=a("strong"),g9o=o("not"),u9o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),iae=a("code"),p9o=o("from_pretrained()"),_9o=o(` to load the model
weights.`),v9o=l(),dae=a("p"),b9o=o("Examples:"),T9o=l(),f(w5.$$.fragment),F9o=l(),Ye=a("div"),f(A5.$$.fragment),E9o=l(),cae=a("p"),C9o=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),M9o=l(),za=a("p"),y9o=o("The model class to instantiate is selected based on the "),fae=a("code"),w9o=o("model_type"),A9o=o(` property of the config object (either
passed as an argument or loaded from `),mae=a("code"),L9o=o("pretrained_model_name_or_path"),B9o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),hae=a("code"),x9o=o("pretrained_model_name_or_path"),k9o=o(":"),R9o=l(),gae=a("ul"),Z4=a("li"),uae=a("strong"),P9o=o("detr"),S9o=o(" \u2014 "),WP=a("a"),$9o=o("DetrForSegmentation"),I9o=o(" (DETR model)"),D9o=l(),ev=a("p"),N9o=o("The model is set in evaluation mode by default using "),pae=a("code"),j9o=o("model.eval()"),O9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_ae=a("code"),G9o=o("model.train()"),q9o=l(),vae=a("p"),z9o=o("Examples:"),X9o=l(),f(L5.$$.fragment),lye=l(),fd=a("h2"),ov=a("a"),bae=a("span"),f(B5.$$.fragment),Q9o=l(),Tae=a("span"),V9o=o("TFAutoModel"),iye=l(),sr=a("div"),f(x5.$$.fragment),W9o=l(),md=a("p"),H9o=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Fae=a("code"),U9o=o("from_pretrained()"),J9o=o(` class method or the
`),Eae=a("code"),K9o=o("from_config()"),Y9o=o(" class method."),Z9o=l(),k5=a("p"),exo=o("This class cannot be instantiated directly using "),Cae=a("code"),oxo=o("__init__()"),rxo=o(" (throws an error)."),txo=l(),Ur=a("div"),f(R5.$$.fragment),axo=l(),Mae=a("p"),nxo=o("Instantiates one of the base model classes of the library from a configuration."),sxo=l(),hd=a("p"),lxo=o(`Note:
Loading a model from its configuration file does `),yae=a("strong"),ixo=o("not"),dxo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wae=a("code"),cxo=o("from_pretrained()"),fxo=o(` to load the model
weights.`),mxo=l(),Aae=a("p"),hxo=o("Examples:"),gxo=l(),f(P5.$$.fragment),uxo=l(),lo=a("div"),f(S5.$$.fragment),pxo=l(),Lae=a("p"),_xo=o("Instantiate one of the base model classes of the library from a pretrained model."),vxo=l(),Xa=a("p"),bxo=o("The model class to instantiate is selected based on the "),Bae=a("code"),Txo=o("model_type"),Fxo=o(` property of the config object (either
passed as an argument or loaded from `),xae=a("code"),Exo=o("pretrained_model_name_or_path"),Cxo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kae=a("code"),Mxo=o("pretrained_model_name_or_path"),yxo=o(":"),wxo=l(),B=a("ul"),rv=a("li"),Rae=a("strong"),Axo=o("albert"),Lxo=o(" \u2014 "),HP=a("a"),Bxo=o("TFAlbertModel"),xxo=o(" (ALBERT model)"),kxo=l(),tv=a("li"),Pae=a("strong"),Rxo=o("bart"),Pxo=o(" \u2014 "),UP=a("a"),Sxo=o("TFBartModel"),$xo=o(" (BART model)"),Ixo=l(),av=a("li"),Sae=a("strong"),Dxo=o("bert"),Nxo=o(" \u2014 "),JP=a("a"),jxo=o("TFBertModel"),Oxo=o(" (BERT model)"),Gxo=l(),nv=a("li"),$ae=a("strong"),qxo=o("blenderbot"),zxo=o(" \u2014 "),KP=a("a"),Xxo=o("TFBlenderbotModel"),Qxo=o(" (Blenderbot model)"),Vxo=l(),sv=a("li"),Iae=a("strong"),Wxo=o("blenderbot-small"),Hxo=o(" \u2014 "),YP=a("a"),Uxo=o("TFBlenderbotSmallModel"),Jxo=o(" (BlenderbotSmall model)"),Kxo=l(),lv=a("li"),Dae=a("strong"),Yxo=o("camembert"),Zxo=o(" \u2014 "),ZP=a("a"),eko=o("TFCamembertModel"),oko=o(" (CamemBERT model)"),rko=l(),iv=a("li"),Nae=a("strong"),tko=o("convbert"),ako=o(" \u2014 "),eS=a("a"),nko=o("TFConvBertModel"),sko=o(" (ConvBERT model)"),lko=l(),dv=a("li"),jae=a("strong"),iko=o("ctrl"),dko=o(" \u2014 "),oS=a("a"),cko=o("TFCTRLModel"),fko=o(" (CTRL model)"),mko=l(),cv=a("li"),Oae=a("strong"),hko=o("deberta"),gko=o(" \u2014 "),rS=a("a"),uko=o("TFDebertaModel"),pko=o(" (DeBERTa model)"),_ko=l(),fv=a("li"),Gae=a("strong"),vko=o("deberta-v2"),bko=o(" \u2014 "),tS=a("a"),Tko=o("TFDebertaV2Model"),Fko=o(" (DeBERTa-v2 model)"),Eko=l(),mv=a("li"),qae=a("strong"),Cko=o("distilbert"),Mko=o(" \u2014 "),aS=a("a"),yko=o("TFDistilBertModel"),wko=o(" (DistilBERT model)"),Ako=l(),hv=a("li"),zae=a("strong"),Lko=o("dpr"),Bko=o(" \u2014 "),nS=a("a"),xko=o("TFDPRQuestionEncoder"),kko=o(" (DPR model)"),Rko=l(),gv=a("li"),Xae=a("strong"),Pko=o("electra"),Sko=o(" \u2014 "),sS=a("a"),$ko=o("TFElectraModel"),Iko=o(" (ELECTRA model)"),Dko=l(),uv=a("li"),Qae=a("strong"),Nko=o("flaubert"),jko=o(" \u2014 "),lS=a("a"),Oko=o("TFFlaubertModel"),Gko=o(" (FlauBERT model)"),qko=l(),cs=a("li"),Vae=a("strong"),zko=o("funnel"),Xko=o(" \u2014 "),iS=a("a"),Qko=o("TFFunnelModel"),Vko=o(" or "),dS=a("a"),Wko=o("TFFunnelBaseModel"),Hko=o(" (Funnel Transformer model)"),Uko=l(),pv=a("li"),Wae=a("strong"),Jko=o("gpt2"),Kko=o(" \u2014 "),cS=a("a"),Yko=o("TFGPT2Model"),Zko=o(" (OpenAI GPT-2 model)"),eRo=l(),_v=a("li"),Hae=a("strong"),oRo=o("hubert"),rRo=o(" \u2014 "),fS=a("a"),tRo=o("TFHubertModel"),aRo=o(" (Hubert model)"),nRo=l(),vv=a("li"),Uae=a("strong"),sRo=o("layoutlm"),lRo=o(" \u2014 "),mS=a("a"),iRo=o("TFLayoutLMModel"),dRo=o(" (LayoutLM model)"),cRo=l(),bv=a("li"),Jae=a("strong"),fRo=o("led"),mRo=o(" \u2014 "),hS=a("a"),hRo=o("TFLEDModel"),gRo=o(" (LED model)"),uRo=l(),Tv=a("li"),Kae=a("strong"),pRo=o("longformer"),_Ro=o(" \u2014 "),gS=a("a"),vRo=o("TFLongformerModel"),bRo=o(" (Longformer model)"),TRo=l(),Fv=a("li"),Yae=a("strong"),FRo=o("lxmert"),ERo=o(" \u2014 "),uS=a("a"),CRo=o("TFLxmertModel"),MRo=o(" (LXMERT model)"),yRo=l(),Ev=a("li"),Zae=a("strong"),wRo=o("marian"),ARo=o(" \u2014 "),pS=a("a"),LRo=o("TFMarianModel"),BRo=o(" (Marian model)"),xRo=l(),Cv=a("li"),ene=a("strong"),kRo=o("mbart"),RRo=o(" \u2014 "),_S=a("a"),PRo=o("TFMBartModel"),SRo=o(" (mBART model)"),$Ro=l(),Mv=a("li"),one=a("strong"),IRo=o("mobilebert"),DRo=o(" \u2014 "),vS=a("a"),NRo=o("TFMobileBertModel"),jRo=o(" (MobileBERT model)"),ORo=l(),yv=a("li"),rne=a("strong"),GRo=o("mpnet"),qRo=o(" \u2014 "),bS=a("a"),zRo=o("TFMPNetModel"),XRo=o(" (MPNet model)"),QRo=l(),wv=a("li"),tne=a("strong"),VRo=o("mt5"),WRo=o(" \u2014 "),TS=a("a"),HRo=o("TFMT5Model"),URo=o(" (mT5 model)"),JRo=l(),Av=a("li"),ane=a("strong"),KRo=o("openai-gpt"),YRo=o(" \u2014 "),FS=a("a"),ZRo=o("TFOpenAIGPTModel"),ePo=o(" (OpenAI GPT model)"),oPo=l(),Lv=a("li"),nne=a("strong"),rPo=o("pegasus"),tPo=o(" \u2014 "),ES=a("a"),aPo=o("TFPegasusModel"),nPo=o(" (Pegasus model)"),sPo=l(),Bv=a("li"),sne=a("strong"),lPo=o("rembert"),iPo=o(" \u2014 "),CS=a("a"),dPo=o("TFRemBertModel"),cPo=o(" (RemBERT model)"),fPo=l(),xv=a("li"),lne=a("strong"),mPo=o("roberta"),hPo=o(" \u2014 "),MS=a("a"),gPo=o("TFRobertaModel"),uPo=o(" (RoBERTa model)"),pPo=l(),kv=a("li"),ine=a("strong"),_Po=o("roformer"),vPo=o(" \u2014 "),yS=a("a"),bPo=o("TFRoFormerModel"),TPo=o(" (RoFormer model)"),FPo=l(),Rv=a("li"),dne=a("strong"),EPo=o("t5"),CPo=o(" \u2014 "),wS=a("a"),MPo=o("TFT5Model"),yPo=o(" (T5 model)"),wPo=l(),Pv=a("li"),cne=a("strong"),APo=o("tapas"),LPo=o(" \u2014 "),AS=a("a"),BPo=o("TFTapasModel"),xPo=o(" (TAPAS model)"),kPo=l(),Sv=a("li"),fne=a("strong"),RPo=o("transfo-xl"),PPo=o(" \u2014 "),LS=a("a"),SPo=o("TFTransfoXLModel"),$Po=o(" (Transformer-XL model)"),IPo=l(),$v=a("li"),mne=a("strong"),DPo=o("vit"),NPo=o(" \u2014 "),BS=a("a"),jPo=o("TFViTModel"),OPo=o(" (ViT model)"),GPo=l(),Iv=a("li"),hne=a("strong"),qPo=o("wav2vec2"),zPo=o(" \u2014 "),xS=a("a"),XPo=o("TFWav2Vec2Model"),QPo=o(" (Wav2Vec2 model)"),VPo=l(),Dv=a("li"),gne=a("strong"),WPo=o("xlm"),HPo=o(" \u2014 "),kS=a("a"),UPo=o("TFXLMModel"),JPo=o(" (XLM model)"),KPo=l(),Nv=a("li"),une=a("strong"),YPo=o("xlm-roberta"),ZPo=o(" \u2014 "),RS=a("a"),eSo=o("TFXLMRobertaModel"),oSo=o(" (XLM-RoBERTa model)"),rSo=l(),jv=a("li"),pne=a("strong"),tSo=o("xlnet"),aSo=o(" \u2014 "),PS=a("a"),nSo=o("TFXLNetModel"),sSo=o(" (XLNet model)"),lSo=l(),_ne=a("p"),iSo=o("Examples:"),dSo=l(),f($5.$$.fragment),dye=l(),gd=a("h2"),Ov=a("a"),vne=a("span"),f(I5.$$.fragment),cSo=l(),bne=a("span"),fSo=o("TFAutoModelForPreTraining"),cye=l(),lr=a("div"),f(D5.$$.fragment),mSo=l(),ud=a("p"),hSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Tne=a("code"),gSo=o("from_pretrained()"),uSo=o(` class method or the
`),Fne=a("code"),pSo=o("from_config()"),_So=o(" class method."),vSo=l(),N5=a("p"),bSo=o("This class cannot be instantiated directly using "),Ene=a("code"),TSo=o("__init__()"),FSo=o(" (throws an error)."),ESo=l(),Jr=a("div"),f(j5.$$.fragment),CSo=l(),Cne=a("p"),MSo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),ySo=l(),pd=a("p"),wSo=o(`Note:
Loading a model from its configuration file does `),Mne=a("strong"),ASo=o("not"),LSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yne=a("code"),BSo=o("from_pretrained()"),xSo=o(` to load the model
weights.`),kSo=l(),wne=a("p"),RSo=o("Examples:"),PSo=l(),f(O5.$$.fragment),SSo=l(),io=a("div"),f(G5.$$.fragment),$So=l(),Ane=a("p"),ISo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),DSo=l(),Qa=a("p"),NSo=o("The model class to instantiate is selected based on the "),Lne=a("code"),jSo=o("model_type"),OSo=o(` property of the config object (either
passed as an argument or loaded from `),Bne=a("code"),GSo=o("pretrained_model_name_or_path"),qSo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),xne=a("code"),zSo=o("pretrained_model_name_or_path"),XSo=o(":"),QSo=l(),K=a("ul"),Gv=a("li"),kne=a("strong"),VSo=o("albert"),WSo=o(" \u2014 "),SS=a("a"),HSo=o("TFAlbertForPreTraining"),USo=o(" (ALBERT model)"),JSo=l(),qv=a("li"),Rne=a("strong"),KSo=o("bart"),YSo=o(" \u2014 "),$S=a("a"),ZSo=o("TFBartForConditionalGeneration"),e$o=o(" (BART model)"),o$o=l(),zv=a("li"),Pne=a("strong"),r$o=o("bert"),t$o=o(" \u2014 "),IS=a("a"),a$o=o("TFBertForPreTraining"),n$o=o(" (BERT model)"),s$o=l(),Xv=a("li"),Sne=a("strong"),l$o=o("camembert"),i$o=o(" \u2014 "),DS=a("a"),d$o=o("TFCamembertForMaskedLM"),c$o=o(" (CamemBERT model)"),f$o=l(),Qv=a("li"),$ne=a("strong"),m$o=o("ctrl"),h$o=o(" \u2014 "),NS=a("a"),g$o=o("TFCTRLLMHeadModel"),u$o=o(" (CTRL model)"),p$o=l(),Vv=a("li"),Ine=a("strong"),_$o=o("distilbert"),v$o=o(" \u2014 "),jS=a("a"),b$o=o("TFDistilBertForMaskedLM"),T$o=o(" (DistilBERT model)"),F$o=l(),Wv=a("li"),Dne=a("strong"),E$o=o("electra"),C$o=o(" \u2014 "),OS=a("a"),M$o=o("TFElectraForPreTraining"),y$o=o(" (ELECTRA model)"),w$o=l(),Hv=a("li"),Nne=a("strong"),A$o=o("flaubert"),L$o=o(" \u2014 "),GS=a("a"),B$o=o("TFFlaubertWithLMHeadModel"),x$o=o(" (FlauBERT model)"),k$o=l(),Uv=a("li"),jne=a("strong"),R$o=o("funnel"),P$o=o(" \u2014 "),qS=a("a"),S$o=o("TFFunnelForPreTraining"),$$o=o(" (Funnel Transformer model)"),I$o=l(),Jv=a("li"),One=a("strong"),D$o=o("gpt2"),N$o=o(" \u2014 "),zS=a("a"),j$o=o("TFGPT2LMHeadModel"),O$o=o(" (OpenAI GPT-2 model)"),G$o=l(),Kv=a("li"),Gne=a("strong"),q$o=o("layoutlm"),z$o=o(" \u2014 "),XS=a("a"),X$o=o("TFLayoutLMForMaskedLM"),Q$o=o(" (LayoutLM model)"),V$o=l(),Yv=a("li"),qne=a("strong"),W$o=o("lxmert"),H$o=o(" \u2014 "),QS=a("a"),U$o=o("TFLxmertForPreTraining"),J$o=o(" (LXMERT model)"),K$o=l(),Zv=a("li"),zne=a("strong"),Y$o=o("mobilebert"),Z$o=o(" \u2014 "),VS=a("a"),eIo=o("TFMobileBertForPreTraining"),oIo=o(" (MobileBERT model)"),rIo=l(),eb=a("li"),Xne=a("strong"),tIo=o("mpnet"),aIo=o(" \u2014 "),WS=a("a"),nIo=o("TFMPNetForMaskedLM"),sIo=o(" (MPNet model)"),lIo=l(),ob=a("li"),Qne=a("strong"),iIo=o("openai-gpt"),dIo=o(" \u2014 "),HS=a("a"),cIo=o("TFOpenAIGPTLMHeadModel"),fIo=o(" (OpenAI GPT model)"),mIo=l(),rb=a("li"),Vne=a("strong"),hIo=o("roberta"),gIo=o(" \u2014 "),US=a("a"),uIo=o("TFRobertaForMaskedLM"),pIo=o(" (RoBERTa model)"),_Io=l(),tb=a("li"),Wne=a("strong"),vIo=o("t5"),bIo=o(" \u2014 "),JS=a("a"),TIo=o("TFT5ForConditionalGeneration"),FIo=o(" (T5 model)"),EIo=l(),ab=a("li"),Hne=a("strong"),CIo=o("tapas"),MIo=o(" \u2014 "),KS=a("a"),yIo=o("TFTapasForMaskedLM"),wIo=o(" (TAPAS model)"),AIo=l(),nb=a("li"),Une=a("strong"),LIo=o("transfo-xl"),BIo=o(" \u2014 "),YS=a("a"),xIo=o("TFTransfoXLLMHeadModel"),kIo=o(" (Transformer-XL model)"),RIo=l(),sb=a("li"),Jne=a("strong"),PIo=o("xlm"),SIo=o(" \u2014 "),ZS=a("a"),$Io=o("TFXLMWithLMHeadModel"),IIo=o(" (XLM model)"),DIo=l(),lb=a("li"),Kne=a("strong"),NIo=o("xlm-roberta"),jIo=o(" \u2014 "),e$=a("a"),OIo=o("TFXLMRobertaForMaskedLM"),GIo=o(" (XLM-RoBERTa model)"),qIo=l(),ib=a("li"),Yne=a("strong"),zIo=o("xlnet"),XIo=o(" \u2014 "),o$=a("a"),QIo=o("TFXLNetLMHeadModel"),VIo=o(" (XLNet model)"),WIo=l(),Zne=a("p"),HIo=o("Examples:"),UIo=l(),f(q5.$$.fragment),fye=l(),_d=a("h2"),db=a("a"),ese=a("span"),f(z5.$$.fragment),JIo=l(),ose=a("span"),KIo=o("TFAutoModelForCausalLM"),mye=l(),ir=a("div"),f(X5.$$.fragment),YIo=l(),vd=a("p"),ZIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),rse=a("code"),eDo=o("from_pretrained()"),oDo=o(` class method or the
`),tse=a("code"),rDo=o("from_config()"),tDo=o(" class method."),aDo=l(),Q5=a("p"),nDo=o("This class cannot be instantiated directly using "),ase=a("code"),sDo=o("__init__()"),lDo=o(" (throws an error)."),iDo=l(),Kr=a("div"),f(V5.$$.fragment),dDo=l(),nse=a("p"),cDo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),fDo=l(),bd=a("p"),mDo=o(`Note:
Loading a model from its configuration file does `),sse=a("strong"),hDo=o("not"),gDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lse=a("code"),uDo=o("from_pretrained()"),pDo=o(` to load the model
weights.`),_Do=l(),ise=a("p"),vDo=o("Examples:"),bDo=l(),f(W5.$$.fragment),TDo=l(),co=a("div"),f(H5.$$.fragment),FDo=l(),dse=a("p"),EDo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),CDo=l(),Va=a("p"),MDo=o("The model class to instantiate is selected based on the "),cse=a("code"),yDo=o("model_type"),wDo=o(` property of the config object (either
passed as an argument or loaded from `),fse=a("code"),ADo=o("pretrained_model_name_or_path"),LDo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),mse=a("code"),BDo=o("pretrained_model_name_or_path"),xDo=o(":"),kDo=l(),ve=a("ul"),cb=a("li"),hse=a("strong"),RDo=o("bert"),PDo=o(" \u2014 "),r$=a("a"),SDo=o("TFBertLMHeadModel"),$Do=o(" (BERT model)"),IDo=l(),fb=a("li"),gse=a("strong"),DDo=o("ctrl"),NDo=o(" \u2014 "),t$=a("a"),jDo=o("TFCTRLLMHeadModel"),ODo=o(" (CTRL model)"),GDo=l(),mb=a("li"),use=a("strong"),qDo=o("gpt2"),zDo=o(" \u2014 "),a$=a("a"),XDo=o("TFGPT2LMHeadModel"),QDo=o(" (OpenAI GPT-2 model)"),VDo=l(),hb=a("li"),pse=a("strong"),WDo=o("openai-gpt"),HDo=o(" \u2014 "),n$=a("a"),UDo=o("TFOpenAIGPTLMHeadModel"),JDo=o(" (OpenAI GPT model)"),KDo=l(),gb=a("li"),_se=a("strong"),YDo=o("rembert"),ZDo=o(" \u2014 "),s$=a("a"),eNo=o("TFRemBertForCausalLM"),oNo=o(" (RemBERT model)"),rNo=l(),ub=a("li"),vse=a("strong"),tNo=o("roberta"),aNo=o(" \u2014 "),l$=a("a"),nNo=o("TFRobertaForCausalLM"),sNo=o(" (RoBERTa model)"),lNo=l(),pb=a("li"),bse=a("strong"),iNo=o("roformer"),dNo=o(" \u2014 "),i$=a("a"),cNo=o("TFRoFormerForCausalLM"),fNo=o(" (RoFormer model)"),mNo=l(),_b=a("li"),Tse=a("strong"),hNo=o("transfo-xl"),gNo=o(" \u2014 "),d$=a("a"),uNo=o("TFTransfoXLLMHeadModel"),pNo=o(" (Transformer-XL model)"),_No=l(),vb=a("li"),Fse=a("strong"),vNo=o("xlm"),bNo=o(" \u2014 "),c$=a("a"),TNo=o("TFXLMWithLMHeadModel"),FNo=o(" (XLM model)"),ENo=l(),bb=a("li"),Ese=a("strong"),CNo=o("xlnet"),MNo=o(" \u2014 "),f$=a("a"),yNo=o("TFXLNetLMHeadModel"),wNo=o(" (XLNet model)"),ANo=l(),Cse=a("p"),LNo=o("Examples:"),BNo=l(),f(U5.$$.fragment),hye=l(),Td=a("h2"),Tb=a("a"),Mse=a("span"),f(J5.$$.fragment),xNo=l(),yse=a("span"),kNo=o("TFAutoModelForImageClassification"),gye=l(),dr=a("div"),f(K5.$$.fragment),RNo=l(),Fd=a("p"),PNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),wse=a("code"),SNo=o("from_pretrained()"),$No=o(` class method or the
`),Ase=a("code"),INo=o("from_config()"),DNo=o(" class method."),NNo=l(),Y5=a("p"),jNo=o("This class cannot be instantiated directly using "),Lse=a("code"),ONo=o("__init__()"),GNo=o(" (throws an error)."),qNo=l(),Yr=a("div"),f(Z5.$$.fragment),zNo=l(),Bse=a("p"),XNo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),QNo=l(),Ed=a("p"),VNo=o(`Note:
Loading a model from its configuration file does `),xse=a("strong"),WNo=o("not"),HNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kse=a("code"),UNo=o("from_pretrained()"),JNo=o(` to load the model
weights.`),KNo=l(),Rse=a("p"),YNo=o("Examples:"),ZNo=l(),f(ey.$$.fragment),ejo=l(),fo=a("div"),f(oy.$$.fragment),ojo=l(),Pse=a("p"),rjo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),tjo=l(),Wa=a("p"),ajo=o("The model class to instantiate is selected based on the "),Sse=a("code"),njo=o("model_type"),sjo=o(` property of the config object (either
passed as an argument or loaded from `),$se=a("code"),ljo=o("pretrained_model_name_or_path"),ijo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ise=a("code"),djo=o("pretrained_model_name_or_path"),cjo=o(":"),fjo=l(),Dse=a("ul"),Fb=a("li"),Nse=a("strong"),mjo=o("vit"),hjo=o(" \u2014 "),m$=a("a"),gjo=o("TFViTForImageClassification"),ujo=o(" (ViT model)"),pjo=l(),jse=a("p"),_jo=o("Examples:"),vjo=l(),f(ry.$$.fragment),uye=l(),Cd=a("h2"),Eb=a("a"),Ose=a("span"),f(ty.$$.fragment),bjo=l(),Gse=a("span"),Tjo=o("TFAutoModelForMaskedLM"),pye=l(),cr=a("div"),f(ay.$$.fragment),Fjo=l(),Md=a("p"),Ejo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),qse=a("code"),Cjo=o("from_pretrained()"),Mjo=o(` class method or the
`),zse=a("code"),yjo=o("from_config()"),wjo=o(" class method."),Ajo=l(),ny=a("p"),Ljo=o("This class cannot be instantiated directly using "),Xse=a("code"),Bjo=o("__init__()"),xjo=o(" (throws an error)."),kjo=l(),Zr=a("div"),f(sy.$$.fragment),Rjo=l(),Qse=a("p"),Pjo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Sjo=l(),yd=a("p"),$jo=o(`Note:
Loading a model from its configuration file does `),Vse=a("strong"),Ijo=o("not"),Djo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=a("code"),Njo=o("from_pretrained()"),jjo=o(` to load the model
weights.`),Ojo=l(),Hse=a("p"),Gjo=o("Examples:"),qjo=l(),f(ly.$$.fragment),zjo=l(),mo=a("div"),f(iy.$$.fragment),Xjo=l(),Use=a("p"),Qjo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Vjo=l(),Ha=a("p"),Wjo=o("The model class to instantiate is selected based on the "),Jse=a("code"),Hjo=o("model_type"),Ujo=o(` property of the config object (either
passed as an argument or loaded from `),Kse=a("code"),Jjo=o("pretrained_model_name_or_path"),Kjo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Yse=a("code"),Yjo=o("pretrained_model_name_or_path"),Zjo=o(":"),eOo=l(),te=a("ul"),Cb=a("li"),Zse=a("strong"),oOo=o("albert"),rOo=o(" \u2014 "),h$=a("a"),tOo=o("TFAlbertForMaskedLM"),aOo=o(" (ALBERT model)"),nOo=l(),Mb=a("li"),ele=a("strong"),sOo=o("bert"),lOo=o(" \u2014 "),g$=a("a"),iOo=o("TFBertForMaskedLM"),dOo=o(" (BERT model)"),cOo=l(),yb=a("li"),ole=a("strong"),fOo=o("camembert"),mOo=o(" \u2014 "),u$=a("a"),hOo=o("TFCamembertForMaskedLM"),gOo=o(" (CamemBERT model)"),uOo=l(),wb=a("li"),rle=a("strong"),pOo=o("convbert"),_Oo=o(" \u2014 "),p$=a("a"),vOo=o("TFConvBertForMaskedLM"),bOo=o(" (ConvBERT model)"),TOo=l(),Ab=a("li"),tle=a("strong"),FOo=o("deberta"),EOo=o(" \u2014 "),_$=a("a"),COo=o("TFDebertaForMaskedLM"),MOo=o(" (DeBERTa model)"),yOo=l(),Lb=a("li"),ale=a("strong"),wOo=o("deberta-v2"),AOo=o(" \u2014 "),v$=a("a"),LOo=o("TFDebertaV2ForMaskedLM"),BOo=o(" (DeBERTa-v2 model)"),xOo=l(),Bb=a("li"),nle=a("strong"),kOo=o("distilbert"),ROo=o(" \u2014 "),b$=a("a"),POo=o("TFDistilBertForMaskedLM"),SOo=o(" (DistilBERT model)"),$Oo=l(),xb=a("li"),sle=a("strong"),IOo=o("electra"),DOo=o(" \u2014 "),T$=a("a"),NOo=o("TFElectraForMaskedLM"),jOo=o(" (ELECTRA model)"),OOo=l(),kb=a("li"),lle=a("strong"),GOo=o("flaubert"),qOo=o(" \u2014 "),F$=a("a"),zOo=o("TFFlaubertWithLMHeadModel"),XOo=o(" (FlauBERT model)"),QOo=l(),Rb=a("li"),ile=a("strong"),VOo=o("funnel"),WOo=o(" \u2014 "),E$=a("a"),HOo=o("TFFunnelForMaskedLM"),UOo=o(" (Funnel Transformer model)"),JOo=l(),Pb=a("li"),dle=a("strong"),KOo=o("layoutlm"),YOo=o(" \u2014 "),C$=a("a"),ZOo=o("TFLayoutLMForMaskedLM"),eGo=o(" (LayoutLM model)"),oGo=l(),Sb=a("li"),cle=a("strong"),rGo=o("longformer"),tGo=o(" \u2014 "),M$=a("a"),aGo=o("TFLongformerForMaskedLM"),nGo=o(" (Longformer model)"),sGo=l(),$b=a("li"),fle=a("strong"),lGo=o("mobilebert"),iGo=o(" \u2014 "),y$=a("a"),dGo=o("TFMobileBertForMaskedLM"),cGo=o(" (MobileBERT model)"),fGo=l(),Ib=a("li"),mle=a("strong"),mGo=o("mpnet"),hGo=o(" \u2014 "),w$=a("a"),gGo=o("TFMPNetForMaskedLM"),uGo=o(" (MPNet model)"),pGo=l(),Db=a("li"),hle=a("strong"),_Go=o("rembert"),vGo=o(" \u2014 "),A$=a("a"),bGo=o("TFRemBertForMaskedLM"),TGo=o(" (RemBERT model)"),FGo=l(),Nb=a("li"),gle=a("strong"),EGo=o("roberta"),CGo=o(" \u2014 "),L$=a("a"),MGo=o("TFRobertaForMaskedLM"),yGo=o(" (RoBERTa model)"),wGo=l(),jb=a("li"),ule=a("strong"),AGo=o("roformer"),LGo=o(" \u2014 "),B$=a("a"),BGo=o("TFRoFormerForMaskedLM"),xGo=o(" (RoFormer model)"),kGo=l(),Ob=a("li"),ple=a("strong"),RGo=o("tapas"),PGo=o(" \u2014 "),x$=a("a"),SGo=o("TFTapasForMaskedLM"),$Go=o(" (TAPAS model)"),IGo=l(),Gb=a("li"),_le=a("strong"),DGo=o("xlm"),NGo=o(" \u2014 "),k$=a("a"),jGo=o("TFXLMWithLMHeadModel"),OGo=o(" (XLM model)"),GGo=l(),qb=a("li"),vle=a("strong"),qGo=o("xlm-roberta"),zGo=o(" \u2014 "),R$=a("a"),XGo=o("TFXLMRobertaForMaskedLM"),QGo=o(" (XLM-RoBERTa model)"),VGo=l(),ble=a("p"),WGo=o("Examples:"),HGo=l(),f(dy.$$.fragment),_ye=l(),wd=a("h2"),zb=a("a"),Tle=a("span"),f(cy.$$.fragment),UGo=l(),Fle=a("span"),JGo=o("TFAutoModelForSeq2SeqLM"),vye=l(),fr=a("div"),f(fy.$$.fragment),KGo=l(),Ad=a("p"),YGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Ele=a("code"),ZGo=o("from_pretrained()"),eqo=o(` class method or the
`),Cle=a("code"),oqo=o("from_config()"),rqo=o(" class method."),tqo=l(),my=a("p"),aqo=o("This class cannot be instantiated directly using "),Mle=a("code"),nqo=o("__init__()"),sqo=o(" (throws an error)."),lqo=l(),et=a("div"),f(hy.$$.fragment),iqo=l(),yle=a("p"),dqo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),cqo=l(),Ld=a("p"),fqo=o(`Note:
Loading a model from its configuration file does `),wle=a("strong"),mqo=o("not"),hqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ale=a("code"),gqo=o("from_pretrained()"),uqo=o(` to load the model
weights.`),pqo=l(),Lle=a("p"),_qo=o("Examples:"),vqo=l(),f(gy.$$.fragment),bqo=l(),ho=a("div"),f(uy.$$.fragment),Tqo=l(),Ble=a("p"),Fqo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Eqo=l(),Ua=a("p"),Cqo=o("The model class to instantiate is selected based on the "),xle=a("code"),Mqo=o("model_type"),yqo=o(` property of the config object (either
passed as an argument or loaded from `),kle=a("code"),wqo=o("pretrained_model_name_or_path"),Aqo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Rle=a("code"),Lqo=o("pretrained_model_name_or_path"),Bqo=o(":"),xqo=l(),be=a("ul"),Xb=a("li"),Ple=a("strong"),kqo=o("bart"),Rqo=o(" \u2014 "),P$=a("a"),Pqo=o("TFBartForConditionalGeneration"),Sqo=o(" (BART model)"),$qo=l(),Qb=a("li"),Sle=a("strong"),Iqo=o("blenderbot"),Dqo=o(" \u2014 "),S$=a("a"),Nqo=o("TFBlenderbotForConditionalGeneration"),jqo=o(" (Blenderbot model)"),Oqo=l(),Vb=a("li"),$le=a("strong"),Gqo=o("blenderbot-small"),qqo=o(" \u2014 "),$$=a("a"),zqo=o("TFBlenderbotSmallForConditionalGeneration"),Xqo=o(" (BlenderbotSmall model)"),Qqo=l(),Wb=a("li"),Ile=a("strong"),Vqo=o("encoder-decoder"),Wqo=o(" \u2014 "),I$=a("a"),Hqo=o("TFEncoderDecoderModel"),Uqo=o(" (Encoder decoder model)"),Jqo=l(),Hb=a("li"),Dle=a("strong"),Kqo=o("led"),Yqo=o(" \u2014 "),D$=a("a"),Zqo=o("TFLEDForConditionalGeneration"),ezo=o(" (LED model)"),ozo=l(),Ub=a("li"),Nle=a("strong"),rzo=o("marian"),tzo=o(" \u2014 "),N$=a("a"),azo=o("TFMarianMTModel"),nzo=o(" (Marian model)"),szo=l(),Jb=a("li"),jle=a("strong"),lzo=o("mbart"),izo=o(" \u2014 "),j$=a("a"),dzo=o("TFMBartForConditionalGeneration"),czo=o(" (mBART model)"),fzo=l(),Kb=a("li"),Ole=a("strong"),mzo=o("mt5"),hzo=o(" \u2014 "),O$=a("a"),gzo=o("TFMT5ForConditionalGeneration"),uzo=o(" (mT5 model)"),pzo=l(),Yb=a("li"),Gle=a("strong"),_zo=o("pegasus"),vzo=o(" \u2014 "),G$=a("a"),bzo=o("TFPegasusForConditionalGeneration"),Tzo=o(" (Pegasus model)"),Fzo=l(),Zb=a("li"),qle=a("strong"),Ezo=o("t5"),Czo=o(" \u2014 "),q$=a("a"),Mzo=o("TFT5ForConditionalGeneration"),yzo=o(" (T5 model)"),wzo=l(),zle=a("p"),Azo=o("Examples:"),Lzo=l(),f(py.$$.fragment),bye=l(),Bd=a("h2"),e2=a("a"),Xle=a("span"),f(_y.$$.fragment),Bzo=l(),Qle=a("span"),xzo=o("TFAutoModelForSequenceClassification"),Tye=l(),mr=a("div"),f(vy.$$.fragment),kzo=l(),xd=a("p"),Rzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Vle=a("code"),Pzo=o("from_pretrained()"),Szo=o(` class method or the
`),Wle=a("code"),$zo=o("from_config()"),Izo=o(" class method."),Dzo=l(),by=a("p"),Nzo=o("This class cannot be instantiated directly using "),Hle=a("code"),jzo=o("__init__()"),Ozo=o(" (throws an error)."),Gzo=l(),ot=a("div"),f(Ty.$$.fragment),qzo=l(),Ule=a("p"),zzo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xzo=l(),kd=a("p"),Qzo=o(`Note:
Loading a model from its configuration file does `),Jle=a("strong"),Vzo=o("not"),Wzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=a("code"),Hzo=o("from_pretrained()"),Uzo=o(` to load the model
weights.`),Jzo=l(),Yle=a("p"),Kzo=o("Examples:"),Yzo=l(),f(Fy.$$.fragment),Zzo=l(),go=a("div"),f(Ey.$$.fragment),eXo=l(),Zle=a("p"),oXo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),rXo=l(),Ja=a("p"),tXo=o("The model class to instantiate is selected based on the "),eie=a("code"),aXo=o("model_type"),nXo=o(` property of the config object (either
passed as an argument or loaded from `),oie=a("code"),sXo=o("pretrained_model_name_or_path"),lXo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),rie=a("code"),iXo=o("pretrained_model_name_or_path"),dXo=o(":"),cXo=l(),W=a("ul"),o2=a("li"),tie=a("strong"),fXo=o("albert"),mXo=o(" \u2014 "),z$=a("a"),hXo=o("TFAlbertForSequenceClassification"),gXo=o(" (ALBERT model)"),uXo=l(),r2=a("li"),aie=a("strong"),pXo=o("bert"),_Xo=o(" \u2014 "),X$=a("a"),vXo=o("TFBertForSequenceClassification"),bXo=o(" (BERT model)"),TXo=l(),t2=a("li"),nie=a("strong"),FXo=o("camembert"),EXo=o(" \u2014 "),Q$=a("a"),CXo=o("TFCamembertForSequenceClassification"),MXo=o(" (CamemBERT model)"),yXo=l(),a2=a("li"),sie=a("strong"),wXo=o("convbert"),AXo=o(" \u2014 "),V$=a("a"),LXo=o("TFConvBertForSequenceClassification"),BXo=o(" (ConvBERT model)"),xXo=l(),n2=a("li"),lie=a("strong"),kXo=o("ctrl"),RXo=o(" \u2014 "),W$=a("a"),PXo=o("TFCTRLForSequenceClassification"),SXo=o(" (CTRL model)"),$Xo=l(),s2=a("li"),iie=a("strong"),IXo=o("deberta"),DXo=o(" \u2014 "),H$=a("a"),NXo=o("TFDebertaForSequenceClassification"),jXo=o(" (DeBERTa model)"),OXo=l(),l2=a("li"),die=a("strong"),GXo=o("deberta-v2"),qXo=o(" \u2014 "),U$=a("a"),zXo=o("TFDebertaV2ForSequenceClassification"),XXo=o(" (DeBERTa-v2 model)"),QXo=l(),i2=a("li"),cie=a("strong"),VXo=o("distilbert"),WXo=o(" \u2014 "),J$=a("a"),HXo=o("TFDistilBertForSequenceClassification"),UXo=o(" (DistilBERT model)"),JXo=l(),d2=a("li"),fie=a("strong"),KXo=o("electra"),YXo=o(" \u2014 "),K$=a("a"),ZXo=o("TFElectraForSequenceClassification"),eQo=o(" (ELECTRA model)"),oQo=l(),c2=a("li"),mie=a("strong"),rQo=o("flaubert"),tQo=o(" \u2014 "),Y$=a("a"),aQo=o("TFFlaubertForSequenceClassification"),nQo=o(" (FlauBERT model)"),sQo=l(),f2=a("li"),hie=a("strong"),lQo=o("funnel"),iQo=o(" \u2014 "),Z$=a("a"),dQo=o("TFFunnelForSequenceClassification"),cQo=o(" (Funnel Transformer model)"),fQo=l(),m2=a("li"),gie=a("strong"),mQo=o("gpt2"),hQo=o(" \u2014 "),eI=a("a"),gQo=o("TFGPT2ForSequenceClassification"),uQo=o(" (OpenAI GPT-2 model)"),pQo=l(),h2=a("li"),uie=a("strong"),_Qo=o("layoutlm"),vQo=o(" \u2014 "),oI=a("a"),bQo=o("TFLayoutLMForSequenceClassification"),TQo=o(" (LayoutLM model)"),FQo=l(),g2=a("li"),pie=a("strong"),EQo=o("longformer"),CQo=o(" \u2014 "),rI=a("a"),MQo=o("TFLongformerForSequenceClassification"),yQo=o(" (Longformer model)"),wQo=l(),u2=a("li"),_ie=a("strong"),AQo=o("mobilebert"),LQo=o(" \u2014 "),tI=a("a"),BQo=o("TFMobileBertForSequenceClassification"),xQo=o(" (MobileBERT model)"),kQo=l(),p2=a("li"),vie=a("strong"),RQo=o("mpnet"),PQo=o(" \u2014 "),aI=a("a"),SQo=o("TFMPNetForSequenceClassification"),$Qo=o(" (MPNet model)"),IQo=l(),_2=a("li"),bie=a("strong"),DQo=o("openai-gpt"),NQo=o(" \u2014 "),nI=a("a"),jQo=o("TFOpenAIGPTForSequenceClassification"),OQo=o(" (OpenAI GPT model)"),GQo=l(),v2=a("li"),Tie=a("strong"),qQo=o("rembert"),zQo=o(" \u2014 "),sI=a("a"),XQo=o("TFRemBertForSequenceClassification"),QQo=o(" (RemBERT model)"),VQo=l(),b2=a("li"),Fie=a("strong"),WQo=o("roberta"),HQo=o(" \u2014 "),lI=a("a"),UQo=o("TFRobertaForSequenceClassification"),JQo=o(" (RoBERTa model)"),KQo=l(),T2=a("li"),Eie=a("strong"),YQo=o("roformer"),ZQo=o(" \u2014 "),iI=a("a"),eVo=o("TFRoFormerForSequenceClassification"),oVo=o(" (RoFormer model)"),rVo=l(),F2=a("li"),Cie=a("strong"),tVo=o("tapas"),aVo=o(" \u2014 "),dI=a("a"),nVo=o("TFTapasForSequenceClassification"),sVo=o(" (TAPAS model)"),lVo=l(),E2=a("li"),Mie=a("strong"),iVo=o("transfo-xl"),dVo=o(" \u2014 "),cI=a("a"),cVo=o("TFTransfoXLForSequenceClassification"),fVo=o(" (Transformer-XL model)"),mVo=l(),C2=a("li"),yie=a("strong"),hVo=o("xlm"),gVo=o(" \u2014 "),fI=a("a"),uVo=o("TFXLMForSequenceClassification"),pVo=o(" (XLM model)"),_Vo=l(),M2=a("li"),wie=a("strong"),vVo=o("xlm-roberta"),bVo=o(" \u2014 "),mI=a("a"),TVo=o("TFXLMRobertaForSequenceClassification"),FVo=o(" (XLM-RoBERTa model)"),EVo=l(),y2=a("li"),Aie=a("strong"),CVo=o("xlnet"),MVo=o(" \u2014 "),hI=a("a"),yVo=o("TFXLNetForSequenceClassification"),wVo=o(" (XLNet model)"),AVo=l(),Lie=a("p"),LVo=o("Examples:"),BVo=l(),f(Cy.$$.fragment),Fye=l(),Rd=a("h2"),w2=a("a"),Bie=a("span"),f(My.$$.fragment),xVo=l(),xie=a("span"),kVo=o("TFAutoModelForMultipleChoice"),Eye=l(),hr=a("div"),f(yy.$$.fragment),RVo=l(),Pd=a("p"),PVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kie=a("code"),SVo=o("from_pretrained()"),$Vo=o(` class method or the
`),Rie=a("code"),IVo=o("from_config()"),DVo=o(" class method."),NVo=l(),wy=a("p"),jVo=o("This class cannot be instantiated directly using "),Pie=a("code"),OVo=o("__init__()"),GVo=o(" (throws an error)."),qVo=l(),rt=a("div"),f(Ay.$$.fragment),zVo=l(),Sie=a("p"),XVo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),QVo=l(),Sd=a("p"),VVo=o(`Note:
Loading a model from its configuration file does `),$ie=a("strong"),WVo=o("not"),HVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Iie=a("code"),UVo=o("from_pretrained()"),JVo=o(` to load the model
weights.`),KVo=l(),Die=a("p"),YVo=o("Examples:"),ZVo=l(),f(Ly.$$.fragment),eWo=l(),uo=a("div"),f(By.$$.fragment),oWo=l(),Nie=a("p"),rWo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tWo=l(),Ka=a("p"),aWo=o("The model class to instantiate is selected based on the "),jie=a("code"),nWo=o("model_type"),sWo=o(` property of the config object (either
passed as an argument or loaded from `),Oie=a("code"),lWo=o("pretrained_model_name_or_path"),iWo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gie=a("code"),dWo=o("pretrained_model_name_or_path"),cWo=o(":"),fWo=l(),de=a("ul"),A2=a("li"),qie=a("strong"),mWo=o("albert"),hWo=o(" \u2014 "),gI=a("a"),gWo=o("TFAlbertForMultipleChoice"),uWo=o(" (ALBERT model)"),pWo=l(),L2=a("li"),zie=a("strong"),_Wo=o("bert"),vWo=o(" \u2014 "),uI=a("a"),bWo=o("TFBertForMultipleChoice"),TWo=o(" (BERT model)"),FWo=l(),B2=a("li"),Xie=a("strong"),EWo=o("camembert"),CWo=o(" \u2014 "),pI=a("a"),MWo=o("TFCamembertForMultipleChoice"),yWo=o(" (CamemBERT model)"),wWo=l(),x2=a("li"),Qie=a("strong"),AWo=o("convbert"),LWo=o(" \u2014 "),_I=a("a"),BWo=o("TFConvBertForMultipleChoice"),xWo=o(" (ConvBERT model)"),kWo=l(),k2=a("li"),Vie=a("strong"),RWo=o("distilbert"),PWo=o(" \u2014 "),vI=a("a"),SWo=o("TFDistilBertForMultipleChoice"),$Wo=o(" (DistilBERT model)"),IWo=l(),R2=a("li"),Wie=a("strong"),DWo=o("electra"),NWo=o(" \u2014 "),bI=a("a"),jWo=o("TFElectraForMultipleChoice"),OWo=o(" (ELECTRA model)"),GWo=l(),P2=a("li"),Hie=a("strong"),qWo=o("flaubert"),zWo=o(" \u2014 "),TI=a("a"),XWo=o("TFFlaubertForMultipleChoice"),QWo=o(" (FlauBERT model)"),VWo=l(),S2=a("li"),Uie=a("strong"),WWo=o("funnel"),HWo=o(" \u2014 "),FI=a("a"),UWo=o("TFFunnelForMultipleChoice"),JWo=o(" (Funnel Transformer model)"),KWo=l(),$2=a("li"),Jie=a("strong"),YWo=o("longformer"),ZWo=o(" \u2014 "),EI=a("a"),eHo=o("TFLongformerForMultipleChoice"),oHo=o(" (Longformer model)"),rHo=l(),I2=a("li"),Kie=a("strong"),tHo=o("mobilebert"),aHo=o(" \u2014 "),CI=a("a"),nHo=o("TFMobileBertForMultipleChoice"),sHo=o(" (MobileBERT model)"),lHo=l(),D2=a("li"),Yie=a("strong"),iHo=o("mpnet"),dHo=o(" \u2014 "),MI=a("a"),cHo=o("TFMPNetForMultipleChoice"),fHo=o(" (MPNet model)"),mHo=l(),N2=a("li"),Zie=a("strong"),hHo=o("rembert"),gHo=o(" \u2014 "),yI=a("a"),uHo=o("TFRemBertForMultipleChoice"),pHo=o(" (RemBERT model)"),_Ho=l(),j2=a("li"),ede=a("strong"),vHo=o("roberta"),bHo=o(" \u2014 "),wI=a("a"),THo=o("TFRobertaForMultipleChoice"),FHo=o(" (RoBERTa model)"),EHo=l(),O2=a("li"),ode=a("strong"),CHo=o("roformer"),MHo=o(" \u2014 "),AI=a("a"),yHo=o("TFRoFormerForMultipleChoice"),wHo=o(" (RoFormer model)"),AHo=l(),G2=a("li"),rde=a("strong"),LHo=o("xlm"),BHo=o(" \u2014 "),LI=a("a"),xHo=o("TFXLMForMultipleChoice"),kHo=o(" (XLM model)"),RHo=l(),q2=a("li"),tde=a("strong"),PHo=o("xlm-roberta"),SHo=o(" \u2014 "),BI=a("a"),$Ho=o("TFXLMRobertaForMultipleChoice"),IHo=o(" (XLM-RoBERTa model)"),DHo=l(),z2=a("li"),ade=a("strong"),NHo=o("xlnet"),jHo=o(" \u2014 "),xI=a("a"),OHo=o("TFXLNetForMultipleChoice"),GHo=o(" (XLNet model)"),qHo=l(),nde=a("p"),zHo=o("Examples:"),XHo=l(),f(xy.$$.fragment),Cye=l(),$d=a("h2"),X2=a("a"),sde=a("span"),f(ky.$$.fragment),QHo=l(),lde=a("span"),VHo=o("TFAutoModelForTableQuestionAnswering"),Mye=l(),gr=a("div"),f(Ry.$$.fragment),WHo=l(),Id=a("p"),HHo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),ide=a("code"),UHo=o("from_pretrained()"),JHo=o(` class method or the
`),dde=a("code"),KHo=o("from_config()"),YHo=o(" class method."),ZHo=l(),Py=a("p"),eUo=o("This class cannot be instantiated directly using "),cde=a("code"),oUo=o("__init__()"),rUo=o(" (throws an error)."),tUo=l(),tt=a("div"),f(Sy.$$.fragment),aUo=l(),fde=a("p"),nUo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),sUo=l(),Dd=a("p"),lUo=o(`Note:
Loading a model from its configuration file does `),mde=a("strong"),iUo=o("not"),dUo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),hde=a("code"),cUo=o("from_pretrained()"),fUo=o(` to load the model
weights.`),mUo=l(),gde=a("p"),hUo=o("Examples:"),gUo=l(),f($y.$$.fragment),uUo=l(),po=a("div"),f(Iy.$$.fragment),pUo=l(),ude=a("p"),_Uo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),vUo=l(),Ya=a("p"),bUo=o("The model class to instantiate is selected based on the "),pde=a("code"),TUo=o("model_type"),FUo=o(` property of the config object (either
passed as an argument or loaded from `),_de=a("code"),EUo=o("pretrained_model_name_or_path"),CUo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),vde=a("code"),MUo=o("pretrained_model_name_or_path"),yUo=o(":"),wUo=l(),bde=a("ul"),Q2=a("li"),Tde=a("strong"),AUo=o("tapas"),LUo=o(" \u2014 "),kI=a("a"),BUo=o("TFTapasForQuestionAnswering"),xUo=o(" (TAPAS model)"),kUo=l(),Fde=a("p"),RUo=o("Examples:"),PUo=l(),f(Dy.$$.fragment),yye=l(),Nd=a("h2"),V2=a("a"),Ede=a("span"),f(Ny.$$.fragment),SUo=l(),Cde=a("span"),$Uo=o("TFAutoModelForTokenClassification"),wye=l(),ur=a("div"),f(jy.$$.fragment),IUo=l(),jd=a("p"),DUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Mde=a("code"),NUo=o("from_pretrained()"),jUo=o(` class method or the
`),yde=a("code"),OUo=o("from_config()"),GUo=o(" class method."),qUo=l(),Oy=a("p"),zUo=o("This class cannot be instantiated directly using "),wde=a("code"),XUo=o("__init__()"),QUo=o(" (throws an error)."),VUo=l(),at=a("div"),f(Gy.$$.fragment),WUo=l(),Ade=a("p"),HUo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),UUo=l(),Od=a("p"),JUo=o(`Note:
Loading a model from its configuration file does `),Lde=a("strong"),KUo=o("not"),YUo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bde=a("code"),ZUo=o("from_pretrained()"),eJo=o(` to load the model
weights.`),oJo=l(),xde=a("p"),rJo=o("Examples:"),tJo=l(),f(qy.$$.fragment),aJo=l(),_o=a("div"),f(zy.$$.fragment),nJo=l(),kde=a("p"),sJo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),lJo=l(),Za=a("p"),iJo=o("The model class to instantiate is selected based on the "),Rde=a("code"),dJo=o("model_type"),cJo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),fJo=o("pretrained_model_name_or_path"),mJo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Sde=a("code"),hJo=o("pretrained_model_name_or_path"),gJo=o(":"),uJo=l(),ae=a("ul"),W2=a("li"),$de=a("strong"),pJo=o("albert"),_Jo=o(" \u2014 "),RI=a("a"),vJo=o("TFAlbertForTokenClassification"),bJo=o(" (ALBERT model)"),TJo=l(),H2=a("li"),Ide=a("strong"),FJo=o("bert"),EJo=o(" \u2014 "),PI=a("a"),CJo=o("TFBertForTokenClassification"),MJo=o(" (BERT model)"),yJo=l(),U2=a("li"),Dde=a("strong"),wJo=o("camembert"),AJo=o(" \u2014 "),SI=a("a"),LJo=o("TFCamembertForTokenClassification"),BJo=o(" (CamemBERT model)"),xJo=l(),J2=a("li"),Nde=a("strong"),kJo=o("convbert"),RJo=o(" \u2014 "),$I=a("a"),PJo=o("TFConvBertForTokenClassification"),SJo=o(" (ConvBERT model)"),$Jo=l(),K2=a("li"),jde=a("strong"),IJo=o("deberta"),DJo=o(" \u2014 "),II=a("a"),NJo=o("TFDebertaForTokenClassification"),jJo=o(" (DeBERTa model)"),OJo=l(),Y2=a("li"),Ode=a("strong"),GJo=o("deberta-v2"),qJo=o(" \u2014 "),DI=a("a"),zJo=o("TFDebertaV2ForTokenClassification"),XJo=o(" (DeBERTa-v2 model)"),QJo=l(),Z2=a("li"),Gde=a("strong"),VJo=o("distilbert"),WJo=o(" \u2014 "),NI=a("a"),HJo=o("TFDistilBertForTokenClassification"),UJo=o(" (DistilBERT model)"),JJo=l(),eT=a("li"),qde=a("strong"),KJo=o("electra"),YJo=o(" \u2014 "),jI=a("a"),ZJo=o("TFElectraForTokenClassification"),eKo=o(" (ELECTRA model)"),oKo=l(),oT=a("li"),zde=a("strong"),rKo=o("flaubert"),tKo=o(" \u2014 "),OI=a("a"),aKo=o("TFFlaubertForTokenClassification"),nKo=o(" (FlauBERT model)"),sKo=l(),rT=a("li"),Xde=a("strong"),lKo=o("funnel"),iKo=o(" \u2014 "),GI=a("a"),dKo=o("TFFunnelForTokenClassification"),cKo=o(" (Funnel Transformer model)"),fKo=l(),tT=a("li"),Qde=a("strong"),mKo=o("layoutlm"),hKo=o(" \u2014 "),qI=a("a"),gKo=o("TFLayoutLMForTokenClassification"),uKo=o(" (LayoutLM model)"),pKo=l(),aT=a("li"),Vde=a("strong"),_Ko=o("longformer"),vKo=o(" \u2014 "),zI=a("a"),bKo=o("TFLongformerForTokenClassification"),TKo=o(" (Longformer model)"),FKo=l(),nT=a("li"),Wde=a("strong"),EKo=o("mobilebert"),CKo=o(" \u2014 "),XI=a("a"),MKo=o("TFMobileBertForTokenClassification"),yKo=o(" (MobileBERT model)"),wKo=l(),sT=a("li"),Hde=a("strong"),AKo=o("mpnet"),LKo=o(" \u2014 "),QI=a("a"),BKo=o("TFMPNetForTokenClassification"),xKo=o(" (MPNet model)"),kKo=l(),lT=a("li"),Ude=a("strong"),RKo=o("rembert"),PKo=o(" \u2014 "),VI=a("a"),SKo=o("TFRemBertForTokenClassification"),$Ko=o(" (RemBERT model)"),IKo=l(),iT=a("li"),Jde=a("strong"),DKo=o("roberta"),NKo=o(" \u2014 "),WI=a("a"),jKo=o("TFRobertaForTokenClassification"),OKo=o(" (RoBERTa model)"),GKo=l(),dT=a("li"),Kde=a("strong"),qKo=o("roformer"),zKo=o(" \u2014 "),HI=a("a"),XKo=o("TFRoFormerForTokenClassification"),QKo=o(" (RoFormer model)"),VKo=l(),cT=a("li"),Yde=a("strong"),WKo=o("xlm"),HKo=o(" \u2014 "),UI=a("a"),UKo=o("TFXLMForTokenClassification"),JKo=o(" (XLM model)"),KKo=l(),fT=a("li"),Zde=a("strong"),YKo=o("xlm-roberta"),ZKo=o(" \u2014 "),JI=a("a"),eYo=o("TFXLMRobertaForTokenClassification"),oYo=o(" (XLM-RoBERTa model)"),rYo=l(),mT=a("li"),ece=a("strong"),tYo=o("xlnet"),aYo=o(" \u2014 "),KI=a("a"),nYo=o("TFXLNetForTokenClassification"),sYo=o(" (XLNet model)"),lYo=l(),oce=a("p"),iYo=o("Examples:"),dYo=l(),f(Xy.$$.fragment),Aye=l(),Gd=a("h2"),hT=a("a"),rce=a("span"),f(Qy.$$.fragment),cYo=l(),tce=a("span"),fYo=o("TFAutoModelForQuestionAnswering"),Lye=l(),pr=a("div"),f(Vy.$$.fragment),mYo=l(),qd=a("p"),hYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),ace=a("code"),gYo=o("from_pretrained()"),uYo=o(` class method or the
`),nce=a("code"),pYo=o("from_config()"),_Yo=o(" class method."),vYo=l(),Wy=a("p"),bYo=o("This class cannot be instantiated directly using "),sce=a("code"),TYo=o("__init__()"),FYo=o(" (throws an error)."),EYo=l(),nt=a("div"),f(Hy.$$.fragment),CYo=l(),lce=a("p"),MYo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),yYo=l(),zd=a("p"),wYo=o(`Note:
Loading a model from its configuration file does `),ice=a("strong"),AYo=o("not"),LYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dce=a("code"),BYo=o("from_pretrained()"),xYo=o(` to load the model
weights.`),kYo=l(),cce=a("p"),RYo=o("Examples:"),PYo=l(),f(Uy.$$.fragment),SYo=l(),vo=a("div"),f(Jy.$$.fragment),$Yo=l(),fce=a("p"),IYo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),DYo=l(),en=a("p"),NYo=o("The model class to instantiate is selected based on the "),mce=a("code"),jYo=o("model_type"),OYo=o(` property of the config object (either
passed as an argument or loaded from `),hce=a("code"),GYo=o("pretrained_model_name_or_path"),qYo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),gce=a("code"),zYo=o("pretrained_model_name_or_path"),XYo=o(":"),QYo=l(),ne=a("ul"),gT=a("li"),uce=a("strong"),VYo=o("albert"),WYo=o(" \u2014 "),YI=a("a"),HYo=o("TFAlbertForQuestionAnswering"),UYo=o(" (ALBERT model)"),JYo=l(),uT=a("li"),pce=a("strong"),KYo=o("bert"),YYo=o(" \u2014 "),ZI=a("a"),ZYo=o("TFBertForQuestionAnswering"),eZo=o(" (BERT model)"),oZo=l(),pT=a("li"),_ce=a("strong"),rZo=o("camembert"),tZo=o(" \u2014 "),eD=a("a"),aZo=o("TFCamembertForQuestionAnswering"),nZo=o(" (CamemBERT model)"),sZo=l(),_T=a("li"),vce=a("strong"),lZo=o("convbert"),iZo=o(" \u2014 "),oD=a("a"),dZo=o("TFConvBertForQuestionAnswering"),cZo=o(" (ConvBERT model)"),fZo=l(),vT=a("li"),bce=a("strong"),mZo=o("deberta"),hZo=o(" \u2014 "),rD=a("a"),gZo=o("TFDebertaForQuestionAnswering"),uZo=o(" (DeBERTa model)"),pZo=l(),bT=a("li"),Tce=a("strong"),_Zo=o("deberta-v2"),vZo=o(" \u2014 "),tD=a("a"),bZo=o("TFDebertaV2ForQuestionAnswering"),TZo=o(" (DeBERTa-v2 model)"),FZo=l(),TT=a("li"),Fce=a("strong"),EZo=o("distilbert"),CZo=o(" \u2014 "),aD=a("a"),MZo=o("TFDistilBertForQuestionAnswering"),yZo=o(" (DistilBERT model)"),wZo=l(),FT=a("li"),Ece=a("strong"),AZo=o("electra"),LZo=o(" \u2014 "),nD=a("a"),BZo=o("TFElectraForQuestionAnswering"),xZo=o(" (ELECTRA model)"),kZo=l(),ET=a("li"),Cce=a("strong"),RZo=o("flaubert"),PZo=o(" \u2014 "),sD=a("a"),SZo=o("TFFlaubertForQuestionAnsweringSimple"),$Zo=o(" (FlauBERT model)"),IZo=l(),CT=a("li"),Mce=a("strong"),DZo=o("funnel"),NZo=o(" \u2014 "),lD=a("a"),jZo=o("TFFunnelForQuestionAnswering"),OZo=o(" (Funnel Transformer model)"),GZo=l(),MT=a("li"),yce=a("strong"),qZo=o("longformer"),zZo=o(" \u2014 "),iD=a("a"),XZo=o("TFLongformerForQuestionAnswering"),QZo=o(" (Longformer model)"),VZo=l(),yT=a("li"),wce=a("strong"),WZo=o("mobilebert"),HZo=o(" \u2014 "),dD=a("a"),UZo=o("TFMobileBertForQuestionAnswering"),JZo=o(" (MobileBERT model)"),KZo=l(),wT=a("li"),Ace=a("strong"),YZo=o("mpnet"),ZZo=o(" \u2014 "),cD=a("a"),eer=o("TFMPNetForQuestionAnswering"),oer=o(" (MPNet model)"),rer=l(),AT=a("li"),Lce=a("strong"),ter=o("rembert"),aer=o(" \u2014 "),fD=a("a"),ner=o("TFRemBertForQuestionAnswering"),ser=o(" (RemBERT model)"),ler=l(),LT=a("li"),Bce=a("strong"),ier=o("roberta"),der=o(" \u2014 "),mD=a("a"),cer=o("TFRobertaForQuestionAnswering"),fer=o(" (RoBERTa model)"),mer=l(),BT=a("li"),xce=a("strong"),her=o("roformer"),ger=o(" \u2014 "),hD=a("a"),uer=o("TFRoFormerForQuestionAnswering"),per=o(" (RoFormer model)"),_er=l(),xT=a("li"),kce=a("strong"),ver=o("xlm"),ber=o(" \u2014 "),gD=a("a"),Ter=o("TFXLMForQuestionAnsweringSimple"),Fer=o(" (XLM model)"),Eer=l(),kT=a("li"),Rce=a("strong"),Cer=o("xlm-roberta"),Mer=o(" \u2014 "),uD=a("a"),yer=o("TFXLMRobertaForQuestionAnswering"),wer=o(" (XLM-RoBERTa model)"),Aer=l(),RT=a("li"),Pce=a("strong"),Ler=o("xlnet"),Ber=o(" \u2014 "),pD=a("a"),xer=o("TFXLNetForQuestionAnsweringSimple"),ker=o(" (XLNet model)"),Rer=l(),Sce=a("p"),Per=o("Examples:"),Ser=l(),f(Ky.$$.fragment),Bye=l(),Xd=a("h2"),PT=a("a"),$ce=a("span"),f(Yy.$$.fragment),$er=l(),Ice=a("span"),Ier=o("FlaxAutoModel"),xye=l(),_r=a("div"),f(Zy.$$.fragment),Der=l(),Qd=a("p"),Ner=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dce=a("code"),jer=o("from_pretrained()"),Oer=o(` class method or the
`),Nce=a("code"),Ger=o("from_config()"),qer=o(" class method."),zer=l(),ew=a("p"),Xer=o("This class cannot be instantiated directly using "),jce=a("code"),Qer=o("__init__()"),Ver=o(" (throws an error)."),Wer=l(),st=a("div"),f(ow.$$.fragment),Her=l(),Oce=a("p"),Uer=o("Instantiates one of the base model classes of the library from a configuration."),Jer=l(),Vd=a("p"),Ker=o(`Note:
Loading a model from its configuration file does `),Gce=a("strong"),Yer=o("not"),Zer=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qce=a("code"),eor=o("from_pretrained()"),oor=o(` to load the model
weights.`),ror=l(),zce=a("p"),tor=o("Examples:"),aor=l(),f(rw.$$.fragment),nor=l(),bo=a("div"),f(tw.$$.fragment),sor=l(),Xce=a("p"),lor=o("Instantiate one of the base model classes of the library from a pretrained model."),ior=l(),on=a("p"),dor=o("The model class to instantiate is selected based on the "),Qce=a("code"),cor=o("model_type"),mor=o(` property of the config object (either
passed as an argument or loaded from `),Vce=a("code"),hor=o("pretrained_model_name_or_path"),gor=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Wce=a("code"),uor=o("pretrained_model_name_or_path"),por=o(":"),_or=l(),Y=a("ul"),ST=a("li"),Hce=a("strong"),vor=o("albert"),bor=o(" \u2014 "),_D=a("a"),Tor=o("FlaxAlbertModel"),For=o(" (ALBERT model)"),Eor=l(),$T=a("li"),Uce=a("strong"),Cor=o("bart"),Mor=o(" \u2014 "),vD=a("a"),yor=o("FlaxBartModel"),wor=o(" (BART model)"),Aor=l(),IT=a("li"),Jce=a("strong"),Lor=o("beit"),Bor=o(" \u2014 "),bD=a("a"),xor=o("FlaxBeitModel"),kor=o(" (BEiT model)"),Ror=l(),DT=a("li"),Kce=a("strong"),Por=o("bert"),Sor=o(" \u2014 "),TD=a("a"),$or=o("FlaxBertModel"),Ior=o(" (BERT model)"),Dor=l(),NT=a("li"),Yce=a("strong"),Nor=o("big_bird"),jor=o(" \u2014 "),FD=a("a"),Oor=o("FlaxBigBirdModel"),Gor=o(" (BigBird model)"),qor=l(),jT=a("li"),Zce=a("strong"),zor=o("blenderbot"),Xor=o(" \u2014 "),ED=a("a"),Qor=o("FlaxBlenderbotModel"),Vor=o(" (Blenderbot model)"),Wor=l(),OT=a("li"),efe=a("strong"),Hor=o("blenderbot-small"),Uor=o(" \u2014 "),CD=a("a"),Jor=o("FlaxBlenderbotSmallModel"),Kor=o(" (BlenderbotSmall model)"),Yor=l(),GT=a("li"),ofe=a("strong"),Zor=o("clip"),err=o(" \u2014 "),MD=a("a"),orr=o("FlaxCLIPModel"),rrr=o(" (CLIP model)"),trr=l(),qT=a("li"),rfe=a("strong"),arr=o("distilbert"),nrr=o(" \u2014 "),yD=a("a"),srr=o("FlaxDistilBertModel"),lrr=o(" (DistilBERT model)"),irr=l(),zT=a("li"),tfe=a("strong"),drr=o("electra"),crr=o(" \u2014 "),wD=a("a"),frr=o("FlaxElectraModel"),mrr=o(" (ELECTRA model)"),hrr=l(),XT=a("li"),afe=a("strong"),grr=o("gpt2"),urr=o(" \u2014 "),AD=a("a"),prr=o("FlaxGPT2Model"),_rr=o(" (OpenAI GPT-2 model)"),vrr=l(),QT=a("li"),nfe=a("strong"),brr=o("gpt_neo"),Trr=o(" \u2014 "),LD=a("a"),Frr=o("FlaxGPTNeoModel"),Err=o(" (GPT Neo model)"),Crr=l(),VT=a("li"),sfe=a("strong"),Mrr=o("gptj"),yrr=o(" \u2014 "),BD=a("a"),wrr=o("FlaxGPTJModel"),Arr=o(" (GPT-J model)"),Lrr=l(),WT=a("li"),lfe=a("strong"),Brr=o("marian"),xrr=o(" \u2014 "),xD=a("a"),krr=o("FlaxMarianModel"),Rrr=o(" (Marian model)"),Prr=l(),HT=a("li"),ife=a("strong"),Srr=o("mbart"),$rr=o(" \u2014 "),kD=a("a"),Irr=o("FlaxMBartModel"),Drr=o(" (mBART model)"),Nrr=l(),UT=a("li"),dfe=a("strong"),jrr=o("mt5"),Orr=o(" \u2014 "),RD=a("a"),Grr=o("FlaxMT5Model"),qrr=o(" (mT5 model)"),zrr=l(),JT=a("li"),cfe=a("strong"),Xrr=o("pegasus"),Qrr=o(" \u2014 "),PD=a("a"),Vrr=o("FlaxPegasusModel"),Wrr=o(" (Pegasus model)"),Hrr=l(),KT=a("li"),ffe=a("strong"),Urr=o("roberta"),Jrr=o(" \u2014 "),SD=a("a"),Krr=o("FlaxRobertaModel"),Yrr=o(" (RoBERTa model)"),Zrr=l(),YT=a("li"),mfe=a("strong"),etr=o("t5"),otr=o(" \u2014 "),$D=a("a"),rtr=o("FlaxT5Model"),ttr=o(" (T5 model)"),atr=l(),ZT=a("li"),hfe=a("strong"),ntr=o("vision-text-dual-encoder"),str=o(" \u2014 "),ID=a("a"),ltr=o("FlaxVisionTextDualEncoderModel"),itr=o(" (VisionTextDualEncoder model)"),dtr=l(),eF=a("li"),gfe=a("strong"),ctr=o("vit"),ftr=o(" \u2014 "),DD=a("a"),mtr=o("FlaxViTModel"),htr=o(" (ViT model)"),gtr=l(),oF=a("li"),ufe=a("strong"),utr=o("wav2vec2"),ptr=o(" \u2014 "),ND=a("a"),_tr=o("FlaxWav2Vec2Model"),vtr=o(" (Wav2Vec2 model)"),btr=l(),pfe=a("p"),Ttr=o("Examples:"),Ftr=l(),f(aw.$$.fragment),kye=l(),Wd=a("h2"),rF=a("a"),_fe=a("span"),f(nw.$$.fragment),Etr=l(),vfe=a("span"),Ctr=o("FlaxAutoModelForCausalLM"),Rye=l(),vr=a("div"),f(sw.$$.fragment),Mtr=l(),Hd=a("p"),ytr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),bfe=a("code"),wtr=o("from_pretrained()"),Atr=o(` class method or the
`),Tfe=a("code"),Ltr=o("from_config()"),Btr=o(" class method."),xtr=l(),lw=a("p"),ktr=o("This class cannot be instantiated directly using "),Ffe=a("code"),Rtr=o("__init__()"),Ptr=o(" (throws an error)."),Str=l(),lt=a("div"),f(iw.$$.fragment),$tr=l(),Efe=a("p"),Itr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Dtr=l(),Ud=a("p"),Ntr=o(`Note:
Loading a model from its configuration file does `),Cfe=a("strong"),jtr=o("not"),Otr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mfe=a("code"),Gtr=o("from_pretrained()"),qtr=o(` to load the model
weights.`),ztr=l(),yfe=a("p"),Xtr=o("Examples:"),Qtr=l(),f(dw.$$.fragment),Vtr=l(),To=a("div"),f(cw.$$.fragment),Wtr=l(),wfe=a("p"),Htr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Utr=l(),rn=a("p"),Jtr=o("The model class to instantiate is selected based on the "),Afe=a("code"),Ktr=o("model_type"),Ytr=o(` property of the config object (either
passed as an argument or loaded from `),Lfe=a("code"),Ztr=o("pretrained_model_name_or_path"),ear=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Bfe=a("code"),oar=o("pretrained_model_name_or_path"),rar=o(":"),tar=l(),Jd=a("ul"),tF=a("li"),xfe=a("strong"),aar=o("gpt2"),nar=o(" \u2014 "),jD=a("a"),sar=o("FlaxGPT2LMHeadModel"),lar=o(" (OpenAI GPT-2 model)"),iar=l(),aF=a("li"),kfe=a("strong"),dar=o("gpt_neo"),car=o(" \u2014 "),OD=a("a"),far=o("FlaxGPTNeoForCausalLM"),mar=o(" (GPT Neo model)"),har=l(),nF=a("li"),Rfe=a("strong"),gar=o("gptj"),uar=o(" \u2014 "),GD=a("a"),par=o("FlaxGPTJForCausalLM"),_ar=o(" (GPT-J model)"),bar=l(),Pfe=a("p"),Tar=o("Examples:"),Far=l(),f(fw.$$.fragment),Pye=l(),Kd=a("h2"),sF=a("a"),Sfe=a("span"),f(mw.$$.fragment),Ear=l(),$fe=a("span"),Car=o("FlaxAutoModelForPreTraining"),Sye=l(),br=a("div"),f(hw.$$.fragment),Mar=l(),Yd=a("p"),yar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Ife=a("code"),war=o("from_pretrained()"),Aar=o(` class method or the
`),Dfe=a("code"),Lar=o("from_config()"),Bar=o(" class method."),xar=l(),gw=a("p"),kar=o("This class cannot be instantiated directly using "),Nfe=a("code"),Rar=o("__init__()"),Par=o(" (throws an error)."),Sar=l(),it=a("div"),f(uw.$$.fragment),$ar=l(),jfe=a("p"),Iar=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Dar=l(),Zd=a("p"),Nar=o(`Note:
Loading a model from its configuration file does `),Ofe=a("strong"),jar=o("not"),Oar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gfe=a("code"),Gar=o("from_pretrained()"),qar=o(` to load the model
weights.`),zar=l(),qfe=a("p"),Xar=o("Examples:"),Qar=l(),f(pw.$$.fragment),Var=l(),Fo=a("div"),f(_w.$$.fragment),War=l(),zfe=a("p"),Har=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Uar=l(),tn=a("p"),Jar=o("The model class to instantiate is selected based on the "),Xfe=a("code"),Kar=o("model_type"),Yar=o(` property of the config object (either
passed as an argument or loaded from `),Qfe=a("code"),Zar=o("pretrained_model_name_or_path"),enr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Vfe=a("code"),onr=o("pretrained_model_name_or_path"),rnr=o(":"),tnr=l(),Te=a("ul"),lF=a("li"),Wfe=a("strong"),anr=o("albert"),nnr=o(" \u2014 "),qD=a("a"),snr=o("FlaxAlbertForPreTraining"),lnr=o(" (ALBERT model)"),inr=l(),iF=a("li"),Hfe=a("strong"),dnr=o("bart"),cnr=o(" \u2014 "),zD=a("a"),fnr=o("FlaxBartForConditionalGeneration"),mnr=o(" (BART model)"),hnr=l(),dF=a("li"),Ufe=a("strong"),gnr=o("bert"),unr=o(" \u2014 "),XD=a("a"),pnr=o("FlaxBertForPreTraining"),_nr=o(" (BERT model)"),vnr=l(),cF=a("li"),Jfe=a("strong"),bnr=o("big_bird"),Tnr=o(" \u2014 "),QD=a("a"),Fnr=o("FlaxBigBirdForPreTraining"),Enr=o(" (BigBird model)"),Cnr=l(),fF=a("li"),Kfe=a("strong"),Mnr=o("electra"),ynr=o(" \u2014 "),VD=a("a"),wnr=o("FlaxElectraForPreTraining"),Anr=o(" (ELECTRA model)"),Lnr=l(),mF=a("li"),Yfe=a("strong"),Bnr=o("mbart"),xnr=o(" \u2014 "),WD=a("a"),knr=o("FlaxMBartForConditionalGeneration"),Rnr=o(" (mBART model)"),Pnr=l(),hF=a("li"),Zfe=a("strong"),Snr=o("mt5"),$nr=o(" \u2014 "),HD=a("a"),Inr=o("FlaxMT5ForConditionalGeneration"),Dnr=o(" (mT5 model)"),Nnr=l(),gF=a("li"),eme=a("strong"),jnr=o("roberta"),Onr=o(" \u2014 "),UD=a("a"),Gnr=o("FlaxRobertaForMaskedLM"),qnr=o(" (RoBERTa model)"),znr=l(),uF=a("li"),ome=a("strong"),Xnr=o("t5"),Qnr=o(" \u2014 "),JD=a("a"),Vnr=o("FlaxT5ForConditionalGeneration"),Wnr=o(" (T5 model)"),Hnr=l(),pF=a("li"),rme=a("strong"),Unr=o("wav2vec2"),Jnr=o(" \u2014 "),KD=a("a"),Knr=o("FlaxWav2Vec2ForPreTraining"),Ynr=o(" (Wav2Vec2 model)"),Znr=l(),tme=a("p"),esr=o("Examples:"),osr=l(),f(vw.$$.fragment),$ye=l(),ec=a("h2"),_F=a("a"),ame=a("span"),f(bw.$$.fragment),rsr=l(),nme=a("span"),tsr=o("FlaxAutoModelForMaskedLM"),Iye=l(),Tr=a("div"),f(Tw.$$.fragment),asr=l(),oc=a("p"),nsr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),sme=a("code"),ssr=o("from_pretrained()"),lsr=o(` class method or the
`),lme=a("code"),isr=o("from_config()"),dsr=o(" class method."),csr=l(),Fw=a("p"),fsr=o("This class cannot be instantiated directly using "),ime=a("code"),msr=o("__init__()"),hsr=o(" (throws an error)."),gsr=l(),dt=a("div"),f(Ew.$$.fragment),usr=l(),dme=a("p"),psr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),_sr=l(),rc=a("p"),vsr=o(`Note:
Loading a model from its configuration file does `),cme=a("strong"),bsr=o("not"),Tsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fme=a("code"),Fsr=o("from_pretrained()"),Esr=o(` to load the model
weights.`),Csr=l(),mme=a("p"),Msr=o("Examples:"),ysr=l(),f(Cw.$$.fragment),wsr=l(),Eo=a("div"),f(Mw.$$.fragment),Asr=l(),hme=a("p"),Lsr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Bsr=l(),an=a("p"),xsr=o("The model class to instantiate is selected based on the "),gme=a("code"),ksr=o("model_type"),Rsr=o(` property of the config object (either
passed as an argument or loaded from `),ume=a("code"),Psr=o("pretrained_model_name_or_path"),Ssr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),pme=a("code"),$sr=o("pretrained_model_name_or_path"),Isr=o(":"),Dsr=l(),ke=a("ul"),vF=a("li"),_me=a("strong"),Nsr=o("albert"),jsr=o(" \u2014 "),YD=a("a"),Osr=o("FlaxAlbertForMaskedLM"),Gsr=o(" (ALBERT model)"),qsr=l(),bF=a("li"),vme=a("strong"),zsr=o("bart"),Xsr=o(" \u2014 "),ZD=a("a"),Qsr=o("FlaxBartForConditionalGeneration"),Vsr=o(" (BART model)"),Wsr=l(),TF=a("li"),bme=a("strong"),Hsr=o("bert"),Usr=o(" \u2014 "),eN=a("a"),Jsr=o("FlaxBertForMaskedLM"),Ksr=o(" (BERT model)"),Ysr=l(),FF=a("li"),Tme=a("strong"),Zsr=o("big_bird"),elr=o(" \u2014 "),oN=a("a"),olr=o("FlaxBigBirdForMaskedLM"),rlr=o(" (BigBird model)"),tlr=l(),EF=a("li"),Fme=a("strong"),alr=o("distilbert"),nlr=o(" \u2014 "),rN=a("a"),slr=o("FlaxDistilBertForMaskedLM"),llr=o(" (DistilBERT model)"),ilr=l(),CF=a("li"),Eme=a("strong"),dlr=o("electra"),clr=o(" \u2014 "),tN=a("a"),flr=o("FlaxElectraForMaskedLM"),mlr=o(" (ELECTRA model)"),hlr=l(),MF=a("li"),Cme=a("strong"),glr=o("mbart"),ulr=o(" \u2014 "),aN=a("a"),plr=o("FlaxMBartForConditionalGeneration"),_lr=o(" (mBART model)"),vlr=l(),yF=a("li"),Mme=a("strong"),blr=o("roberta"),Tlr=o(" \u2014 "),nN=a("a"),Flr=o("FlaxRobertaForMaskedLM"),Elr=o(" (RoBERTa model)"),Clr=l(),yme=a("p"),Mlr=o("Examples:"),ylr=l(),f(yw.$$.fragment),Dye=l(),tc=a("h2"),wF=a("a"),wme=a("span"),f(ww.$$.fragment),wlr=l(),Ame=a("span"),Alr=o("FlaxAutoModelForSeq2SeqLM"),Nye=l(),Fr=a("div"),f(Aw.$$.fragment),Llr=l(),ac=a("p"),Blr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Lme=a("code"),xlr=o("from_pretrained()"),klr=o(` class method or the
`),Bme=a("code"),Rlr=o("from_config()"),Plr=o(" class method."),Slr=l(),Lw=a("p"),$lr=o("This class cannot be instantiated directly using "),xme=a("code"),Ilr=o("__init__()"),Dlr=o(" (throws an error)."),Nlr=l(),ct=a("div"),f(Bw.$$.fragment),jlr=l(),kme=a("p"),Olr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Glr=l(),nc=a("p"),qlr=o(`Note:
Loading a model from its configuration file does `),Rme=a("strong"),zlr=o("not"),Xlr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pme=a("code"),Qlr=o("from_pretrained()"),Vlr=o(` to load the model
weights.`),Wlr=l(),Sme=a("p"),Hlr=o("Examples:"),Ulr=l(),f(xw.$$.fragment),Jlr=l(),Co=a("div"),f(kw.$$.fragment),Klr=l(),$me=a("p"),Ylr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Zlr=l(),nn=a("p"),eir=o("The model class to instantiate is selected based on the "),Ime=a("code"),oir=o("model_type"),rir=o(` property of the config object (either
passed as an argument or loaded from `),Dme=a("code"),tir=o("pretrained_model_name_or_path"),air=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Nme=a("code"),nir=o("pretrained_model_name_or_path"),sir=o(":"),lir=l(),Me=a("ul"),AF=a("li"),jme=a("strong"),iir=o("bart"),dir=o(" \u2014 "),sN=a("a"),cir=o("FlaxBartForConditionalGeneration"),fir=o(" (BART model)"),mir=l(),LF=a("li"),Ome=a("strong"),hir=o("blenderbot"),gir=o(" \u2014 "),lN=a("a"),uir=o("FlaxBlenderbotForConditionalGeneration"),pir=o(" (Blenderbot model)"),_ir=l(),BF=a("li"),Gme=a("strong"),vir=o("blenderbot-small"),bir=o(" \u2014 "),iN=a("a"),Tir=o("FlaxBlenderbotSmallForConditionalGeneration"),Fir=o(" (BlenderbotSmall model)"),Eir=l(),xF=a("li"),qme=a("strong"),Cir=o("encoder-decoder"),Mir=o(" \u2014 "),dN=a("a"),yir=o("FlaxEncoderDecoderModel"),wir=o(" (Encoder decoder model)"),Air=l(),kF=a("li"),zme=a("strong"),Lir=o("marian"),Bir=o(" \u2014 "),cN=a("a"),xir=o("FlaxMarianMTModel"),kir=o(" (Marian model)"),Rir=l(),RF=a("li"),Xme=a("strong"),Pir=o("mbart"),Sir=o(" \u2014 "),fN=a("a"),$ir=o("FlaxMBartForConditionalGeneration"),Iir=o(" (mBART model)"),Dir=l(),PF=a("li"),Qme=a("strong"),Nir=o("mt5"),jir=o(" \u2014 "),mN=a("a"),Oir=o("FlaxMT5ForConditionalGeneration"),Gir=o(" (mT5 model)"),qir=l(),SF=a("li"),Vme=a("strong"),zir=o("pegasus"),Xir=o(" \u2014 "),hN=a("a"),Qir=o("FlaxPegasusForConditionalGeneration"),Vir=o(" (Pegasus model)"),Wir=l(),$F=a("li"),Wme=a("strong"),Hir=o("t5"),Uir=o(" \u2014 "),gN=a("a"),Jir=o("FlaxT5ForConditionalGeneration"),Kir=o(" (T5 model)"),Yir=l(),Hme=a("p"),Zir=o("Examples:"),edr=l(),f(Rw.$$.fragment),jye=l(),sc=a("h2"),IF=a("a"),Ume=a("span"),f(Pw.$$.fragment),odr=l(),Jme=a("span"),rdr=o("FlaxAutoModelForSequenceClassification"),Oye=l(),Er=a("div"),f(Sw.$$.fragment),tdr=l(),lc=a("p"),adr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Kme=a("code"),ndr=o("from_pretrained()"),sdr=o(` class method or the
`),Yme=a("code"),ldr=o("from_config()"),idr=o(" class method."),ddr=l(),$w=a("p"),cdr=o("This class cannot be instantiated directly using "),Zme=a("code"),fdr=o("__init__()"),mdr=o(" (throws an error)."),hdr=l(),ft=a("div"),f(Iw.$$.fragment),gdr=l(),ehe=a("p"),udr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),pdr=l(),ic=a("p"),_dr=o(`Note:
Loading a model from its configuration file does `),ohe=a("strong"),vdr=o("not"),bdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rhe=a("code"),Tdr=o("from_pretrained()"),Fdr=o(` to load the model
weights.`),Edr=l(),the=a("p"),Cdr=o("Examples:"),Mdr=l(),f(Dw.$$.fragment),ydr=l(),Mo=a("div"),f(Nw.$$.fragment),wdr=l(),ahe=a("p"),Adr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Ldr=l(),sn=a("p"),Bdr=o("The model class to instantiate is selected based on the "),nhe=a("code"),xdr=o("model_type"),kdr=o(` property of the config object (either
passed as an argument or loaded from `),she=a("code"),Rdr=o("pretrained_model_name_or_path"),Pdr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),lhe=a("code"),Sdr=o("pretrained_model_name_or_path"),$dr=o(":"),Idr=l(),Re=a("ul"),DF=a("li"),ihe=a("strong"),Ddr=o("albert"),Ndr=o(" \u2014 "),uN=a("a"),jdr=o("FlaxAlbertForSequenceClassification"),Odr=o(" (ALBERT model)"),Gdr=l(),NF=a("li"),dhe=a("strong"),qdr=o("bart"),zdr=o(" \u2014 "),pN=a("a"),Xdr=o("FlaxBartForSequenceClassification"),Qdr=o(" (BART model)"),Vdr=l(),jF=a("li"),che=a("strong"),Wdr=o("bert"),Hdr=o(" \u2014 "),_N=a("a"),Udr=o("FlaxBertForSequenceClassification"),Jdr=o(" (BERT model)"),Kdr=l(),OF=a("li"),fhe=a("strong"),Ydr=o("big_bird"),Zdr=o(" \u2014 "),vN=a("a"),ecr=o("FlaxBigBirdForSequenceClassification"),ocr=o(" (BigBird model)"),rcr=l(),GF=a("li"),mhe=a("strong"),tcr=o("distilbert"),acr=o(" \u2014 "),bN=a("a"),ncr=o("FlaxDistilBertForSequenceClassification"),scr=o(" (DistilBERT model)"),lcr=l(),qF=a("li"),hhe=a("strong"),icr=o("electra"),dcr=o(" \u2014 "),TN=a("a"),ccr=o("FlaxElectraForSequenceClassification"),fcr=o(" (ELECTRA model)"),mcr=l(),zF=a("li"),ghe=a("strong"),hcr=o("mbart"),gcr=o(" \u2014 "),FN=a("a"),ucr=o("FlaxMBartForSequenceClassification"),pcr=o(" (mBART model)"),_cr=l(),XF=a("li"),uhe=a("strong"),vcr=o("roberta"),bcr=o(" \u2014 "),EN=a("a"),Tcr=o("FlaxRobertaForSequenceClassification"),Fcr=o(" (RoBERTa model)"),Ecr=l(),phe=a("p"),Ccr=o("Examples:"),Mcr=l(),f(jw.$$.fragment),Gye=l(),dc=a("h2"),QF=a("a"),_he=a("span"),f(Ow.$$.fragment),ycr=l(),vhe=a("span"),wcr=o("FlaxAutoModelForQuestionAnswering"),qye=l(),Cr=a("div"),f(Gw.$$.fragment),Acr=l(),cc=a("p"),Lcr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),bhe=a("code"),Bcr=o("from_pretrained()"),xcr=o(` class method or the
`),The=a("code"),kcr=o("from_config()"),Rcr=o(" class method."),Pcr=l(),qw=a("p"),Scr=o("This class cannot be instantiated directly using "),Fhe=a("code"),$cr=o("__init__()"),Icr=o(" (throws an error)."),Dcr=l(),mt=a("div"),f(zw.$$.fragment),Ncr=l(),Ehe=a("p"),jcr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Ocr=l(),fc=a("p"),Gcr=o(`Note:
Loading a model from its configuration file does `),Che=a("strong"),qcr=o("not"),zcr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mhe=a("code"),Xcr=o("from_pretrained()"),Qcr=o(` to load the model
weights.`),Vcr=l(),yhe=a("p"),Wcr=o("Examples:"),Hcr=l(),f(Xw.$$.fragment),Ucr=l(),yo=a("div"),f(Qw.$$.fragment),Jcr=l(),whe=a("p"),Kcr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Ycr=l(),ln=a("p"),Zcr=o("The model class to instantiate is selected based on the "),Ahe=a("code"),efr=o("model_type"),ofr=o(` property of the config object (either
passed as an argument or loaded from `),Lhe=a("code"),rfr=o("pretrained_model_name_or_path"),tfr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Bhe=a("code"),afr=o("pretrained_model_name_or_path"),nfr=o(":"),sfr=l(),Pe=a("ul"),VF=a("li"),xhe=a("strong"),lfr=o("albert"),ifr=o(" \u2014 "),CN=a("a"),dfr=o("FlaxAlbertForQuestionAnswering"),cfr=o(" (ALBERT model)"),ffr=l(),WF=a("li"),khe=a("strong"),mfr=o("bart"),hfr=o(" \u2014 "),MN=a("a"),gfr=o("FlaxBartForQuestionAnswering"),ufr=o(" (BART model)"),pfr=l(),HF=a("li"),Rhe=a("strong"),_fr=o("bert"),vfr=o(" \u2014 "),yN=a("a"),bfr=o("FlaxBertForQuestionAnswering"),Tfr=o(" (BERT model)"),Ffr=l(),UF=a("li"),Phe=a("strong"),Efr=o("big_bird"),Cfr=o(" \u2014 "),wN=a("a"),Mfr=o("FlaxBigBirdForQuestionAnswering"),yfr=o(" (BigBird model)"),wfr=l(),JF=a("li"),She=a("strong"),Afr=o("distilbert"),Lfr=o(" \u2014 "),AN=a("a"),Bfr=o("FlaxDistilBertForQuestionAnswering"),xfr=o(" (DistilBERT model)"),kfr=l(),KF=a("li"),$he=a("strong"),Rfr=o("electra"),Pfr=o(" \u2014 "),LN=a("a"),Sfr=o("FlaxElectraForQuestionAnswering"),$fr=o(" (ELECTRA model)"),Ifr=l(),YF=a("li"),Ihe=a("strong"),Dfr=o("mbart"),Nfr=o(" \u2014 "),BN=a("a"),jfr=o("FlaxMBartForQuestionAnswering"),Ofr=o(" (mBART model)"),Gfr=l(),ZF=a("li"),Dhe=a("strong"),qfr=o("roberta"),zfr=o(" \u2014 "),xN=a("a"),Xfr=o("FlaxRobertaForQuestionAnswering"),Qfr=o(" (RoBERTa model)"),Vfr=l(),Nhe=a("p"),Wfr=o("Examples:"),Hfr=l(),f(Vw.$$.fragment),zye=l(),mc=a("h2"),eE=a("a"),jhe=a("span"),f(Ww.$$.fragment),Ufr=l(),Ohe=a("span"),Jfr=o("FlaxAutoModelForTokenClassification"),Xye=l(),Mr=a("div"),f(Hw.$$.fragment),Kfr=l(),hc=a("p"),Yfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Ghe=a("code"),Zfr=o("from_pretrained()"),emr=o(` class method or the
`),qhe=a("code"),omr=o("from_config()"),rmr=o(" class method."),tmr=l(),Uw=a("p"),amr=o("This class cannot be instantiated directly using "),zhe=a("code"),nmr=o("__init__()"),smr=o(" (throws an error)."),lmr=l(),ht=a("div"),f(Jw.$$.fragment),imr=l(),Xhe=a("p"),dmr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),cmr=l(),gc=a("p"),fmr=o(`Note:
Loading a model from its configuration file does `),Qhe=a("strong"),mmr=o("not"),hmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vhe=a("code"),gmr=o("from_pretrained()"),umr=o(` to load the model
weights.`),pmr=l(),Whe=a("p"),_mr=o("Examples:"),vmr=l(),f(Kw.$$.fragment),bmr=l(),wo=a("div"),f(Yw.$$.fragment),Tmr=l(),Hhe=a("p"),Fmr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Emr=l(),dn=a("p"),Cmr=o("The model class to instantiate is selected based on the "),Uhe=a("code"),Mmr=o("model_type"),ymr=o(` property of the config object (either
passed as an argument or loaded from `),Jhe=a("code"),wmr=o("pretrained_model_name_or_path"),Amr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Khe=a("code"),Lmr=o("pretrained_model_name_or_path"),Bmr=o(":"),xmr=l(),yr=a("ul"),oE=a("li"),Yhe=a("strong"),kmr=o("albert"),Rmr=o(" \u2014 "),kN=a("a"),Pmr=o("FlaxAlbertForTokenClassification"),Smr=o(" (ALBERT model)"),$mr=l(),rE=a("li"),Zhe=a("strong"),Imr=o("bert"),Dmr=o(" \u2014 "),RN=a("a"),Nmr=o("FlaxBertForTokenClassification"),jmr=o(" (BERT model)"),Omr=l(),tE=a("li"),ege=a("strong"),Gmr=o("big_bird"),qmr=o(" \u2014 "),PN=a("a"),zmr=o("FlaxBigBirdForTokenClassification"),Xmr=o(" (BigBird model)"),Qmr=l(),aE=a("li"),oge=a("strong"),Vmr=o("distilbert"),Wmr=o(" \u2014 "),SN=a("a"),Hmr=o("FlaxDistilBertForTokenClassification"),Umr=o(" (DistilBERT model)"),Jmr=l(),nE=a("li"),rge=a("strong"),Kmr=o("electra"),Ymr=o(" \u2014 "),$N=a("a"),Zmr=o("FlaxElectraForTokenClassification"),ehr=o(" (ELECTRA model)"),ohr=l(),sE=a("li"),tge=a("strong"),rhr=o("roberta"),thr=o(" \u2014 "),IN=a("a"),ahr=o("FlaxRobertaForTokenClassification"),nhr=o(" (RoBERTa model)"),shr=l(),age=a("p"),lhr=o("Examples:"),ihr=l(),f(Zw.$$.fragment),Qye=l(),uc=a("h2"),lE=a("a"),nge=a("span"),f(e7.$$.fragment),dhr=l(),sge=a("span"),chr=o("FlaxAutoModelForMultipleChoice"),Vye=l(),wr=a("div"),f(o7.$$.fragment),fhr=l(),pc=a("p"),mhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),lge=a("code"),hhr=o("from_pretrained()"),ghr=o(` class method or the
`),ige=a("code"),uhr=o("from_config()"),phr=o(" class method."),_hr=l(),r7=a("p"),vhr=o("This class cannot be instantiated directly using "),dge=a("code"),bhr=o("__init__()"),Thr=o(" (throws an error)."),Fhr=l(),gt=a("div"),f(t7.$$.fragment),Ehr=l(),cge=a("p"),Chr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Mhr=l(),_c=a("p"),yhr=o(`Note:
Loading a model from its configuration file does `),fge=a("strong"),whr=o("not"),Ahr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mge=a("code"),Lhr=o("from_pretrained()"),Bhr=o(` to load the model
weights.`),xhr=l(),hge=a("p"),khr=o("Examples:"),Rhr=l(),f(a7.$$.fragment),Phr=l(),Ao=a("div"),f(n7.$$.fragment),Shr=l(),gge=a("p"),$hr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ihr=l(),cn=a("p"),Dhr=o("The model class to instantiate is selected based on the "),uge=a("code"),Nhr=o("model_type"),jhr=o(` property of the config object (either
passed as an argument or loaded from `),pge=a("code"),Ohr=o("pretrained_model_name_or_path"),Ghr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),_ge=a("code"),qhr=o("pretrained_model_name_or_path"),zhr=o(":"),Xhr=l(),Ar=a("ul"),iE=a("li"),vge=a("strong"),Qhr=o("albert"),Vhr=o(" \u2014 "),DN=a("a"),Whr=o("FlaxAlbertForMultipleChoice"),Hhr=o(" (ALBERT model)"),Uhr=l(),dE=a("li"),bge=a("strong"),Jhr=o("bert"),Khr=o(" \u2014 "),NN=a("a"),Yhr=o("FlaxBertForMultipleChoice"),Zhr=o(" (BERT model)"),egr=l(),cE=a("li"),Tge=a("strong"),ogr=o("big_bird"),rgr=o(" \u2014 "),jN=a("a"),tgr=o("FlaxBigBirdForMultipleChoice"),agr=o(" (BigBird model)"),ngr=l(),fE=a("li"),Fge=a("strong"),sgr=o("distilbert"),lgr=o(" \u2014 "),ON=a("a"),igr=o("FlaxDistilBertForMultipleChoice"),dgr=o(" (DistilBERT model)"),cgr=l(),mE=a("li"),Ege=a("strong"),fgr=o("electra"),mgr=o(" \u2014 "),GN=a("a"),hgr=o("FlaxElectraForMultipleChoice"),ggr=o(" (ELECTRA model)"),ugr=l(),hE=a("li"),Cge=a("strong"),pgr=o("roberta"),_gr=o(" \u2014 "),qN=a("a"),vgr=o("FlaxRobertaForMultipleChoice"),bgr=o(" (RoBERTa model)"),Tgr=l(),Mge=a("p"),Fgr=o("Examples:"),Egr=l(),f(s7.$$.fragment),Wye=l(),vc=a("h2"),gE=a("a"),yge=a("span"),f(l7.$$.fragment),Cgr=l(),wge=a("span"),Mgr=o("FlaxAutoModelForNextSentencePrediction"),Hye=l(),Lr=a("div"),f(i7.$$.fragment),ygr=l(),bc=a("p"),wgr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Age=a("code"),Agr=o("from_pretrained()"),Lgr=o(` class method or the
`),Lge=a("code"),Bgr=o("from_config()"),xgr=o(" class method."),kgr=l(),d7=a("p"),Rgr=o("This class cannot be instantiated directly using "),Bge=a("code"),Pgr=o("__init__()"),Sgr=o(" (throws an error)."),$gr=l(),ut=a("div"),f(c7.$$.fragment),Igr=l(),xge=a("p"),Dgr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Ngr=l(),Tc=a("p"),jgr=o(`Note:
Loading a model from its configuration file does `),kge=a("strong"),Ogr=o("not"),Ggr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Rge=a("code"),qgr=o("from_pretrained()"),zgr=o(` to load the model
weights.`),Xgr=l(),Pge=a("p"),Qgr=o("Examples:"),Vgr=l(),f(f7.$$.fragment),Wgr=l(),Lo=a("div"),f(m7.$$.fragment),Hgr=l(),Sge=a("p"),Ugr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Jgr=l(),fn=a("p"),Kgr=o("The model class to instantiate is selected based on the "),$ge=a("code"),Ygr=o("model_type"),Zgr=o(` property of the config object (either
passed as an argument or loaded from `),Ige=a("code"),eur=o("pretrained_model_name_or_path"),our=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Dge=a("code"),rur=o("pretrained_model_name_or_path"),tur=o(":"),aur=l(),Nge=a("ul"),uE=a("li"),jge=a("strong"),nur=o("bert"),sur=o(" \u2014 "),zN=a("a"),lur=o("FlaxBertForNextSentencePrediction"),iur=o(" (BERT model)"),dur=l(),Oge=a("p"),cur=o("Examples:"),fur=l(),f(h7.$$.fragment),Uye=l(),Fc=a("h2"),pE=a("a"),Gge=a("span"),f(g7.$$.fragment),mur=l(),qge=a("span"),hur=o("FlaxAutoModelForImageClassification"),Jye=l(),Br=a("div"),f(u7.$$.fragment),gur=l(),Ec=a("p"),uur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),zge=a("code"),pur=o("from_pretrained()"),_ur=o(` class method or the
`),Xge=a("code"),vur=o("from_config()"),bur=o(" class method."),Tur=l(),p7=a("p"),Fur=o("This class cannot be instantiated directly using "),Qge=a("code"),Eur=o("__init__()"),Cur=o(" (throws an error)."),Mur=l(),pt=a("div"),f(_7.$$.fragment),yur=l(),Vge=a("p"),wur=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Aur=l(),Cc=a("p"),Lur=o(`Note:
Loading a model from its configuration file does `),Wge=a("strong"),Bur=o("not"),xur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hge=a("code"),kur=o("from_pretrained()"),Rur=o(` to load the model
weights.`),Pur=l(),Uge=a("p"),Sur=o("Examples:"),$ur=l(),f(v7.$$.fragment),Iur=l(),Bo=a("div"),f(b7.$$.fragment),Dur=l(),Jge=a("p"),Nur=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),jur=l(),mn=a("p"),Our=o("The model class to instantiate is selected based on the "),Kge=a("code"),Gur=o("model_type"),qur=o(` property of the config object (either
passed as an argument or loaded from `),Yge=a("code"),zur=o("pretrained_model_name_or_path"),Xur=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Zge=a("code"),Qur=o("pretrained_model_name_or_path"),Vur=o(":"),Wur=l(),T7=a("ul"),_E=a("li"),eue=a("strong"),Hur=o("beit"),Uur=o(" \u2014 "),XN=a("a"),Jur=o("FlaxBeitForImageClassification"),Kur=o(" (BEiT model)"),Yur=l(),vE=a("li"),oue=a("strong"),Zur=o("vit"),epr=o(" \u2014 "),QN=a("a"),opr=o("FlaxViTForImageClassification"),rpr=o(" (ViT model)"),tpr=l(),rue=a("p"),apr=o("Examples:"),npr=l(),f(F7.$$.fragment),Kye=l(),Mc=a("h2"),bE=a("a"),tue=a("span"),f(E7.$$.fragment),spr=l(),aue=a("span"),lpr=o("FlaxAutoModelForVision2Seq"),Yye=l(),xr=a("div"),f(C7.$$.fragment),ipr=l(),yc=a("p"),dpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),nue=a("code"),cpr=o("from_pretrained()"),fpr=o(` class method or the
`),sue=a("code"),mpr=o("from_config()"),hpr=o(" class method."),gpr=l(),M7=a("p"),upr=o("This class cannot be instantiated directly using "),lue=a("code"),ppr=o("__init__()"),_pr=o(" (throws an error)."),vpr=l(),_t=a("div"),f(y7.$$.fragment),bpr=l(),iue=a("p"),Tpr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Fpr=l(),wc=a("p"),Epr=o(`Note:
Loading a model from its configuration file does `),due=a("strong"),Cpr=o("not"),Mpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cue=a("code"),ypr=o("from_pretrained()"),wpr=o(` to load the model
weights.`),Apr=l(),fue=a("p"),Lpr=o("Examples:"),Bpr=l(),f(w7.$$.fragment),xpr=l(),xo=a("div"),f(A7.$$.fragment),kpr=l(),mue=a("p"),Rpr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Ppr=l(),hn=a("p"),Spr=o("The model class to instantiate is selected based on the "),hue=a("code"),$pr=o("model_type"),Ipr=o(` property of the config object (either
passed as an argument or loaded from `),gue=a("code"),Dpr=o("pretrained_model_name_or_path"),Npr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),uue=a("code"),jpr=o("pretrained_model_name_or_path"),Opr=o(":"),Gpr=l(),pue=a("ul"),TE=a("li"),_ue=a("strong"),qpr=o("vision-encoder-decoder"),zpr=o(" \u2014 "),VN=a("a"),Xpr=o("FlaxVisionEncoderDecoderModel"),Qpr=o(" (Vision Encoder decoder model)"),Vpr=l(),vue=a("p"),Wpr=o("Examples:"),Hpr=l(),f(L7.$$.fragment),this.h()},l(c){const _=aYr('[data-svelte="svelte-1phssyn"]',document.head);re=n(_,"META",{name:!0,content:!0}),_.forEach(t),Se=i(c),me=n(c,"H1",{class:!0});var B7=s(me);ue=n(B7,"A",{id:!0,class:!0,href:!0});var bue=s(ue);ro=n(bue,"SPAN",{});var Tue=s(ro);m(ge.$$.fragment,Tue),Tue.forEach(t),bue.forEach(t),Ce=i(B7),$o=n(B7,"SPAN",{});var Jpr=s($o);zl=r(Jpr,"Auto Classes"),Jpr.forEach(t),B7.forEach(t),Lc=i(c),Ot=n(c,"P",{});var ewe=s(Ot);Xl=r(ewe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Ql=n(ewe,"CODE",{});var Kpr=s(Ql);vC=r(Kpr,"from_pretrained()"),Kpr.forEach(t),Bc=r(ewe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ewe.forEach(t),xe=i(c),ao=n(c,"P",{});var FE=s(ao);Vl=r(FE,"Instantiating one of "),gn=n(FE,"A",{href:!0});var Ypr=s(gn);bC=r(Ypr,"AutoConfig"),Ypr.forEach(t),un=r(FE,", "),pn=n(FE,"A",{href:!0});var Zpr=s(pn);TC=r(Zpr,"AutoModel"),Zpr.forEach(t),Wl=r(FE,`, and
`),_n=n(FE,"A",{href:!0});var e_r=s(_n);FC=r(e_r,"AutoTokenizer"),e_r.forEach(t),Hl=r(FE," will directly create a class of the relevant architecture. For instance"),FE.forEach(t),xc=i(c),m(_a.$$.fragment,c),no=i(c),pe=n(c,"P",{});var owe=s(pe);E0=r(owe,"will create a model that is an instance of "),Ul=n(owe,"A",{href:!0});var o_r=s(Ul);C0=r(o_r,"BertModel"),o_r.forEach(t),M0=r(owe,"."),owe.forEach(t),Io=i(c),va=n(c,"P",{});var rwe=s(va);y0=r(rwe,"There is one class of "),kc=n(rwe,"CODE",{});var r_r=s(kc);w0=r(r_r,"AutoModel"),r_r.forEach(t),i0e=r(rwe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),rwe.forEach(t),m5e=i(c),Jl=n(c,"H2",{class:!0});var twe=s(Jl);Rc=n(twe,"A",{id:!0,class:!0,href:!0});var t_r=s(Rc);PO=n(t_r,"SPAN",{});var a_r=s(PO);m(EC.$$.fragment,a_r),a_r.forEach(t),t_r.forEach(t),d0e=i(twe),SO=n(twe,"SPAN",{});var n_r=s(SO);c0e=r(n_r,"Extending the Auto Classes"),n_r.forEach(t),twe.forEach(t),h5e=i(c),vn=n(c,"P",{});var WN=s(vn);f0e=r(WN,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),$O=n(WN,"CODE",{});var s_r=s($O);m0e=r(s_r,"NewModel"),s_r.forEach(t),h0e=r(WN,", make sure you have a "),IO=n(WN,"CODE",{});var l_r=s(IO);g0e=r(l_r,"NewModelConfig"),l_r.forEach(t),u0e=r(WN,` then you can add those to the auto
classes like this:`),WN.forEach(t),g5e=i(c),m(CC.$$.fragment,c),u5e=i(c),A0=n(c,"P",{});var i_r=s(A0);p0e=r(i_r,"You will then be able to use the auto classes like you would usually do!"),i_r.forEach(t),p5e=i(c),m(Pc.$$.fragment,c),_5e=i(c),Kl=n(c,"H2",{class:!0});var awe=s(Kl);Sc=n(awe,"A",{id:!0,class:!0,href:!0});var d_r=s(Sc);DO=n(d_r,"SPAN",{});var c_r=s(DO);m(MC.$$.fragment,c_r),c_r.forEach(t),d_r.forEach(t),_0e=i(awe),NO=n(awe,"SPAN",{});var f_r=s(NO);v0e=r(f_r,"AutoConfig"),f_r.forEach(t),awe.forEach(t),v5e=i(c),Do=n(c,"DIV",{class:!0});var fs=s(Do);m(yC.$$.fragment,fs),b0e=i(fs),wC=n(fs,"P",{});var nwe=s(wC);T0e=r(nwe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),L0=n(nwe,"A",{href:!0});var m_r=s(L0);F0e=r(m_r,"from_pretrained()"),m_r.forEach(t),E0e=r(nwe," class method."),nwe.forEach(t),C0e=i(fs),AC=n(fs,"P",{});var swe=s(AC);M0e=r(swe,"This class cannot be instantiated directly using "),jO=n(swe,"CODE",{});var h_r=s(jO);y0e=r(h_r,"__init__()"),h_r.forEach(t),w0e=r(swe," (throws an error)."),swe.forEach(t),A0e=i(fs),so=n(fs,"DIV",{class:!0});var qt=s(so);m(LC.$$.fragment,qt),L0e=i(qt),OO=n(qt,"P",{});var g_r=s(OO);B0e=r(g_r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),g_r.forEach(t),x0e=i(qt),Yl=n(qt,"P",{});var HN=s(Yl);k0e=r(HN,"The configuration class to instantiate is selected based on the "),GO=n(HN,"CODE",{});var u_r=s(GO);R0e=r(u_r,"model_type"),u_r.forEach(t),P0e=r(HN,` property of the config object
that is loaded, or when it\u2019s missing, by falling back to using pattern matching on
`),qO=n(HN,"CODE",{});var p_r=s(qO);S0e=r(p_r,"pretrained_model_name_or_path"),p_r.forEach(t),$0e=r(HN,":"),HN.forEach(t),I0e=i(qt),b=n(qt,"UL",{});var T=s(b);$c=n(T,"LI",{});var Fue=s($c);zO=n(Fue,"STRONG",{});var __r=s(zO);D0e=r(__r,"albert"),__r.forEach(t),N0e=r(Fue," \u2014 "),B0=n(Fue,"A",{href:!0});var v_r=s(B0);j0e=r(v_r,"AlbertConfig"),v_r.forEach(t),O0e=r(Fue," (ALBERT model)"),Fue.forEach(t),G0e=i(T),Ic=n(T,"LI",{});var Eue=s(Ic);XO=n(Eue,"STRONG",{});var b_r=s(XO);q0e=r(b_r,"bart"),b_r.forEach(t),z0e=r(Eue," \u2014 "),x0=n(Eue,"A",{href:!0});var T_r=s(x0);X0e=r(T_r,"BartConfig"),T_r.forEach(t),Q0e=r(Eue," (BART model)"),Eue.forEach(t),V0e=i(T),Dc=n(T,"LI",{});var Cue=s(Dc);QO=n(Cue,"STRONG",{});var F_r=s(QO);W0e=r(F_r,"beit"),F_r.forEach(t),H0e=r(Cue," \u2014 "),k0=n(Cue,"A",{href:!0});var E_r=s(k0);U0e=r(E_r,"BeitConfig"),E_r.forEach(t),J0e=r(Cue," (BEiT model)"),Cue.forEach(t),K0e=i(T),Nc=n(T,"LI",{});var Mue=s(Nc);VO=n(Mue,"STRONG",{});var C_r=s(VO);Y0e=r(C_r,"bert"),C_r.forEach(t),Z0e=r(Mue," \u2014 "),R0=n(Mue,"A",{href:!0});var M_r=s(R0);eAe=r(M_r,"BertConfig"),M_r.forEach(t),oAe=r(Mue," (BERT model)"),Mue.forEach(t),rAe=i(T),jc=n(T,"LI",{});var yue=s(jc);WO=n(yue,"STRONG",{});var y_r=s(WO);tAe=r(y_r,"bert-generation"),y_r.forEach(t),aAe=r(yue," \u2014 "),P0=n(yue,"A",{href:!0});var w_r=s(P0);nAe=r(w_r,"BertGenerationConfig"),w_r.forEach(t),sAe=r(yue," (Bert Generation model)"),yue.forEach(t),lAe=i(T),Oc=n(T,"LI",{});var wue=s(Oc);HO=n(wue,"STRONG",{});var A_r=s(HO);iAe=r(A_r,"big_bird"),A_r.forEach(t),dAe=r(wue," \u2014 "),S0=n(wue,"A",{href:!0});var L_r=s(S0);cAe=r(L_r,"BigBirdConfig"),L_r.forEach(t),fAe=r(wue," (BigBird model)"),wue.forEach(t),mAe=i(T),Gc=n(T,"LI",{});var Aue=s(Gc);UO=n(Aue,"STRONG",{});var B_r=s(UO);hAe=r(B_r,"bigbird_pegasus"),B_r.forEach(t),gAe=r(Aue," \u2014 "),$0=n(Aue,"A",{href:!0});var x_r=s($0);uAe=r(x_r,"BigBirdPegasusConfig"),x_r.forEach(t),pAe=r(Aue," (BigBirdPegasus model)"),Aue.forEach(t),_Ae=i(T),qc=n(T,"LI",{});var Lue=s(qc);JO=n(Lue,"STRONG",{});var k_r=s(JO);vAe=r(k_r,"blenderbot"),k_r.forEach(t),bAe=r(Lue," \u2014 "),I0=n(Lue,"A",{href:!0});var R_r=s(I0);TAe=r(R_r,"BlenderbotConfig"),R_r.forEach(t),FAe=r(Lue," (Blenderbot model)"),Lue.forEach(t),EAe=i(T),zc=n(T,"LI",{});var Bue=s(zc);KO=n(Bue,"STRONG",{});var P_r=s(KO);CAe=r(P_r,"blenderbot-small"),P_r.forEach(t),MAe=r(Bue," \u2014 "),D0=n(Bue,"A",{href:!0});var S_r=s(D0);yAe=r(S_r,"BlenderbotSmallConfig"),S_r.forEach(t),wAe=r(Bue," (BlenderbotSmall model)"),Bue.forEach(t),AAe=i(T),Xc=n(T,"LI",{});var xue=s(Xc);YO=n(xue,"STRONG",{});var $_r=s(YO);LAe=r($_r,"camembert"),$_r.forEach(t),BAe=r(xue," \u2014 "),N0=n(xue,"A",{href:!0});var I_r=s(N0);xAe=r(I_r,"CamembertConfig"),I_r.forEach(t),kAe=r(xue," (CamemBERT model)"),xue.forEach(t),RAe=i(T),Qc=n(T,"LI",{});var kue=s(Qc);ZO=n(kue,"STRONG",{});var D_r=s(ZO);PAe=r(D_r,"canine"),D_r.forEach(t),SAe=r(kue," \u2014 "),j0=n(kue,"A",{href:!0});var N_r=s(j0);$Ae=r(N_r,"CanineConfig"),N_r.forEach(t),IAe=r(kue," (Canine model)"),kue.forEach(t),DAe=i(T),Vc=n(T,"LI",{});var Rue=s(Vc);eG=n(Rue,"STRONG",{});var j_r=s(eG);NAe=r(j_r,"clip"),j_r.forEach(t),jAe=r(Rue," \u2014 "),O0=n(Rue,"A",{href:!0});var O_r=s(O0);OAe=r(O_r,"CLIPConfig"),O_r.forEach(t),GAe=r(Rue," (CLIP model)"),Rue.forEach(t),qAe=i(T),Wc=n(T,"LI",{});var Pue=s(Wc);oG=n(Pue,"STRONG",{});var G_r=s(oG);zAe=r(G_r,"convbert"),G_r.forEach(t),XAe=r(Pue," \u2014 "),G0=n(Pue,"A",{href:!0});var q_r=s(G0);QAe=r(q_r,"ConvBertConfig"),q_r.forEach(t),VAe=r(Pue," (ConvBERT model)"),Pue.forEach(t),WAe=i(T),Hc=n(T,"LI",{});var Sue=s(Hc);rG=n(Sue,"STRONG",{});var z_r=s(rG);HAe=r(z_r,"ctrl"),z_r.forEach(t),UAe=r(Sue," \u2014 "),q0=n(Sue,"A",{href:!0});var X_r=s(q0);JAe=r(X_r,"CTRLConfig"),X_r.forEach(t),KAe=r(Sue," (CTRL model)"),Sue.forEach(t),YAe=i(T),Uc=n(T,"LI",{});var $ue=s(Uc);tG=n($ue,"STRONG",{});var Q_r=s(tG);ZAe=r(Q_r,"deberta"),Q_r.forEach(t),e6e=r($ue," \u2014 "),z0=n($ue,"A",{href:!0});var V_r=s(z0);o6e=r(V_r,"DebertaConfig"),V_r.forEach(t),r6e=r($ue," (DeBERTa model)"),$ue.forEach(t),t6e=i(T),Jc=n(T,"LI",{});var Iue=s(Jc);aG=n(Iue,"STRONG",{});var W_r=s(aG);a6e=r(W_r,"deberta-v2"),W_r.forEach(t),n6e=r(Iue," \u2014 "),X0=n(Iue,"A",{href:!0});var H_r=s(X0);s6e=r(H_r,"DebertaV2Config"),H_r.forEach(t),l6e=r(Iue," (DeBERTa-v2 model)"),Iue.forEach(t),i6e=i(T),Kc=n(T,"LI",{});var Due=s(Kc);nG=n(Due,"STRONG",{});var U_r=s(nG);d6e=r(U_r,"deit"),U_r.forEach(t),c6e=r(Due," \u2014 "),Q0=n(Due,"A",{href:!0});var J_r=s(Q0);f6e=r(J_r,"DeiTConfig"),J_r.forEach(t),m6e=r(Due," (DeiT model)"),Due.forEach(t),h6e=i(T),Yc=n(T,"LI",{});var Nue=s(Yc);sG=n(Nue,"STRONG",{});var K_r=s(sG);g6e=r(K_r,"detr"),K_r.forEach(t),u6e=r(Nue," \u2014 "),V0=n(Nue,"A",{href:!0});var Y_r=s(V0);p6e=r(Y_r,"DetrConfig"),Y_r.forEach(t),_6e=r(Nue," (DETR model)"),Nue.forEach(t),v6e=i(T),Zc=n(T,"LI",{});var jue=s(Zc);lG=n(jue,"STRONG",{});var Z_r=s(lG);b6e=r(Z_r,"distilbert"),Z_r.forEach(t),T6e=r(jue," \u2014 "),W0=n(jue,"A",{href:!0});var e1r=s(W0);F6e=r(e1r,"DistilBertConfig"),e1r.forEach(t),E6e=r(jue," (DistilBERT model)"),jue.forEach(t),C6e=i(T),ef=n(T,"LI",{});var Oue=s(ef);iG=n(Oue,"STRONG",{});var o1r=s(iG);M6e=r(o1r,"dpr"),o1r.forEach(t),y6e=r(Oue," \u2014 "),H0=n(Oue,"A",{href:!0});var r1r=s(H0);w6e=r(r1r,"DPRConfig"),r1r.forEach(t),A6e=r(Oue," (DPR model)"),Oue.forEach(t),L6e=i(T),of=n(T,"LI",{});var Gue=s(of);dG=n(Gue,"STRONG",{});var t1r=s(dG);B6e=r(t1r,"electra"),t1r.forEach(t),x6e=r(Gue," \u2014 "),U0=n(Gue,"A",{href:!0});var a1r=s(U0);k6e=r(a1r,"ElectraConfig"),a1r.forEach(t),R6e=r(Gue," (ELECTRA model)"),Gue.forEach(t),P6e=i(T),rf=n(T,"LI",{});var que=s(rf);cG=n(que,"STRONG",{});var n1r=s(cG);S6e=r(n1r,"encoder-decoder"),n1r.forEach(t),$6e=r(que," \u2014 "),J0=n(que,"A",{href:!0});var s1r=s(J0);I6e=r(s1r,"EncoderDecoderConfig"),s1r.forEach(t),D6e=r(que," (Encoder decoder model)"),que.forEach(t),N6e=i(T),tf=n(T,"LI",{});var zue=s(tf);fG=n(zue,"STRONG",{});var l1r=s(fG);j6e=r(l1r,"flaubert"),l1r.forEach(t),O6e=r(zue," \u2014 "),K0=n(zue,"A",{href:!0});var i1r=s(K0);G6e=r(i1r,"FlaubertConfig"),i1r.forEach(t),q6e=r(zue," (FlauBERT model)"),zue.forEach(t),z6e=i(T),af=n(T,"LI",{});var Xue=s(af);mG=n(Xue,"STRONG",{});var d1r=s(mG);X6e=r(d1r,"fnet"),d1r.forEach(t),Q6e=r(Xue," \u2014 "),Y0=n(Xue,"A",{href:!0});var c1r=s(Y0);V6e=r(c1r,"FNetConfig"),c1r.forEach(t),W6e=r(Xue," (FNet model)"),Xue.forEach(t),H6e=i(T),nf=n(T,"LI",{});var Que=s(nf);hG=n(Que,"STRONG",{});var f1r=s(hG);U6e=r(f1r,"fsmt"),f1r.forEach(t),J6e=r(Que," \u2014 "),Z0=n(Que,"A",{href:!0});var m1r=s(Z0);K6e=r(m1r,"FSMTConfig"),m1r.forEach(t),Y6e=r(Que," (FairSeq Machine-Translation model)"),Que.forEach(t),Z6e=i(T),sf=n(T,"LI",{});var Vue=s(sf);gG=n(Vue,"STRONG",{});var h1r=s(gG);eLe=r(h1r,"funnel"),h1r.forEach(t),oLe=r(Vue," \u2014 "),eA=n(Vue,"A",{href:!0});var g1r=s(eA);rLe=r(g1r,"FunnelConfig"),g1r.forEach(t),tLe=r(Vue," (Funnel Transformer model)"),Vue.forEach(t),aLe=i(T),lf=n(T,"LI",{});var Wue=s(lf);uG=n(Wue,"STRONG",{});var u1r=s(uG);nLe=r(u1r,"gpt2"),u1r.forEach(t),sLe=r(Wue," \u2014 "),oA=n(Wue,"A",{href:!0});var p1r=s(oA);lLe=r(p1r,"GPT2Config"),p1r.forEach(t),iLe=r(Wue," (OpenAI GPT-2 model)"),Wue.forEach(t),dLe=i(T),df=n(T,"LI",{});var Hue=s(df);pG=n(Hue,"STRONG",{});var _1r=s(pG);cLe=r(_1r,"gpt_neo"),_1r.forEach(t),fLe=r(Hue," \u2014 "),rA=n(Hue,"A",{href:!0});var v1r=s(rA);mLe=r(v1r,"GPTNeoConfig"),v1r.forEach(t),hLe=r(Hue," (GPT Neo model)"),Hue.forEach(t),gLe=i(T),cf=n(T,"LI",{});var Uue=s(cf);_G=n(Uue,"STRONG",{});var b1r=s(_G);uLe=r(b1r,"gptj"),b1r.forEach(t),pLe=r(Uue," \u2014 "),tA=n(Uue,"A",{href:!0});var T1r=s(tA);_Le=r(T1r,"GPTJConfig"),T1r.forEach(t),vLe=r(Uue," (GPT-J model)"),Uue.forEach(t),bLe=i(T),ff=n(T,"LI",{});var Jue=s(ff);vG=n(Jue,"STRONG",{});var F1r=s(vG);TLe=r(F1r,"hubert"),F1r.forEach(t),FLe=r(Jue," \u2014 "),aA=n(Jue,"A",{href:!0});var E1r=s(aA);ELe=r(E1r,"HubertConfig"),E1r.forEach(t),CLe=r(Jue," (Hubert model)"),Jue.forEach(t),MLe=i(T),mf=n(T,"LI",{});var Kue=s(mf);bG=n(Kue,"STRONG",{});var C1r=s(bG);yLe=r(C1r,"ibert"),C1r.forEach(t),wLe=r(Kue," \u2014 "),nA=n(Kue,"A",{href:!0});var M1r=s(nA);ALe=r(M1r,"IBertConfig"),M1r.forEach(t),LLe=r(Kue," (I-BERT model)"),Kue.forEach(t),BLe=i(T),hf=n(T,"LI",{});var Yue=s(hf);TG=n(Yue,"STRONG",{});var y1r=s(TG);xLe=r(y1r,"imagegpt"),y1r.forEach(t),kLe=r(Yue," \u2014 "),sA=n(Yue,"A",{href:!0});var w1r=s(sA);RLe=r(w1r,"ImageGPTConfig"),w1r.forEach(t),PLe=r(Yue," (ImageGPT model)"),Yue.forEach(t),SLe=i(T),gf=n(T,"LI",{});var Zue=s(gf);FG=n(Zue,"STRONG",{});var A1r=s(FG);$Le=r(A1r,"layoutlm"),A1r.forEach(t),ILe=r(Zue," \u2014 "),lA=n(Zue,"A",{href:!0});var L1r=s(lA);DLe=r(L1r,"LayoutLMConfig"),L1r.forEach(t),NLe=r(Zue," (LayoutLM model)"),Zue.forEach(t),jLe=i(T),uf=n(T,"LI",{});var epe=s(uf);EG=n(epe,"STRONG",{});var B1r=s(EG);OLe=r(B1r,"layoutlmv2"),B1r.forEach(t),GLe=r(epe," \u2014 "),iA=n(epe,"A",{href:!0});var x1r=s(iA);qLe=r(x1r,"LayoutLMv2Config"),x1r.forEach(t),zLe=r(epe," (LayoutLMv2 model)"),epe.forEach(t),XLe=i(T),pf=n(T,"LI",{});var ope=s(pf);CG=n(ope,"STRONG",{});var k1r=s(CG);QLe=r(k1r,"led"),k1r.forEach(t),VLe=r(ope," \u2014 "),dA=n(ope,"A",{href:!0});var R1r=s(dA);WLe=r(R1r,"LEDConfig"),R1r.forEach(t),HLe=r(ope," (LED model)"),ope.forEach(t),ULe=i(T),_f=n(T,"LI",{});var rpe=s(_f);MG=n(rpe,"STRONG",{});var P1r=s(MG);JLe=r(P1r,"longformer"),P1r.forEach(t),KLe=r(rpe," \u2014 "),cA=n(rpe,"A",{href:!0});var S1r=s(cA);YLe=r(S1r,"LongformerConfig"),S1r.forEach(t),ZLe=r(rpe," (Longformer model)"),rpe.forEach(t),e8e=i(T),vf=n(T,"LI",{});var tpe=s(vf);yG=n(tpe,"STRONG",{});var $1r=s(yG);o8e=r($1r,"luke"),$1r.forEach(t),r8e=r(tpe," \u2014 "),fA=n(tpe,"A",{href:!0});var I1r=s(fA);t8e=r(I1r,"LukeConfig"),I1r.forEach(t),a8e=r(tpe," (LUKE model)"),tpe.forEach(t),n8e=i(T),bf=n(T,"LI",{});var ape=s(bf);wG=n(ape,"STRONG",{});var D1r=s(wG);s8e=r(D1r,"lxmert"),D1r.forEach(t),l8e=r(ape," \u2014 "),mA=n(ape,"A",{href:!0});var N1r=s(mA);i8e=r(N1r,"LxmertConfig"),N1r.forEach(t),d8e=r(ape," (LXMERT model)"),ape.forEach(t),c8e=i(T),Tf=n(T,"LI",{});var npe=s(Tf);AG=n(npe,"STRONG",{});var j1r=s(AG);f8e=r(j1r,"m2m_100"),j1r.forEach(t),m8e=r(npe," \u2014 "),hA=n(npe,"A",{href:!0});var O1r=s(hA);h8e=r(O1r,"M2M100Config"),O1r.forEach(t),g8e=r(npe," (M2M100 model)"),npe.forEach(t),u8e=i(T),Ff=n(T,"LI",{});var spe=s(Ff);LG=n(spe,"STRONG",{});var G1r=s(LG);p8e=r(G1r,"marian"),G1r.forEach(t),_8e=r(spe," \u2014 "),gA=n(spe,"A",{href:!0});var q1r=s(gA);v8e=r(q1r,"MarianConfig"),q1r.forEach(t),b8e=r(spe," (Marian model)"),spe.forEach(t),T8e=i(T),Ef=n(T,"LI",{});var lpe=s(Ef);BG=n(lpe,"STRONG",{});var z1r=s(BG);F8e=r(z1r,"mbart"),z1r.forEach(t),E8e=r(lpe," \u2014 "),uA=n(lpe,"A",{href:!0});var X1r=s(uA);C8e=r(X1r,"MBartConfig"),X1r.forEach(t),M8e=r(lpe," (mBART model)"),lpe.forEach(t),y8e=i(T),Cf=n(T,"LI",{});var ipe=s(Cf);xG=n(ipe,"STRONG",{});var Q1r=s(xG);w8e=r(Q1r,"megatron-bert"),Q1r.forEach(t),A8e=r(ipe," \u2014 "),pA=n(ipe,"A",{href:!0});var V1r=s(pA);L8e=r(V1r,"MegatronBertConfig"),V1r.forEach(t),B8e=r(ipe," (MegatronBert model)"),ipe.forEach(t),x8e=i(T),Mf=n(T,"LI",{});var dpe=s(Mf);kG=n(dpe,"STRONG",{});var W1r=s(kG);k8e=r(W1r,"mobilebert"),W1r.forEach(t),R8e=r(dpe," \u2014 "),_A=n(dpe,"A",{href:!0});var H1r=s(_A);P8e=r(H1r,"MobileBertConfig"),H1r.forEach(t),S8e=r(dpe," (MobileBERT model)"),dpe.forEach(t),$8e=i(T),yf=n(T,"LI",{});var cpe=s(yf);RG=n(cpe,"STRONG",{});var U1r=s(RG);I8e=r(U1r,"mpnet"),U1r.forEach(t),D8e=r(cpe," \u2014 "),vA=n(cpe,"A",{href:!0});var J1r=s(vA);N8e=r(J1r,"MPNetConfig"),J1r.forEach(t),j8e=r(cpe," (MPNet model)"),cpe.forEach(t),O8e=i(T),wf=n(T,"LI",{});var fpe=s(wf);PG=n(fpe,"STRONG",{});var K1r=s(PG);G8e=r(K1r,"mt5"),K1r.forEach(t),q8e=r(fpe," \u2014 "),bA=n(fpe,"A",{href:!0});var Y1r=s(bA);z8e=r(Y1r,"MT5Config"),Y1r.forEach(t),X8e=r(fpe," (mT5 model)"),fpe.forEach(t),Q8e=i(T),Af=n(T,"LI",{});var mpe=s(Af);SG=n(mpe,"STRONG",{});var Z1r=s(SG);V8e=r(Z1r,"openai-gpt"),Z1r.forEach(t),W8e=r(mpe," \u2014 "),TA=n(mpe,"A",{href:!0});var e4r=s(TA);H8e=r(e4r,"OpenAIGPTConfig"),e4r.forEach(t),U8e=r(mpe," (OpenAI GPT model)"),mpe.forEach(t),J8e=i(T),Lf=n(T,"LI",{});var hpe=s(Lf);$G=n(hpe,"STRONG",{});var o4r=s($G);K8e=r(o4r,"pegasus"),o4r.forEach(t),Y8e=r(hpe," \u2014 "),FA=n(hpe,"A",{href:!0});var r4r=s(FA);Z8e=r(r4r,"PegasusConfig"),r4r.forEach(t),eBe=r(hpe," (Pegasus model)"),hpe.forEach(t),oBe=i(T),Bf=n(T,"LI",{});var gpe=s(Bf);IG=n(gpe,"STRONG",{});var t4r=s(IG);rBe=r(t4r,"perceiver"),t4r.forEach(t),tBe=r(gpe," \u2014 "),EA=n(gpe,"A",{href:!0});var a4r=s(EA);aBe=r(a4r,"PerceiverConfig"),a4r.forEach(t),nBe=r(gpe," (Perceiver model)"),gpe.forEach(t),sBe=i(T),xf=n(T,"LI",{});var upe=s(xf);DG=n(upe,"STRONG",{});var n4r=s(DG);lBe=r(n4r,"prophetnet"),n4r.forEach(t),iBe=r(upe," \u2014 "),CA=n(upe,"A",{href:!0});var s4r=s(CA);dBe=r(s4r,"ProphetNetConfig"),s4r.forEach(t),cBe=r(upe," (ProphetNet model)"),upe.forEach(t),fBe=i(T),kf=n(T,"LI",{});var ppe=s(kf);NG=n(ppe,"STRONG",{});var l4r=s(NG);mBe=r(l4r,"qdqbert"),l4r.forEach(t),hBe=r(ppe," \u2014 "),MA=n(ppe,"A",{href:!0});var i4r=s(MA);gBe=r(i4r,"QDQBertConfig"),i4r.forEach(t),uBe=r(ppe," (QDQBert model)"),ppe.forEach(t),pBe=i(T),Rf=n(T,"LI",{});var _pe=s(Rf);jG=n(_pe,"STRONG",{});var d4r=s(jG);_Be=r(d4r,"rag"),d4r.forEach(t),vBe=r(_pe," \u2014 "),yA=n(_pe,"A",{href:!0});var c4r=s(yA);bBe=r(c4r,"RagConfig"),c4r.forEach(t),TBe=r(_pe," (RAG model)"),_pe.forEach(t),FBe=i(T),Pf=n(T,"LI",{});var vpe=s(Pf);OG=n(vpe,"STRONG",{});var f4r=s(OG);EBe=r(f4r,"reformer"),f4r.forEach(t),CBe=r(vpe," \u2014 "),wA=n(vpe,"A",{href:!0});var m4r=s(wA);MBe=r(m4r,"ReformerConfig"),m4r.forEach(t),yBe=r(vpe," (Reformer model)"),vpe.forEach(t),wBe=i(T),Sf=n(T,"LI",{});var bpe=s(Sf);GG=n(bpe,"STRONG",{});var h4r=s(GG);ABe=r(h4r,"rembert"),h4r.forEach(t),LBe=r(bpe," \u2014 "),AA=n(bpe,"A",{href:!0});var g4r=s(AA);BBe=r(g4r,"RemBertConfig"),g4r.forEach(t),xBe=r(bpe," (RemBERT model)"),bpe.forEach(t),kBe=i(T),$f=n(T,"LI",{});var Tpe=s($f);qG=n(Tpe,"STRONG",{});var u4r=s(qG);RBe=r(u4r,"retribert"),u4r.forEach(t),PBe=r(Tpe," \u2014 "),LA=n(Tpe,"A",{href:!0});var p4r=s(LA);SBe=r(p4r,"RetriBertConfig"),p4r.forEach(t),$Be=r(Tpe," (RetriBERT model)"),Tpe.forEach(t),IBe=i(T),If=n(T,"LI",{});var Fpe=s(If);zG=n(Fpe,"STRONG",{});var _4r=s(zG);DBe=r(_4r,"roberta"),_4r.forEach(t),NBe=r(Fpe," \u2014 "),BA=n(Fpe,"A",{href:!0});var v4r=s(BA);jBe=r(v4r,"RobertaConfig"),v4r.forEach(t),OBe=r(Fpe," (RoBERTa model)"),Fpe.forEach(t),GBe=i(T),Df=n(T,"LI",{});var Epe=s(Df);XG=n(Epe,"STRONG",{});var b4r=s(XG);qBe=r(b4r,"roformer"),b4r.forEach(t),zBe=r(Epe," \u2014 "),xA=n(Epe,"A",{href:!0});var T4r=s(xA);XBe=r(T4r,"RoFormerConfig"),T4r.forEach(t),QBe=r(Epe," (RoFormer model)"),Epe.forEach(t),VBe=i(T),Nf=n(T,"LI",{});var Cpe=s(Nf);QG=n(Cpe,"STRONG",{});var F4r=s(QG);WBe=r(F4r,"segformer"),F4r.forEach(t),HBe=r(Cpe," \u2014 "),kA=n(Cpe,"A",{href:!0});var E4r=s(kA);UBe=r(E4r,"SegformerConfig"),E4r.forEach(t),JBe=r(Cpe," (SegFormer model)"),Cpe.forEach(t),KBe=i(T),jf=n(T,"LI",{});var Mpe=s(jf);VG=n(Mpe,"STRONG",{});var C4r=s(VG);YBe=r(C4r,"sew"),C4r.forEach(t),ZBe=r(Mpe," \u2014 "),RA=n(Mpe,"A",{href:!0});var M4r=s(RA);e9e=r(M4r,"SEWConfig"),M4r.forEach(t),o9e=r(Mpe," (SEW model)"),Mpe.forEach(t),r9e=i(T),Of=n(T,"LI",{});var ype=s(Of);WG=n(ype,"STRONG",{});var y4r=s(WG);t9e=r(y4r,"sew-d"),y4r.forEach(t),a9e=r(ype," \u2014 "),PA=n(ype,"A",{href:!0});var w4r=s(PA);n9e=r(w4r,"SEWDConfig"),w4r.forEach(t),s9e=r(ype," (SEW-D model)"),ype.forEach(t),l9e=i(T),Gf=n(T,"LI",{});var wpe=s(Gf);HG=n(wpe,"STRONG",{});var A4r=s(HG);i9e=r(A4r,"speech-encoder-decoder"),A4r.forEach(t),d9e=r(wpe," \u2014 "),SA=n(wpe,"A",{href:!0});var L4r=s(SA);c9e=r(L4r,"SpeechEncoderDecoderConfig"),L4r.forEach(t),f9e=r(wpe," (Speech Encoder decoder model)"),wpe.forEach(t),m9e=i(T),qf=n(T,"LI",{});var Ape=s(qf);UG=n(Ape,"STRONG",{});var B4r=s(UG);h9e=r(B4r,"speech_to_text"),B4r.forEach(t),g9e=r(Ape," \u2014 "),$A=n(Ape,"A",{href:!0});var x4r=s($A);u9e=r(x4r,"Speech2TextConfig"),x4r.forEach(t),p9e=r(Ape," (Speech2Text model)"),Ape.forEach(t),_9e=i(T),zf=n(T,"LI",{});var Lpe=s(zf);JG=n(Lpe,"STRONG",{});var k4r=s(JG);v9e=r(k4r,"speech_to_text_2"),k4r.forEach(t),b9e=r(Lpe," \u2014 "),IA=n(Lpe,"A",{href:!0});var R4r=s(IA);T9e=r(R4r,"Speech2Text2Config"),R4r.forEach(t),F9e=r(Lpe," (Speech2Text2 model)"),Lpe.forEach(t),E9e=i(T),Xf=n(T,"LI",{});var Bpe=s(Xf);KG=n(Bpe,"STRONG",{});var P4r=s(KG);C9e=r(P4r,"splinter"),P4r.forEach(t),M9e=r(Bpe," \u2014 "),DA=n(Bpe,"A",{href:!0});var S4r=s(DA);y9e=r(S4r,"SplinterConfig"),S4r.forEach(t),w9e=r(Bpe," (Splinter model)"),Bpe.forEach(t),A9e=i(T),Qf=n(T,"LI",{});var xpe=s(Qf);YG=n(xpe,"STRONG",{});var $4r=s(YG);L9e=r($4r,"squeezebert"),$4r.forEach(t),B9e=r(xpe," \u2014 "),NA=n(xpe,"A",{href:!0});var I4r=s(NA);x9e=r(I4r,"SqueezeBertConfig"),I4r.forEach(t),k9e=r(xpe," (SqueezeBERT model)"),xpe.forEach(t),R9e=i(T),Vf=n(T,"LI",{});var kpe=s(Vf);ZG=n(kpe,"STRONG",{});var D4r=s(ZG);P9e=r(D4r,"t5"),D4r.forEach(t),S9e=r(kpe," \u2014 "),jA=n(kpe,"A",{href:!0});var N4r=s(jA);$9e=r(N4r,"T5Config"),N4r.forEach(t),I9e=r(kpe," (T5 model)"),kpe.forEach(t),D9e=i(T),Wf=n(T,"LI",{});var Rpe=s(Wf);eq=n(Rpe,"STRONG",{});var j4r=s(eq);N9e=r(j4r,"tapas"),j4r.forEach(t),j9e=r(Rpe," \u2014 "),OA=n(Rpe,"A",{href:!0});var O4r=s(OA);O9e=r(O4r,"TapasConfig"),O4r.forEach(t),G9e=r(Rpe," (TAPAS model)"),Rpe.forEach(t),q9e=i(T),Hf=n(T,"LI",{});var Ppe=s(Hf);oq=n(Ppe,"STRONG",{});var G4r=s(oq);z9e=r(G4r,"transfo-xl"),G4r.forEach(t),X9e=r(Ppe," \u2014 "),GA=n(Ppe,"A",{href:!0});var q4r=s(GA);Q9e=r(q4r,"TransfoXLConfig"),q4r.forEach(t),V9e=r(Ppe," (Transformer-XL model)"),Ppe.forEach(t),W9e=i(T),Uf=n(T,"LI",{});var Spe=s(Uf);rq=n(Spe,"STRONG",{});var z4r=s(rq);H9e=r(z4r,"trocr"),z4r.forEach(t),U9e=r(Spe," \u2014 "),qA=n(Spe,"A",{href:!0});var X4r=s(qA);J9e=r(X4r,"TrOCRConfig"),X4r.forEach(t),K9e=r(Spe," (TrOCR model)"),Spe.forEach(t),Y9e=i(T),Jf=n(T,"LI",{});var $pe=s(Jf);tq=n($pe,"STRONG",{});var Q4r=s(tq);Z9e=r(Q4r,"unispeech"),Q4r.forEach(t),exe=r($pe," \u2014 "),zA=n($pe,"A",{href:!0});var V4r=s(zA);oxe=r(V4r,"UniSpeechConfig"),V4r.forEach(t),rxe=r($pe," (UniSpeech model)"),$pe.forEach(t),txe=i(T),Kf=n(T,"LI",{});var Ipe=s(Kf);aq=n(Ipe,"STRONG",{});var W4r=s(aq);axe=r(W4r,"unispeech-sat"),W4r.forEach(t),nxe=r(Ipe," \u2014 "),XA=n(Ipe,"A",{href:!0});var H4r=s(XA);sxe=r(H4r,"UniSpeechSatConfig"),H4r.forEach(t),lxe=r(Ipe," (UniSpeechSat model)"),Ipe.forEach(t),ixe=i(T),Yf=n(T,"LI",{});var Dpe=s(Yf);nq=n(Dpe,"STRONG",{});var U4r=s(nq);dxe=r(U4r,"vision-encoder-decoder"),U4r.forEach(t),cxe=r(Dpe," \u2014 "),QA=n(Dpe,"A",{href:!0});var J4r=s(QA);fxe=r(J4r,"VisionEncoderDecoderConfig"),J4r.forEach(t),mxe=r(Dpe," (Vision Encoder decoder model)"),Dpe.forEach(t),hxe=i(T),Zf=n(T,"LI",{});var Npe=s(Zf);sq=n(Npe,"STRONG",{});var K4r=s(sq);gxe=r(K4r,"vision-text-dual-encoder"),K4r.forEach(t),uxe=r(Npe," \u2014 "),VA=n(Npe,"A",{href:!0});var Y4r=s(VA);pxe=r(Y4r,"VisionTextDualEncoderConfig"),Y4r.forEach(t),_xe=r(Npe," (VisionTextDualEncoder model)"),Npe.forEach(t),vxe=i(T),em=n(T,"LI",{});var jpe=s(em);lq=n(jpe,"STRONG",{});var Z4r=s(lq);bxe=r(Z4r,"visual_bert"),Z4r.forEach(t),Txe=r(jpe," \u2014 "),WA=n(jpe,"A",{href:!0});var evr=s(WA);Fxe=r(evr,"VisualBertConfig"),evr.forEach(t),Exe=r(jpe," (VisualBert model)"),jpe.forEach(t),Cxe=i(T),om=n(T,"LI",{});var Ope=s(om);iq=n(Ope,"STRONG",{});var ovr=s(iq);Mxe=r(ovr,"vit"),ovr.forEach(t),yxe=r(Ope," \u2014 "),HA=n(Ope,"A",{href:!0});var rvr=s(HA);wxe=r(rvr,"ViTConfig"),rvr.forEach(t),Axe=r(Ope," (ViT model)"),Ope.forEach(t),Lxe=i(T),rm=n(T,"LI",{});var Gpe=s(rm);dq=n(Gpe,"STRONG",{});var tvr=s(dq);Bxe=r(tvr,"wav2vec2"),tvr.forEach(t),xxe=r(Gpe," \u2014 "),UA=n(Gpe,"A",{href:!0});var avr=s(UA);kxe=r(avr,"Wav2Vec2Config"),avr.forEach(t),Rxe=r(Gpe," (Wav2Vec2 model)"),Gpe.forEach(t),Pxe=i(T),tm=n(T,"LI",{});var qpe=s(tm);cq=n(qpe,"STRONG",{});var nvr=s(cq);Sxe=r(nvr,"xlm"),nvr.forEach(t),$xe=r(qpe," \u2014 "),JA=n(qpe,"A",{href:!0});var svr=s(JA);Ixe=r(svr,"XLMConfig"),svr.forEach(t),Dxe=r(qpe," (XLM model)"),qpe.forEach(t),Nxe=i(T),am=n(T,"LI",{});var zpe=s(am);fq=n(zpe,"STRONG",{});var lvr=s(fq);jxe=r(lvr,"xlm-prophetnet"),lvr.forEach(t),Oxe=r(zpe," \u2014 "),KA=n(zpe,"A",{href:!0});var ivr=s(KA);Gxe=r(ivr,"XLMProphetNetConfig"),ivr.forEach(t),qxe=r(zpe," (XLMProphetNet model)"),zpe.forEach(t),zxe=i(T),nm=n(T,"LI",{});var Xpe=s(nm);mq=n(Xpe,"STRONG",{});var dvr=s(mq);Xxe=r(dvr,"xlm-roberta"),dvr.forEach(t),Qxe=r(Xpe," \u2014 "),YA=n(Xpe,"A",{href:!0});var cvr=s(YA);Vxe=r(cvr,"XLMRobertaConfig"),cvr.forEach(t),Wxe=r(Xpe," (XLM-RoBERTa model)"),Xpe.forEach(t),Hxe=i(T),sm=n(T,"LI",{});var Qpe=s(sm);hq=n(Qpe,"STRONG",{});var fvr=s(hq);Uxe=r(fvr,"xlnet"),fvr.forEach(t),Jxe=r(Qpe," \u2014 "),ZA=n(Qpe,"A",{href:!0});var mvr=s(ZA);Kxe=r(mvr,"XLNetConfig"),mvr.forEach(t),Yxe=r(Qpe," (XLNet model)"),Qpe.forEach(t),T.forEach(t),Zxe=i(qt),gq=n(qt,"P",{});var hvr=s(gq);eke=r(hvr,"Examples:"),hvr.forEach(t),oke=i(qt),m(BC.$$.fragment,qt),qt.forEach(t),rke=i(fs),lm=n(fs,"DIV",{class:!0});var lwe=s(lm);m(xC.$$.fragment,lwe),tke=i(lwe),uq=n(lwe,"P",{});var gvr=s(uq);ake=r(gvr,"Register a new configuration for this class."),gvr.forEach(t),lwe.forEach(t),fs.forEach(t),b5e=i(c),Zl=n(c,"H2",{class:!0});var iwe=s(Zl);im=n(iwe,"A",{id:!0,class:!0,href:!0});var uvr=s(im);pq=n(uvr,"SPAN",{});var pvr=s(pq);m(kC.$$.fragment,pvr),pvr.forEach(t),uvr.forEach(t),nke=i(iwe),_q=n(iwe,"SPAN",{});var _vr=s(_q);ske=r(_vr,"AutoTokenizer"),_vr.forEach(t),iwe.forEach(t),T5e=i(c),No=n(c,"DIV",{class:!0});var ms=s(No);m(RC.$$.fragment,ms),lke=i(ms),PC=n(ms,"P",{});var dwe=s(PC);ike=r(dwe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),e6=n(dwe,"A",{href:!0});var vvr=s(e6);dke=r(vvr,"AutoTokenizer.from_pretrained()"),vvr.forEach(t),cke=r(dwe," class method."),dwe.forEach(t),fke=i(ms),SC=n(ms,"P",{});var cwe=s(SC);mke=r(cwe,"This class cannot be instantiated directly using "),vq=n(cwe,"CODE",{});var bvr=s(vq);hke=r(bvr,"__init__()"),bvr.forEach(t),gke=r(cwe," (throws an error)."),cwe.forEach(t),uke=i(ms),ye=n(ms,"DIV",{class:!0});var ko=s(ye);m($C.$$.fragment,ko),pke=i(ko),bq=n(ko,"P",{});var Tvr=s(bq);_ke=r(Tvr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Tvr.forEach(t),vke=i(ko),ba=n(ko,"P",{});var EE=s(ba);bke=r(EE,"The tokenizer class to instantiate is selected based on the "),Tq=n(EE,"CODE",{});var Fvr=s(Tq);Tke=r(Fvr,"model_type"),Fvr.forEach(t),Fke=r(EE,` property of the config object
(either passed as an argument or loaded from `),Fq=n(EE,"CODE",{});var Evr=s(Fq);Eke=r(Evr,"pretrained_model_name_or_path"),Evr.forEach(t),Cke=r(EE,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),Eq=n(EE,"CODE",{});var Cvr=s(Eq);Mke=r(Cvr,"pretrained_model_name_or_path"),Cvr.forEach(t),yke=r(EE,":"),EE.forEach(t),wke=i(ko),C=n(ko,"UL",{});var M=s(C);bn=n(M,"LI",{});var x7=s(bn);Cq=n(x7,"STRONG",{});var Mvr=s(Cq);Ake=r(Mvr,"albert"),Mvr.forEach(t),Lke=r(x7," \u2014 "),o6=n(x7,"A",{href:!0});var yvr=s(o6);Bke=r(yvr,"AlbertTokenizer"),yvr.forEach(t),xke=r(x7," or "),r6=n(x7,"A",{href:!0});var wvr=s(r6);kke=r(wvr,"AlbertTokenizerFast"),wvr.forEach(t),Rke=r(x7," (ALBERT model)"),x7.forEach(t),Pke=i(M),Tn=n(M,"LI",{});var k7=s(Tn);Mq=n(k7,"STRONG",{});var Avr=s(Mq);Ske=r(Avr,"bart"),Avr.forEach(t),$ke=r(k7," \u2014 "),t6=n(k7,"A",{href:!0});var Lvr=s(t6);Ike=r(Lvr,"BartTokenizer"),Lvr.forEach(t),Dke=r(k7," or "),a6=n(k7,"A",{href:!0});var Bvr=s(a6);Nke=r(Bvr,"BartTokenizerFast"),Bvr.forEach(t),jke=r(k7," (BART model)"),k7.forEach(t),Oke=i(M),Fn=n(M,"LI",{});var R7=s(Fn);yq=n(R7,"STRONG",{});var xvr=s(yq);Gke=r(xvr,"barthez"),xvr.forEach(t),qke=r(R7," \u2014 "),n6=n(R7,"A",{href:!0});var kvr=s(n6);zke=r(kvr,"BarthezTokenizer"),kvr.forEach(t),Xke=r(R7," or "),s6=n(R7,"A",{href:!0});var Rvr=s(s6);Qke=r(Rvr,"BarthezTokenizerFast"),Rvr.forEach(t),Vke=r(R7," (BARThez model)"),R7.forEach(t),Wke=i(M),dm=n(M,"LI",{});var Vpe=s(dm);wq=n(Vpe,"STRONG",{});var Pvr=s(wq);Hke=r(Pvr,"bartpho"),Pvr.forEach(t),Uke=r(Vpe," \u2014 "),l6=n(Vpe,"A",{href:!0});var Svr=s(l6);Jke=r(Svr,"BartphoTokenizer"),Svr.forEach(t),Kke=r(Vpe," (BARTpho model)"),Vpe.forEach(t),Yke=i(M),En=n(M,"LI",{});var P7=s(En);Aq=n(P7,"STRONG",{});var $vr=s(Aq);Zke=r($vr,"bert"),$vr.forEach(t),eRe=r(P7," \u2014 "),i6=n(P7,"A",{href:!0});var Ivr=s(i6);oRe=r(Ivr,"BertTokenizer"),Ivr.forEach(t),rRe=r(P7," or "),d6=n(P7,"A",{href:!0});var Dvr=s(d6);tRe=r(Dvr,"BertTokenizerFast"),Dvr.forEach(t),aRe=r(P7," (BERT model)"),P7.forEach(t),nRe=i(M),cm=n(M,"LI",{});var Wpe=s(cm);Lq=n(Wpe,"STRONG",{});var Nvr=s(Lq);sRe=r(Nvr,"bert-generation"),Nvr.forEach(t),lRe=r(Wpe," \u2014 "),c6=n(Wpe,"A",{href:!0});var jvr=s(c6);iRe=r(jvr,"BertGenerationTokenizer"),jvr.forEach(t),dRe=r(Wpe," (Bert Generation model)"),Wpe.forEach(t),cRe=i(M),fm=n(M,"LI",{});var Hpe=s(fm);Bq=n(Hpe,"STRONG",{});var Ovr=s(Bq);fRe=r(Ovr,"bert-japanese"),Ovr.forEach(t),mRe=r(Hpe," \u2014 "),f6=n(Hpe,"A",{href:!0});var Gvr=s(f6);hRe=r(Gvr,"BertJapaneseTokenizer"),Gvr.forEach(t),gRe=r(Hpe," (BertJapanese model)"),Hpe.forEach(t),uRe=i(M),mm=n(M,"LI",{});var Upe=s(mm);xq=n(Upe,"STRONG",{});var qvr=s(xq);pRe=r(qvr,"bertweet"),qvr.forEach(t),_Re=r(Upe," \u2014 "),m6=n(Upe,"A",{href:!0});var zvr=s(m6);vRe=r(zvr,"BertweetTokenizer"),zvr.forEach(t),bRe=r(Upe," (Bertweet model)"),Upe.forEach(t),TRe=i(M),Cn=n(M,"LI",{});var S7=s(Cn);kq=n(S7,"STRONG",{});var Xvr=s(kq);FRe=r(Xvr,"big_bird"),Xvr.forEach(t),ERe=r(S7," \u2014 "),h6=n(S7,"A",{href:!0});var Qvr=s(h6);CRe=r(Qvr,"BigBirdTokenizer"),Qvr.forEach(t),MRe=r(S7," or "),g6=n(S7,"A",{href:!0});var Vvr=s(g6);yRe=r(Vvr,"BigBirdTokenizerFast"),Vvr.forEach(t),wRe=r(S7," (BigBird model)"),S7.forEach(t),ARe=i(M),Mn=n(M,"LI",{});var $7=s(Mn);Rq=n($7,"STRONG",{});var Wvr=s(Rq);LRe=r(Wvr,"bigbird_pegasus"),Wvr.forEach(t),BRe=r($7," \u2014 "),u6=n($7,"A",{href:!0});var Hvr=s(u6);xRe=r(Hvr,"PegasusTokenizer"),Hvr.forEach(t),kRe=r($7," or "),p6=n($7,"A",{href:!0});var Uvr=s(p6);RRe=r(Uvr,"PegasusTokenizerFast"),Uvr.forEach(t),PRe=r($7," (BigBirdPegasus model)"),$7.forEach(t),SRe=i(M),yn=n(M,"LI",{});var I7=s(yn);Pq=n(I7,"STRONG",{});var Jvr=s(Pq);$Re=r(Jvr,"blenderbot"),Jvr.forEach(t),IRe=r(I7," \u2014 "),_6=n(I7,"A",{href:!0});var Kvr=s(_6);DRe=r(Kvr,"BlenderbotTokenizer"),Kvr.forEach(t),NRe=r(I7," or "),v6=n(I7,"A",{href:!0});var Yvr=s(v6);jRe=r(Yvr,"BlenderbotTokenizerFast"),Yvr.forEach(t),ORe=r(I7," (Blenderbot model)"),I7.forEach(t),GRe=i(M),hm=n(M,"LI",{});var Jpe=s(hm);Sq=n(Jpe,"STRONG",{});var Zvr=s(Sq);qRe=r(Zvr,"blenderbot-small"),Zvr.forEach(t),zRe=r(Jpe," \u2014 "),b6=n(Jpe,"A",{href:!0});var ebr=s(b6);XRe=r(ebr,"BlenderbotSmallTokenizer"),ebr.forEach(t),QRe=r(Jpe," (BlenderbotSmall model)"),Jpe.forEach(t),VRe=i(M),gm=n(M,"LI",{});var Kpe=s(gm);$q=n(Kpe,"STRONG",{});var obr=s($q);WRe=r(obr,"byt5"),obr.forEach(t),HRe=r(Kpe," \u2014 "),T6=n(Kpe,"A",{href:!0});var rbr=s(T6);URe=r(rbr,"ByT5Tokenizer"),rbr.forEach(t),JRe=r(Kpe," (ByT5 model)"),Kpe.forEach(t),KRe=i(M),wn=n(M,"LI",{});var D7=s(wn);Iq=n(D7,"STRONG",{});var tbr=s(Iq);YRe=r(tbr,"camembert"),tbr.forEach(t),ZRe=r(D7," \u2014 "),F6=n(D7,"A",{href:!0});var abr=s(F6);ePe=r(abr,"CamembertTokenizer"),abr.forEach(t),oPe=r(D7," or "),E6=n(D7,"A",{href:!0});var nbr=s(E6);rPe=r(nbr,"CamembertTokenizerFast"),nbr.forEach(t),tPe=r(D7," (CamemBERT model)"),D7.forEach(t),aPe=i(M),um=n(M,"LI",{});var Ype=s(um);Dq=n(Ype,"STRONG",{});var sbr=s(Dq);nPe=r(sbr,"canine"),sbr.forEach(t),sPe=r(Ype," \u2014 "),C6=n(Ype,"A",{href:!0});var lbr=s(C6);lPe=r(lbr,"CanineTokenizer"),lbr.forEach(t),iPe=r(Ype," (Canine model)"),Ype.forEach(t),dPe=i(M),An=n(M,"LI",{});var N7=s(An);Nq=n(N7,"STRONG",{});var ibr=s(Nq);cPe=r(ibr,"clip"),ibr.forEach(t),fPe=r(N7," \u2014 "),M6=n(N7,"A",{href:!0});var dbr=s(M6);mPe=r(dbr,"CLIPTokenizer"),dbr.forEach(t),hPe=r(N7," or "),y6=n(N7,"A",{href:!0});var cbr=s(y6);gPe=r(cbr,"CLIPTokenizerFast"),cbr.forEach(t),uPe=r(N7," (CLIP model)"),N7.forEach(t),pPe=i(M),Ln=n(M,"LI",{});var j7=s(Ln);jq=n(j7,"STRONG",{});var fbr=s(jq);_Pe=r(fbr,"convbert"),fbr.forEach(t),vPe=r(j7," \u2014 "),w6=n(j7,"A",{href:!0});var mbr=s(w6);bPe=r(mbr,"ConvBertTokenizer"),mbr.forEach(t),TPe=r(j7," or "),A6=n(j7,"A",{href:!0});var hbr=s(A6);FPe=r(hbr,"ConvBertTokenizerFast"),hbr.forEach(t),EPe=r(j7," (ConvBERT model)"),j7.forEach(t),CPe=i(M),Bn=n(M,"LI",{});var O7=s(Bn);Oq=n(O7,"STRONG",{});var gbr=s(Oq);MPe=r(gbr,"cpm"),gbr.forEach(t),yPe=r(O7," \u2014 "),L6=n(O7,"A",{href:!0});var ubr=s(L6);wPe=r(ubr,"CpmTokenizer"),ubr.forEach(t),APe=r(O7," or "),Gq=n(O7,"CODE",{});var pbr=s(Gq);LPe=r(pbr,"CpmTokenizerFast"),pbr.forEach(t),BPe=r(O7," (CPM model)"),O7.forEach(t),xPe=i(M),pm=n(M,"LI",{});var Zpe=s(pm);qq=n(Zpe,"STRONG",{});var _br=s(qq);kPe=r(_br,"ctrl"),_br.forEach(t),RPe=r(Zpe," \u2014 "),B6=n(Zpe,"A",{href:!0});var vbr=s(B6);PPe=r(vbr,"CTRLTokenizer"),vbr.forEach(t),SPe=r(Zpe," (CTRL model)"),Zpe.forEach(t),$Pe=i(M),xn=n(M,"LI",{});var G7=s(xn);zq=n(G7,"STRONG",{});var bbr=s(zq);IPe=r(bbr,"deberta"),bbr.forEach(t),DPe=r(G7," \u2014 "),x6=n(G7,"A",{href:!0});var Tbr=s(x6);NPe=r(Tbr,"DebertaTokenizer"),Tbr.forEach(t),jPe=r(G7," or "),k6=n(G7,"A",{href:!0});var Fbr=s(k6);OPe=r(Fbr,"DebertaTokenizerFast"),Fbr.forEach(t),GPe=r(G7," (DeBERTa model)"),G7.forEach(t),qPe=i(M),_m=n(M,"LI",{});var e_e=s(_m);Xq=n(e_e,"STRONG",{});var Ebr=s(Xq);zPe=r(Ebr,"deberta-v2"),Ebr.forEach(t),XPe=r(e_e," \u2014 "),R6=n(e_e,"A",{href:!0});var Cbr=s(R6);QPe=r(Cbr,"DebertaV2Tokenizer"),Cbr.forEach(t),VPe=r(e_e," (DeBERTa-v2 model)"),e_e.forEach(t),WPe=i(M),kn=n(M,"LI",{});var q7=s(kn);Qq=n(q7,"STRONG",{});var Mbr=s(Qq);HPe=r(Mbr,"distilbert"),Mbr.forEach(t),UPe=r(q7," \u2014 "),P6=n(q7,"A",{href:!0});var ybr=s(P6);JPe=r(ybr,"DistilBertTokenizer"),ybr.forEach(t),KPe=r(q7," or "),S6=n(q7,"A",{href:!0});var wbr=s(S6);YPe=r(wbr,"DistilBertTokenizerFast"),wbr.forEach(t),ZPe=r(q7," (DistilBERT model)"),q7.forEach(t),eSe=i(M),Rn=n(M,"LI",{});var z7=s(Rn);Vq=n(z7,"STRONG",{});var Abr=s(Vq);oSe=r(Abr,"dpr"),Abr.forEach(t),rSe=r(z7," \u2014 "),$6=n(z7,"A",{href:!0});var Lbr=s($6);tSe=r(Lbr,"DPRQuestionEncoderTokenizer"),Lbr.forEach(t),aSe=r(z7," or "),I6=n(z7,"A",{href:!0});var Bbr=s(I6);nSe=r(Bbr,"DPRQuestionEncoderTokenizerFast"),Bbr.forEach(t),sSe=r(z7," (DPR model)"),z7.forEach(t),lSe=i(M),Pn=n(M,"LI",{});var X7=s(Pn);Wq=n(X7,"STRONG",{});var xbr=s(Wq);iSe=r(xbr,"electra"),xbr.forEach(t),dSe=r(X7," \u2014 "),D6=n(X7,"A",{href:!0});var kbr=s(D6);cSe=r(kbr,"ElectraTokenizer"),kbr.forEach(t),fSe=r(X7," or "),N6=n(X7,"A",{href:!0});var Rbr=s(N6);mSe=r(Rbr,"ElectraTokenizerFast"),Rbr.forEach(t),hSe=r(X7," (ELECTRA model)"),X7.forEach(t),gSe=i(M),vm=n(M,"LI",{});var o_e=s(vm);Hq=n(o_e,"STRONG",{});var Pbr=s(Hq);uSe=r(Pbr,"flaubert"),Pbr.forEach(t),pSe=r(o_e," \u2014 "),j6=n(o_e,"A",{href:!0});var Sbr=s(j6);_Se=r(Sbr,"FlaubertTokenizer"),Sbr.forEach(t),vSe=r(o_e," (FlauBERT model)"),o_e.forEach(t),bSe=i(M),Sn=n(M,"LI",{});var Q7=s(Sn);Uq=n(Q7,"STRONG",{});var $br=s(Uq);TSe=r($br,"fnet"),$br.forEach(t),FSe=r(Q7," \u2014 "),O6=n(Q7,"A",{href:!0});var Ibr=s(O6);ESe=r(Ibr,"FNetTokenizer"),Ibr.forEach(t),CSe=r(Q7," or "),G6=n(Q7,"A",{href:!0});var Dbr=s(G6);MSe=r(Dbr,"FNetTokenizerFast"),Dbr.forEach(t),ySe=r(Q7," (FNet model)"),Q7.forEach(t),wSe=i(M),bm=n(M,"LI",{});var r_e=s(bm);Jq=n(r_e,"STRONG",{});var Nbr=s(Jq);ASe=r(Nbr,"fsmt"),Nbr.forEach(t),LSe=r(r_e," \u2014 "),q6=n(r_e,"A",{href:!0});var jbr=s(q6);BSe=r(jbr,"FSMTTokenizer"),jbr.forEach(t),xSe=r(r_e," (FairSeq Machine-Translation model)"),r_e.forEach(t),kSe=i(M),$n=n(M,"LI",{});var V7=s($n);Kq=n(V7,"STRONG",{});var Obr=s(Kq);RSe=r(Obr,"funnel"),Obr.forEach(t),PSe=r(V7," \u2014 "),z6=n(V7,"A",{href:!0});var Gbr=s(z6);SSe=r(Gbr,"FunnelTokenizer"),Gbr.forEach(t),$Se=r(V7," or "),X6=n(V7,"A",{href:!0});var qbr=s(X6);ISe=r(qbr,"FunnelTokenizerFast"),qbr.forEach(t),DSe=r(V7," (Funnel Transformer model)"),V7.forEach(t),NSe=i(M),In=n(M,"LI",{});var W7=s(In);Yq=n(W7,"STRONG",{});var zbr=s(Yq);jSe=r(zbr,"gpt2"),zbr.forEach(t),OSe=r(W7," \u2014 "),Q6=n(W7,"A",{href:!0});var Xbr=s(Q6);GSe=r(Xbr,"GPT2Tokenizer"),Xbr.forEach(t),qSe=r(W7," or "),V6=n(W7,"A",{href:!0});var Qbr=s(V6);zSe=r(Qbr,"GPT2TokenizerFast"),Qbr.forEach(t),XSe=r(W7," (OpenAI GPT-2 model)"),W7.forEach(t),QSe=i(M),Dn=n(M,"LI",{});var H7=s(Dn);Zq=n(H7,"STRONG",{});var Vbr=s(Zq);VSe=r(Vbr,"gpt_neo"),Vbr.forEach(t),WSe=r(H7," \u2014 "),W6=n(H7,"A",{href:!0});var Wbr=s(W6);HSe=r(Wbr,"GPT2Tokenizer"),Wbr.forEach(t),USe=r(H7," or "),H6=n(H7,"A",{href:!0});var Hbr=s(H6);JSe=r(Hbr,"GPT2TokenizerFast"),Hbr.forEach(t),KSe=r(H7," (GPT Neo model)"),H7.forEach(t),YSe=i(M),Tm=n(M,"LI",{});var t_e=s(Tm);ez=n(t_e,"STRONG",{});var Ubr=s(ez);ZSe=r(Ubr,"hubert"),Ubr.forEach(t),e$e=r(t_e," \u2014 "),U6=n(t_e,"A",{href:!0});var Jbr=s(U6);o$e=r(Jbr,"Wav2Vec2CTCTokenizer"),Jbr.forEach(t),r$e=r(t_e," (Hubert model)"),t_e.forEach(t),t$e=i(M),Nn=n(M,"LI",{});var U7=s(Nn);oz=n(U7,"STRONG",{});var Kbr=s(oz);a$e=r(Kbr,"ibert"),Kbr.forEach(t),n$e=r(U7," \u2014 "),J6=n(U7,"A",{href:!0});var Ybr=s(J6);s$e=r(Ybr,"RobertaTokenizer"),Ybr.forEach(t),l$e=r(U7," or "),K6=n(U7,"A",{href:!0});var Zbr=s(K6);i$e=r(Zbr,"RobertaTokenizerFast"),Zbr.forEach(t),d$e=r(U7," (I-BERT model)"),U7.forEach(t),c$e=i(M),jn=n(M,"LI",{});var J7=s(jn);rz=n(J7,"STRONG",{});var e2r=s(rz);f$e=r(e2r,"layoutlm"),e2r.forEach(t),m$e=r(J7," \u2014 "),Y6=n(J7,"A",{href:!0});var o2r=s(Y6);h$e=r(o2r,"LayoutLMTokenizer"),o2r.forEach(t),g$e=r(J7," or "),Z6=n(J7,"A",{href:!0});var r2r=s(Z6);u$e=r(r2r,"LayoutLMTokenizerFast"),r2r.forEach(t),p$e=r(J7," (LayoutLM model)"),J7.forEach(t),_$e=i(M),On=n(M,"LI",{});var K7=s(On);tz=n(K7,"STRONG",{});var t2r=s(tz);v$e=r(t2r,"layoutlmv2"),t2r.forEach(t),b$e=r(K7," \u2014 "),eL=n(K7,"A",{href:!0});var a2r=s(eL);T$e=r(a2r,"LayoutLMv2Tokenizer"),a2r.forEach(t),F$e=r(K7," or "),oL=n(K7,"A",{href:!0});var n2r=s(oL);E$e=r(n2r,"LayoutLMv2TokenizerFast"),n2r.forEach(t),C$e=r(K7," (LayoutLMv2 model)"),K7.forEach(t),M$e=i(M),Gn=n(M,"LI",{});var Y7=s(Gn);az=n(Y7,"STRONG",{});var s2r=s(az);y$e=r(s2r,"led"),s2r.forEach(t),w$e=r(Y7," \u2014 "),rL=n(Y7,"A",{href:!0});var l2r=s(rL);A$e=r(l2r,"LEDTokenizer"),l2r.forEach(t),L$e=r(Y7," or "),tL=n(Y7,"A",{href:!0});var i2r=s(tL);B$e=r(i2r,"LEDTokenizerFast"),i2r.forEach(t),x$e=r(Y7," (LED model)"),Y7.forEach(t),k$e=i(M),qn=n(M,"LI",{});var Z7=s(qn);nz=n(Z7,"STRONG",{});var d2r=s(nz);R$e=r(d2r,"longformer"),d2r.forEach(t),P$e=r(Z7," \u2014 "),aL=n(Z7,"A",{href:!0});var c2r=s(aL);S$e=r(c2r,"LongformerTokenizer"),c2r.forEach(t),$$e=r(Z7," or "),nL=n(Z7,"A",{href:!0});var f2r=s(nL);I$e=r(f2r,"LongformerTokenizerFast"),f2r.forEach(t),D$e=r(Z7," (Longformer model)"),Z7.forEach(t),N$e=i(M),Fm=n(M,"LI",{});var a_e=s(Fm);sz=n(a_e,"STRONG",{});var m2r=s(sz);j$e=r(m2r,"luke"),m2r.forEach(t),O$e=r(a_e," \u2014 "),sL=n(a_e,"A",{href:!0});var h2r=s(sL);G$e=r(h2r,"LukeTokenizer"),h2r.forEach(t),q$e=r(a_e," (LUKE model)"),a_e.forEach(t),z$e=i(M),zn=n(M,"LI",{});var e0=s(zn);lz=n(e0,"STRONG",{});var g2r=s(lz);X$e=r(g2r,"lxmert"),g2r.forEach(t),Q$e=r(e0," \u2014 "),lL=n(e0,"A",{href:!0});var u2r=s(lL);V$e=r(u2r,"LxmertTokenizer"),u2r.forEach(t),W$e=r(e0," or "),iL=n(e0,"A",{href:!0});var p2r=s(iL);H$e=r(p2r,"LxmertTokenizerFast"),p2r.forEach(t),U$e=r(e0," (LXMERT model)"),e0.forEach(t),J$e=i(M),Em=n(M,"LI",{});var n_e=s(Em);iz=n(n_e,"STRONG",{});var _2r=s(iz);K$e=r(_2r,"m2m_100"),_2r.forEach(t),Y$e=r(n_e," \u2014 "),dL=n(n_e,"A",{href:!0});var v2r=s(dL);Z$e=r(v2r,"M2M100Tokenizer"),v2r.forEach(t),eIe=r(n_e," (M2M100 model)"),n_e.forEach(t),oIe=i(M),Cm=n(M,"LI",{});var s_e=s(Cm);dz=n(s_e,"STRONG",{});var b2r=s(dz);rIe=r(b2r,"marian"),b2r.forEach(t),tIe=r(s_e," \u2014 "),cL=n(s_e,"A",{href:!0});var T2r=s(cL);aIe=r(T2r,"MarianTokenizer"),T2r.forEach(t),nIe=r(s_e," (Marian model)"),s_e.forEach(t),sIe=i(M),Xn=n(M,"LI",{});var o0=s(Xn);cz=n(o0,"STRONG",{});var F2r=s(cz);lIe=r(F2r,"mbart"),F2r.forEach(t),iIe=r(o0," \u2014 "),fL=n(o0,"A",{href:!0});var E2r=s(fL);dIe=r(E2r,"MBartTokenizer"),E2r.forEach(t),cIe=r(o0," or "),mL=n(o0,"A",{href:!0});var C2r=s(mL);fIe=r(C2r,"MBartTokenizerFast"),C2r.forEach(t),mIe=r(o0," (mBART model)"),o0.forEach(t),hIe=i(M),Qn=n(M,"LI",{});var r0=s(Qn);fz=n(r0,"STRONG",{});var M2r=s(fz);gIe=r(M2r,"mbart50"),M2r.forEach(t),uIe=r(r0," \u2014 "),hL=n(r0,"A",{href:!0});var y2r=s(hL);pIe=r(y2r,"MBart50Tokenizer"),y2r.forEach(t),_Ie=r(r0," or "),gL=n(r0,"A",{href:!0});var w2r=s(gL);vIe=r(w2r,"MBart50TokenizerFast"),w2r.forEach(t),bIe=r(r0," (mBART-50 model)"),r0.forEach(t),TIe=i(M),Vn=n(M,"LI",{});var t0=s(Vn);mz=n(t0,"STRONG",{});var A2r=s(mz);FIe=r(A2r,"mobilebert"),A2r.forEach(t),EIe=r(t0," \u2014 "),uL=n(t0,"A",{href:!0});var L2r=s(uL);CIe=r(L2r,"MobileBertTokenizer"),L2r.forEach(t),MIe=r(t0," or "),pL=n(t0,"A",{href:!0});var B2r=s(pL);yIe=r(B2r,"MobileBertTokenizerFast"),B2r.forEach(t),wIe=r(t0," (MobileBERT model)"),t0.forEach(t),AIe=i(M),Wn=n(M,"LI",{});var a0=s(Wn);hz=n(a0,"STRONG",{});var x2r=s(hz);LIe=r(x2r,"mpnet"),x2r.forEach(t),BIe=r(a0," \u2014 "),_L=n(a0,"A",{href:!0});var k2r=s(_L);xIe=r(k2r,"MPNetTokenizer"),k2r.forEach(t),kIe=r(a0," or "),vL=n(a0,"A",{href:!0});var R2r=s(vL);RIe=r(R2r,"MPNetTokenizerFast"),R2r.forEach(t),PIe=r(a0," (MPNet model)"),a0.forEach(t),SIe=i(M),Hn=n(M,"LI",{});var n0=s(Hn);gz=n(n0,"STRONG",{});var P2r=s(gz);$Ie=r(P2r,"mt5"),P2r.forEach(t),IIe=r(n0," \u2014 "),bL=n(n0,"A",{href:!0});var S2r=s(bL);DIe=r(S2r,"MT5Tokenizer"),S2r.forEach(t),NIe=r(n0," or "),TL=n(n0,"A",{href:!0});var $2r=s(TL);jIe=r($2r,"MT5TokenizerFast"),$2r.forEach(t),OIe=r(n0," (mT5 model)"),n0.forEach(t),GIe=i(M),Un=n(M,"LI",{});var s0=s(Un);uz=n(s0,"STRONG",{});var I2r=s(uz);qIe=r(I2r,"openai-gpt"),I2r.forEach(t),zIe=r(s0," \u2014 "),FL=n(s0,"A",{href:!0});var D2r=s(FL);XIe=r(D2r,"OpenAIGPTTokenizer"),D2r.forEach(t),QIe=r(s0," or "),EL=n(s0,"A",{href:!0});var N2r=s(EL);VIe=r(N2r,"OpenAIGPTTokenizerFast"),N2r.forEach(t),WIe=r(s0," (OpenAI GPT model)"),s0.forEach(t),HIe=i(M),Jn=n(M,"LI",{});var l0=s(Jn);pz=n(l0,"STRONG",{});var j2r=s(pz);UIe=r(j2r,"pegasus"),j2r.forEach(t),JIe=r(l0," \u2014 "),CL=n(l0,"A",{href:!0});var O2r=s(CL);KIe=r(O2r,"PegasusTokenizer"),O2r.forEach(t),YIe=r(l0," or "),ML=n(l0,"A",{href:!0});var G2r=s(ML);ZIe=r(G2r,"PegasusTokenizerFast"),G2r.forEach(t),eDe=r(l0," (Pegasus model)"),l0.forEach(t),oDe=i(M),Mm=n(M,"LI",{});var l_e=s(Mm);_z=n(l_e,"STRONG",{});var q2r=s(_z);rDe=r(q2r,"perceiver"),q2r.forEach(t),tDe=r(l_e," \u2014 "),yL=n(l_e,"A",{href:!0});var z2r=s(yL);aDe=r(z2r,"PerceiverTokenizer"),z2r.forEach(t),nDe=r(l_e," (Perceiver model)"),l_e.forEach(t),sDe=i(M),ym=n(M,"LI",{});var i_e=s(ym);vz=n(i_e,"STRONG",{});var X2r=s(vz);lDe=r(X2r,"phobert"),X2r.forEach(t),iDe=r(i_e," \u2014 "),wL=n(i_e,"A",{href:!0});var Q2r=s(wL);dDe=r(Q2r,"PhobertTokenizer"),Q2r.forEach(t),cDe=r(i_e," (PhoBERT model)"),i_e.forEach(t),fDe=i(M),wm=n(M,"LI",{});var d_e=s(wm);bz=n(d_e,"STRONG",{});var V2r=s(bz);mDe=r(V2r,"prophetnet"),V2r.forEach(t),hDe=r(d_e," \u2014 "),AL=n(d_e,"A",{href:!0});var W2r=s(AL);gDe=r(W2r,"ProphetNetTokenizer"),W2r.forEach(t),uDe=r(d_e," (ProphetNet model)"),d_e.forEach(t),pDe=i(M),Kn=n(M,"LI",{});var i0=s(Kn);Tz=n(i0,"STRONG",{});var H2r=s(Tz);_De=r(H2r,"qdqbert"),H2r.forEach(t),vDe=r(i0," \u2014 "),LL=n(i0,"A",{href:!0});var U2r=s(LL);bDe=r(U2r,"BertTokenizer"),U2r.forEach(t),TDe=r(i0," or "),BL=n(i0,"A",{href:!0});var J2r=s(BL);FDe=r(J2r,"BertTokenizerFast"),J2r.forEach(t),EDe=r(i0," (QDQBert model)"),i0.forEach(t),CDe=i(M),Am=n(M,"LI",{});var c_e=s(Am);Fz=n(c_e,"STRONG",{});var K2r=s(Fz);MDe=r(K2r,"rag"),K2r.forEach(t),yDe=r(c_e," \u2014 "),xL=n(c_e,"A",{href:!0});var Y2r=s(xL);wDe=r(Y2r,"RagTokenizer"),Y2r.forEach(t),ADe=r(c_e," (RAG model)"),c_e.forEach(t),LDe=i(M),Yn=n(M,"LI",{});var d0=s(Yn);Ez=n(d0,"STRONG",{});var Z2r=s(Ez);BDe=r(Z2r,"reformer"),Z2r.forEach(t),xDe=r(d0," \u2014 "),kL=n(d0,"A",{href:!0});var eTr=s(kL);kDe=r(eTr,"ReformerTokenizer"),eTr.forEach(t),RDe=r(d0," or "),RL=n(d0,"A",{href:!0});var oTr=s(RL);PDe=r(oTr,"ReformerTokenizerFast"),oTr.forEach(t),SDe=r(d0," (Reformer model)"),d0.forEach(t),$De=i(M),Zn=n(M,"LI",{});var c0=s(Zn);Cz=n(c0,"STRONG",{});var rTr=s(Cz);IDe=r(rTr,"rembert"),rTr.forEach(t),DDe=r(c0," \u2014 "),PL=n(c0,"A",{href:!0});var tTr=s(PL);NDe=r(tTr,"RemBertTokenizer"),tTr.forEach(t),jDe=r(c0," or "),SL=n(c0,"A",{href:!0});var aTr=s(SL);ODe=r(aTr,"RemBertTokenizerFast"),aTr.forEach(t),GDe=r(c0," (RemBERT model)"),c0.forEach(t),qDe=i(M),es=n(M,"LI",{});var f0=s(es);Mz=n(f0,"STRONG",{});var nTr=s(Mz);zDe=r(nTr,"retribert"),nTr.forEach(t),XDe=r(f0," \u2014 "),$L=n(f0,"A",{href:!0});var sTr=s($L);QDe=r(sTr,"RetriBertTokenizer"),sTr.forEach(t),VDe=r(f0," or "),IL=n(f0,"A",{href:!0});var lTr=s(IL);WDe=r(lTr,"RetriBertTokenizerFast"),lTr.forEach(t),HDe=r(f0," (RetriBERT model)"),f0.forEach(t),UDe=i(M),os=n(M,"LI",{});var m0=s(os);yz=n(m0,"STRONG",{});var iTr=s(yz);JDe=r(iTr,"roberta"),iTr.forEach(t),KDe=r(m0," \u2014 "),DL=n(m0,"A",{href:!0});var dTr=s(DL);YDe=r(dTr,"RobertaTokenizer"),dTr.forEach(t),ZDe=r(m0," or "),NL=n(m0,"A",{href:!0});var cTr=s(NL);eNe=r(cTr,"RobertaTokenizerFast"),cTr.forEach(t),oNe=r(m0," (RoBERTa model)"),m0.forEach(t),rNe=i(M),rs=n(M,"LI",{});var h0=s(rs);wz=n(h0,"STRONG",{});var fTr=s(wz);tNe=r(fTr,"roformer"),fTr.forEach(t),aNe=r(h0," \u2014 "),jL=n(h0,"A",{href:!0});var mTr=s(jL);nNe=r(mTr,"RoFormerTokenizer"),mTr.forEach(t),sNe=r(h0," or "),OL=n(h0,"A",{href:!0});var hTr=s(OL);lNe=r(hTr,"RoFormerTokenizerFast"),hTr.forEach(t),iNe=r(h0," (RoFormer model)"),h0.forEach(t),dNe=i(M),Lm=n(M,"LI",{});var f_e=s(Lm);Az=n(f_e,"STRONG",{});var gTr=s(Az);cNe=r(gTr,"speech_to_text"),gTr.forEach(t),fNe=r(f_e," \u2014 "),GL=n(f_e,"A",{href:!0});var uTr=s(GL);mNe=r(uTr,"Speech2TextTokenizer"),uTr.forEach(t),hNe=r(f_e," (Speech2Text model)"),f_e.forEach(t),gNe=i(M),Bm=n(M,"LI",{});var m_e=s(Bm);Lz=n(m_e,"STRONG",{});var pTr=s(Lz);uNe=r(pTr,"speech_to_text_2"),pTr.forEach(t),pNe=r(m_e," \u2014 "),qL=n(m_e,"A",{href:!0});var _Tr=s(qL);_Ne=r(_Tr,"Speech2Text2Tokenizer"),_Tr.forEach(t),vNe=r(m_e," (Speech2Text2 model)"),m_e.forEach(t),bNe=i(M),ts=n(M,"LI",{});var g0=s(ts);Bz=n(g0,"STRONG",{});var vTr=s(Bz);TNe=r(vTr,"splinter"),vTr.forEach(t),FNe=r(g0," \u2014 "),zL=n(g0,"A",{href:!0});var bTr=s(zL);ENe=r(bTr,"SplinterTokenizer"),bTr.forEach(t),CNe=r(g0," or "),XL=n(g0,"A",{href:!0});var TTr=s(XL);MNe=r(TTr,"SplinterTokenizerFast"),TTr.forEach(t),yNe=r(g0," (Splinter model)"),g0.forEach(t),wNe=i(M),as=n(M,"LI",{});var u0=s(as);xz=n(u0,"STRONG",{});var FTr=s(xz);ANe=r(FTr,"squeezebert"),FTr.forEach(t),LNe=r(u0," \u2014 "),QL=n(u0,"A",{href:!0});var ETr=s(QL);BNe=r(ETr,"SqueezeBertTokenizer"),ETr.forEach(t),xNe=r(u0," or "),VL=n(u0,"A",{href:!0});var CTr=s(VL);kNe=r(CTr,"SqueezeBertTokenizerFast"),CTr.forEach(t),RNe=r(u0," (SqueezeBERT model)"),u0.forEach(t),PNe=i(M),ns=n(M,"LI",{});var p0=s(ns);kz=n(p0,"STRONG",{});var MTr=s(kz);SNe=r(MTr,"t5"),MTr.forEach(t),$Ne=r(p0," \u2014 "),WL=n(p0,"A",{href:!0});var yTr=s(WL);INe=r(yTr,"T5Tokenizer"),yTr.forEach(t),DNe=r(p0," or "),HL=n(p0,"A",{href:!0});var wTr=s(HL);NNe=r(wTr,"T5TokenizerFast"),wTr.forEach(t),jNe=r(p0," (T5 model)"),p0.forEach(t),ONe=i(M),xm=n(M,"LI",{});var h_e=s(xm);Rz=n(h_e,"STRONG",{});var ATr=s(Rz);GNe=r(ATr,"tapas"),ATr.forEach(t),qNe=r(h_e," \u2014 "),UL=n(h_e,"A",{href:!0});var LTr=s(UL);zNe=r(LTr,"TapasTokenizer"),LTr.forEach(t),XNe=r(h_e," (TAPAS model)"),h_e.forEach(t),QNe=i(M),km=n(M,"LI",{});var g_e=s(km);Pz=n(g_e,"STRONG",{});var BTr=s(Pz);VNe=r(BTr,"transfo-xl"),BTr.forEach(t),WNe=r(g_e," \u2014 "),JL=n(g_e,"A",{href:!0});var xTr=s(JL);HNe=r(xTr,"TransfoXLTokenizer"),xTr.forEach(t),UNe=r(g_e," (Transformer-XL model)"),g_e.forEach(t),JNe=i(M),Rm=n(M,"LI",{});var u_e=s(Rm);Sz=n(u_e,"STRONG",{});var kTr=s(Sz);KNe=r(kTr,"wav2vec2"),kTr.forEach(t),YNe=r(u_e," \u2014 "),KL=n(u_e,"A",{href:!0});var RTr=s(KL);ZNe=r(RTr,"Wav2Vec2CTCTokenizer"),RTr.forEach(t),eje=r(u_e," (Wav2Vec2 model)"),u_e.forEach(t),oje=i(M),Pm=n(M,"LI",{});var p_e=s(Pm);$z=n(p_e,"STRONG",{});var PTr=s($z);rje=r(PTr,"xlm"),PTr.forEach(t),tje=r(p_e," \u2014 "),YL=n(p_e,"A",{href:!0});var STr=s(YL);aje=r(STr,"XLMTokenizer"),STr.forEach(t),nje=r(p_e," (XLM model)"),p_e.forEach(t),sje=i(M),Sm=n(M,"LI",{});var __e=s(Sm);Iz=n(__e,"STRONG",{});var $Tr=s(Iz);lje=r($Tr,"xlm-prophetnet"),$Tr.forEach(t),ije=r(__e," \u2014 "),ZL=n(__e,"A",{href:!0});var ITr=s(ZL);dje=r(ITr,"XLMProphetNetTokenizer"),ITr.forEach(t),cje=r(__e," (XLMProphetNet model)"),__e.forEach(t),fje=i(M),ss=n(M,"LI",{});var _0=s(ss);Dz=n(_0,"STRONG",{});var DTr=s(Dz);mje=r(DTr,"xlm-roberta"),DTr.forEach(t),hje=r(_0," \u2014 "),e8=n(_0,"A",{href:!0});var NTr=s(e8);gje=r(NTr,"XLMRobertaTokenizer"),NTr.forEach(t),uje=r(_0," or "),o8=n(_0,"A",{href:!0});var jTr=s(o8);pje=r(jTr,"XLMRobertaTokenizerFast"),jTr.forEach(t),_je=r(_0," (XLM-RoBERTa model)"),_0.forEach(t),vje=i(M),ls=n(M,"LI",{});var v0=s(ls);Nz=n(v0,"STRONG",{});var OTr=s(Nz);bje=r(OTr,"xlnet"),OTr.forEach(t),Tje=r(v0," \u2014 "),r8=n(v0,"A",{href:!0});var GTr=s(r8);Fje=r(GTr,"XLNetTokenizer"),GTr.forEach(t),Eje=r(v0," or "),t8=n(v0,"A",{href:!0});var qTr=s(t8);Cje=r(qTr,"XLNetTokenizerFast"),qTr.forEach(t),Mje=r(v0," (XLNet model)"),v0.forEach(t),M.forEach(t),yje=i(ko),ei=n(ko,"P",{});var UN=s(ei);wje=r(UN,`Params:
pretrained_model_name_or_path (`),jz=n(UN,"CODE",{});var zTr=s(jz);Aje=r(zTr,"str"),zTr.forEach(t),Lje=r(UN," or "),Oz=n(UN,"CODE",{});var XTr=s(Oz);Bje=r(XTr,"os.PathLike"),XTr.forEach(t),xje=r(UN,`):
Can be either:`),UN.forEach(t),kje=i(ko),oi=n(ko,"UL",{});var JN=s(oi);Ta=n(JN,"LI",{});var CE=s(Ta);Rje=r(CE,"A string, the "),Gz=n(CE,"EM",{});var QTr=s(Gz);Pje=r(QTr,"model id"),QTr.forEach(t),Sje=r(CE,` of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),qz=n(CE,"CODE",{});var VTr=s(qz);$je=r(VTr,"bert-base-uncased"),VTr.forEach(t),Ije=r(CE,`, or namespaced under
a user or organization name, like `),zz=n(CE,"CODE",{});var WTr=s(zz);Dje=r(WTr,"dbmdz/bert-base-german-cased"),WTr.forEach(t),Nje=r(CE,"."),CE.forEach(t),jje=i(JN),Fa=n(JN,"LI",{});var ME=s(Fa);Oje=r(ME,"A path to a "),Xz=n(ME,"EM",{});var HTr=s(Xz);Gje=r(HTr,"directory"),HTr.forEach(t),qje=r(ME,` containing vocabulary files required by the tokenizer, for instance saved
using the `),a8=n(ME,"A",{href:!0});var UTr=s(a8);zje=r(UTr,"save_pretrained()"),UTr.forEach(t),Xje=r(ME,` method, e.g.,
`),Qz=n(ME,"CODE",{});var JTr=s(Qz);Qje=r(JTr,"./my_model_directory/"),JTr.forEach(t),Vje=r(ME,"."),ME.forEach(t),Wje=i(JN),k=n(JN,"LI",{});var P=s(k);Hje=r(P,`A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: `),Vz=n(P,"CODE",{});var KTr=s(Vz);Uje=r(KTr,"./my_model_directory/vocab.txt"),KTr.forEach(t),Jje=r(P,`. (Not
applicable to all derived classes)
inputs (additional positional arguments, `),Wz=n(P,"EM",{});var YTr=s(Wz);Kje=r(YTr,"optional"),YTr.forEach(t),Yje=r(P,`):
Will be passed along to the Tokenizer `),Hz=n(P,"CODE",{});var ZTr=s(Hz);Zje=r(ZTr,"__init__()"),ZTr.forEach(t),eOe=r(P,` method.
config (`),n8=n(P,"A",{href:!0});var eFr=s(n8);oOe=r(eFr,"PretrainedConfig"),eFr.forEach(t),rOe=r(P,", "),Uz=n(P,"EM",{});var oFr=s(Uz);tOe=r(oFr,"optional"),oFr.forEach(t),aOe=r(P,`)
The configuration object used to dertermine the tokenizer class to instantiate.
cache`),ri=n(P,"EM",{});var KN=s(ri);nOe=r(KN,"dir ("),Jz=n(KN,"CODE",{});var rFr=s(Jz);sOe=r(rFr,"str"),rFr.forEach(t),lOe=r(KN," or "),Kz=n(KN,"CODE",{});var tFr=s(Kz);iOe=r(tFr,"os.PathLike"),tFr.forEach(t),dOe=r(KN,", _optional"),KN.forEach(t),cOe=r(P,`):
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.
force`),IC=n(P,"EM",{});var fwe=s(IC);fOe=r(fwe,"download ("),Yz=n(fwe,"CODE",{});var aFr=s(Yz);mOe=r(aFr,"bool"),aFr.forEach(t),hOe=r(fwe,", _optional"),fwe.forEach(t),gOe=r(P,", defaults to "),Zz=n(P,"CODE",{});var nFr=s(Zz);uOe=r(nFr,"False"),nFr.forEach(t),pOe=r(P,`):
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.
resume`),DC=n(P,"EM",{});var mwe=s(DC);_Oe=r(mwe,"download ("),eX=n(mwe,"CODE",{});var sFr=s(eX);vOe=r(sFr,"bool"),sFr.forEach(t),bOe=r(mwe,", _optional"),mwe.forEach(t),TOe=r(P,", defaults to "),oX=n(P,"CODE",{});var lFr=s(oX);FOe=r(lFr,"False"),lFr.forEach(t),EOe=r(P,`):
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.
proxies (`),rX=n(P,"CODE",{});var iFr=s(rX);COe=r(iFr,"Dict[str, str]"),iFr.forEach(t),MOe=r(P,", "),tX=n(P,"EM",{});var dFr=s(tX);yOe=r(dFr,"optional"),dFr.forEach(t),wOe=r(P,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),aX=n(P,"CODE",{});var cFr=s(aX);AOe=r(cFr,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}"),cFr.forEach(t),LOe=r(P,`. The proxies are used on each request.
revision(`),nX=n(P,"CODE",{});var fFr=s(nX);BOe=r(fFr,"str"),fFr.forEach(t),xOe=r(P,", "),sX=n(P,"EM",{});var mFr=s(sX);kOe=r(mFr,"optional"),mFr.forEach(t),ROe=r(P,", defaults to "),lX=n(P,"CODE",{});var hFr=s(lX);POe=r(hFr,'"main"'),hFr.forEach(t),SOe=r(P,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),iX=n(P,"CODE",{});var gFr=s(iX);$Oe=r(gFr,"revision"),gFr.forEach(t),IOe=r(P,` can be any
identifier allowed by git.
subfolder (`),dX=n(P,"CODE",{});var uFr=s(dX);DOe=r(uFr,"str"),uFr.forEach(t),NOe=r(P,", "),cX=n(P,"EM",{});var pFr=s(cX);jOe=r(pFr,"optional"),pFr.forEach(t),OOe=r(P,`):
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.
use`),NC=n(P,"EM",{});var hwe=s(NC);GOe=r(hwe,"fast ("),fX=n(hwe,"CODE",{});var _Fr=s(fX);qOe=r(_Fr,"bool"),_Fr.forEach(t),zOe=r(hwe,", _optional"),hwe.forEach(t),XOe=r(P,", defaults to "),mX=n(P,"CODE",{});var vFr=s(mX);QOe=r(vFr,"True"),vFr.forEach(t),VOe=r(P,`):
Whether or not to try to load the fast version of the tokenizer.
tokenizer`),jC=n(P,"EM",{});var gwe=s(jC);WOe=r(gwe,"type ("),hX=n(gwe,"CODE",{});var bFr=s(hX);HOe=r(bFr,"str"),bFr.forEach(t),UOe=r(gwe,", _optional"),gwe.forEach(t),JOe=r(P,`):
Tokenizer type to be loaded.
trust`),OC=n(P,"EM",{});var uwe=s(OC);KOe=r(uwe,"remote_code ("),gX=n(uwe,"CODE",{});var TFr=s(gX);YOe=r(TFr,"bool"),TFr.forEach(t),ZOe=r(uwe,", _optional"),uwe.forEach(t),eGe=r(P,", defaults to "),uX=n(P,"CODE",{});var FFr=s(uX);oGe=r(FFr,"False"),FFr.forEach(t),rGe=r(P,`):
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to `),pX=n(P,"CODE",{});var EFr=s(pX);tGe=r(EFr,"True"),EFr.forEach(t),aGe=r(P,` for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.
kwargs (additional keyword arguments, `),_X=n(P,"EM",{});var CFr=s(_X);nGe=r(CFr,"optional"),CFr.forEach(t),sGe=r(P,`):
Will be passed to the Tokenizer `),vX=n(P,"CODE",{});var MFr=s(vX);lGe=r(MFr,"__init__()"),MFr.forEach(t),iGe=r(P,` method. Can be used to set special tokens like
`),bX=n(P,"CODE",{});var yFr=s(bX);dGe=r(yFr,"bos_token"),yFr.forEach(t),cGe=r(P,", "),TX=n(P,"CODE",{});var wFr=s(TX);fGe=r(wFr,"eos_token"),wFr.forEach(t),mGe=r(P,", "),FX=n(P,"CODE",{});var AFr=s(FX);hGe=r(AFr,"unk_token"),AFr.forEach(t),gGe=r(P,", "),EX=n(P,"CODE",{});var LFr=s(EX);uGe=r(LFr,"sep_token"),LFr.forEach(t),pGe=r(P,", "),CX=n(P,"CODE",{});var BFr=s(CX);_Ge=r(BFr,"pad_token"),BFr.forEach(t),vGe=r(P,", "),MX=n(P,"CODE",{});var xFr=s(MX);bGe=r(xFr,"cls_token"),xFr.forEach(t),TGe=r(P,`,
`),yX=n(P,"CODE",{});var kFr=s(yX);FGe=r(kFr,"mask_token"),kFr.forEach(t),EGe=r(P,", "),wX=n(P,"CODE",{});var RFr=s(wX);CGe=r(RFr,"additional_special_tokens"),RFr.forEach(t),MGe=r(P,". See parameters in the "),AX=n(P,"CODE",{});var PFr=s(AX);yGe=r(PFr,"__init__()"),PFr.forEach(t),wGe=r(P," for more details."),P.forEach(t),JN.forEach(t),AGe=i(ko),LX=n(ko,"P",{});var SFr=s(LX);LGe=r(SFr,"Examples:"),SFr.forEach(t),BGe=i(ko),m(GC.$$.fragment,ko),ko.forEach(t),xGe=i(ms),$m=n(ms,"DIV",{class:!0});var pwe=s($m);m(qC.$$.fragment,pwe),kGe=i(pwe),BX=n(pwe,"P",{});var $Fr=s(BX);RGe=r($Fr,"Register a new tokenizer in this mapping."),$Fr.forEach(t),pwe.forEach(t),ms.forEach(t),F5e=i(c),ti=n(c,"H2",{class:!0});var _we=s(ti);Im=n(_we,"A",{id:!0,class:!0,href:!0});var IFr=s(Im);xX=n(IFr,"SPAN",{});var DFr=s(xX);m(zC.$$.fragment,DFr),DFr.forEach(t),IFr.forEach(t),PGe=i(_we),kX=n(_we,"SPAN",{});var NFr=s(kX);SGe=r(NFr,"AutoFeatureExtractor"),NFr.forEach(t),_we.forEach(t),E5e=i(c),Dt=n(c,"DIV",{class:!0});var yE=s(Dt);m(XC.$$.fragment,yE),$Ge=i(yE),QC=n(yE,"P",{});var vwe=s(QC);IGe=r(vwe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),s8=n(vwe,"A",{href:!0});var jFr=s(s8);DGe=r(jFr,"AutoFeatureExtractor.from_pretrained()"),jFr.forEach(t),NGe=r(vwe," class method."),vwe.forEach(t),jGe=i(yE),VC=n(yE,"P",{});var bwe=s(VC);OGe=r(bwe,"This class cannot be instantiated directly using "),RX=n(bwe,"CODE",{});var OFr=s(RX);GGe=r(OFr,"__init__()"),OFr.forEach(t),qGe=r(bwe," (throws an error)."),bwe.forEach(t),zGe=i(yE),Fe=n(yE,"DIV",{class:!0});var Ze=s(Fe);m(WC.$$.fragment,Ze),XGe=i(Ze),PX=n(Ze,"P",{});var GFr=s(PX);QGe=r(GFr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),GFr.forEach(t),VGe=i(Ze),Ea=n(Ze,"P",{});var wE=s(Ea);WGe=r(wE,"The feature extractor class to instantiate is selected based on the "),SX=n(wE,"CODE",{});var qFr=s(SX);HGe=r(qFr,"model_type"),qFr.forEach(t),UGe=r(wE,` property of the config
object (either passed as an argument or loaded from `),$X=n(wE,"CODE",{});var zFr=s($X);JGe=r(zFr,"pretrained_model_name_or_path"),zFr.forEach(t),KGe=r(wE,` if possible), or when
it\u2019s missing, by falling back to using pattern matching on `),IX=n(wE,"CODE",{});var XFr=s(IX);YGe=r(XFr,"pretrained_model_name_or_path"),XFr.forEach(t),ZGe=r(wE,":"),wE.forEach(t),eqe=i(Ze),_e=n(Ze,"UL",{});var we=s(_e);Dm=n(we,"LI",{});var v_e=s(Dm);DX=n(v_e,"STRONG",{});var QFr=s(DX);oqe=r(QFr,"beit"),QFr.forEach(t),rqe=r(v_e," \u2014 "),l8=n(v_e,"A",{href:!0});var VFr=s(l8);tqe=r(VFr,"BeitFeatureExtractor"),VFr.forEach(t),aqe=r(v_e," (BEiT model)"),v_e.forEach(t),nqe=i(we),Nm=n(we,"LI",{});var b_e=s(Nm);NX=n(b_e,"STRONG",{});var WFr=s(NX);sqe=r(WFr,"clip"),WFr.forEach(t),lqe=r(b_e," \u2014 "),i8=n(b_e,"A",{href:!0});var HFr=s(i8);iqe=r(HFr,"CLIPFeatureExtractor"),HFr.forEach(t),dqe=r(b_e," (CLIP model)"),b_e.forEach(t),cqe=i(we),jm=n(we,"LI",{});var T_e=s(jm);jX=n(T_e,"STRONG",{});var UFr=s(jX);fqe=r(UFr,"deit"),UFr.forEach(t),mqe=r(T_e," \u2014 "),d8=n(T_e,"A",{href:!0});var JFr=s(d8);hqe=r(JFr,"DeiTFeatureExtractor"),JFr.forEach(t),gqe=r(T_e," (DeiT model)"),T_e.forEach(t),uqe=i(we),Om=n(we,"LI",{});var F_e=s(Om);OX=n(F_e,"STRONG",{});var KFr=s(OX);pqe=r(KFr,"detr"),KFr.forEach(t),_qe=r(F_e," \u2014 "),c8=n(F_e,"A",{href:!0});var YFr=s(c8);vqe=r(YFr,"DetrFeatureExtractor"),YFr.forEach(t),bqe=r(F_e," (DETR model)"),F_e.forEach(t),Tqe=i(we),Gm=n(we,"LI",{});var E_e=s(Gm);GX=n(E_e,"STRONG",{});var ZFr=s(GX);Fqe=r(ZFr,"hubert"),ZFr.forEach(t),Eqe=r(E_e," \u2014 "),f8=n(E_e,"A",{href:!0});var eEr=s(f8);Cqe=r(eEr,"Wav2Vec2FeatureExtractor"),eEr.forEach(t),Mqe=r(E_e," (Hubert model)"),E_e.forEach(t),yqe=i(we),qm=n(we,"LI",{});var C_e=s(qm);qX=n(C_e,"STRONG",{});var oEr=s(qX);wqe=r(oEr,"layoutlmv2"),oEr.forEach(t),Aqe=r(C_e," \u2014 "),m8=n(C_e,"A",{href:!0});var rEr=s(m8);Lqe=r(rEr,"LayoutLMv2FeatureExtractor"),rEr.forEach(t),Bqe=r(C_e," (LayoutLMv2 model)"),C_e.forEach(t),xqe=i(we),zm=n(we,"LI",{});var M_e=s(zm);zX=n(M_e,"STRONG",{});var tEr=s(zX);kqe=r(tEr,"perceiver"),tEr.forEach(t),Rqe=r(M_e," \u2014 "),h8=n(M_e,"A",{href:!0});var aEr=s(h8);Pqe=r(aEr,"PerceiverFeatureExtractor"),aEr.forEach(t),Sqe=r(M_e," (Perceiver model)"),M_e.forEach(t),$qe=i(we),Xm=n(we,"LI",{});var y_e=s(Xm);XX=n(y_e,"STRONG",{});var nEr=s(XX);Iqe=r(nEr,"speech_to_text"),nEr.forEach(t),Dqe=r(y_e," \u2014 "),g8=n(y_e,"A",{href:!0});var sEr=s(g8);Nqe=r(sEr,"Speech2TextFeatureExtractor"),sEr.forEach(t),jqe=r(y_e," (Speech2Text model)"),y_e.forEach(t),Oqe=i(we),Qm=n(we,"LI",{});var w_e=s(Qm);QX=n(w_e,"STRONG",{});var lEr=s(QX);Gqe=r(lEr,"vit"),lEr.forEach(t),qqe=r(w_e," \u2014 "),u8=n(w_e,"A",{href:!0});var iEr=s(u8);zqe=r(iEr,"ViTFeatureExtractor"),iEr.forEach(t),Xqe=r(w_e," (ViT model)"),w_e.forEach(t),Qqe=i(we),Vm=n(we,"LI",{});var A_e=s(Vm);VX=n(A_e,"STRONG",{});var dEr=s(VX);Vqe=r(dEr,"wav2vec2"),dEr.forEach(t),Wqe=r(A_e," \u2014 "),p8=n(A_e,"A",{href:!0});var cEr=s(p8);Hqe=r(cEr,"Wav2Vec2FeatureExtractor"),cEr.forEach(t),Uqe=r(A_e," (Wav2Vec2 model)"),A_e.forEach(t),we.forEach(t),Jqe=i(Ze),ai=n(Ze,"P",{});var YN=s(ai);Kqe=r(YN,`Params:
pretrained_model_name_or_path (`),WX=n(YN,"CODE",{});var fEr=s(WX);Yqe=r(fEr,"str"),fEr.forEach(t),Zqe=r(YN," or "),HX=n(YN,"CODE",{});var mEr=s(HX);eze=r(mEr,"os.PathLike"),mEr.forEach(t),oze=r(YN,`):
This can be either:`),YN.forEach(t),rze=i(Ze),ni=n(Ze,"UL",{});var ZN=s(ni);Ca=n(ZN,"LI",{});var AE=s(Ca);tze=r(AE,"a string, the "),UX=n(AE,"EM",{});var hEr=s(UX);aze=r(hEr,"model id"),hEr.forEach(t),nze=r(AE,` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),JX=n(AE,"CODE",{});var gEr=s(JX);sze=r(gEr,"bert-base-uncased"),gEr.forEach(t),lze=r(AE,`, or
namespaced under a user or organization name, like `),KX=n(AE,"CODE",{});var uEr=s(KX);ize=r(uEr,"dbmdz/bert-base-german-cased"),uEr.forEach(t),dze=r(AE,"."),AE.forEach(t),cze=i(ZN),Ma=n(ZN,"LI",{});var LE=s(Ma);fze=r(LE,"a path to a "),YX=n(LE,"EM",{});var pEr=s(YX);mze=r(pEr,"directory"),pEr.forEach(t),hze=r(LE,` containing a feature extractor file saved using the
`),_8=n(LE,"A",{href:!0});var _Er=s(_8);gze=r(_Er,"save_pretrained()"),_Er.forEach(t),uze=r(LE,` method, e.g.,
`),ZX=n(LE,"CODE",{});var vEr=s(ZX);pze=r(vEr,"./my_model_directory/"),vEr.forEach(t),_ze=r(LE,"."),LE.forEach(t),vze=i(ZN),N=n(ZN,"LI",{});var G=s(N);bze=r(G,"a path or url to a saved feature extractor JSON "),eQ=n(G,"EM",{});var bEr=s(eQ);Tze=r(bEr,"file"),bEr.forEach(t),Fze=r(G,`, e.g.,
`),oQ=n(G,"CODE",{});var TEr=s(oQ);Eze=r(TEr,"./my_model_directory/preprocessor_config.json"),TEr.forEach(t),Cze=r(G,`.
cache`),si=n(G,"EM",{});var ej=s(si);Mze=r(ej,"dir ("),rQ=n(ej,"CODE",{});var FEr=s(rQ);yze=r(FEr,"str"),FEr.forEach(t),wze=r(ej," or "),tQ=n(ej,"CODE",{});var EEr=s(tQ);Aze=r(EEr,"os.PathLike"),EEr.forEach(t),Lze=r(ej,", _optional"),ej.forEach(t),Bze=r(G,`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),HC=n(G,"EM",{});var Twe=s(HC);xze=r(Twe,"download ("),aQ=n(Twe,"CODE",{});var CEr=s(aQ);kze=r(CEr,"bool"),CEr.forEach(t),Rze=r(Twe,", _optional"),Twe.forEach(t),Pze=r(G,", defaults to "),nQ=n(G,"CODE",{});var MEr=s(nQ);Sze=r(MEr,"False"),MEr.forEach(t),$ze=r(G,`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),UC=n(G,"EM",{});var Fwe=s(UC);Ize=r(Fwe,"download ("),sQ=n(Fwe,"CODE",{});var yEr=s(sQ);Dze=r(yEr,"bool"),yEr.forEach(t),Nze=r(Fwe,", _optional"),Fwe.forEach(t),jze=r(G,", defaults to "),lQ=n(G,"CODE",{});var wEr=s(lQ);Oze=r(wEr,"False"),wEr.forEach(t),Gze=r(G,`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),iQ=n(G,"CODE",{});var AEr=s(iQ);qze=r(AEr,"Dict[str, str]"),AEr.forEach(t),zze=r(G,", "),dQ=n(G,"EM",{});var LEr=s(dQ);Xze=r(LEr,"optional"),LEr.forEach(t),Qze=r(G,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),cQ=n(G,"CODE",{});var BEr=s(cQ);Vze=r(BEr,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),BEr.forEach(t),Wze=r(G,` The proxies are used on each request.
use`),JC=n(G,"EM",{});var Ewe=s(JC);Hze=r(Ewe,"auth_token ("),fQ=n(Ewe,"CODE",{});var xEr=s(fQ);Uze=r(xEr,"str"),xEr.forEach(t),Jze=r(Ewe," or _bool"),Ewe.forEach(t),Kze=r(G,", "),mQ=n(G,"EM",{});var kEr=s(mQ);Yze=r(kEr,"optional"),kEr.forEach(t),Zze=r(G,`):
The token to use as HTTP bearer authorization for remote files. If `),hQ=n(G,"CODE",{});var REr=s(hQ);eXe=r(REr,"True"),REr.forEach(t),oXe=r(G,`, will use the token
generated when running `),gQ=n(G,"CODE",{});var PEr=s(gQ);rXe=r(PEr,"transformers-cli login"),PEr.forEach(t),tXe=r(G," (stored in "),uQ=n(G,"CODE",{});var SEr=s(uQ);aXe=r(SEr,"~/.huggingface"),SEr.forEach(t),nXe=r(G,`).
revision(`),pQ=n(G,"CODE",{});var $Er=s(pQ);sXe=r($Er,"str"),$Er.forEach(t),lXe=r(G,", "),_Q=n(G,"EM",{});var IEr=s(_Q);iXe=r(IEr,"optional"),IEr.forEach(t),dXe=r(G,", defaults to "),vQ=n(G,"CODE",{});var DEr=s(vQ);cXe=r(DEr,'"main"'),DEr.forEach(t),fXe=r(G,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),bQ=n(G,"CODE",{});var NEr=s(bQ);mXe=r(NEr,"revision"),NEr.forEach(t),hXe=r(G,` can be any
identifier allowed by git.
return`),KC=n(G,"EM",{});var Cwe=s(KC);gXe=r(Cwe,"unused_kwargs ("),TQ=n(Cwe,"CODE",{});var jEr=s(TQ);uXe=r(jEr,"bool"),jEr.forEach(t),pXe=r(Cwe,", _optional"),Cwe.forEach(t),_Xe=r(G,", defaults to "),FQ=n(G,"CODE",{});var OEr=s(FQ);vXe=r(OEr,"False"),OEr.forEach(t),bXe=r(G,`):
If `),EQ=n(G,"CODE",{});var GEr=s(EQ);TXe=r(GEr,"False"),GEr.forEach(t),FXe=r(G,", then this function returns just the final feature extractor object. If "),CQ=n(G,"CODE",{});var qEr=s(CQ);EXe=r(qEr,"True"),qEr.forEach(t),CXe=r(G,`,
then this functions returns a `),MQ=n(G,"CODE",{});var zEr=s(MQ);MXe=r(zEr,"Tuple(feature_extractor, unused_kwargs)"),zEr.forEach(t),yXe=r(G," where "),yQ=n(G,"EM",{});var XEr=s(yQ);wXe=r(XEr,"unused_kwargs"),XEr.forEach(t),AXe=r(G,` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),wQ=n(G,"CODE",{});var QEr=s(wQ);LXe=r(QEr,"kwargs"),QEr.forEach(t),BXe=r(G," which has not been used to update "),AQ=n(G,"CODE",{});var VEr=s(AQ);xXe=r(VEr,"feature_extractor"),VEr.forEach(t),kXe=r(G,` and is otherwise ignored.
kwargs (`),LQ=n(G,"CODE",{});var WEr=s(LQ);RXe=r(WEr,"Dict[str, Any]"),WEr.forEach(t),PXe=r(G,", "),BQ=n(G,"EM",{});var HEr=s(BQ);SXe=r(HEr,"optional"),HEr.forEach(t),$Xe=r(G,`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),xQ=n(G,"EM",{});var UEr=s(xQ);IXe=r(UEr,"not"),UEr.forEach(t),DXe=r(G,` feature extractor attributes is
controlled by the `),kQ=n(G,"CODE",{});var JEr=s(kQ);NXe=r(JEr,"return_unused_kwargs"),JEr.forEach(t),jXe=r(G," keyword parameter."),G.forEach(t),ZN.forEach(t),OXe=i(Ze),m(Wm.$$.fragment,Ze),GXe=i(Ze),RQ=n(Ze,"P",{});var KEr=s(RQ);qXe=r(KEr,"Examples:"),KEr.forEach(t),zXe=i(Ze),m(YC.$$.fragment,Ze),Ze.forEach(t),yE.forEach(t),C5e=i(c),li=n(c,"H2",{class:!0});var Mwe=s(li);Hm=n(Mwe,"A",{id:!0,class:!0,href:!0});var YEr=s(Hm);PQ=n(YEr,"SPAN",{});var ZEr=s(PQ);m(ZC.$$.fragment,ZEr),ZEr.forEach(t),YEr.forEach(t),XXe=i(Mwe),SQ=n(Mwe,"SPAN",{});var eCr=s(SQ);QXe=r(eCr,"AutoProcessor"),eCr.forEach(t),Mwe.forEach(t),M5e=i(c),Nt=n(c,"DIV",{class:!0});var BE=s(Nt);m(e3.$$.fragment,BE),VXe=i(BE),o3=n(BE,"P",{});var ywe=s(o3);WXe=r(ywe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),v8=n(ywe,"A",{href:!0});var oCr=s(v8);HXe=r(oCr,"AutoProcessor.from_pretrained()"),oCr.forEach(t),UXe=r(ywe," class method."),ywe.forEach(t),JXe=i(BE),r3=n(BE,"P",{});var wwe=s(r3);KXe=r(wwe,"This class cannot be instantiated directly using "),$Q=n(wwe,"CODE",{});var rCr=s($Q);YXe=r(rCr,"__init__()"),rCr.forEach(t),ZXe=r(wwe," (throws an error)."),wwe.forEach(t),eQe=i(BE),Ee=n(BE,"DIV",{class:!0});var eo=s(Ee);m(t3.$$.fragment,eo),oQe=i(eo),IQ=n(eo,"P",{});var tCr=s(IQ);rQe=r(tCr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),tCr.forEach(t),tQe=i(eo),ii=n(eo,"P",{});var oj=s(ii);aQe=r(oj,"The processor class to instantiate is selected based on the "),DQ=n(oj,"CODE",{});var aCr=s(DQ);nQe=r(aCr,"model_type"),aCr.forEach(t),sQe=r(oj,` property of the config object
(either passed as an argument or loaded from `),NQ=n(oj,"CODE",{});var nCr=s(NQ);lQe=r(nCr,"pretrained_model_name_or_path"),nCr.forEach(t),iQe=r(oj," if possible):"),oj.forEach(t),dQe=i(eo),to=n(eo,"UL",{});var vt=s(to);Um=n(vt,"LI",{});var L_e=s(Um);jQ=n(L_e,"STRONG",{});var sCr=s(jQ);cQe=r(sCr,"clip"),sCr.forEach(t),fQe=r(L_e," \u2014 "),b8=n(L_e,"A",{href:!0});var lCr=s(b8);mQe=r(lCr,"CLIPProcessor"),lCr.forEach(t),hQe=r(L_e," (CLIP model)"),L_e.forEach(t),gQe=i(vt),Jm=n(vt,"LI",{});var B_e=s(Jm);OQ=n(B_e,"STRONG",{});var iCr=s(OQ);uQe=r(iCr,"layoutlmv2"),iCr.forEach(t),pQe=r(B_e," \u2014 "),T8=n(B_e,"A",{href:!0});var dCr=s(T8);_Qe=r(dCr,"LayoutLMv2Processor"),dCr.forEach(t),vQe=r(B_e," (LayoutLMv2 model)"),B_e.forEach(t),bQe=i(vt),Km=n(vt,"LI",{});var x_e=s(Km);GQ=n(x_e,"STRONG",{});var cCr=s(GQ);TQe=r(cCr,"speech_to_text"),cCr.forEach(t),FQe=r(x_e," \u2014 "),F8=n(x_e,"A",{href:!0});var fCr=s(F8);EQe=r(fCr,"Speech2TextProcessor"),fCr.forEach(t),CQe=r(x_e," (Speech2Text model)"),x_e.forEach(t),MQe=i(vt),Ym=n(vt,"LI",{});var k_e=s(Ym);qQ=n(k_e,"STRONG",{});var mCr=s(qQ);yQe=r(mCr,"speech_to_text_2"),mCr.forEach(t),wQe=r(k_e," \u2014 "),E8=n(k_e,"A",{href:!0});var hCr=s(E8);AQe=r(hCr,"Speech2Text2Processor"),hCr.forEach(t),LQe=r(k_e," (Speech2Text2 model)"),k_e.forEach(t),BQe=i(vt),Zm=n(vt,"LI",{});var R_e=s(Zm);zQ=n(R_e,"STRONG",{});var gCr=s(zQ);xQe=r(gCr,"trocr"),gCr.forEach(t),kQe=r(R_e," \u2014 "),C8=n(R_e,"A",{href:!0});var uCr=s(C8);RQe=r(uCr,"TrOCRProcessor"),uCr.forEach(t),PQe=r(R_e," (TrOCR model)"),R_e.forEach(t),SQe=i(vt),eh=n(vt,"LI",{});var P_e=s(eh);XQ=n(P_e,"STRONG",{});var pCr=s(XQ);$Qe=r(pCr,"vision-text-dual-encoder"),pCr.forEach(t),IQe=r(P_e," \u2014 "),M8=n(P_e,"A",{href:!0});var _Cr=s(M8);DQe=r(_Cr,"VisionTextDualEncoderProcessor"),_Cr.forEach(t),NQe=r(P_e," (VisionTextDualEncoder model)"),P_e.forEach(t),jQe=i(vt),oh=n(vt,"LI",{});var S_e=s(oh);QQ=n(S_e,"STRONG",{});var vCr=s(QQ);OQe=r(vCr,"wav2vec2"),vCr.forEach(t),GQe=r(S_e," \u2014 "),y8=n(S_e,"A",{href:!0});var bCr=s(y8);qQe=r(bCr,"Wav2Vec2Processor"),bCr.forEach(t),zQe=r(S_e," (Wav2Vec2 model)"),S_e.forEach(t),vt.forEach(t),XQe=i(eo),di=n(eo,"P",{});var rj=s(di);QQe=r(rj,`Params:
pretrained_model_name_or_path (`),VQ=n(rj,"CODE",{});var TCr=s(VQ);VQe=r(TCr,"str"),TCr.forEach(t),WQe=r(rj," or "),WQ=n(rj,"CODE",{});var FCr=s(WQ);HQe=r(FCr,"os.PathLike"),FCr.forEach(t),UQe=r(rj,`):
This can be either:`),rj.forEach(t),JQe=i(eo),a3=n(eo,"UL",{});var Awe=s(a3);ya=n(Awe,"LI",{});var xE=s(ya);KQe=r(xE,"a string, the "),HQ=n(xE,"EM",{});var ECr=s(HQ);YQe=r(ECr,"model id"),ECr.forEach(t),ZQe=r(xE,` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),UQ=n(xE,"CODE",{});var CCr=s(UQ);eVe=r(CCr,"bert-base-uncased"),CCr.forEach(t),oVe=r(xE,`, or
namespaced under a user or organization name, like `),JQ=n(xE,"CODE",{});var MCr=s(JQ);rVe=r(MCr,"dbmdz/bert-base-german-cased"),MCr.forEach(t),tVe=r(xE,"."),xE.forEach(t),aVe=i(Awe),D=n(Awe,"LI",{});var j=s(D);nVe=r(j,"a path to a "),KQ=n(j,"EM",{});var yCr=s(KQ);sVe=r(yCr,"directory"),yCr.forEach(t),lVe=r(j," containing a processor files saved using the "),YQ=n(j,"CODE",{});var wCr=s(YQ);iVe=r(wCr,"save_pretrained()"),wCr.forEach(t),dVe=r(j,` method,
e.g., `),ZQ=n(j,"CODE",{});var ACr=s(ZQ);cVe=r(ACr,"./my_model_directory/"),ACr.forEach(t),fVe=r(j,`.
cache`),ci=n(j,"EM",{});var tj=s(ci);mVe=r(tj,"dir ("),eV=n(tj,"CODE",{});var LCr=s(eV);hVe=r(LCr,"str"),LCr.forEach(t),gVe=r(tj," or "),oV=n(tj,"CODE",{});var BCr=s(oV);uVe=r(BCr,"os.PathLike"),BCr.forEach(t),pVe=r(tj,", _optional"),tj.forEach(t),_Ve=r(j,`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),n3=n(j,"EM",{});var Lwe=s(n3);vVe=r(Lwe,"download ("),rV=n(Lwe,"CODE",{});var xCr=s(rV);bVe=r(xCr,"bool"),xCr.forEach(t),TVe=r(Lwe,", _optional"),Lwe.forEach(t),FVe=r(j,", defaults to "),tV=n(j,"CODE",{});var kCr=s(tV);EVe=r(kCr,"False"),kCr.forEach(t),CVe=r(j,`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),s3=n(j,"EM",{});var Bwe=s(s3);MVe=r(Bwe,"download ("),aV=n(Bwe,"CODE",{});var RCr=s(aV);yVe=r(RCr,"bool"),RCr.forEach(t),wVe=r(Bwe,", _optional"),Bwe.forEach(t),AVe=r(j,", defaults to "),nV=n(j,"CODE",{});var PCr=s(nV);LVe=r(PCr,"False"),PCr.forEach(t),BVe=r(j,`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),sV=n(j,"CODE",{});var SCr=s(sV);xVe=r(SCr,"Dict[str, str]"),SCr.forEach(t),kVe=r(j,", "),lV=n(j,"EM",{});var $Cr=s(lV);RVe=r($Cr,"optional"),$Cr.forEach(t),PVe=r(j,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),iV=n(j,"CODE",{});var ICr=s(iV);SVe=r(ICr,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),ICr.forEach(t),$Ve=r(j,` The proxies are used on each request.
use`),l3=n(j,"EM",{});var xwe=s(l3);IVe=r(xwe,"auth_token ("),dV=n(xwe,"CODE",{});var DCr=s(dV);DVe=r(DCr,"str"),DCr.forEach(t),NVe=r(xwe," or _bool"),xwe.forEach(t),jVe=r(j,", "),cV=n(j,"EM",{});var NCr=s(cV);OVe=r(NCr,"optional"),NCr.forEach(t),GVe=r(j,`):
The token to use as HTTP bearer authorization for remote files. If `),fV=n(j,"CODE",{});var jCr=s(fV);qVe=r(jCr,"True"),jCr.forEach(t),zVe=r(j,`, will use the token
generated when running `),mV=n(j,"CODE",{});var OCr=s(mV);XVe=r(OCr,"transformers-cli login"),OCr.forEach(t),QVe=r(j," (stored in "),hV=n(j,"CODE",{});var GCr=s(hV);VVe=r(GCr,"~/.huggingface"),GCr.forEach(t),WVe=r(j,`).
revision (`),gV=n(j,"CODE",{});var qCr=s(gV);HVe=r(qCr,"str"),qCr.forEach(t),UVe=r(j,", "),uV=n(j,"EM",{});var zCr=s(uV);JVe=r(zCr,"optional"),zCr.forEach(t),KVe=r(j,", defaults to "),pV=n(j,"CODE",{});var XCr=s(pV);YVe=r(XCr,'"main"'),XCr.forEach(t),ZVe=r(j,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),_V=n(j,"CODE",{});var QCr=s(_V);eWe=r(QCr,"revision"),QCr.forEach(t),oWe=r(j,` can be any
identifier allowed by git.
return`),i3=n(j,"EM",{});var kwe=s(i3);rWe=r(kwe,"unused_kwargs ("),vV=n(kwe,"CODE",{});var VCr=s(vV);tWe=r(VCr,"bool"),VCr.forEach(t),aWe=r(kwe,", _optional"),kwe.forEach(t),nWe=r(j,", defaults to "),bV=n(j,"CODE",{});var WCr=s(bV);sWe=r(WCr,"False"),WCr.forEach(t),lWe=r(j,`):
If `),TV=n(j,"CODE",{});var HCr=s(TV);iWe=r(HCr,"False"),HCr.forEach(t),dWe=r(j,", then this function returns just the final feature extractor object. If "),FV=n(j,"CODE",{});var UCr=s(FV);cWe=r(UCr,"True"),UCr.forEach(t),fWe=r(j,`,
then this functions returns a `),EV=n(j,"CODE",{});var JCr=s(EV);mWe=r(JCr,"Tuple(feature_extractor, unused_kwargs)"),JCr.forEach(t),hWe=r(j," where "),CV=n(j,"EM",{});var KCr=s(CV);gWe=r(KCr,"unused_kwargs"),KCr.forEach(t),uWe=r(j,` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),MV=n(j,"CODE",{});var YCr=s(MV);pWe=r(YCr,"kwargs"),YCr.forEach(t),_We=r(j," which has not been used to update "),yV=n(j,"CODE",{});var ZCr=s(yV);vWe=r(ZCr,"feature_extractor"),ZCr.forEach(t),bWe=r(j,` and is otherwise ignored.
kwargs (`),wV=n(j,"CODE",{});var e3r=s(wV);TWe=r(e3r,"Dict[str, Any]"),e3r.forEach(t),FWe=r(j,", "),AV=n(j,"EM",{});var o3r=s(AV);EWe=r(o3r,"optional"),o3r.forEach(t),CWe=r(j,`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),LV=n(j,"EM",{});var r3r=s(LV);MWe=r(r3r,"not"),r3r.forEach(t),yWe=r(j,` feature extractor attributes is
controlled by the `),BV=n(j,"CODE",{});var t3r=s(BV);wWe=r(t3r,"return_unused_kwargs"),t3r.forEach(t),AWe=r(j," keyword parameter."),j.forEach(t),Awe.forEach(t),LWe=i(eo),m(rh.$$.fragment,eo),BWe=i(eo),xV=n(eo,"P",{});var a3r=s(xV);xWe=r(a3r,"Examples:"),a3r.forEach(t),kWe=i(eo),m(d3.$$.fragment,eo),eo.forEach(t),BE.forEach(t),y5e=i(c),fi=n(c,"H2",{class:!0});var Rwe=s(fi);th=n(Rwe,"A",{id:!0,class:!0,href:!0});var n3r=s(th);kV=n(n3r,"SPAN",{});var s3r=s(kV);m(c3.$$.fragment,s3r),s3r.forEach(t),n3r.forEach(t),RWe=i(Rwe),RV=n(Rwe,"SPAN",{});var l3r=s(RV);PWe=r(l3r,"AutoModel"),l3r.forEach(t),Rwe.forEach(t),w5e=i(c),jo=n(c,"DIV",{class:!0});var hs=s(jo);m(f3.$$.fragment,hs),SWe=i(hs),mi=n(hs,"P",{});var aj=s(mi);$We=r(aj,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),PV=n(aj,"CODE",{});var i3r=s(PV);IWe=r(i3r,"from_pretrained()"),i3r.forEach(t),DWe=r(aj,` class method or the
`),SV=n(aj,"CODE",{});var d3r=s(SV);NWe=r(d3r,"from_config()"),d3r.forEach(t),jWe=r(aj," class method."),aj.forEach(t),OWe=i(hs),m3=n(hs,"P",{});var Pwe=s(m3);GWe=r(Pwe,"This class cannot be instantiated directly using "),$V=n(Pwe,"CODE",{});var c3r=s($V);qWe=r(c3r,"__init__()"),c3r.forEach(t),zWe=r(Pwe," (throws an error)."),Pwe.forEach(t),XWe=i(hs),kr=n(hs,"DIV",{class:!0});var gs=s(kr);m(h3.$$.fragment,gs),QWe=i(gs),IV=n(gs,"P",{});var f3r=s(IV);VWe=r(f3r,"Instantiates one of the base model classes of the library from a configuration."),f3r.forEach(t),WWe=i(gs),hi=n(gs,"P",{});var nj=s(hi);HWe=r(nj,`Note:
Loading a model from its configuration file does `),DV=n(nj,"STRONG",{});var m3r=s(DV);UWe=r(m3r,"not"),m3r.forEach(t),JWe=r(nj,` load the model weights. It only affects the
model\u2019s configuration. Use `),NV=n(nj,"CODE",{});var h3r=s(NV);KWe=r(h3r,"from_pretrained()"),h3r.forEach(t),YWe=r(nj,` to load the model
weights.`),nj.forEach(t),ZWe=i(gs),jV=n(gs,"P",{});var g3r=s(jV);eHe=r(g3r,"Examples:"),g3r.forEach(t),oHe=i(gs),m(g3.$$.fragment,gs),gs.forEach(t),rHe=i(hs),$e=n(hs,"DIV",{class:!0});var bt=s($e);m(u3.$$.fragment,bt),tHe=i(bt),OV=n(bt,"P",{});var u3r=s(OV);aHe=r(u3r,"Instantiate one of the base model classes of the library from a pretrained model."),u3r.forEach(t),nHe=i(bt),wa=n(bt,"P",{});var kE=s(wa);sHe=r(kE,"The model class to instantiate is selected based on the "),GV=n(kE,"CODE",{});var p3r=s(GV);lHe=r(p3r,"model_type"),p3r.forEach(t),iHe=r(kE,` property of the config object (either
passed as an argument or loaded from `),qV=n(kE,"CODE",{});var _3r=s(qV);dHe=r(_3r,"pretrained_model_name_or_path"),_3r.forEach(t),cHe=r(kE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zV=n(kE,"CODE",{});var v3r=s(zV);fHe=r(v3r,"pretrained_model_name_or_path"),v3r.forEach(t),mHe=r(kE,":"),kE.forEach(t),hHe=i(bt),F=n(bt,"UL",{});var E=s(F);ah=n(E,"LI",{});var $_e=s(ah);XV=n($_e,"STRONG",{});var b3r=s(XV);gHe=r(b3r,"albert"),b3r.forEach(t),uHe=r($_e," \u2014 "),w8=n($_e,"A",{href:!0});var T3r=s(w8);pHe=r(T3r,"AlbertModel"),T3r.forEach(t),_He=r($_e," (ALBERT model)"),$_e.forEach(t),vHe=i(E),nh=n(E,"LI",{});var I_e=s(nh);QV=n(I_e,"STRONG",{});var F3r=s(QV);bHe=r(F3r,"bart"),F3r.forEach(t),THe=r(I_e," \u2014 "),A8=n(I_e,"A",{href:!0});var E3r=s(A8);FHe=r(E3r,"BartModel"),E3r.forEach(t),EHe=r(I_e," (BART model)"),I_e.forEach(t),CHe=i(E),sh=n(E,"LI",{});var D_e=s(sh);VV=n(D_e,"STRONG",{});var C3r=s(VV);MHe=r(C3r,"beit"),C3r.forEach(t),yHe=r(D_e," \u2014 "),L8=n(D_e,"A",{href:!0});var M3r=s(L8);wHe=r(M3r,"BeitModel"),M3r.forEach(t),AHe=r(D_e," (BEiT model)"),D_e.forEach(t),LHe=i(E),lh=n(E,"LI",{});var N_e=s(lh);WV=n(N_e,"STRONG",{});var y3r=s(WV);BHe=r(y3r,"bert"),y3r.forEach(t),xHe=r(N_e," \u2014 "),B8=n(N_e,"A",{href:!0});var w3r=s(B8);kHe=r(w3r,"BertModel"),w3r.forEach(t),RHe=r(N_e," (BERT model)"),N_e.forEach(t),PHe=i(E),ih=n(E,"LI",{});var j_e=s(ih);HV=n(j_e,"STRONG",{});var A3r=s(HV);SHe=r(A3r,"bert-generation"),A3r.forEach(t),$He=r(j_e," \u2014 "),x8=n(j_e,"A",{href:!0});var L3r=s(x8);IHe=r(L3r,"BertGenerationEncoder"),L3r.forEach(t),DHe=r(j_e," (Bert Generation model)"),j_e.forEach(t),NHe=i(E),dh=n(E,"LI",{});var O_e=s(dh);UV=n(O_e,"STRONG",{});var B3r=s(UV);jHe=r(B3r,"big_bird"),B3r.forEach(t),OHe=r(O_e," \u2014 "),k8=n(O_e,"A",{href:!0});var x3r=s(k8);GHe=r(x3r,"BigBirdModel"),x3r.forEach(t),qHe=r(O_e," (BigBird model)"),O_e.forEach(t),zHe=i(E),ch=n(E,"LI",{});var G_e=s(ch);JV=n(G_e,"STRONG",{});var k3r=s(JV);XHe=r(k3r,"bigbird_pegasus"),k3r.forEach(t),QHe=r(G_e," \u2014 "),R8=n(G_e,"A",{href:!0});var R3r=s(R8);VHe=r(R3r,"BigBirdPegasusModel"),R3r.forEach(t),WHe=r(G_e," (BigBirdPegasus model)"),G_e.forEach(t),HHe=i(E),fh=n(E,"LI",{});var q_e=s(fh);KV=n(q_e,"STRONG",{});var P3r=s(KV);UHe=r(P3r,"blenderbot"),P3r.forEach(t),JHe=r(q_e," \u2014 "),P8=n(q_e,"A",{href:!0});var S3r=s(P8);KHe=r(S3r,"BlenderbotModel"),S3r.forEach(t),YHe=r(q_e," (Blenderbot model)"),q_e.forEach(t),ZHe=i(E),mh=n(E,"LI",{});var z_e=s(mh);YV=n(z_e,"STRONG",{});var $3r=s(YV);eUe=r($3r,"blenderbot-small"),$3r.forEach(t),oUe=r(z_e," \u2014 "),S8=n(z_e,"A",{href:!0});var I3r=s(S8);rUe=r(I3r,"BlenderbotSmallModel"),I3r.forEach(t),tUe=r(z_e," (BlenderbotSmall model)"),z_e.forEach(t),aUe=i(E),hh=n(E,"LI",{});var X_e=s(hh);ZV=n(X_e,"STRONG",{});var D3r=s(ZV);nUe=r(D3r,"camembert"),D3r.forEach(t),sUe=r(X_e," \u2014 "),$8=n(X_e,"A",{href:!0});var N3r=s($8);lUe=r(N3r,"CamembertModel"),N3r.forEach(t),iUe=r(X_e," (CamemBERT model)"),X_e.forEach(t),dUe=i(E),gh=n(E,"LI",{});var Q_e=s(gh);eW=n(Q_e,"STRONG",{});var j3r=s(eW);cUe=r(j3r,"canine"),j3r.forEach(t),fUe=r(Q_e," \u2014 "),I8=n(Q_e,"A",{href:!0});var O3r=s(I8);mUe=r(O3r,"CanineModel"),O3r.forEach(t),hUe=r(Q_e," (Canine model)"),Q_e.forEach(t),gUe=i(E),uh=n(E,"LI",{});var V_e=s(uh);oW=n(V_e,"STRONG",{});var G3r=s(oW);uUe=r(G3r,"clip"),G3r.forEach(t),pUe=r(V_e," \u2014 "),D8=n(V_e,"A",{href:!0});var q3r=s(D8);_Ue=r(q3r,"CLIPModel"),q3r.forEach(t),vUe=r(V_e," (CLIP model)"),V_e.forEach(t),bUe=i(E),ph=n(E,"LI",{});var W_e=s(ph);rW=n(W_e,"STRONG",{});var z3r=s(rW);TUe=r(z3r,"convbert"),z3r.forEach(t),FUe=r(W_e," \u2014 "),N8=n(W_e,"A",{href:!0});var X3r=s(N8);EUe=r(X3r,"ConvBertModel"),X3r.forEach(t),CUe=r(W_e," (ConvBERT model)"),W_e.forEach(t),MUe=i(E),_h=n(E,"LI",{});var H_e=s(_h);tW=n(H_e,"STRONG",{});var Q3r=s(tW);yUe=r(Q3r,"ctrl"),Q3r.forEach(t),wUe=r(H_e," \u2014 "),j8=n(H_e,"A",{href:!0});var V3r=s(j8);AUe=r(V3r,"CTRLModel"),V3r.forEach(t),LUe=r(H_e," (CTRL model)"),H_e.forEach(t),BUe=i(E),vh=n(E,"LI",{});var U_e=s(vh);aW=n(U_e,"STRONG",{});var W3r=s(aW);xUe=r(W3r,"deberta"),W3r.forEach(t),kUe=r(U_e," \u2014 "),O8=n(U_e,"A",{href:!0});var H3r=s(O8);RUe=r(H3r,"DebertaModel"),H3r.forEach(t),PUe=r(U_e," (DeBERTa model)"),U_e.forEach(t),SUe=i(E),bh=n(E,"LI",{});var J_e=s(bh);nW=n(J_e,"STRONG",{});var U3r=s(nW);$Ue=r(U3r,"deberta-v2"),U3r.forEach(t),IUe=r(J_e," \u2014 "),G8=n(J_e,"A",{href:!0});var J3r=s(G8);DUe=r(J3r,"DebertaV2Model"),J3r.forEach(t),NUe=r(J_e," (DeBERTa-v2 model)"),J_e.forEach(t),jUe=i(E),Th=n(E,"LI",{});var K_e=s(Th);sW=n(K_e,"STRONG",{});var K3r=s(sW);OUe=r(K3r,"deit"),K3r.forEach(t),GUe=r(K_e," \u2014 "),q8=n(K_e,"A",{href:!0});var Y3r=s(q8);qUe=r(Y3r,"DeiTModel"),Y3r.forEach(t),zUe=r(K_e," (DeiT model)"),K_e.forEach(t),XUe=i(E),Fh=n(E,"LI",{});var Y_e=s(Fh);lW=n(Y_e,"STRONG",{});var Z3r=s(lW);QUe=r(Z3r,"detr"),Z3r.forEach(t),VUe=r(Y_e," \u2014 "),z8=n(Y_e,"A",{href:!0});var eMr=s(z8);WUe=r(eMr,"DetrModel"),eMr.forEach(t),HUe=r(Y_e," (DETR model)"),Y_e.forEach(t),UUe=i(E),Eh=n(E,"LI",{});var Z_e=s(Eh);iW=n(Z_e,"STRONG",{});var oMr=s(iW);JUe=r(oMr,"distilbert"),oMr.forEach(t),KUe=r(Z_e," \u2014 "),X8=n(Z_e,"A",{href:!0});var rMr=s(X8);YUe=r(rMr,"DistilBertModel"),rMr.forEach(t),ZUe=r(Z_e," (DistilBERT model)"),Z_e.forEach(t),eJe=i(E),Ch=n(E,"LI",{});var e1e=s(Ch);dW=n(e1e,"STRONG",{});var tMr=s(dW);oJe=r(tMr,"dpr"),tMr.forEach(t),rJe=r(e1e," \u2014 "),Q8=n(e1e,"A",{href:!0});var aMr=s(Q8);tJe=r(aMr,"DPRQuestionEncoder"),aMr.forEach(t),aJe=r(e1e," (DPR model)"),e1e.forEach(t),nJe=i(E),Mh=n(E,"LI",{});var o1e=s(Mh);cW=n(o1e,"STRONG",{});var nMr=s(cW);sJe=r(nMr,"electra"),nMr.forEach(t),lJe=r(o1e," \u2014 "),V8=n(o1e,"A",{href:!0});var sMr=s(V8);iJe=r(sMr,"ElectraModel"),sMr.forEach(t),dJe=r(o1e," (ELECTRA model)"),o1e.forEach(t),cJe=i(E),yh=n(E,"LI",{});var r1e=s(yh);fW=n(r1e,"STRONG",{});var lMr=s(fW);fJe=r(lMr,"flaubert"),lMr.forEach(t),mJe=r(r1e," \u2014 "),W8=n(r1e,"A",{href:!0});var iMr=s(W8);hJe=r(iMr,"FlaubertModel"),iMr.forEach(t),gJe=r(r1e," (FlauBERT model)"),r1e.forEach(t),uJe=i(E),wh=n(E,"LI",{});var t1e=s(wh);mW=n(t1e,"STRONG",{});var dMr=s(mW);pJe=r(dMr,"fnet"),dMr.forEach(t),_Je=r(t1e," \u2014 "),H8=n(t1e,"A",{href:!0});var cMr=s(H8);vJe=r(cMr,"FNetModel"),cMr.forEach(t),bJe=r(t1e," (FNet model)"),t1e.forEach(t),TJe=i(E),Ah=n(E,"LI",{});var a1e=s(Ah);hW=n(a1e,"STRONG",{});var fMr=s(hW);FJe=r(fMr,"fsmt"),fMr.forEach(t),EJe=r(a1e," \u2014 "),U8=n(a1e,"A",{href:!0});var mMr=s(U8);CJe=r(mMr,"FSMTModel"),mMr.forEach(t),MJe=r(a1e," (FairSeq Machine-Translation model)"),a1e.forEach(t),yJe=i(E),is=n(E,"LI",{});var b0=s(is);gW=n(b0,"STRONG",{});var hMr=s(gW);wJe=r(hMr,"funnel"),hMr.forEach(t),AJe=r(b0," \u2014 "),J8=n(b0,"A",{href:!0});var gMr=s(J8);LJe=r(gMr,"FunnelModel"),gMr.forEach(t),BJe=r(b0," or "),K8=n(b0,"A",{href:!0});var uMr=s(K8);xJe=r(uMr,"FunnelBaseModel"),uMr.forEach(t),kJe=r(b0," (Funnel Transformer model)"),b0.forEach(t),RJe=i(E),Lh=n(E,"LI",{});var n1e=s(Lh);uW=n(n1e,"STRONG",{});var pMr=s(uW);PJe=r(pMr,"gpt2"),pMr.forEach(t),SJe=r(n1e," \u2014 "),Y8=n(n1e,"A",{href:!0});var _Mr=s(Y8);$Je=r(_Mr,"GPT2Model"),_Mr.forEach(t),IJe=r(n1e," (OpenAI GPT-2 model)"),n1e.forEach(t),DJe=i(E),Bh=n(E,"LI",{});var s1e=s(Bh);pW=n(s1e,"STRONG",{});var vMr=s(pW);NJe=r(vMr,"gpt_neo"),vMr.forEach(t),jJe=r(s1e," \u2014 "),Z8=n(s1e,"A",{href:!0});var bMr=s(Z8);OJe=r(bMr,"GPTNeoModel"),bMr.forEach(t),GJe=r(s1e," (GPT Neo model)"),s1e.forEach(t),qJe=i(E),xh=n(E,"LI",{});var l1e=s(xh);_W=n(l1e,"STRONG",{});var TMr=s(_W);zJe=r(TMr,"gptj"),TMr.forEach(t),XJe=r(l1e," \u2014 "),eB=n(l1e,"A",{href:!0});var FMr=s(eB);QJe=r(FMr,"GPTJModel"),FMr.forEach(t),VJe=r(l1e," (GPT-J model)"),l1e.forEach(t),WJe=i(E),kh=n(E,"LI",{});var i1e=s(kh);vW=n(i1e,"STRONG",{});var EMr=s(vW);HJe=r(EMr,"hubert"),EMr.forEach(t),UJe=r(i1e," \u2014 "),oB=n(i1e,"A",{href:!0});var CMr=s(oB);JJe=r(CMr,"HubertModel"),CMr.forEach(t),KJe=r(i1e," (Hubert model)"),i1e.forEach(t),YJe=i(E),Rh=n(E,"LI",{});var d1e=s(Rh);bW=n(d1e,"STRONG",{});var MMr=s(bW);ZJe=r(MMr,"ibert"),MMr.forEach(t),eKe=r(d1e," \u2014 "),rB=n(d1e,"A",{href:!0});var yMr=s(rB);oKe=r(yMr,"IBertModel"),yMr.forEach(t),rKe=r(d1e," (I-BERT model)"),d1e.forEach(t),tKe=i(E),Ph=n(E,"LI",{});var c1e=s(Ph);TW=n(c1e,"STRONG",{});var wMr=s(TW);aKe=r(wMr,"imagegpt"),wMr.forEach(t),nKe=r(c1e," \u2014 "),tB=n(c1e,"A",{href:!0});var AMr=s(tB);sKe=r(AMr,"ImageGPTModel"),AMr.forEach(t),lKe=r(c1e," (ImageGPT model)"),c1e.forEach(t),iKe=i(E),Sh=n(E,"LI",{});var f1e=s(Sh);FW=n(f1e,"STRONG",{});var LMr=s(FW);dKe=r(LMr,"layoutlm"),LMr.forEach(t),cKe=r(f1e," \u2014 "),aB=n(f1e,"A",{href:!0});var BMr=s(aB);fKe=r(BMr,"LayoutLMModel"),BMr.forEach(t),mKe=r(f1e," (LayoutLM model)"),f1e.forEach(t),hKe=i(E),$h=n(E,"LI",{});var m1e=s($h);EW=n(m1e,"STRONG",{});var xMr=s(EW);gKe=r(xMr,"layoutlmv2"),xMr.forEach(t),uKe=r(m1e," \u2014 "),nB=n(m1e,"A",{href:!0});var kMr=s(nB);pKe=r(kMr,"LayoutLMv2Model"),kMr.forEach(t),_Ke=r(m1e," (LayoutLMv2 model)"),m1e.forEach(t),vKe=i(E),Ih=n(E,"LI",{});var h1e=s(Ih);CW=n(h1e,"STRONG",{});var RMr=s(CW);bKe=r(RMr,"led"),RMr.forEach(t),TKe=r(h1e," \u2014 "),sB=n(h1e,"A",{href:!0});var PMr=s(sB);FKe=r(PMr,"LEDModel"),PMr.forEach(t),EKe=r(h1e," (LED model)"),h1e.forEach(t),CKe=i(E),Dh=n(E,"LI",{});var g1e=s(Dh);MW=n(g1e,"STRONG",{});var SMr=s(MW);MKe=r(SMr,"longformer"),SMr.forEach(t),yKe=r(g1e," \u2014 "),lB=n(g1e,"A",{href:!0});var $Mr=s(lB);wKe=r($Mr,"LongformerModel"),$Mr.forEach(t),AKe=r(g1e," (Longformer model)"),g1e.forEach(t),LKe=i(E),Nh=n(E,"LI",{});var u1e=s(Nh);yW=n(u1e,"STRONG",{});var IMr=s(yW);BKe=r(IMr,"luke"),IMr.forEach(t),xKe=r(u1e," \u2014 "),iB=n(u1e,"A",{href:!0});var DMr=s(iB);kKe=r(DMr,"LukeModel"),DMr.forEach(t),RKe=r(u1e," (LUKE model)"),u1e.forEach(t),PKe=i(E),jh=n(E,"LI",{});var p1e=s(jh);wW=n(p1e,"STRONG",{});var NMr=s(wW);SKe=r(NMr,"lxmert"),NMr.forEach(t),$Ke=r(p1e," \u2014 "),dB=n(p1e,"A",{href:!0});var jMr=s(dB);IKe=r(jMr,"LxmertModel"),jMr.forEach(t),DKe=r(p1e," (LXMERT model)"),p1e.forEach(t),NKe=i(E),Oh=n(E,"LI",{});var _1e=s(Oh);AW=n(_1e,"STRONG",{});var OMr=s(AW);jKe=r(OMr,"m2m_100"),OMr.forEach(t),OKe=r(_1e," \u2014 "),cB=n(_1e,"A",{href:!0});var GMr=s(cB);GKe=r(GMr,"M2M100Model"),GMr.forEach(t),qKe=r(_1e," (M2M100 model)"),_1e.forEach(t),zKe=i(E),Gh=n(E,"LI",{});var v1e=s(Gh);LW=n(v1e,"STRONG",{});var qMr=s(LW);XKe=r(qMr,"marian"),qMr.forEach(t),QKe=r(v1e," \u2014 "),fB=n(v1e,"A",{href:!0});var zMr=s(fB);VKe=r(zMr,"MarianModel"),zMr.forEach(t),WKe=r(v1e," (Marian model)"),v1e.forEach(t),HKe=i(E),qh=n(E,"LI",{});var b1e=s(qh);BW=n(b1e,"STRONG",{});var XMr=s(BW);UKe=r(XMr,"mbart"),XMr.forEach(t),JKe=r(b1e," \u2014 "),mB=n(b1e,"A",{href:!0});var QMr=s(mB);KKe=r(QMr,"MBartModel"),QMr.forEach(t),YKe=r(b1e," (mBART model)"),b1e.forEach(t),ZKe=i(E),zh=n(E,"LI",{});var T1e=s(zh);xW=n(T1e,"STRONG",{});var VMr=s(xW);eYe=r(VMr,"megatron-bert"),VMr.forEach(t),oYe=r(T1e," \u2014 "),hB=n(T1e,"A",{href:!0});var WMr=s(hB);rYe=r(WMr,"MegatronBertModel"),WMr.forEach(t),tYe=r(T1e," (MegatronBert model)"),T1e.forEach(t),aYe=i(E),Xh=n(E,"LI",{});var F1e=s(Xh);kW=n(F1e,"STRONG",{});var HMr=s(kW);nYe=r(HMr,"mobilebert"),HMr.forEach(t),sYe=r(F1e," \u2014 "),gB=n(F1e,"A",{href:!0});var UMr=s(gB);lYe=r(UMr,"MobileBertModel"),UMr.forEach(t),iYe=r(F1e," (MobileBERT model)"),F1e.forEach(t),dYe=i(E),Qh=n(E,"LI",{});var E1e=s(Qh);RW=n(E1e,"STRONG",{});var JMr=s(RW);cYe=r(JMr,"mpnet"),JMr.forEach(t),fYe=r(E1e," \u2014 "),uB=n(E1e,"A",{href:!0});var KMr=s(uB);mYe=r(KMr,"MPNetModel"),KMr.forEach(t),hYe=r(E1e," (MPNet model)"),E1e.forEach(t),gYe=i(E),Vh=n(E,"LI",{});var C1e=s(Vh);PW=n(C1e,"STRONG",{});var YMr=s(PW);uYe=r(YMr,"mt5"),YMr.forEach(t),pYe=r(C1e," \u2014 "),pB=n(C1e,"A",{href:!0});var ZMr=s(pB);_Ye=r(ZMr,"MT5Model"),ZMr.forEach(t),vYe=r(C1e," (mT5 model)"),C1e.forEach(t),bYe=i(E),Wh=n(E,"LI",{});var M1e=s(Wh);SW=n(M1e,"STRONG",{});var e5r=s(SW);TYe=r(e5r,"openai-gpt"),e5r.forEach(t),FYe=r(M1e," \u2014 "),_B=n(M1e,"A",{href:!0});var o5r=s(_B);EYe=r(o5r,"OpenAIGPTModel"),o5r.forEach(t),CYe=r(M1e," (OpenAI GPT model)"),M1e.forEach(t),MYe=i(E),Hh=n(E,"LI",{});var y1e=s(Hh);$W=n(y1e,"STRONG",{});var r5r=s($W);yYe=r(r5r,"pegasus"),r5r.forEach(t),wYe=r(y1e," \u2014 "),vB=n(y1e,"A",{href:!0});var t5r=s(vB);AYe=r(t5r,"PegasusModel"),t5r.forEach(t),LYe=r(y1e," (Pegasus model)"),y1e.forEach(t),BYe=i(E),Uh=n(E,"LI",{});var w1e=s(Uh);IW=n(w1e,"STRONG",{});var a5r=s(IW);xYe=r(a5r,"perceiver"),a5r.forEach(t),kYe=r(w1e," \u2014 "),bB=n(w1e,"A",{href:!0});var n5r=s(bB);RYe=r(n5r,"PerceiverModel"),n5r.forEach(t),PYe=r(w1e," (Perceiver model)"),w1e.forEach(t),SYe=i(E),Jh=n(E,"LI",{});var A1e=s(Jh);DW=n(A1e,"STRONG",{});var s5r=s(DW);$Ye=r(s5r,"prophetnet"),s5r.forEach(t),IYe=r(A1e," \u2014 "),TB=n(A1e,"A",{href:!0});var l5r=s(TB);DYe=r(l5r,"ProphetNetModel"),l5r.forEach(t),NYe=r(A1e," (ProphetNet model)"),A1e.forEach(t),jYe=i(E),Kh=n(E,"LI",{});var L1e=s(Kh);NW=n(L1e,"STRONG",{});var i5r=s(NW);OYe=r(i5r,"qdqbert"),i5r.forEach(t),GYe=r(L1e," \u2014 "),FB=n(L1e,"A",{href:!0});var d5r=s(FB);qYe=r(d5r,"QDQBertModel"),d5r.forEach(t),zYe=r(L1e," (QDQBert model)"),L1e.forEach(t),XYe=i(E),Yh=n(E,"LI",{});var B1e=s(Yh);jW=n(B1e,"STRONG",{});var c5r=s(jW);QYe=r(c5r,"reformer"),c5r.forEach(t),VYe=r(B1e," \u2014 "),EB=n(B1e,"A",{href:!0});var f5r=s(EB);WYe=r(f5r,"ReformerModel"),f5r.forEach(t),HYe=r(B1e," (Reformer model)"),B1e.forEach(t),UYe=i(E),Zh=n(E,"LI",{});var x1e=s(Zh);OW=n(x1e,"STRONG",{});var m5r=s(OW);JYe=r(m5r,"rembert"),m5r.forEach(t),KYe=r(x1e," \u2014 "),CB=n(x1e,"A",{href:!0});var h5r=s(CB);YYe=r(h5r,"RemBertModel"),h5r.forEach(t),ZYe=r(x1e," (RemBERT model)"),x1e.forEach(t),eZe=i(E),eg=n(E,"LI",{});var k1e=s(eg);GW=n(k1e,"STRONG",{});var g5r=s(GW);oZe=r(g5r,"retribert"),g5r.forEach(t),rZe=r(k1e," \u2014 "),MB=n(k1e,"A",{href:!0});var u5r=s(MB);tZe=r(u5r,"RetriBertModel"),u5r.forEach(t),aZe=r(k1e," (RetriBERT model)"),k1e.forEach(t),nZe=i(E),og=n(E,"LI",{});var R1e=s(og);qW=n(R1e,"STRONG",{});var p5r=s(qW);sZe=r(p5r,"roberta"),p5r.forEach(t),lZe=r(R1e," \u2014 "),yB=n(R1e,"A",{href:!0});var _5r=s(yB);iZe=r(_5r,"RobertaModel"),_5r.forEach(t),dZe=r(R1e," (RoBERTa model)"),R1e.forEach(t),cZe=i(E),rg=n(E,"LI",{});var P1e=s(rg);zW=n(P1e,"STRONG",{});var v5r=s(zW);fZe=r(v5r,"roformer"),v5r.forEach(t),mZe=r(P1e," \u2014 "),wB=n(P1e,"A",{href:!0});var b5r=s(wB);hZe=r(b5r,"RoFormerModel"),b5r.forEach(t),gZe=r(P1e," (RoFormer model)"),P1e.forEach(t),uZe=i(E),tg=n(E,"LI",{});var S1e=s(tg);XW=n(S1e,"STRONG",{});var T5r=s(XW);pZe=r(T5r,"segformer"),T5r.forEach(t),_Ze=r(S1e," \u2014 "),AB=n(S1e,"A",{href:!0});var F5r=s(AB);vZe=r(F5r,"SegformerModel"),F5r.forEach(t),bZe=r(S1e," (SegFormer model)"),S1e.forEach(t),TZe=i(E),ag=n(E,"LI",{});var $1e=s(ag);QW=n($1e,"STRONG",{});var E5r=s(QW);FZe=r(E5r,"sew"),E5r.forEach(t),EZe=r($1e," \u2014 "),LB=n($1e,"A",{href:!0});var C5r=s(LB);CZe=r(C5r,"SEWModel"),C5r.forEach(t),MZe=r($1e," (SEW model)"),$1e.forEach(t),yZe=i(E),ng=n(E,"LI",{});var I1e=s(ng);VW=n(I1e,"STRONG",{});var M5r=s(VW);wZe=r(M5r,"sew-d"),M5r.forEach(t),AZe=r(I1e," \u2014 "),BB=n(I1e,"A",{href:!0});var y5r=s(BB);LZe=r(y5r,"SEWDModel"),y5r.forEach(t),BZe=r(I1e," (SEW-D model)"),I1e.forEach(t),xZe=i(E),sg=n(E,"LI",{});var D1e=s(sg);WW=n(D1e,"STRONG",{});var w5r=s(WW);kZe=r(w5r,"speech_to_text"),w5r.forEach(t),RZe=r(D1e," \u2014 "),xB=n(D1e,"A",{href:!0});var A5r=s(xB);PZe=r(A5r,"Speech2TextModel"),A5r.forEach(t),SZe=r(D1e," (Speech2Text model)"),D1e.forEach(t),$Ze=i(E),lg=n(E,"LI",{});var N1e=s(lg);HW=n(N1e,"STRONG",{});var L5r=s(HW);IZe=r(L5r,"splinter"),L5r.forEach(t),DZe=r(N1e," \u2014 "),kB=n(N1e,"A",{href:!0});var B5r=s(kB);NZe=r(B5r,"SplinterModel"),B5r.forEach(t),jZe=r(N1e," (Splinter model)"),N1e.forEach(t),OZe=i(E),ig=n(E,"LI",{});var j1e=s(ig);UW=n(j1e,"STRONG",{});var x5r=s(UW);GZe=r(x5r,"squeezebert"),x5r.forEach(t),qZe=r(j1e," \u2014 "),RB=n(j1e,"A",{href:!0});var k5r=s(RB);zZe=r(k5r,"SqueezeBertModel"),k5r.forEach(t),XZe=r(j1e," (SqueezeBERT model)"),j1e.forEach(t),QZe=i(E),dg=n(E,"LI",{});var O1e=s(dg);JW=n(O1e,"STRONG",{});var R5r=s(JW);VZe=r(R5r,"t5"),R5r.forEach(t),WZe=r(O1e," \u2014 "),PB=n(O1e,"A",{href:!0});var P5r=s(PB);HZe=r(P5r,"T5Model"),P5r.forEach(t),UZe=r(O1e," (T5 model)"),O1e.forEach(t),JZe=i(E),cg=n(E,"LI",{});var G1e=s(cg);KW=n(G1e,"STRONG",{});var S5r=s(KW);KZe=r(S5r,"tapas"),S5r.forEach(t),YZe=r(G1e," \u2014 "),SB=n(G1e,"A",{href:!0});var $5r=s(SB);ZZe=r($5r,"TapasModel"),$5r.forEach(t),eeo=r(G1e," (TAPAS model)"),G1e.forEach(t),oeo=i(E),fg=n(E,"LI",{});var q1e=s(fg);YW=n(q1e,"STRONG",{});var I5r=s(YW);reo=r(I5r,"transfo-xl"),I5r.forEach(t),teo=r(q1e," \u2014 "),$B=n(q1e,"A",{href:!0});var D5r=s($B);aeo=r(D5r,"TransfoXLModel"),D5r.forEach(t),neo=r(q1e," (Transformer-XL model)"),q1e.forEach(t),seo=i(E),mg=n(E,"LI",{});var z1e=s(mg);ZW=n(z1e,"STRONG",{});var N5r=s(ZW);leo=r(N5r,"unispeech"),N5r.forEach(t),ieo=r(z1e," \u2014 "),IB=n(z1e,"A",{href:!0});var j5r=s(IB);deo=r(j5r,"UniSpeechModel"),j5r.forEach(t),ceo=r(z1e," (UniSpeech model)"),z1e.forEach(t),feo=i(E),hg=n(E,"LI",{});var X1e=s(hg);eH=n(X1e,"STRONG",{});var O5r=s(eH);meo=r(O5r,"unispeech-sat"),O5r.forEach(t),heo=r(X1e," \u2014 "),DB=n(X1e,"A",{href:!0});var G5r=s(DB);geo=r(G5r,"UniSpeechSatModel"),G5r.forEach(t),ueo=r(X1e," (UniSpeechSat model)"),X1e.forEach(t),peo=i(E),gg=n(E,"LI",{});var Q1e=s(gg);oH=n(Q1e,"STRONG",{});var q5r=s(oH);_eo=r(q5r,"vision-text-dual-encoder"),q5r.forEach(t),veo=r(Q1e," \u2014 "),NB=n(Q1e,"A",{href:!0});var z5r=s(NB);beo=r(z5r,"VisionTextDualEncoderModel"),z5r.forEach(t),Teo=r(Q1e," (VisionTextDualEncoder model)"),Q1e.forEach(t),Feo=i(E),ug=n(E,"LI",{});var V1e=s(ug);rH=n(V1e,"STRONG",{});var X5r=s(rH);Eeo=r(X5r,"visual_bert"),X5r.forEach(t),Ceo=r(V1e," \u2014 "),jB=n(V1e,"A",{href:!0});var Q5r=s(jB);Meo=r(Q5r,"VisualBertModel"),Q5r.forEach(t),yeo=r(V1e," (VisualBert model)"),V1e.forEach(t),weo=i(E),pg=n(E,"LI",{});var W1e=s(pg);tH=n(W1e,"STRONG",{});var V5r=s(tH);Aeo=r(V5r,"vit"),V5r.forEach(t),Leo=r(W1e," \u2014 "),OB=n(W1e,"A",{href:!0});var W5r=s(OB);Beo=r(W5r,"ViTModel"),W5r.forEach(t),xeo=r(W1e," (ViT model)"),W1e.forEach(t),keo=i(E),_g=n(E,"LI",{});var H1e=s(_g);aH=n(H1e,"STRONG",{});var H5r=s(aH);Reo=r(H5r,"wav2vec2"),H5r.forEach(t),Peo=r(H1e," \u2014 "),GB=n(H1e,"A",{href:!0});var U5r=s(GB);Seo=r(U5r,"Wav2Vec2Model"),U5r.forEach(t),$eo=r(H1e," (Wav2Vec2 model)"),H1e.forEach(t),Ieo=i(E),vg=n(E,"LI",{});var U1e=s(vg);nH=n(U1e,"STRONG",{});var J5r=s(nH);Deo=r(J5r,"xlm"),J5r.forEach(t),Neo=r(U1e," \u2014 "),qB=n(U1e,"A",{href:!0});var K5r=s(qB);jeo=r(K5r,"XLMModel"),K5r.forEach(t),Oeo=r(U1e," (XLM model)"),U1e.forEach(t),Geo=i(E),bg=n(E,"LI",{});var J1e=s(bg);sH=n(J1e,"STRONG",{});var Y5r=s(sH);qeo=r(Y5r,"xlm-prophetnet"),Y5r.forEach(t),zeo=r(J1e," \u2014 "),zB=n(J1e,"A",{href:!0});var Z5r=s(zB);Xeo=r(Z5r,"XLMProphetNetModel"),Z5r.forEach(t),Qeo=r(J1e," (XLMProphetNet model)"),J1e.forEach(t),Veo=i(E),Tg=n(E,"LI",{});var K1e=s(Tg);lH=n(K1e,"STRONG",{});var eyr=s(lH);Weo=r(eyr,"xlm-roberta"),eyr.forEach(t),Heo=r(K1e," \u2014 "),XB=n(K1e,"A",{href:!0});var oyr=s(XB);Ueo=r(oyr,"XLMRobertaModel"),oyr.forEach(t),Jeo=r(K1e," (XLM-RoBERTa model)"),K1e.forEach(t),Keo=i(E),Fg=n(E,"LI",{});var Y1e=s(Fg);iH=n(Y1e,"STRONG",{});var ryr=s(iH);Yeo=r(ryr,"xlnet"),ryr.forEach(t),Zeo=r(Y1e," \u2014 "),QB=n(Y1e,"A",{href:!0});var tyr=s(QB);eoo=r(tyr,"XLNetModel"),tyr.forEach(t),ooo=r(Y1e," (XLNet model)"),Y1e.forEach(t),E.forEach(t),roo=i(bt),Eg=n(bt,"P",{});var Z1e=s(Eg);too=r(Z1e,"The model is set in evaluation mode by default using "),dH=n(Z1e,"CODE",{});var ayr=s(dH);aoo=r(ayr,"model.eval()"),ayr.forEach(t),noo=r(Z1e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cH=n(Z1e,"CODE",{});var nyr=s(cH);soo=r(nyr,"model.train()"),nyr.forEach(t),Z1e.forEach(t),loo=i(bt),fH=n(bt,"P",{});var syr=s(fH);ioo=r(syr,"Examples:"),syr.forEach(t),doo=i(bt),m(p3.$$.fragment,bt),bt.forEach(t),hs.forEach(t),A5e=i(c),gi=n(c,"H2",{class:!0});var Swe=s(gi);Cg=n(Swe,"A",{id:!0,class:!0,href:!0});var lyr=s(Cg);mH=n(lyr,"SPAN",{});var iyr=s(mH);m(_3.$$.fragment,iyr),iyr.forEach(t),lyr.forEach(t),coo=i(Swe),hH=n(Swe,"SPAN",{});var dyr=s(hH);foo=r(dyr,"AutoModelForPreTraining"),dyr.forEach(t),Swe.forEach(t),L5e=i(c),Oo=n(c,"DIV",{class:!0});var us=s(Oo);m(v3.$$.fragment,us),moo=i(us),ui=n(us,"P",{});var sj=s(ui);hoo=r(sj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gH=n(sj,"CODE",{});var cyr=s(gH);goo=r(cyr,"from_pretrained()"),cyr.forEach(t),uoo=r(sj,` class method or the
`),uH=n(sj,"CODE",{});var fyr=s(uH);poo=r(fyr,"from_config()"),fyr.forEach(t),_oo=r(sj," class method."),sj.forEach(t),voo=i(us),b3=n(us,"P",{});var $we=s(b3);boo=r($we,"This class cannot be instantiated directly using "),pH=n($we,"CODE",{});var myr=s(pH);Too=r(myr,"__init__()"),myr.forEach(t),Foo=r($we," (throws an error)."),$we.forEach(t),Eoo=i(us),Rr=n(us,"DIV",{class:!0});var ps=s(Rr);m(T3.$$.fragment,ps),Coo=i(ps),_H=n(ps,"P",{});var hyr=s(_H);Moo=r(hyr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),hyr.forEach(t),yoo=i(ps),pi=n(ps,"P",{});var lj=s(pi);woo=r(lj,`Note:
Loading a model from its configuration file does `),vH=n(lj,"STRONG",{});var gyr=s(vH);Aoo=r(gyr,"not"),gyr.forEach(t),Loo=r(lj,` load the model weights. It only affects the
model\u2019s configuration. Use `),bH=n(lj,"CODE",{});var uyr=s(bH);Boo=r(uyr,"from_pretrained()"),uyr.forEach(t),xoo=r(lj,` to load the model
weights.`),lj.forEach(t),koo=i(ps),TH=n(ps,"P",{});var pyr=s(TH);Roo=r(pyr,"Examples:"),pyr.forEach(t),Poo=i(ps),m(F3.$$.fragment,ps),ps.forEach(t),Soo=i(us),Ie=n(us,"DIV",{class:!0});var Tt=s(Ie);m(E3.$$.fragment,Tt),$oo=i(Tt),FH=n(Tt,"P",{});var _yr=s(FH);Ioo=r(_yr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),_yr.forEach(t),Doo=i(Tt),Aa=n(Tt,"P",{});var RE=s(Aa);Noo=r(RE,"The model class to instantiate is selected based on the "),EH=n(RE,"CODE",{});var vyr=s(EH);joo=r(vyr,"model_type"),vyr.forEach(t),Ooo=r(RE,` property of the config object (either
passed as an argument or loaded from `),CH=n(RE,"CODE",{});var byr=s(CH);Goo=r(byr,"pretrained_model_name_or_path"),byr.forEach(t),qoo=r(RE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),MH=n(RE,"CODE",{});var Tyr=s(MH);zoo=r(Tyr,"pretrained_model_name_or_path"),Tyr.forEach(t),Xoo=r(RE,":"),RE.forEach(t),Qoo=i(Tt),R=n(Tt,"UL",{});var $=s(R);Mg=n($,"LI",{});var e4e=s(Mg);yH=n(e4e,"STRONG",{});var Fyr=s(yH);Voo=r(Fyr,"albert"),Fyr.forEach(t),Woo=r(e4e," \u2014 "),VB=n(e4e,"A",{href:!0});var Eyr=s(VB);Hoo=r(Eyr,"AlbertForPreTraining"),Eyr.forEach(t),Uoo=r(e4e," (ALBERT model)"),e4e.forEach(t),Joo=i($),yg=n($,"LI",{});var o4e=s(yg);wH=n(o4e,"STRONG",{});var Cyr=s(wH);Koo=r(Cyr,"bart"),Cyr.forEach(t),Yoo=r(o4e," \u2014 "),WB=n(o4e,"A",{href:!0});var Myr=s(WB);Zoo=r(Myr,"BartForConditionalGeneration"),Myr.forEach(t),ero=r(o4e," (BART model)"),o4e.forEach(t),oro=i($),wg=n($,"LI",{});var r4e=s(wg);AH=n(r4e,"STRONG",{});var yyr=s(AH);rro=r(yyr,"bert"),yyr.forEach(t),tro=r(r4e," \u2014 "),HB=n(r4e,"A",{href:!0});var wyr=s(HB);aro=r(wyr,"BertForPreTraining"),wyr.forEach(t),nro=r(r4e," (BERT model)"),r4e.forEach(t),sro=i($),Ag=n($,"LI",{});var t4e=s(Ag);LH=n(t4e,"STRONG",{});var Ayr=s(LH);lro=r(Ayr,"big_bird"),Ayr.forEach(t),iro=r(t4e," \u2014 "),UB=n(t4e,"A",{href:!0});var Lyr=s(UB);dro=r(Lyr,"BigBirdForPreTraining"),Lyr.forEach(t),cro=r(t4e," (BigBird model)"),t4e.forEach(t),fro=i($),Lg=n($,"LI",{});var a4e=s(Lg);BH=n(a4e,"STRONG",{});var Byr=s(BH);mro=r(Byr,"camembert"),Byr.forEach(t),hro=r(a4e," \u2014 "),JB=n(a4e,"A",{href:!0});var xyr=s(JB);gro=r(xyr,"CamembertForMaskedLM"),xyr.forEach(t),uro=r(a4e," (CamemBERT model)"),a4e.forEach(t),pro=i($),Bg=n($,"LI",{});var n4e=s(Bg);xH=n(n4e,"STRONG",{});var kyr=s(xH);_ro=r(kyr,"ctrl"),kyr.forEach(t),vro=r(n4e," \u2014 "),KB=n(n4e,"A",{href:!0});var Ryr=s(KB);bro=r(Ryr,"CTRLLMHeadModel"),Ryr.forEach(t),Tro=r(n4e," (CTRL model)"),n4e.forEach(t),Fro=i($),xg=n($,"LI",{});var s4e=s(xg);kH=n(s4e,"STRONG",{});var Pyr=s(kH);Ero=r(Pyr,"deberta"),Pyr.forEach(t),Cro=r(s4e," \u2014 "),YB=n(s4e,"A",{href:!0});var Syr=s(YB);Mro=r(Syr,"DebertaForMaskedLM"),Syr.forEach(t),yro=r(s4e," (DeBERTa model)"),s4e.forEach(t),wro=i($),kg=n($,"LI",{});var l4e=s(kg);RH=n(l4e,"STRONG",{});var $yr=s(RH);Aro=r($yr,"deberta-v2"),$yr.forEach(t),Lro=r(l4e," \u2014 "),ZB=n(l4e,"A",{href:!0});var Iyr=s(ZB);Bro=r(Iyr,"DebertaV2ForMaskedLM"),Iyr.forEach(t),xro=r(l4e," (DeBERTa-v2 model)"),l4e.forEach(t),kro=i($),Rg=n($,"LI",{});var i4e=s(Rg);PH=n(i4e,"STRONG",{});var Dyr=s(PH);Rro=r(Dyr,"distilbert"),Dyr.forEach(t),Pro=r(i4e," \u2014 "),e9=n(i4e,"A",{href:!0});var Nyr=s(e9);Sro=r(Nyr,"DistilBertForMaskedLM"),Nyr.forEach(t),$ro=r(i4e," (DistilBERT model)"),i4e.forEach(t),Iro=i($),Pg=n($,"LI",{});var d4e=s(Pg);SH=n(d4e,"STRONG",{});var jyr=s(SH);Dro=r(jyr,"electra"),jyr.forEach(t),Nro=r(d4e," \u2014 "),o9=n(d4e,"A",{href:!0});var Oyr=s(o9);jro=r(Oyr,"ElectraForPreTraining"),Oyr.forEach(t),Oro=r(d4e," (ELECTRA model)"),d4e.forEach(t),Gro=i($),Sg=n($,"LI",{});var c4e=s(Sg);$H=n(c4e,"STRONG",{});var Gyr=s($H);qro=r(Gyr,"flaubert"),Gyr.forEach(t),zro=r(c4e," \u2014 "),r9=n(c4e,"A",{href:!0});var qyr=s(r9);Xro=r(qyr,"FlaubertWithLMHeadModel"),qyr.forEach(t),Qro=r(c4e," (FlauBERT model)"),c4e.forEach(t),Vro=i($),$g=n($,"LI",{});var f4e=s($g);IH=n(f4e,"STRONG",{});var zyr=s(IH);Wro=r(zyr,"fnet"),zyr.forEach(t),Hro=r(f4e," \u2014 "),t9=n(f4e,"A",{href:!0});var Xyr=s(t9);Uro=r(Xyr,"FNetForPreTraining"),Xyr.forEach(t),Jro=r(f4e," (FNet model)"),f4e.forEach(t),Kro=i($),Ig=n($,"LI",{});var m4e=s(Ig);DH=n(m4e,"STRONG",{});var Qyr=s(DH);Yro=r(Qyr,"fsmt"),Qyr.forEach(t),Zro=r(m4e," \u2014 "),a9=n(m4e,"A",{href:!0});var Vyr=s(a9);eto=r(Vyr,"FSMTForConditionalGeneration"),Vyr.forEach(t),oto=r(m4e," (FairSeq Machine-Translation model)"),m4e.forEach(t),rto=i($),Dg=n($,"LI",{});var h4e=s(Dg);NH=n(h4e,"STRONG",{});var Wyr=s(NH);tto=r(Wyr,"funnel"),Wyr.forEach(t),ato=r(h4e," \u2014 "),n9=n(h4e,"A",{href:!0});var Hyr=s(n9);nto=r(Hyr,"FunnelForPreTraining"),Hyr.forEach(t),sto=r(h4e," (Funnel Transformer model)"),h4e.forEach(t),lto=i($),Ng=n($,"LI",{});var g4e=s(Ng);jH=n(g4e,"STRONG",{});var Uyr=s(jH);ito=r(Uyr,"gpt2"),Uyr.forEach(t),dto=r(g4e," \u2014 "),s9=n(g4e,"A",{href:!0});var Jyr=s(s9);cto=r(Jyr,"GPT2LMHeadModel"),Jyr.forEach(t),fto=r(g4e," (OpenAI GPT-2 model)"),g4e.forEach(t),mto=i($),jg=n($,"LI",{});var u4e=s(jg);OH=n(u4e,"STRONG",{});var Kyr=s(OH);hto=r(Kyr,"ibert"),Kyr.forEach(t),gto=r(u4e," \u2014 "),l9=n(u4e,"A",{href:!0});var Yyr=s(l9);uto=r(Yyr,"IBertForMaskedLM"),Yyr.forEach(t),pto=r(u4e," (I-BERT model)"),u4e.forEach(t),_to=i($),Og=n($,"LI",{});var p4e=s(Og);GH=n(p4e,"STRONG",{});var Zyr=s(GH);vto=r(Zyr,"layoutlm"),Zyr.forEach(t),bto=r(p4e," \u2014 "),i9=n(p4e,"A",{href:!0});var ewr=s(i9);Tto=r(ewr,"LayoutLMForMaskedLM"),ewr.forEach(t),Fto=r(p4e," (LayoutLM model)"),p4e.forEach(t),Eto=i($),Gg=n($,"LI",{});var _4e=s(Gg);qH=n(_4e,"STRONG",{});var owr=s(qH);Cto=r(owr,"longformer"),owr.forEach(t),Mto=r(_4e," \u2014 "),d9=n(_4e,"A",{href:!0});var rwr=s(d9);yto=r(rwr,"LongformerForMaskedLM"),rwr.forEach(t),wto=r(_4e," (Longformer model)"),_4e.forEach(t),Ato=i($),qg=n($,"LI",{});var v4e=s(qg);zH=n(v4e,"STRONG",{});var twr=s(zH);Lto=r(twr,"lxmert"),twr.forEach(t),Bto=r(v4e," \u2014 "),c9=n(v4e,"A",{href:!0});var awr=s(c9);xto=r(awr,"LxmertForPreTraining"),awr.forEach(t),kto=r(v4e," (LXMERT model)"),v4e.forEach(t),Rto=i($),zg=n($,"LI",{});var b4e=s(zg);XH=n(b4e,"STRONG",{});var nwr=s(XH);Pto=r(nwr,"megatron-bert"),nwr.forEach(t),Sto=r(b4e," \u2014 "),f9=n(b4e,"A",{href:!0});var swr=s(f9);$to=r(swr,"MegatronBertForPreTraining"),swr.forEach(t),Ito=r(b4e," (MegatronBert model)"),b4e.forEach(t),Dto=i($),Xg=n($,"LI",{});var T4e=s(Xg);QH=n(T4e,"STRONG",{});var lwr=s(QH);Nto=r(lwr,"mobilebert"),lwr.forEach(t),jto=r(T4e," \u2014 "),m9=n(T4e,"A",{href:!0});var iwr=s(m9);Oto=r(iwr,"MobileBertForPreTraining"),iwr.forEach(t),Gto=r(T4e," (MobileBERT model)"),T4e.forEach(t),qto=i($),Qg=n($,"LI",{});var F4e=s(Qg);VH=n(F4e,"STRONG",{});var dwr=s(VH);zto=r(dwr,"mpnet"),dwr.forEach(t),Xto=r(F4e," \u2014 "),h9=n(F4e,"A",{href:!0});var cwr=s(h9);Qto=r(cwr,"MPNetForMaskedLM"),cwr.forEach(t),Vto=r(F4e," (MPNet model)"),F4e.forEach(t),Wto=i($),Vg=n($,"LI",{});var E4e=s(Vg);WH=n(E4e,"STRONG",{});var fwr=s(WH);Hto=r(fwr,"openai-gpt"),fwr.forEach(t),Uto=r(E4e," \u2014 "),g9=n(E4e,"A",{href:!0});var mwr=s(g9);Jto=r(mwr,"OpenAIGPTLMHeadModel"),mwr.forEach(t),Kto=r(E4e," (OpenAI GPT model)"),E4e.forEach(t),Yto=i($),Wg=n($,"LI",{});var C4e=s(Wg);HH=n(C4e,"STRONG",{});var hwr=s(HH);Zto=r(hwr,"retribert"),hwr.forEach(t),eao=r(C4e," \u2014 "),u9=n(C4e,"A",{href:!0});var gwr=s(u9);oao=r(gwr,"RetriBertModel"),gwr.forEach(t),rao=r(C4e," (RetriBERT model)"),C4e.forEach(t),tao=i($),Hg=n($,"LI",{});var M4e=s(Hg);UH=n(M4e,"STRONG",{});var uwr=s(UH);aao=r(uwr,"roberta"),uwr.forEach(t),nao=r(M4e," \u2014 "),p9=n(M4e,"A",{href:!0});var pwr=s(p9);sao=r(pwr,"RobertaForMaskedLM"),pwr.forEach(t),lao=r(M4e," (RoBERTa model)"),M4e.forEach(t),iao=i($),Ug=n($,"LI",{});var y4e=s(Ug);JH=n(y4e,"STRONG",{});var _wr=s(JH);dao=r(_wr,"squeezebert"),_wr.forEach(t),cao=r(y4e," \u2014 "),_9=n(y4e,"A",{href:!0});var vwr=s(_9);fao=r(vwr,"SqueezeBertForMaskedLM"),vwr.forEach(t),mao=r(y4e," (SqueezeBERT model)"),y4e.forEach(t),hao=i($),Jg=n($,"LI",{});var w4e=s(Jg);KH=n(w4e,"STRONG",{});var bwr=s(KH);gao=r(bwr,"t5"),bwr.forEach(t),uao=r(w4e," \u2014 "),v9=n(w4e,"A",{href:!0});var Twr=s(v9);pao=r(Twr,"T5ForConditionalGeneration"),Twr.forEach(t),_ao=r(w4e," (T5 model)"),w4e.forEach(t),vao=i($),Kg=n($,"LI",{});var A4e=s(Kg);YH=n(A4e,"STRONG",{});var Fwr=s(YH);bao=r(Fwr,"tapas"),Fwr.forEach(t),Tao=r(A4e," \u2014 "),b9=n(A4e,"A",{href:!0});var Ewr=s(b9);Fao=r(Ewr,"TapasForMaskedLM"),Ewr.forEach(t),Eao=r(A4e," (TAPAS model)"),A4e.forEach(t),Cao=i($),Yg=n($,"LI",{});var L4e=s(Yg);ZH=n(L4e,"STRONG",{});var Cwr=s(ZH);Mao=r(Cwr,"transfo-xl"),Cwr.forEach(t),yao=r(L4e," \u2014 "),T9=n(L4e,"A",{href:!0});var Mwr=s(T9);wao=r(Mwr,"TransfoXLLMHeadModel"),Mwr.forEach(t),Aao=r(L4e," (Transformer-XL model)"),L4e.forEach(t),Lao=i($),Zg=n($,"LI",{});var B4e=s(Zg);eU=n(B4e,"STRONG",{});var ywr=s(eU);Bao=r(ywr,"unispeech"),ywr.forEach(t),xao=r(B4e," \u2014 "),F9=n(B4e,"A",{href:!0});var wwr=s(F9);kao=r(wwr,"UniSpeechForPreTraining"),wwr.forEach(t),Rao=r(B4e," (UniSpeech model)"),B4e.forEach(t),Pao=i($),eu=n($,"LI",{});var x4e=s(eu);oU=n(x4e,"STRONG",{});var Awr=s(oU);Sao=r(Awr,"unispeech-sat"),Awr.forEach(t),$ao=r(x4e," \u2014 "),E9=n(x4e,"A",{href:!0});var Lwr=s(E9);Iao=r(Lwr,"UniSpeechSatForPreTraining"),Lwr.forEach(t),Dao=r(x4e," (UniSpeechSat model)"),x4e.forEach(t),Nao=i($),ou=n($,"LI",{});var k4e=s(ou);rU=n(k4e,"STRONG",{});var Bwr=s(rU);jao=r(Bwr,"visual_bert"),Bwr.forEach(t),Oao=r(k4e," \u2014 "),C9=n(k4e,"A",{href:!0});var xwr=s(C9);Gao=r(xwr,"VisualBertForPreTraining"),xwr.forEach(t),qao=r(k4e," (VisualBert model)"),k4e.forEach(t),zao=i($),ru=n($,"LI",{});var R4e=s(ru);tU=n(R4e,"STRONG",{});var kwr=s(tU);Xao=r(kwr,"wav2vec2"),kwr.forEach(t),Qao=r(R4e," \u2014 "),M9=n(R4e,"A",{href:!0});var Rwr=s(M9);Vao=r(Rwr,"Wav2Vec2ForPreTraining"),Rwr.forEach(t),Wao=r(R4e," (Wav2Vec2 model)"),R4e.forEach(t),Hao=i($),tu=n($,"LI",{});var P4e=s(tu);aU=n(P4e,"STRONG",{});var Pwr=s(aU);Uao=r(Pwr,"xlm"),Pwr.forEach(t),Jao=r(P4e," \u2014 "),y9=n(P4e,"A",{href:!0});var Swr=s(y9);Kao=r(Swr,"XLMWithLMHeadModel"),Swr.forEach(t),Yao=r(P4e," (XLM model)"),P4e.forEach(t),Zao=i($),au=n($,"LI",{});var S4e=s(au);nU=n(S4e,"STRONG",{});var $wr=s(nU);eno=r($wr,"xlm-roberta"),$wr.forEach(t),ono=r(S4e," \u2014 "),w9=n(S4e,"A",{href:!0});var Iwr=s(w9);rno=r(Iwr,"XLMRobertaForMaskedLM"),Iwr.forEach(t),tno=r(S4e," (XLM-RoBERTa model)"),S4e.forEach(t),ano=i($),nu=n($,"LI",{});var $4e=s(nu);sU=n($4e,"STRONG",{});var Dwr=s(sU);nno=r(Dwr,"xlnet"),Dwr.forEach(t),sno=r($4e," \u2014 "),A9=n($4e,"A",{href:!0});var Nwr=s(A9);lno=r(Nwr,"XLNetLMHeadModel"),Nwr.forEach(t),ino=r($4e," (XLNet model)"),$4e.forEach(t),$.forEach(t),dno=i(Tt),su=n(Tt,"P",{});var I4e=s(su);cno=r(I4e,"The model is set in evaluation mode by default using "),lU=n(I4e,"CODE",{});var jwr=s(lU);fno=r(jwr,"model.eval()"),jwr.forEach(t),mno=r(I4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iU=n(I4e,"CODE",{});var Owr=s(iU);hno=r(Owr,"model.train()"),Owr.forEach(t),I4e.forEach(t),gno=i(Tt),dU=n(Tt,"P",{});var Gwr=s(dU);uno=r(Gwr,"Examples:"),Gwr.forEach(t),pno=i(Tt),m(C3.$$.fragment,Tt),Tt.forEach(t),us.forEach(t),B5e=i(c),_i=n(c,"H2",{class:!0});var Iwe=s(_i);lu=n(Iwe,"A",{id:!0,class:!0,href:!0});var qwr=s(lu);cU=n(qwr,"SPAN",{});var zwr=s(cU);m(M3.$$.fragment,zwr),zwr.forEach(t),qwr.forEach(t),_no=i(Iwe),fU=n(Iwe,"SPAN",{});var Xwr=s(fU);vno=r(Xwr,"AutoModelForCausalLM"),Xwr.forEach(t),Iwe.forEach(t),x5e=i(c),Go=n(c,"DIV",{class:!0});var _s=s(Go);m(y3.$$.fragment,_s),bno=i(_s),vi=n(_s,"P",{});var ij=s(vi);Tno=r(ij,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),mU=n(ij,"CODE",{});var Qwr=s(mU);Fno=r(Qwr,"from_pretrained()"),Qwr.forEach(t),Eno=r(ij,` class method or the
`),hU=n(ij,"CODE",{});var Vwr=s(hU);Cno=r(Vwr,"from_config()"),Vwr.forEach(t),Mno=r(ij," class method."),ij.forEach(t),yno=i(_s),w3=n(_s,"P",{});var Dwe=s(w3);wno=r(Dwe,"This class cannot be instantiated directly using "),gU=n(Dwe,"CODE",{});var Wwr=s(gU);Ano=r(Wwr,"__init__()"),Wwr.forEach(t),Lno=r(Dwe," (throws an error)."),Dwe.forEach(t),Bno=i(_s),Pr=n(_s,"DIV",{class:!0});var vs=s(Pr);m(A3.$$.fragment,vs),xno=i(vs),uU=n(vs,"P",{});var Hwr=s(uU);kno=r(Hwr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Hwr.forEach(t),Rno=i(vs),bi=n(vs,"P",{});var dj=s(bi);Pno=r(dj,`Note:
Loading a model from its configuration file does `),pU=n(dj,"STRONG",{});var Uwr=s(pU);Sno=r(Uwr,"not"),Uwr.forEach(t),$no=r(dj,` load the model weights. It only affects the
model\u2019s configuration. Use `),_U=n(dj,"CODE",{});var Jwr=s(_U);Ino=r(Jwr,"from_pretrained()"),Jwr.forEach(t),Dno=r(dj,` to load the model
weights.`),dj.forEach(t),Nno=i(vs),vU=n(vs,"P",{});var Kwr=s(vU);jno=r(Kwr,"Examples:"),Kwr.forEach(t),Ono=i(vs),m(L3.$$.fragment,vs),vs.forEach(t),Gno=i(_s),De=n(_s,"DIV",{class:!0});var Ft=s(De);m(B3.$$.fragment,Ft),qno=i(Ft),bU=n(Ft,"P",{});var Ywr=s(bU);zno=r(Ywr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Ywr.forEach(t),Xno=i(Ft),La=n(Ft,"P",{});var PE=s(La);Qno=r(PE,"The model class to instantiate is selected based on the "),TU=n(PE,"CODE",{});var Zwr=s(TU);Vno=r(Zwr,"model_type"),Zwr.forEach(t),Wno=r(PE,` property of the config object (either
passed as an argument or loaded from `),FU=n(PE,"CODE",{});var e7r=s(FU);Hno=r(e7r,"pretrained_model_name_or_path"),e7r.forEach(t),Uno=r(PE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),EU=n(PE,"CODE",{});var o7r=s(EU);Jno=r(o7r,"pretrained_model_name_or_path"),o7r.forEach(t),Kno=r(PE,":"),PE.forEach(t),Yno=i(Ft),q=n(Ft,"UL",{});var Q=s(q);iu=n(Q,"LI",{});var D4e=s(iu);CU=n(D4e,"STRONG",{});var r7r=s(CU);Zno=r(r7r,"bart"),r7r.forEach(t),eso=r(D4e," \u2014 "),L9=n(D4e,"A",{href:!0});var t7r=s(L9);oso=r(t7r,"BartForCausalLM"),t7r.forEach(t),rso=r(D4e," (BART model)"),D4e.forEach(t),tso=i(Q),du=n(Q,"LI",{});var N4e=s(du);MU=n(N4e,"STRONG",{});var a7r=s(MU);aso=r(a7r,"bert"),a7r.forEach(t),nso=r(N4e," \u2014 "),B9=n(N4e,"A",{href:!0});var n7r=s(B9);sso=r(n7r,"BertLMHeadModel"),n7r.forEach(t),lso=r(N4e," (BERT model)"),N4e.forEach(t),iso=i(Q),cu=n(Q,"LI",{});var j4e=s(cu);yU=n(j4e,"STRONG",{});var s7r=s(yU);dso=r(s7r,"bert-generation"),s7r.forEach(t),cso=r(j4e," \u2014 "),x9=n(j4e,"A",{href:!0});var l7r=s(x9);fso=r(l7r,"BertGenerationDecoder"),l7r.forEach(t),mso=r(j4e," (Bert Generation model)"),j4e.forEach(t),hso=i(Q),fu=n(Q,"LI",{});var O4e=s(fu);wU=n(O4e,"STRONG",{});var i7r=s(wU);gso=r(i7r,"big_bird"),i7r.forEach(t),uso=r(O4e," \u2014 "),k9=n(O4e,"A",{href:!0});var d7r=s(k9);pso=r(d7r,"BigBirdForCausalLM"),d7r.forEach(t),_so=r(O4e," (BigBird model)"),O4e.forEach(t),vso=i(Q),mu=n(Q,"LI",{});var G4e=s(mu);AU=n(G4e,"STRONG",{});var c7r=s(AU);bso=r(c7r,"bigbird_pegasus"),c7r.forEach(t),Tso=r(G4e," \u2014 "),R9=n(G4e,"A",{href:!0});var f7r=s(R9);Fso=r(f7r,"BigBirdPegasusForCausalLM"),f7r.forEach(t),Eso=r(G4e," (BigBirdPegasus model)"),G4e.forEach(t),Cso=i(Q),hu=n(Q,"LI",{});var q4e=s(hu);LU=n(q4e,"STRONG",{});var m7r=s(LU);Mso=r(m7r,"blenderbot"),m7r.forEach(t),yso=r(q4e," \u2014 "),P9=n(q4e,"A",{href:!0});var h7r=s(P9);wso=r(h7r,"BlenderbotForCausalLM"),h7r.forEach(t),Aso=r(q4e," (Blenderbot model)"),q4e.forEach(t),Lso=i(Q),gu=n(Q,"LI",{});var z4e=s(gu);BU=n(z4e,"STRONG",{});var g7r=s(BU);Bso=r(g7r,"blenderbot-small"),g7r.forEach(t),xso=r(z4e," \u2014 "),S9=n(z4e,"A",{href:!0});var u7r=s(S9);kso=r(u7r,"BlenderbotSmallForCausalLM"),u7r.forEach(t),Rso=r(z4e," (BlenderbotSmall model)"),z4e.forEach(t),Pso=i(Q),uu=n(Q,"LI",{});var X4e=s(uu);xU=n(X4e,"STRONG",{});var p7r=s(xU);Sso=r(p7r,"camembert"),p7r.forEach(t),$so=r(X4e," \u2014 "),$9=n(X4e,"A",{href:!0});var _7r=s($9);Iso=r(_7r,"CamembertForCausalLM"),_7r.forEach(t),Dso=r(X4e," (CamemBERT model)"),X4e.forEach(t),Nso=i(Q),pu=n(Q,"LI",{});var Q4e=s(pu);kU=n(Q4e,"STRONG",{});var v7r=s(kU);jso=r(v7r,"ctrl"),v7r.forEach(t),Oso=r(Q4e," \u2014 "),I9=n(Q4e,"A",{href:!0});var b7r=s(I9);Gso=r(b7r,"CTRLLMHeadModel"),b7r.forEach(t),qso=r(Q4e," (CTRL model)"),Q4e.forEach(t),zso=i(Q),_u=n(Q,"LI",{});var V4e=s(_u);RU=n(V4e,"STRONG",{});var T7r=s(RU);Xso=r(T7r,"gpt2"),T7r.forEach(t),Qso=r(V4e," \u2014 "),D9=n(V4e,"A",{href:!0});var F7r=s(D9);Vso=r(F7r,"GPT2LMHeadModel"),F7r.forEach(t),Wso=r(V4e," (OpenAI GPT-2 model)"),V4e.forEach(t),Hso=i(Q),vu=n(Q,"LI",{});var W4e=s(vu);PU=n(W4e,"STRONG",{});var E7r=s(PU);Uso=r(E7r,"gpt_neo"),E7r.forEach(t),Jso=r(W4e," \u2014 "),N9=n(W4e,"A",{href:!0});var C7r=s(N9);Kso=r(C7r,"GPTNeoForCausalLM"),C7r.forEach(t),Yso=r(W4e," (GPT Neo model)"),W4e.forEach(t),Zso=i(Q),bu=n(Q,"LI",{});var H4e=s(bu);SU=n(H4e,"STRONG",{});var M7r=s(SU);elo=r(M7r,"gptj"),M7r.forEach(t),olo=r(H4e," \u2014 "),j9=n(H4e,"A",{href:!0});var y7r=s(j9);rlo=r(y7r,"GPTJForCausalLM"),y7r.forEach(t),tlo=r(H4e," (GPT-J model)"),H4e.forEach(t),alo=i(Q),Tu=n(Q,"LI",{});var U4e=s(Tu);$U=n(U4e,"STRONG",{});var w7r=s($U);nlo=r(w7r,"marian"),w7r.forEach(t),slo=r(U4e," \u2014 "),O9=n(U4e,"A",{href:!0});var A7r=s(O9);llo=r(A7r,"MarianForCausalLM"),A7r.forEach(t),ilo=r(U4e," (Marian model)"),U4e.forEach(t),dlo=i(Q),Fu=n(Q,"LI",{});var J4e=s(Fu);IU=n(J4e,"STRONG",{});var L7r=s(IU);clo=r(L7r,"mbart"),L7r.forEach(t),flo=r(J4e," \u2014 "),G9=n(J4e,"A",{href:!0});var B7r=s(G9);mlo=r(B7r,"MBartForCausalLM"),B7r.forEach(t),hlo=r(J4e," (mBART model)"),J4e.forEach(t),glo=i(Q),Eu=n(Q,"LI",{});var K4e=s(Eu);DU=n(K4e,"STRONG",{});var x7r=s(DU);ulo=r(x7r,"megatron-bert"),x7r.forEach(t),plo=r(K4e," \u2014 "),q9=n(K4e,"A",{href:!0});var k7r=s(q9);_lo=r(k7r,"MegatronBertForCausalLM"),k7r.forEach(t),vlo=r(K4e," (MegatronBert model)"),K4e.forEach(t),blo=i(Q),Cu=n(Q,"LI",{});var Y4e=s(Cu);NU=n(Y4e,"STRONG",{});var R7r=s(NU);Tlo=r(R7r,"openai-gpt"),R7r.forEach(t),Flo=r(Y4e," \u2014 "),z9=n(Y4e,"A",{href:!0});var P7r=s(z9);Elo=r(P7r,"OpenAIGPTLMHeadModel"),P7r.forEach(t),Clo=r(Y4e," (OpenAI GPT model)"),Y4e.forEach(t),Mlo=i(Q),Mu=n(Q,"LI",{});var Z4e=s(Mu);jU=n(Z4e,"STRONG",{});var S7r=s(jU);ylo=r(S7r,"pegasus"),S7r.forEach(t),wlo=r(Z4e," \u2014 "),X9=n(Z4e,"A",{href:!0});var $7r=s(X9);Alo=r($7r,"PegasusForCausalLM"),$7r.forEach(t),Llo=r(Z4e," (Pegasus model)"),Z4e.forEach(t),Blo=i(Q),yu=n(Q,"LI",{});var eve=s(yu);OU=n(eve,"STRONG",{});var I7r=s(OU);xlo=r(I7r,"prophetnet"),I7r.forEach(t),klo=r(eve," \u2014 "),Q9=n(eve,"A",{href:!0});var D7r=s(Q9);Rlo=r(D7r,"ProphetNetForCausalLM"),D7r.forEach(t),Plo=r(eve," (ProphetNet model)"),eve.forEach(t),Slo=i(Q),wu=n(Q,"LI",{});var ove=s(wu);GU=n(ove,"STRONG",{});var N7r=s(GU);$lo=r(N7r,"qdqbert"),N7r.forEach(t),Ilo=r(ove," \u2014 "),V9=n(ove,"A",{href:!0});var j7r=s(V9);Dlo=r(j7r,"QDQBertLMHeadModel"),j7r.forEach(t),Nlo=r(ove," (QDQBert model)"),ove.forEach(t),jlo=i(Q),Au=n(Q,"LI",{});var rve=s(Au);qU=n(rve,"STRONG",{});var O7r=s(qU);Olo=r(O7r,"reformer"),O7r.forEach(t),Glo=r(rve," \u2014 "),W9=n(rve,"A",{href:!0});var G7r=s(W9);qlo=r(G7r,"ReformerModelWithLMHead"),G7r.forEach(t),zlo=r(rve," (Reformer model)"),rve.forEach(t),Xlo=i(Q),Lu=n(Q,"LI",{});var tve=s(Lu);zU=n(tve,"STRONG",{});var q7r=s(zU);Qlo=r(q7r,"rembert"),q7r.forEach(t),Vlo=r(tve," \u2014 "),H9=n(tve,"A",{href:!0});var z7r=s(H9);Wlo=r(z7r,"RemBertForCausalLM"),z7r.forEach(t),Hlo=r(tve," (RemBERT model)"),tve.forEach(t),Ulo=i(Q),Bu=n(Q,"LI",{});var ave=s(Bu);XU=n(ave,"STRONG",{});var X7r=s(XU);Jlo=r(X7r,"roberta"),X7r.forEach(t),Klo=r(ave," \u2014 "),U9=n(ave,"A",{href:!0});var Q7r=s(U9);Ylo=r(Q7r,"RobertaForCausalLM"),Q7r.forEach(t),Zlo=r(ave," (RoBERTa model)"),ave.forEach(t),eio=i(Q),xu=n(Q,"LI",{});var nve=s(xu);QU=n(nve,"STRONG",{});var V7r=s(QU);oio=r(V7r,"roformer"),V7r.forEach(t),rio=r(nve," \u2014 "),J9=n(nve,"A",{href:!0});var W7r=s(J9);tio=r(W7r,"RoFormerForCausalLM"),W7r.forEach(t),aio=r(nve," (RoFormer model)"),nve.forEach(t),nio=i(Q),ku=n(Q,"LI",{});var sve=s(ku);VU=n(sve,"STRONG",{});var H7r=s(VU);sio=r(H7r,"speech_to_text_2"),H7r.forEach(t),lio=r(sve," \u2014 "),K9=n(sve,"A",{href:!0});var U7r=s(K9);iio=r(U7r,"Speech2Text2ForCausalLM"),U7r.forEach(t),dio=r(sve," (Speech2Text2 model)"),sve.forEach(t),cio=i(Q),Ru=n(Q,"LI",{});var lve=s(Ru);WU=n(lve,"STRONG",{});var J7r=s(WU);fio=r(J7r,"transfo-xl"),J7r.forEach(t),mio=r(lve," \u2014 "),Y9=n(lve,"A",{href:!0});var K7r=s(Y9);hio=r(K7r,"TransfoXLLMHeadModel"),K7r.forEach(t),gio=r(lve," (Transformer-XL model)"),lve.forEach(t),uio=i(Q),Pu=n(Q,"LI",{});var ive=s(Pu);HU=n(ive,"STRONG",{});var Y7r=s(HU);pio=r(Y7r,"trocr"),Y7r.forEach(t),_io=r(ive," \u2014 "),Z9=n(ive,"A",{href:!0});var Z7r=s(Z9);vio=r(Z7r,"TrOCRForCausalLM"),Z7r.forEach(t),bio=r(ive," (TrOCR model)"),ive.forEach(t),Tio=i(Q),Su=n(Q,"LI",{});var dve=s(Su);UU=n(dve,"STRONG",{});var e0r=s(UU);Fio=r(e0r,"xlm"),e0r.forEach(t),Eio=r(dve," \u2014 "),ex=n(dve,"A",{href:!0});var o0r=s(ex);Cio=r(o0r,"XLMWithLMHeadModel"),o0r.forEach(t),Mio=r(dve," (XLM model)"),dve.forEach(t),yio=i(Q),$u=n(Q,"LI",{});var cve=s($u);JU=n(cve,"STRONG",{});var r0r=s(JU);wio=r(r0r,"xlm-prophetnet"),r0r.forEach(t),Aio=r(cve," \u2014 "),ox=n(cve,"A",{href:!0});var t0r=s(ox);Lio=r(t0r,"XLMProphetNetForCausalLM"),t0r.forEach(t),Bio=r(cve," (XLMProphetNet model)"),cve.forEach(t),xio=i(Q),Iu=n(Q,"LI",{});var fve=s(Iu);KU=n(fve,"STRONG",{});var a0r=s(KU);kio=r(a0r,"xlm-roberta"),a0r.forEach(t),Rio=r(fve," \u2014 "),rx=n(fve,"A",{href:!0});var n0r=s(rx);Pio=r(n0r,"XLMRobertaForCausalLM"),n0r.forEach(t),Sio=r(fve," (XLM-RoBERTa model)"),fve.forEach(t),$io=i(Q),Du=n(Q,"LI",{});var mve=s(Du);YU=n(mve,"STRONG",{});var s0r=s(YU);Iio=r(s0r,"xlnet"),s0r.forEach(t),Dio=r(mve," \u2014 "),tx=n(mve,"A",{href:!0});var l0r=s(tx);Nio=r(l0r,"XLNetLMHeadModel"),l0r.forEach(t),jio=r(mve," (XLNet model)"),mve.forEach(t),Q.forEach(t),Oio=i(Ft),Nu=n(Ft,"P",{});var hve=s(Nu);Gio=r(hve,"The model is set in evaluation mode by default using "),ZU=n(hve,"CODE",{});var i0r=s(ZU);qio=r(i0r,"model.eval()"),i0r.forEach(t),zio=r(hve,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),eJ=n(hve,"CODE",{});var d0r=s(eJ);Xio=r(d0r,"model.train()"),d0r.forEach(t),hve.forEach(t),Qio=i(Ft),oJ=n(Ft,"P",{});var c0r=s(oJ);Vio=r(c0r,"Examples:"),c0r.forEach(t),Wio=i(Ft),m(x3.$$.fragment,Ft),Ft.forEach(t),_s.forEach(t),k5e=i(c),Ti=n(c,"H2",{class:!0});var Nwe=s(Ti);ju=n(Nwe,"A",{id:!0,class:!0,href:!0});var f0r=s(ju);rJ=n(f0r,"SPAN",{});var m0r=s(rJ);m(k3.$$.fragment,m0r),m0r.forEach(t),f0r.forEach(t),Hio=i(Nwe),tJ=n(Nwe,"SPAN",{});var h0r=s(tJ);Uio=r(h0r,"AutoModelForMaskedLM"),h0r.forEach(t),Nwe.forEach(t),R5e=i(c),qo=n(c,"DIV",{class:!0});var bs=s(qo);m(R3.$$.fragment,bs),Jio=i(bs),Fi=n(bs,"P",{});var cj=s(Fi);Kio=r(cj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),aJ=n(cj,"CODE",{});var g0r=s(aJ);Yio=r(g0r,"from_pretrained()"),g0r.forEach(t),Zio=r(cj,` class method or the
`),nJ=n(cj,"CODE",{});var u0r=s(nJ);edo=r(u0r,"from_config()"),u0r.forEach(t),odo=r(cj," class method."),cj.forEach(t),rdo=i(bs),P3=n(bs,"P",{});var jwe=s(P3);tdo=r(jwe,"This class cannot be instantiated directly using "),sJ=n(jwe,"CODE",{});var p0r=s(sJ);ado=r(p0r,"__init__()"),p0r.forEach(t),ndo=r(jwe," (throws an error)."),jwe.forEach(t),sdo=i(bs),Sr=n(bs,"DIV",{class:!0});var Ts=s(Sr);m(S3.$$.fragment,Ts),ldo=i(Ts),lJ=n(Ts,"P",{});var _0r=s(lJ);ido=r(_0r,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),_0r.forEach(t),ddo=i(Ts),Ei=n(Ts,"P",{});var fj=s(Ei);cdo=r(fj,`Note:
Loading a model from its configuration file does `),iJ=n(fj,"STRONG",{});var v0r=s(iJ);fdo=r(v0r,"not"),v0r.forEach(t),mdo=r(fj,` load the model weights. It only affects the
model\u2019s configuration. Use `),dJ=n(fj,"CODE",{});var b0r=s(dJ);hdo=r(b0r,"from_pretrained()"),b0r.forEach(t),gdo=r(fj,` to load the model
weights.`),fj.forEach(t),udo=i(Ts),cJ=n(Ts,"P",{});var T0r=s(cJ);pdo=r(T0r,"Examples:"),T0r.forEach(t),_do=i(Ts),m($3.$$.fragment,Ts),Ts.forEach(t),vdo=i(bs),Ne=n(bs,"DIV",{class:!0});var Et=s(Ne);m(I3.$$.fragment,Et),bdo=i(Et),fJ=n(Et,"P",{});var F0r=s(fJ);Tdo=r(F0r,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),F0r.forEach(t),Fdo=i(Et),Ba=n(Et,"P",{});var SE=s(Ba);Edo=r(SE,"The model class to instantiate is selected based on the "),mJ=n(SE,"CODE",{});var E0r=s(mJ);Cdo=r(E0r,"model_type"),E0r.forEach(t),Mdo=r(SE,` property of the config object (either
passed as an argument or loaded from `),hJ=n(SE,"CODE",{});var C0r=s(hJ);ydo=r(C0r,"pretrained_model_name_or_path"),C0r.forEach(t),wdo=r(SE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),gJ=n(SE,"CODE",{});var M0r=s(gJ);Ado=r(M0r,"pretrained_model_name_or_path"),M0r.forEach(t),Ldo=r(SE,":"),SE.forEach(t),Bdo=i(Et),O=n(Et,"UL",{});var z=s(O);Ou=n(z,"LI",{});var gve=s(Ou);uJ=n(gve,"STRONG",{});var y0r=s(uJ);xdo=r(y0r,"albert"),y0r.forEach(t),kdo=r(gve," \u2014 "),ax=n(gve,"A",{href:!0});var w0r=s(ax);Rdo=r(w0r,"AlbertForMaskedLM"),w0r.forEach(t),Pdo=r(gve," (ALBERT model)"),gve.forEach(t),Sdo=i(z),Gu=n(z,"LI",{});var uve=s(Gu);pJ=n(uve,"STRONG",{});var A0r=s(pJ);$do=r(A0r,"bart"),A0r.forEach(t),Ido=r(uve," \u2014 "),nx=n(uve,"A",{href:!0});var L0r=s(nx);Ddo=r(L0r,"BartForConditionalGeneration"),L0r.forEach(t),Ndo=r(uve," (BART model)"),uve.forEach(t),jdo=i(z),qu=n(z,"LI",{});var pve=s(qu);_J=n(pve,"STRONG",{});var B0r=s(_J);Odo=r(B0r,"bert"),B0r.forEach(t),Gdo=r(pve," \u2014 "),sx=n(pve,"A",{href:!0});var x0r=s(sx);qdo=r(x0r,"BertForMaskedLM"),x0r.forEach(t),zdo=r(pve," (BERT model)"),pve.forEach(t),Xdo=i(z),zu=n(z,"LI",{});var _ve=s(zu);vJ=n(_ve,"STRONG",{});var k0r=s(vJ);Qdo=r(k0r,"big_bird"),k0r.forEach(t),Vdo=r(_ve," \u2014 "),lx=n(_ve,"A",{href:!0});var R0r=s(lx);Wdo=r(R0r,"BigBirdForMaskedLM"),R0r.forEach(t),Hdo=r(_ve," (BigBird model)"),_ve.forEach(t),Udo=i(z),Xu=n(z,"LI",{});var vve=s(Xu);bJ=n(vve,"STRONG",{});var P0r=s(bJ);Jdo=r(P0r,"camembert"),P0r.forEach(t),Kdo=r(vve," \u2014 "),ix=n(vve,"A",{href:!0});var S0r=s(ix);Ydo=r(S0r,"CamembertForMaskedLM"),S0r.forEach(t),Zdo=r(vve," (CamemBERT model)"),vve.forEach(t),eco=i(z),Qu=n(z,"LI",{});var bve=s(Qu);TJ=n(bve,"STRONG",{});var $0r=s(TJ);oco=r($0r,"convbert"),$0r.forEach(t),rco=r(bve," \u2014 "),dx=n(bve,"A",{href:!0});var I0r=s(dx);tco=r(I0r,"ConvBertForMaskedLM"),I0r.forEach(t),aco=r(bve," (ConvBERT model)"),bve.forEach(t),nco=i(z),Vu=n(z,"LI",{});var Tve=s(Vu);FJ=n(Tve,"STRONG",{});var D0r=s(FJ);sco=r(D0r,"deberta"),D0r.forEach(t),lco=r(Tve," \u2014 "),cx=n(Tve,"A",{href:!0});var N0r=s(cx);ico=r(N0r,"DebertaForMaskedLM"),N0r.forEach(t),dco=r(Tve," (DeBERTa model)"),Tve.forEach(t),cco=i(z),Wu=n(z,"LI",{});var Fve=s(Wu);EJ=n(Fve,"STRONG",{});var j0r=s(EJ);fco=r(j0r,"deberta-v2"),j0r.forEach(t),mco=r(Fve," \u2014 "),fx=n(Fve,"A",{href:!0});var O0r=s(fx);hco=r(O0r,"DebertaV2ForMaskedLM"),O0r.forEach(t),gco=r(Fve," (DeBERTa-v2 model)"),Fve.forEach(t),uco=i(z),Hu=n(z,"LI",{});var Eve=s(Hu);CJ=n(Eve,"STRONG",{});var G0r=s(CJ);pco=r(G0r,"distilbert"),G0r.forEach(t),_co=r(Eve," \u2014 "),mx=n(Eve,"A",{href:!0});var q0r=s(mx);vco=r(q0r,"DistilBertForMaskedLM"),q0r.forEach(t),bco=r(Eve," (DistilBERT model)"),Eve.forEach(t),Tco=i(z),Uu=n(z,"LI",{});var Cve=s(Uu);MJ=n(Cve,"STRONG",{});var z0r=s(MJ);Fco=r(z0r,"electra"),z0r.forEach(t),Eco=r(Cve," \u2014 "),hx=n(Cve,"A",{href:!0});var X0r=s(hx);Cco=r(X0r,"ElectraForMaskedLM"),X0r.forEach(t),Mco=r(Cve," (ELECTRA model)"),Cve.forEach(t),yco=i(z),Ju=n(z,"LI",{});var Mve=s(Ju);yJ=n(Mve,"STRONG",{});var Q0r=s(yJ);wco=r(Q0r,"flaubert"),Q0r.forEach(t),Aco=r(Mve," \u2014 "),gx=n(Mve,"A",{href:!0});var V0r=s(gx);Lco=r(V0r,"FlaubertWithLMHeadModel"),V0r.forEach(t),Bco=r(Mve," (FlauBERT model)"),Mve.forEach(t),xco=i(z),Ku=n(z,"LI",{});var yve=s(Ku);wJ=n(yve,"STRONG",{});var W0r=s(wJ);kco=r(W0r,"fnet"),W0r.forEach(t),Rco=r(yve," \u2014 "),ux=n(yve,"A",{href:!0});var H0r=s(ux);Pco=r(H0r,"FNetForMaskedLM"),H0r.forEach(t),Sco=r(yve," (FNet model)"),yve.forEach(t),$co=i(z),Yu=n(z,"LI",{});var wve=s(Yu);AJ=n(wve,"STRONG",{});var U0r=s(AJ);Ico=r(U0r,"funnel"),U0r.forEach(t),Dco=r(wve," \u2014 "),px=n(wve,"A",{href:!0});var J0r=s(px);Nco=r(J0r,"FunnelForMaskedLM"),J0r.forEach(t),jco=r(wve," (Funnel Transformer model)"),wve.forEach(t),Oco=i(z),Zu=n(z,"LI",{});var Ave=s(Zu);LJ=n(Ave,"STRONG",{});var K0r=s(LJ);Gco=r(K0r,"ibert"),K0r.forEach(t),qco=r(Ave," \u2014 "),_x=n(Ave,"A",{href:!0});var Y0r=s(_x);zco=r(Y0r,"IBertForMaskedLM"),Y0r.forEach(t),Xco=r(Ave," (I-BERT model)"),Ave.forEach(t),Qco=i(z),ep=n(z,"LI",{});var Lve=s(ep);BJ=n(Lve,"STRONG",{});var Z0r=s(BJ);Vco=r(Z0r,"layoutlm"),Z0r.forEach(t),Wco=r(Lve," \u2014 "),vx=n(Lve,"A",{href:!0});var eAr=s(vx);Hco=r(eAr,"LayoutLMForMaskedLM"),eAr.forEach(t),Uco=r(Lve," (LayoutLM model)"),Lve.forEach(t),Jco=i(z),op=n(z,"LI",{});var Bve=s(op);xJ=n(Bve,"STRONG",{});var oAr=s(xJ);Kco=r(oAr,"longformer"),oAr.forEach(t),Yco=r(Bve," \u2014 "),bx=n(Bve,"A",{href:!0});var rAr=s(bx);Zco=r(rAr,"LongformerForMaskedLM"),rAr.forEach(t),efo=r(Bve," (Longformer model)"),Bve.forEach(t),ofo=i(z),rp=n(z,"LI",{});var xve=s(rp);kJ=n(xve,"STRONG",{});var tAr=s(kJ);rfo=r(tAr,"mbart"),tAr.forEach(t),tfo=r(xve," \u2014 "),Tx=n(xve,"A",{href:!0});var aAr=s(Tx);afo=r(aAr,"MBartForConditionalGeneration"),aAr.forEach(t),nfo=r(xve," (mBART model)"),xve.forEach(t),sfo=i(z),tp=n(z,"LI",{});var kve=s(tp);RJ=n(kve,"STRONG",{});var nAr=s(RJ);lfo=r(nAr,"megatron-bert"),nAr.forEach(t),ifo=r(kve," \u2014 "),Fx=n(kve,"A",{href:!0});var sAr=s(Fx);dfo=r(sAr,"MegatronBertForMaskedLM"),sAr.forEach(t),cfo=r(kve," (MegatronBert model)"),kve.forEach(t),ffo=i(z),ap=n(z,"LI",{});var Rve=s(ap);PJ=n(Rve,"STRONG",{});var lAr=s(PJ);mfo=r(lAr,"mobilebert"),lAr.forEach(t),hfo=r(Rve," \u2014 "),Ex=n(Rve,"A",{href:!0});var iAr=s(Ex);gfo=r(iAr,"MobileBertForMaskedLM"),iAr.forEach(t),ufo=r(Rve," (MobileBERT model)"),Rve.forEach(t),pfo=i(z),np=n(z,"LI",{});var Pve=s(np);SJ=n(Pve,"STRONG",{});var dAr=s(SJ);_fo=r(dAr,"mpnet"),dAr.forEach(t),vfo=r(Pve," \u2014 "),Cx=n(Pve,"A",{href:!0});var cAr=s(Cx);bfo=r(cAr,"MPNetForMaskedLM"),cAr.forEach(t),Tfo=r(Pve," (MPNet model)"),Pve.forEach(t),Ffo=i(z),sp=n(z,"LI",{});var Sve=s(sp);$J=n(Sve,"STRONG",{});var fAr=s($J);Efo=r(fAr,"perceiver"),fAr.forEach(t),Cfo=r(Sve," \u2014 "),Mx=n(Sve,"A",{href:!0});var mAr=s(Mx);Mfo=r(mAr,"PerceiverForMaskedLM"),mAr.forEach(t),yfo=r(Sve," (Perceiver model)"),Sve.forEach(t),wfo=i(z),lp=n(z,"LI",{});var $ve=s(lp);IJ=n($ve,"STRONG",{});var hAr=s(IJ);Afo=r(hAr,"qdqbert"),hAr.forEach(t),Lfo=r($ve," \u2014 "),yx=n($ve,"A",{href:!0});var gAr=s(yx);Bfo=r(gAr,"QDQBertForMaskedLM"),gAr.forEach(t),xfo=r($ve," (QDQBert model)"),$ve.forEach(t),kfo=i(z),ip=n(z,"LI",{});var Ive=s(ip);DJ=n(Ive,"STRONG",{});var uAr=s(DJ);Rfo=r(uAr,"reformer"),uAr.forEach(t),Pfo=r(Ive," \u2014 "),wx=n(Ive,"A",{href:!0});var pAr=s(wx);Sfo=r(pAr,"ReformerForMaskedLM"),pAr.forEach(t),$fo=r(Ive," (Reformer model)"),Ive.forEach(t),Ifo=i(z),dp=n(z,"LI",{});var Dve=s(dp);NJ=n(Dve,"STRONG",{});var _Ar=s(NJ);Dfo=r(_Ar,"rembert"),_Ar.forEach(t),Nfo=r(Dve," \u2014 "),Ax=n(Dve,"A",{href:!0});var vAr=s(Ax);jfo=r(vAr,"RemBertForMaskedLM"),vAr.forEach(t),Ofo=r(Dve," (RemBERT model)"),Dve.forEach(t),Gfo=i(z),cp=n(z,"LI",{});var Nve=s(cp);jJ=n(Nve,"STRONG",{});var bAr=s(jJ);qfo=r(bAr,"roberta"),bAr.forEach(t),zfo=r(Nve," \u2014 "),Lx=n(Nve,"A",{href:!0});var TAr=s(Lx);Xfo=r(TAr,"RobertaForMaskedLM"),TAr.forEach(t),Qfo=r(Nve," (RoBERTa model)"),Nve.forEach(t),Vfo=i(z),fp=n(z,"LI",{});var jve=s(fp);OJ=n(jve,"STRONG",{});var FAr=s(OJ);Wfo=r(FAr,"roformer"),FAr.forEach(t),Hfo=r(jve," \u2014 "),Bx=n(jve,"A",{href:!0});var EAr=s(Bx);Ufo=r(EAr,"RoFormerForMaskedLM"),EAr.forEach(t),Jfo=r(jve," (RoFormer model)"),jve.forEach(t),Kfo=i(z),mp=n(z,"LI",{});var Ove=s(mp);GJ=n(Ove,"STRONG",{});var CAr=s(GJ);Yfo=r(CAr,"squeezebert"),CAr.forEach(t),Zfo=r(Ove," \u2014 "),xx=n(Ove,"A",{href:!0});var MAr=s(xx);emo=r(MAr,"SqueezeBertForMaskedLM"),MAr.forEach(t),omo=r(Ove," (SqueezeBERT model)"),Ove.forEach(t),rmo=i(z),hp=n(z,"LI",{});var Gve=s(hp);qJ=n(Gve,"STRONG",{});var yAr=s(qJ);tmo=r(yAr,"tapas"),yAr.forEach(t),amo=r(Gve," \u2014 "),kx=n(Gve,"A",{href:!0});var wAr=s(kx);nmo=r(wAr,"TapasForMaskedLM"),wAr.forEach(t),smo=r(Gve," (TAPAS model)"),Gve.forEach(t),lmo=i(z),gp=n(z,"LI",{});var qve=s(gp);zJ=n(qve,"STRONG",{});var AAr=s(zJ);imo=r(AAr,"wav2vec2"),AAr.forEach(t),dmo=r(qve," \u2014 "),XJ=n(qve,"CODE",{});var LAr=s(XJ);cmo=r(LAr,"Wav2Vec2ForMaskedLM"),LAr.forEach(t),fmo=r(qve," (Wav2Vec2 model)"),qve.forEach(t),mmo=i(z),up=n(z,"LI",{});var zve=s(up);QJ=n(zve,"STRONG",{});var BAr=s(QJ);hmo=r(BAr,"xlm"),BAr.forEach(t),gmo=r(zve," \u2014 "),Rx=n(zve,"A",{href:!0});var xAr=s(Rx);umo=r(xAr,"XLMWithLMHeadModel"),xAr.forEach(t),pmo=r(zve," (XLM model)"),zve.forEach(t),_mo=i(z),pp=n(z,"LI",{});var Xve=s(pp);VJ=n(Xve,"STRONG",{});var kAr=s(VJ);vmo=r(kAr,"xlm-roberta"),kAr.forEach(t),bmo=r(Xve," \u2014 "),Px=n(Xve,"A",{href:!0});var RAr=s(Px);Tmo=r(RAr,"XLMRobertaForMaskedLM"),RAr.forEach(t),Fmo=r(Xve," (XLM-RoBERTa model)"),Xve.forEach(t),z.forEach(t),Emo=i(Et),_p=n(Et,"P",{});var Qve=s(_p);Cmo=r(Qve,"The model is set in evaluation mode by default using "),WJ=n(Qve,"CODE",{});var PAr=s(WJ);Mmo=r(PAr,"model.eval()"),PAr.forEach(t),ymo=r(Qve,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HJ=n(Qve,"CODE",{});var SAr=s(HJ);wmo=r(SAr,"model.train()"),SAr.forEach(t),Qve.forEach(t),Amo=i(Et),UJ=n(Et,"P",{});var $Ar=s(UJ);Lmo=r($Ar,"Examples:"),$Ar.forEach(t),Bmo=i(Et),m(D3.$$.fragment,Et),Et.forEach(t),bs.forEach(t),P5e=i(c),Ci=n(c,"H2",{class:!0});var Owe=s(Ci);vp=n(Owe,"A",{id:!0,class:!0,href:!0});var IAr=s(vp);JJ=n(IAr,"SPAN",{});var DAr=s(JJ);m(N3.$$.fragment,DAr),DAr.forEach(t),IAr.forEach(t),xmo=i(Owe),KJ=n(Owe,"SPAN",{});var NAr=s(KJ);kmo=r(NAr,"AutoModelForSeq2SeqLM"),NAr.forEach(t),Owe.forEach(t),S5e=i(c),zo=n(c,"DIV",{class:!0});var Fs=s(zo);m(j3.$$.fragment,Fs),Rmo=i(Fs),Mi=n(Fs,"P",{});var mj=s(Mi);Pmo=r(mj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),YJ=n(mj,"CODE",{});var jAr=s(YJ);Smo=r(jAr,"from_pretrained()"),jAr.forEach(t),$mo=r(mj,` class method or the
`),ZJ=n(mj,"CODE",{});var OAr=s(ZJ);Imo=r(OAr,"from_config()"),OAr.forEach(t),Dmo=r(mj," class method."),mj.forEach(t),Nmo=i(Fs),O3=n(Fs,"P",{});var Gwe=s(O3);jmo=r(Gwe,"This class cannot be instantiated directly using "),eK=n(Gwe,"CODE",{});var GAr=s(eK);Omo=r(GAr,"__init__()"),GAr.forEach(t),Gmo=r(Gwe," (throws an error)."),Gwe.forEach(t),qmo=i(Fs),$r=n(Fs,"DIV",{class:!0});var Es=s($r);m(G3.$$.fragment,Es),zmo=i(Es),oK=n(Es,"P",{});var qAr=s(oK);Xmo=r(qAr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),qAr.forEach(t),Qmo=i(Es),yi=n(Es,"P",{});var hj=s(yi);Vmo=r(hj,`Note:
Loading a model from its configuration file does `),rK=n(hj,"STRONG",{});var zAr=s(rK);Wmo=r(zAr,"not"),zAr.forEach(t),Hmo=r(hj,` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=n(hj,"CODE",{});var XAr=s(tK);Umo=r(XAr,"from_pretrained()"),XAr.forEach(t),Jmo=r(hj,` to load the model
weights.`),hj.forEach(t),Kmo=i(Es),aK=n(Es,"P",{});var QAr=s(aK);Ymo=r(QAr,"Examples:"),QAr.forEach(t),Zmo=i(Es),m(q3.$$.fragment,Es),Es.forEach(t),eho=i(Fs),je=n(Fs,"DIV",{class:!0});var Ct=s(je);m(z3.$$.fragment,Ct),oho=i(Ct),nK=n(Ct,"P",{});var VAr=s(nK);rho=r(VAr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),VAr.forEach(t),tho=i(Ct),xa=n(Ct,"P",{});var $E=s(xa);aho=r($E,"The model class to instantiate is selected based on the "),sK=n($E,"CODE",{});var WAr=s(sK);nho=r(WAr,"model_type"),WAr.forEach(t),sho=r($E,` property of the config object (either
passed as an argument or loaded from `),lK=n($E,"CODE",{});var HAr=s(lK);lho=r(HAr,"pretrained_model_name_or_path"),HAr.forEach(t),iho=r($E,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),iK=n($E,"CODE",{});var UAr=s(iK);dho=r(UAr,"pretrained_model_name_or_path"),UAr.forEach(t),cho=r($E,":"),$E.forEach(t),fho=i(Ct),fe=n(Ct,"UL",{});var he=s(fe);bp=n(he,"LI",{});var Vve=s(bp);dK=n(Vve,"STRONG",{});var JAr=s(dK);mho=r(JAr,"bart"),JAr.forEach(t),hho=r(Vve," \u2014 "),Sx=n(Vve,"A",{href:!0});var KAr=s(Sx);gho=r(KAr,"BartForConditionalGeneration"),KAr.forEach(t),uho=r(Vve," (BART model)"),Vve.forEach(t),pho=i(he),Tp=n(he,"LI",{});var Wve=s(Tp);cK=n(Wve,"STRONG",{});var YAr=s(cK);_ho=r(YAr,"bigbird_pegasus"),YAr.forEach(t),vho=r(Wve," \u2014 "),$x=n(Wve,"A",{href:!0});var ZAr=s($x);bho=r(ZAr,"BigBirdPegasusForConditionalGeneration"),ZAr.forEach(t),Tho=r(Wve," (BigBirdPegasus model)"),Wve.forEach(t),Fho=i(he),Fp=n(he,"LI",{});var Hve=s(Fp);fK=n(Hve,"STRONG",{});var e6r=s(fK);Eho=r(e6r,"blenderbot"),e6r.forEach(t),Cho=r(Hve," \u2014 "),Ix=n(Hve,"A",{href:!0});var o6r=s(Ix);Mho=r(o6r,"BlenderbotForConditionalGeneration"),o6r.forEach(t),yho=r(Hve," (Blenderbot model)"),Hve.forEach(t),who=i(he),Ep=n(he,"LI",{});var Uve=s(Ep);mK=n(Uve,"STRONG",{});var r6r=s(mK);Aho=r(r6r,"blenderbot-small"),r6r.forEach(t),Lho=r(Uve," \u2014 "),Dx=n(Uve,"A",{href:!0});var t6r=s(Dx);Bho=r(t6r,"BlenderbotSmallForConditionalGeneration"),t6r.forEach(t),xho=r(Uve," (BlenderbotSmall model)"),Uve.forEach(t),kho=i(he),Cp=n(he,"LI",{});var Jve=s(Cp);hK=n(Jve,"STRONG",{});var a6r=s(hK);Rho=r(a6r,"encoder-decoder"),a6r.forEach(t),Pho=r(Jve," \u2014 "),Nx=n(Jve,"A",{href:!0});var n6r=s(Nx);Sho=r(n6r,"EncoderDecoderModel"),n6r.forEach(t),$ho=r(Jve," (Encoder decoder model)"),Jve.forEach(t),Iho=i(he),Mp=n(he,"LI",{});var Kve=s(Mp);gK=n(Kve,"STRONG",{});var s6r=s(gK);Dho=r(s6r,"fsmt"),s6r.forEach(t),Nho=r(Kve," \u2014 "),jx=n(Kve,"A",{href:!0});var l6r=s(jx);jho=r(l6r,"FSMTForConditionalGeneration"),l6r.forEach(t),Oho=r(Kve," (FairSeq Machine-Translation model)"),Kve.forEach(t),Gho=i(he),yp=n(he,"LI",{});var Yve=s(yp);uK=n(Yve,"STRONG",{});var i6r=s(uK);qho=r(i6r,"led"),i6r.forEach(t),zho=r(Yve," \u2014 "),Ox=n(Yve,"A",{href:!0});var d6r=s(Ox);Xho=r(d6r,"LEDForConditionalGeneration"),d6r.forEach(t),Qho=r(Yve," (LED model)"),Yve.forEach(t),Vho=i(he),wp=n(he,"LI",{});var Zve=s(wp);pK=n(Zve,"STRONG",{});var c6r=s(pK);Who=r(c6r,"m2m_100"),c6r.forEach(t),Hho=r(Zve," \u2014 "),Gx=n(Zve,"A",{href:!0});var f6r=s(Gx);Uho=r(f6r,"M2M100ForConditionalGeneration"),f6r.forEach(t),Jho=r(Zve," (M2M100 model)"),Zve.forEach(t),Kho=i(he),Ap=n(he,"LI",{});var ebe=s(Ap);_K=n(ebe,"STRONG",{});var m6r=s(_K);Yho=r(m6r,"marian"),m6r.forEach(t),Zho=r(ebe," \u2014 "),qx=n(ebe,"A",{href:!0});var h6r=s(qx);ego=r(h6r,"MarianMTModel"),h6r.forEach(t),ogo=r(ebe," (Marian model)"),ebe.forEach(t),rgo=i(he),Lp=n(he,"LI",{});var obe=s(Lp);vK=n(obe,"STRONG",{});var g6r=s(vK);tgo=r(g6r,"mbart"),g6r.forEach(t),ago=r(obe," \u2014 "),zx=n(obe,"A",{href:!0});var u6r=s(zx);ngo=r(u6r,"MBartForConditionalGeneration"),u6r.forEach(t),sgo=r(obe," (mBART model)"),obe.forEach(t),lgo=i(he),Bp=n(he,"LI",{});var rbe=s(Bp);bK=n(rbe,"STRONG",{});var p6r=s(bK);igo=r(p6r,"mt5"),p6r.forEach(t),dgo=r(rbe," \u2014 "),Xx=n(rbe,"A",{href:!0});var _6r=s(Xx);cgo=r(_6r,"MT5ForConditionalGeneration"),_6r.forEach(t),fgo=r(rbe," (mT5 model)"),rbe.forEach(t),mgo=i(he),xp=n(he,"LI",{});var tbe=s(xp);TK=n(tbe,"STRONG",{});var v6r=s(TK);hgo=r(v6r,"pegasus"),v6r.forEach(t),ggo=r(tbe," \u2014 "),Qx=n(tbe,"A",{href:!0});var b6r=s(Qx);ugo=r(b6r,"PegasusForConditionalGeneration"),b6r.forEach(t),pgo=r(tbe," (Pegasus model)"),tbe.forEach(t),_go=i(he),kp=n(he,"LI",{});var abe=s(kp);FK=n(abe,"STRONG",{});var T6r=s(FK);vgo=r(T6r,"prophetnet"),T6r.forEach(t),bgo=r(abe," \u2014 "),Vx=n(abe,"A",{href:!0});var F6r=s(Vx);Tgo=r(F6r,"ProphetNetForConditionalGeneration"),F6r.forEach(t),Fgo=r(abe," (ProphetNet model)"),abe.forEach(t),Ego=i(he),Rp=n(he,"LI",{});var nbe=s(Rp);EK=n(nbe,"STRONG",{});var E6r=s(EK);Cgo=r(E6r,"t5"),E6r.forEach(t),Mgo=r(nbe," \u2014 "),Wx=n(nbe,"A",{href:!0});var C6r=s(Wx);ygo=r(C6r,"T5ForConditionalGeneration"),C6r.forEach(t),wgo=r(nbe," (T5 model)"),nbe.forEach(t),Ago=i(he),Pp=n(he,"LI",{});var sbe=s(Pp);CK=n(sbe,"STRONG",{});var M6r=s(CK);Lgo=r(M6r,"xlm-prophetnet"),M6r.forEach(t),Bgo=r(sbe," \u2014 "),Hx=n(sbe,"A",{href:!0});var y6r=s(Hx);xgo=r(y6r,"XLMProphetNetForConditionalGeneration"),y6r.forEach(t),kgo=r(sbe," (XLMProphetNet model)"),sbe.forEach(t),he.forEach(t),Rgo=i(Ct),Sp=n(Ct,"P",{});var lbe=s(Sp);Pgo=r(lbe,"The model is set in evaluation mode by default using "),MK=n(lbe,"CODE",{});var w6r=s(MK);Sgo=r(w6r,"model.eval()"),w6r.forEach(t),$go=r(lbe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yK=n(lbe,"CODE",{});var A6r=s(yK);Igo=r(A6r,"model.train()"),A6r.forEach(t),lbe.forEach(t),Dgo=i(Ct),wK=n(Ct,"P",{});var L6r=s(wK);Ngo=r(L6r,"Examples:"),L6r.forEach(t),jgo=i(Ct),m(X3.$$.fragment,Ct),Ct.forEach(t),Fs.forEach(t),$5e=i(c),wi=n(c,"H2",{class:!0});var qwe=s(wi);$p=n(qwe,"A",{id:!0,class:!0,href:!0});var B6r=s($p);AK=n(B6r,"SPAN",{});var x6r=s(AK);m(Q3.$$.fragment,x6r),x6r.forEach(t),B6r.forEach(t),Ogo=i(qwe),LK=n(qwe,"SPAN",{});var k6r=s(LK);Ggo=r(k6r,"AutoModelForSequenceClassification"),k6r.forEach(t),qwe.forEach(t),I5e=i(c),Xo=n(c,"DIV",{class:!0});var Cs=s(Xo);m(V3.$$.fragment,Cs),qgo=i(Cs),Ai=n(Cs,"P",{});var gj=s(Ai);zgo=r(gj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),BK=n(gj,"CODE",{});var R6r=s(BK);Xgo=r(R6r,"from_pretrained()"),R6r.forEach(t),Qgo=r(gj,` class method or the
`),xK=n(gj,"CODE",{});var P6r=s(xK);Vgo=r(P6r,"from_config()"),P6r.forEach(t),Wgo=r(gj," class method."),gj.forEach(t),Hgo=i(Cs),W3=n(Cs,"P",{});var zwe=s(W3);Ugo=r(zwe,"This class cannot be instantiated directly using "),kK=n(zwe,"CODE",{});var S6r=s(kK);Jgo=r(S6r,"__init__()"),S6r.forEach(t),Kgo=r(zwe," (throws an error)."),zwe.forEach(t),Ygo=i(Cs),Ir=n(Cs,"DIV",{class:!0});var Ms=s(Ir);m(H3.$$.fragment,Ms),Zgo=i(Ms),RK=n(Ms,"P",{});var $6r=s(RK);euo=r($6r,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),$6r.forEach(t),ouo=i(Ms),Li=n(Ms,"P",{});var uj=s(Li);ruo=r(uj,`Note:
Loading a model from its configuration file does `),PK=n(uj,"STRONG",{});var I6r=s(PK);tuo=r(I6r,"not"),I6r.forEach(t),auo=r(uj,` load the model weights. It only affects the
model\u2019s configuration. Use `),SK=n(uj,"CODE",{});var D6r=s(SK);nuo=r(D6r,"from_pretrained()"),D6r.forEach(t),suo=r(uj,` to load the model
weights.`),uj.forEach(t),luo=i(Ms),$K=n(Ms,"P",{});var N6r=s($K);iuo=r(N6r,"Examples:"),N6r.forEach(t),duo=i(Ms),m(U3.$$.fragment,Ms),Ms.forEach(t),cuo=i(Cs),Oe=n(Cs,"DIV",{class:!0});var Mt=s(Oe);m(J3.$$.fragment,Mt),fuo=i(Mt),IK=n(Mt,"P",{});var j6r=s(IK);muo=r(j6r,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),j6r.forEach(t),huo=i(Mt),ka=n(Mt,"P",{});var IE=s(ka);guo=r(IE,"The model class to instantiate is selected based on the "),DK=n(IE,"CODE",{});var O6r=s(DK);uuo=r(O6r,"model_type"),O6r.forEach(t),puo=r(IE,` property of the config object (either
passed as an argument or loaded from `),NK=n(IE,"CODE",{});var G6r=s(NK);_uo=r(G6r,"pretrained_model_name_or_path"),G6r.forEach(t),vuo=r(IE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),jK=n(IE,"CODE",{});var q6r=s(jK);buo=r(q6r,"pretrained_model_name_or_path"),q6r.forEach(t),Tuo=r(IE,":"),IE.forEach(t),Fuo=i(Mt),A=n(Mt,"UL",{});var L=s(A);Ip=n(L,"LI",{});var ibe=s(Ip);OK=n(ibe,"STRONG",{});var z6r=s(OK);Euo=r(z6r,"albert"),z6r.forEach(t),Cuo=r(ibe," \u2014 "),Ux=n(ibe,"A",{href:!0});var X6r=s(Ux);Muo=r(X6r,"AlbertForSequenceClassification"),X6r.forEach(t),yuo=r(ibe," (ALBERT model)"),ibe.forEach(t),wuo=i(L),Dp=n(L,"LI",{});var dbe=s(Dp);GK=n(dbe,"STRONG",{});var Q6r=s(GK);Auo=r(Q6r,"bart"),Q6r.forEach(t),Luo=r(dbe," \u2014 "),Jx=n(dbe,"A",{href:!0});var V6r=s(Jx);Buo=r(V6r,"BartForSequenceClassification"),V6r.forEach(t),xuo=r(dbe," (BART model)"),dbe.forEach(t),kuo=i(L),Np=n(L,"LI",{});var cbe=s(Np);qK=n(cbe,"STRONG",{});var W6r=s(qK);Ruo=r(W6r,"bert"),W6r.forEach(t),Puo=r(cbe," \u2014 "),Kx=n(cbe,"A",{href:!0});var H6r=s(Kx);Suo=r(H6r,"BertForSequenceClassification"),H6r.forEach(t),$uo=r(cbe," (BERT model)"),cbe.forEach(t),Iuo=i(L),jp=n(L,"LI",{});var fbe=s(jp);zK=n(fbe,"STRONG",{});var U6r=s(zK);Duo=r(U6r,"big_bird"),U6r.forEach(t),Nuo=r(fbe," \u2014 "),Yx=n(fbe,"A",{href:!0});var J6r=s(Yx);juo=r(J6r,"BigBirdForSequenceClassification"),J6r.forEach(t),Ouo=r(fbe," (BigBird model)"),fbe.forEach(t),Guo=i(L),Op=n(L,"LI",{});var mbe=s(Op);XK=n(mbe,"STRONG",{});var K6r=s(XK);quo=r(K6r,"bigbird_pegasus"),K6r.forEach(t),zuo=r(mbe," \u2014 "),Zx=n(mbe,"A",{href:!0});var Y6r=s(Zx);Xuo=r(Y6r,"BigBirdPegasusForSequenceClassification"),Y6r.forEach(t),Quo=r(mbe," (BigBirdPegasus model)"),mbe.forEach(t),Vuo=i(L),Gp=n(L,"LI",{});var hbe=s(Gp);QK=n(hbe,"STRONG",{});var Z6r=s(QK);Wuo=r(Z6r,"camembert"),Z6r.forEach(t),Huo=r(hbe," \u2014 "),ek=n(hbe,"A",{href:!0});var eLr=s(ek);Uuo=r(eLr,"CamembertForSequenceClassification"),eLr.forEach(t),Juo=r(hbe," (CamemBERT model)"),hbe.forEach(t),Kuo=i(L),qp=n(L,"LI",{});var gbe=s(qp);VK=n(gbe,"STRONG",{});var oLr=s(VK);Yuo=r(oLr,"canine"),oLr.forEach(t),Zuo=r(gbe," \u2014 "),ok=n(gbe,"A",{href:!0});var rLr=s(ok);epo=r(rLr,"CanineForSequenceClassification"),rLr.forEach(t),opo=r(gbe," (Canine model)"),gbe.forEach(t),rpo=i(L),zp=n(L,"LI",{});var ube=s(zp);WK=n(ube,"STRONG",{});var tLr=s(WK);tpo=r(tLr,"convbert"),tLr.forEach(t),apo=r(ube," \u2014 "),rk=n(ube,"A",{href:!0});var aLr=s(rk);npo=r(aLr,"ConvBertForSequenceClassification"),aLr.forEach(t),spo=r(ube," (ConvBERT model)"),ube.forEach(t),lpo=i(L),Xp=n(L,"LI",{});var pbe=s(Xp);HK=n(pbe,"STRONG",{});var nLr=s(HK);ipo=r(nLr,"ctrl"),nLr.forEach(t),dpo=r(pbe," \u2014 "),tk=n(pbe,"A",{href:!0});var sLr=s(tk);cpo=r(sLr,"CTRLForSequenceClassification"),sLr.forEach(t),fpo=r(pbe," (CTRL model)"),pbe.forEach(t),mpo=i(L),Qp=n(L,"LI",{});var _be=s(Qp);UK=n(_be,"STRONG",{});var lLr=s(UK);hpo=r(lLr,"deberta"),lLr.forEach(t),gpo=r(_be," \u2014 "),ak=n(_be,"A",{href:!0});var iLr=s(ak);upo=r(iLr,"DebertaForSequenceClassification"),iLr.forEach(t),ppo=r(_be," (DeBERTa model)"),_be.forEach(t),_po=i(L),Vp=n(L,"LI",{});var vbe=s(Vp);JK=n(vbe,"STRONG",{});var dLr=s(JK);vpo=r(dLr,"deberta-v2"),dLr.forEach(t),bpo=r(vbe," \u2014 "),nk=n(vbe,"A",{href:!0});var cLr=s(nk);Tpo=r(cLr,"DebertaV2ForSequenceClassification"),cLr.forEach(t),Fpo=r(vbe," (DeBERTa-v2 model)"),vbe.forEach(t),Epo=i(L),Wp=n(L,"LI",{});var bbe=s(Wp);KK=n(bbe,"STRONG",{});var fLr=s(KK);Cpo=r(fLr,"distilbert"),fLr.forEach(t),Mpo=r(bbe," \u2014 "),sk=n(bbe,"A",{href:!0});var mLr=s(sk);ypo=r(mLr,"DistilBertForSequenceClassification"),mLr.forEach(t),wpo=r(bbe," (DistilBERT model)"),bbe.forEach(t),Apo=i(L),Hp=n(L,"LI",{});var Tbe=s(Hp);YK=n(Tbe,"STRONG",{});var hLr=s(YK);Lpo=r(hLr,"electra"),hLr.forEach(t),Bpo=r(Tbe," \u2014 "),lk=n(Tbe,"A",{href:!0});var gLr=s(lk);xpo=r(gLr,"ElectraForSequenceClassification"),gLr.forEach(t),kpo=r(Tbe," (ELECTRA model)"),Tbe.forEach(t),Rpo=i(L),Up=n(L,"LI",{});var Fbe=s(Up);ZK=n(Fbe,"STRONG",{});var uLr=s(ZK);Ppo=r(uLr,"flaubert"),uLr.forEach(t),Spo=r(Fbe," \u2014 "),ik=n(Fbe,"A",{href:!0});var pLr=s(ik);$po=r(pLr,"FlaubertForSequenceClassification"),pLr.forEach(t),Ipo=r(Fbe," (FlauBERT model)"),Fbe.forEach(t),Dpo=i(L),Jp=n(L,"LI",{});var Ebe=s(Jp);eY=n(Ebe,"STRONG",{});var _Lr=s(eY);Npo=r(_Lr,"fnet"),_Lr.forEach(t),jpo=r(Ebe," \u2014 "),dk=n(Ebe,"A",{href:!0});var vLr=s(dk);Opo=r(vLr,"FNetForSequenceClassification"),vLr.forEach(t),Gpo=r(Ebe," (FNet model)"),Ebe.forEach(t),qpo=i(L),Kp=n(L,"LI",{});var Cbe=s(Kp);oY=n(Cbe,"STRONG",{});var bLr=s(oY);zpo=r(bLr,"funnel"),bLr.forEach(t),Xpo=r(Cbe," \u2014 "),ck=n(Cbe,"A",{href:!0});var TLr=s(ck);Qpo=r(TLr,"FunnelForSequenceClassification"),TLr.forEach(t),Vpo=r(Cbe," (Funnel Transformer model)"),Cbe.forEach(t),Wpo=i(L),Yp=n(L,"LI",{});var Mbe=s(Yp);rY=n(Mbe,"STRONG",{});var FLr=s(rY);Hpo=r(FLr,"gpt2"),FLr.forEach(t),Upo=r(Mbe," \u2014 "),fk=n(Mbe,"A",{href:!0});var ELr=s(fk);Jpo=r(ELr,"GPT2ForSequenceClassification"),ELr.forEach(t),Kpo=r(Mbe," (OpenAI GPT-2 model)"),Mbe.forEach(t),Ypo=i(L),Zp=n(L,"LI",{});var ybe=s(Zp);tY=n(ybe,"STRONG",{});var CLr=s(tY);Zpo=r(CLr,"gpt_neo"),CLr.forEach(t),e_o=r(ybe," \u2014 "),mk=n(ybe,"A",{href:!0});var MLr=s(mk);o_o=r(MLr,"GPTNeoForSequenceClassification"),MLr.forEach(t),r_o=r(ybe," (GPT Neo model)"),ybe.forEach(t),t_o=i(L),e_=n(L,"LI",{});var wbe=s(e_);aY=n(wbe,"STRONG",{});var yLr=s(aY);a_o=r(yLr,"gptj"),yLr.forEach(t),n_o=r(wbe," \u2014 "),hk=n(wbe,"A",{href:!0});var wLr=s(hk);s_o=r(wLr,"GPTJForSequenceClassification"),wLr.forEach(t),l_o=r(wbe," (GPT-J model)"),wbe.forEach(t),i_o=i(L),o_=n(L,"LI",{});var Abe=s(o_);nY=n(Abe,"STRONG",{});var ALr=s(nY);d_o=r(ALr,"ibert"),ALr.forEach(t),c_o=r(Abe," \u2014 "),gk=n(Abe,"A",{href:!0});var LLr=s(gk);f_o=r(LLr,"IBertForSequenceClassification"),LLr.forEach(t),m_o=r(Abe," (I-BERT model)"),Abe.forEach(t),h_o=i(L),r_=n(L,"LI",{});var Lbe=s(r_);sY=n(Lbe,"STRONG",{});var BLr=s(sY);g_o=r(BLr,"layoutlm"),BLr.forEach(t),u_o=r(Lbe," \u2014 "),uk=n(Lbe,"A",{href:!0});var xLr=s(uk);p_o=r(xLr,"LayoutLMForSequenceClassification"),xLr.forEach(t),__o=r(Lbe," (LayoutLM model)"),Lbe.forEach(t),v_o=i(L),t_=n(L,"LI",{});var Bbe=s(t_);lY=n(Bbe,"STRONG",{});var kLr=s(lY);b_o=r(kLr,"layoutlmv2"),kLr.forEach(t),T_o=r(Bbe," \u2014 "),pk=n(Bbe,"A",{href:!0});var RLr=s(pk);F_o=r(RLr,"LayoutLMv2ForSequenceClassification"),RLr.forEach(t),E_o=r(Bbe," (LayoutLMv2 model)"),Bbe.forEach(t),C_o=i(L),a_=n(L,"LI",{});var xbe=s(a_);iY=n(xbe,"STRONG",{});var PLr=s(iY);M_o=r(PLr,"led"),PLr.forEach(t),y_o=r(xbe," \u2014 "),_k=n(xbe,"A",{href:!0});var SLr=s(_k);w_o=r(SLr,"LEDForSequenceClassification"),SLr.forEach(t),A_o=r(xbe," (LED model)"),xbe.forEach(t),L_o=i(L),n_=n(L,"LI",{});var kbe=s(n_);dY=n(kbe,"STRONG",{});var $Lr=s(dY);B_o=r($Lr,"longformer"),$Lr.forEach(t),x_o=r(kbe," \u2014 "),vk=n(kbe,"A",{href:!0});var ILr=s(vk);k_o=r(ILr,"LongformerForSequenceClassification"),ILr.forEach(t),R_o=r(kbe," (Longformer model)"),kbe.forEach(t),P_o=i(L),s_=n(L,"LI",{});var Rbe=s(s_);cY=n(Rbe,"STRONG",{});var DLr=s(cY);S_o=r(DLr,"mbart"),DLr.forEach(t),$_o=r(Rbe," \u2014 "),bk=n(Rbe,"A",{href:!0});var NLr=s(bk);I_o=r(NLr,"MBartForSequenceClassification"),NLr.forEach(t),D_o=r(Rbe," (mBART model)"),Rbe.forEach(t),N_o=i(L),l_=n(L,"LI",{});var Pbe=s(l_);fY=n(Pbe,"STRONG",{});var jLr=s(fY);j_o=r(jLr,"megatron-bert"),jLr.forEach(t),O_o=r(Pbe," \u2014 "),Tk=n(Pbe,"A",{href:!0});var OLr=s(Tk);G_o=r(OLr,"MegatronBertForSequenceClassification"),OLr.forEach(t),q_o=r(Pbe," (MegatronBert model)"),Pbe.forEach(t),z_o=i(L),i_=n(L,"LI",{});var Sbe=s(i_);mY=n(Sbe,"STRONG",{});var GLr=s(mY);X_o=r(GLr,"mobilebert"),GLr.forEach(t),Q_o=r(Sbe," \u2014 "),Fk=n(Sbe,"A",{href:!0});var qLr=s(Fk);V_o=r(qLr,"MobileBertForSequenceClassification"),qLr.forEach(t),W_o=r(Sbe," (MobileBERT model)"),Sbe.forEach(t),H_o=i(L),d_=n(L,"LI",{});var $be=s(d_);hY=n($be,"STRONG",{});var zLr=s(hY);U_o=r(zLr,"mpnet"),zLr.forEach(t),J_o=r($be," \u2014 "),Ek=n($be,"A",{href:!0});var XLr=s(Ek);K_o=r(XLr,"MPNetForSequenceClassification"),XLr.forEach(t),Y_o=r($be," (MPNet model)"),$be.forEach(t),Z_o=i(L),c_=n(L,"LI",{});var Ibe=s(c_);gY=n(Ibe,"STRONG",{});var QLr=s(gY);e1o=r(QLr,"openai-gpt"),QLr.forEach(t),o1o=r(Ibe," \u2014 "),Ck=n(Ibe,"A",{href:!0});var VLr=s(Ck);r1o=r(VLr,"OpenAIGPTForSequenceClassification"),VLr.forEach(t),t1o=r(Ibe," (OpenAI GPT model)"),Ibe.forEach(t),a1o=i(L),f_=n(L,"LI",{});var Dbe=s(f_);uY=n(Dbe,"STRONG",{});var WLr=s(uY);n1o=r(WLr,"perceiver"),WLr.forEach(t),s1o=r(Dbe," \u2014 "),Mk=n(Dbe,"A",{href:!0});var HLr=s(Mk);l1o=r(HLr,"PerceiverForSequenceClassification"),HLr.forEach(t),i1o=r(Dbe," (Perceiver model)"),Dbe.forEach(t),d1o=i(L),m_=n(L,"LI",{});var Nbe=s(m_);pY=n(Nbe,"STRONG",{});var ULr=s(pY);c1o=r(ULr,"qdqbert"),ULr.forEach(t),f1o=r(Nbe," \u2014 "),yk=n(Nbe,"A",{href:!0});var JLr=s(yk);m1o=r(JLr,"QDQBertForSequenceClassification"),JLr.forEach(t),h1o=r(Nbe," (QDQBert model)"),Nbe.forEach(t),g1o=i(L),h_=n(L,"LI",{});var jbe=s(h_);_Y=n(jbe,"STRONG",{});var KLr=s(_Y);u1o=r(KLr,"reformer"),KLr.forEach(t),p1o=r(jbe," \u2014 "),wk=n(jbe,"A",{href:!0});var YLr=s(wk);_1o=r(YLr,"ReformerForSequenceClassification"),YLr.forEach(t),v1o=r(jbe," (Reformer model)"),jbe.forEach(t),b1o=i(L),g_=n(L,"LI",{});var Obe=s(g_);vY=n(Obe,"STRONG",{});var ZLr=s(vY);T1o=r(ZLr,"rembert"),ZLr.forEach(t),F1o=r(Obe," \u2014 "),Ak=n(Obe,"A",{href:!0});var e8r=s(Ak);E1o=r(e8r,"RemBertForSequenceClassification"),e8r.forEach(t),C1o=r(Obe," (RemBERT model)"),Obe.forEach(t),M1o=i(L),u_=n(L,"LI",{});var Gbe=s(u_);bY=n(Gbe,"STRONG",{});var o8r=s(bY);y1o=r(o8r,"roberta"),o8r.forEach(t),w1o=r(Gbe," \u2014 "),Lk=n(Gbe,"A",{href:!0});var r8r=s(Lk);A1o=r(r8r,"RobertaForSequenceClassification"),r8r.forEach(t),L1o=r(Gbe," (RoBERTa model)"),Gbe.forEach(t),B1o=i(L),p_=n(L,"LI",{});var qbe=s(p_);TY=n(qbe,"STRONG",{});var t8r=s(TY);x1o=r(t8r,"roformer"),t8r.forEach(t),k1o=r(qbe," \u2014 "),Bk=n(qbe,"A",{href:!0});var a8r=s(Bk);R1o=r(a8r,"RoFormerForSequenceClassification"),a8r.forEach(t),P1o=r(qbe," (RoFormer model)"),qbe.forEach(t),S1o=i(L),__=n(L,"LI",{});var zbe=s(__);FY=n(zbe,"STRONG",{});var n8r=s(FY);$1o=r(n8r,"squeezebert"),n8r.forEach(t),I1o=r(zbe," \u2014 "),xk=n(zbe,"A",{href:!0});var s8r=s(xk);D1o=r(s8r,"SqueezeBertForSequenceClassification"),s8r.forEach(t),N1o=r(zbe," (SqueezeBERT model)"),zbe.forEach(t),j1o=i(L),v_=n(L,"LI",{});var Xbe=s(v_);EY=n(Xbe,"STRONG",{});var l8r=s(EY);O1o=r(l8r,"tapas"),l8r.forEach(t),G1o=r(Xbe," \u2014 "),kk=n(Xbe,"A",{href:!0});var i8r=s(kk);q1o=r(i8r,"TapasForSequenceClassification"),i8r.forEach(t),z1o=r(Xbe," (TAPAS model)"),Xbe.forEach(t),X1o=i(L),b_=n(L,"LI",{});var Qbe=s(b_);CY=n(Qbe,"STRONG",{});var d8r=s(CY);Q1o=r(d8r,"transfo-xl"),d8r.forEach(t),V1o=r(Qbe," \u2014 "),Rk=n(Qbe,"A",{href:!0});var c8r=s(Rk);W1o=r(c8r,"TransfoXLForSequenceClassification"),c8r.forEach(t),H1o=r(Qbe," (Transformer-XL model)"),Qbe.forEach(t),U1o=i(L),T_=n(L,"LI",{});var Vbe=s(T_);MY=n(Vbe,"STRONG",{});var f8r=s(MY);J1o=r(f8r,"xlm"),f8r.forEach(t),K1o=r(Vbe," \u2014 "),Pk=n(Vbe,"A",{href:!0});var m8r=s(Pk);Y1o=r(m8r,"XLMForSequenceClassification"),m8r.forEach(t),Z1o=r(Vbe," (XLM model)"),Vbe.forEach(t),e4o=i(L),F_=n(L,"LI",{});var Wbe=s(F_);yY=n(Wbe,"STRONG",{});var h8r=s(yY);o4o=r(h8r,"xlm-roberta"),h8r.forEach(t),r4o=r(Wbe," \u2014 "),Sk=n(Wbe,"A",{href:!0});var g8r=s(Sk);t4o=r(g8r,"XLMRobertaForSequenceClassification"),g8r.forEach(t),a4o=r(Wbe," (XLM-RoBERTa model)"),Wbe.forEach(t),n4o=i(L),E_=n(L,"LI",{});var Hbe=s(E_);wY=n(Hbe,"STRONG",{});var u8r=s(wY);s4o=r(u8r,"xlnet"),u8r.forEach(t),l4o=r(Hbe," \u2014 "),$k=n(Hbe,"A",{href:!0});var p8r=s($k);i4o=r(p8r,"XLNetForSequenceClassification"),p8r.forEach(t),d4o=r(Hbe," (XLNet model)"),Hbe.forEach(t),L.forEach(t),c4o=i(Mt),C_=n(Mt,"P",{});var Ube=s(C_);f4o=r(Ube,"The model is set in evaluation mode by default using "),AY=n(Ube,"CODE",{});var _8r=s(AY);m4o=r(_8r,"model.eval()"),_8r.forEach(t),h4o=r(Ube,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),LY=n(Ube,"CODE",{});var v8r=s(LY);g4o=r(v8r,"model.train()"),v8r.forEach(t),Ube.forEach(t),u4o=i(Mt),BY=n(Mt,"P",{});var b8r=s(BY);p4o=r(b8r,"Examples:"),b8r.forEach(t),_4o=i(Mt),m(K3.$$.fragment,Mt),Mt.forEach(t),Cs.forEach(t),D5e=i(c),Bi=n(c,"H2",{class:!0});var Xwe=s(Bi);M_=n(Xwe,"A",{id:!0,class:!0,href:!0});var T8r=s(M_);xY=n(T8r,"SPAN",{});var F8r=s(xY);m(Y3.$$.fragment,F8r),F8r.forEach(t),T8r.forEach(t),v4o=i(Xwe),kY=n(Xwe,"SPAN",{});var E8r=s(kY);b4o=r(E8r,"AutoModelForMultipleChoice"),E8r.forEach(t),Xwe.forEach(t),N5e=i(c),Qo=n(c,"DIV",{class:!0});var ys=s(Qo);m(Z3.$$.fragment,ys),T4o=i(ys),xi=n(ys,"P",{});var pj=s(xi);F4o=r(pj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),RY=n(pj,"CODE",{});var C8r=s(RY);E4o=r(C8r,"from_pretrained()"),C8r.forEach(t),C4o=r(pj,` class method or the
`),PY=n(pj,"CODE",{});var M8r=s(PY);M4o=r(M8r,"from_config()"),M8r.forEach(t),y4o=r(pj," class method."),pj.forEach(t),w4o=i(ys),eM=n(ys,"P",{});var Qwe=s(eM);A4o=r(Qwe,"This class cannot be instantiated directly using "),SY=n(Qwe,"CODE",{});var y8r=s(SY);L4o=r(y8r,"__init__()"),y8r.forEach(t),B4o=r(Qwe," (throws an error)."),Qwe.forEach(t),x4o=i(ys),Dr=n(ys,"DIV",{class:!0});var ws=s(Dr);m(oM.$$.fragment,ws),k4o=i(ws),$Y=n(ws,"P",{});var w8r=s($Y);R4o=r(w8r,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),w8r.forEach(t),P4o=i(ws),ki=n(ws,"P",{});var _j=s(ki);S4o=r(_j,`Note:
Loading a model from its configuration file does `),IY=n(_j,"STRONG",{});var A8r=s(IY);$4o=r(A8r,"not"),A8r.forEach(t),I4o=r(_j,` load the model weights. It only affects the
model\u2019s configuration. Use `),DY=n(_j,"CODE",{});var L8r=s(DY);D4o=r(L8r,"from_pretrained()"),L8r.forEach(t),N4o=r(_j,` to load the model
weights.`),_j.forEach(t),j4o=i(ws),NY=n(ws,"P",{});var B8r=s(NY);O4o=r(B8r,"Examples:"),B8r.forEach(t),G4o=i(ws),m(rM.$$.fragment,ws),ws.forEach(t),q4o=i(ys),Ge=n(ys,"DIV",{class:!0});var yt=s(Ge);m(tM.$$.fragment,yt),z4o=i(yt),jY=n(yt,"P",{});var x8r=s(jY);X4o=r(x8r,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),x8r.forEach(t),Q4o=i(yt),Ra=n(yt,"P",{});var DE=s(Ra);V4o=r(DE,"The model class to instantiate is selected based on the "),OY=n(DE,"CODE",{});var k8r=s(OY);W4o=r(k8r,"model_type"),k8r.forEach(t),H4o=r(DE,` property of the config object (either
passed as an argument or loaded from `),GY=n(DE,"CODE",{});var R8r=s(GY);U4o=r(R8r,"pretrained_model_name_or_path"),R8r.forEach(t),J4o=r(DE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),qY=n(DE,"CODE",{});var P8r=s(qY);K4o=r(P8r,"pretrained_model_name_or_path"),P8r.forEach(t),Y4o=r(DE,":"),DE.forEach(t),Z4o=i(yt),H=n(yt,"UL",{});var J=s(H);y_=n(J,"LI",{});var Jbe=s(y_);zY=n(Jbe,"STRONG",{});var S8r=s(zY);evo=r(S8r,"albert"),S8r.forEach(t),ovo=r(Jbe," \u2014 "),Ik=n(Jbe,"A",{href:!0});var $8r=s(Ik);rvo=r($8r,"AlbertForMultipleChoice"),$8r.forEach(t),tvo=r(Jbe," (ALBERT model)"),Jbe.forEach(t),avo=i(J),w_=n(J,"LI",{});var Kbe=s(w_);XY=n(Kbe,"STRONG",{});var I8r=s(XY);nvo=r(I8r,"bert"),I8r.forEach(t),svo=r(Kbe," \u2014 "),Dk=n(Kbe,"A",{href:!0});var D8r=s(Dk);lvo=r(D8r,"BertForMultipleChoice"),D8r.forEach(t),ivo=r(Kbe," (BERT model)"),Kbe.forEach(t),dvo=i(J),A_=n(J,"LI",{});var Ybe=s(A_);QY=n(Ybe,"STRONG",{});var N8r=s(QY);cvo=r(N8r,"big_bird"),N8r.forEach(t),fvo=r(Ybe," \u2014 "),Nk=n(Ybe,"A",{href:!0});var j8r=s(Nk);mvo=r(j8r,"BigBirdForMultipleChoice"),j8r.forEach(t),hvo=r(Ybe," (BigBird model)"),Ybe.forEach(t),gvo=i(J),L_=n(J,"LI",{});var Zbe=s(L_);VY=n(Zbe,"STRONG",{});var O8r=s(VY);uvo=r(O8r,"camembert"),O8r.forEach(t),pvo=r(Zbe," \u2014 "),jk=n(Zbe,"A",{href:!0});var G8r=s(jk);_vo=r(G8r,"CamembertForMultipleChoice"),G8r.forEach(t),vvo=r(Zbe," (CamemBERT model)"),Zbe.forEach(t),bvo=i(J),B_=n(J,"LI",{});var e2e=s(B_);WY=n(e2e,"STRONG",{});var q8r=s(WY);Tvo=r(q8r,"canine"),q8r.forEach(t),Fvo=r(e2e," \u2014 "),Ok=n(e2e,"A",{href:!0});var z8r=s(Ok);Evo=r(z8r,"CanineForMultipleChoice"),z8r.forEach(t),Cvo=r(e2e," (Canine model)"),e2e.forEach(t),Mvo=i(J),x_=n(J,"LI",{});var o2e=s(x_);HY=n(o2e,"STRONG",{});var X8r=s(HY);yvo=r(X8r,"convbert"),X8r.forEach(t),wvo=r(o2e," \u2014 "),Gk=n(o2e,"A",{href:!0});var Q8r=s(Gk);Avo=r(Q8r,"ConvBertForMultipleChoice"),Q8r.forEach(t),Lvo=r(o2e," (ConvBERT model)"),o2e.forEach(t),Bvo=i(J),k_=n(J,"LI",{});var r2e=s(k_);UY=n(r2e,"STRONG",{});var V8r=s(UY);xvo=r(V8r,"distilbert"),V8r.forEach(t),kvo=r(r2e," \u2014 "),qk=n(r2e,"A",{href:!0});var W8r=s(qk);Rvo=r(W8r,"DistilBertForMultipleChoice"),W8r.forEach(t),Pvo=r(r2e," (DistilBERT model)"),r2e.forEach(t),Svo=i(J),R_=n(J,"LI",{});var t2e=s(R_);JY=n(t2e,"STRONG",{});var H8r=s(JY);$vo=r(H8r,"electra"),H8r.forEach(t),Ivo=r(t2e," \u2014 "),zk=n(t2e,"A",{href:!0});var U8r=s(zk);Dvo=r(U8r,"ElectraForMultipleChoice"),U8r.forEach(t),Nvo=r(t2e," (ELECTRA model)"),t2e.forEach(t),jvo=i(J),P_=n(J,"LI",{});var a2e=s(P_);KY=n(a2e,"STRONG",{});var J8r=s(KY);Ovo=r(J8r,"flaubert"),J8r.forEach(t),Gvo=r(a2e," \u2014 "),Xk=n(a2e,"A",{href:!0});var K8r=s(Xk);qvo=r(K8r,"FlaubertForMultipleChoice"),K8r.forEach(t),zvo=r(a2e," (FlauBERT model)"),a2e.forEach(t),Xvo=i(J),S_=n(J,"LI",{});var n2e=s(S_);YY=n(n2e,"STRONG",{});var Y8r=s(YY);Qvo=r(Y8r,"fnet"),Y8r.forEach(t),Vvo=r(n2e," \u2014 "),Qk=n(n2e,"A",{href:!0});var Z8r=s(Qk);Wvo=r(Z8r,"FNetForMultipleChoice"),Z8r.forEach(t),Hvo=r(n2e," (FNet model)"),n2e.forEach(t),Uvo=i(J),$_=n(J,"LI",{});var s2e=s($_);ZY=n(s2e,"STRONG",{});var eBr=s(ZY);Jvo=r(eBr,"funnel"),eBr.forEach(t),Kvo=r(s2e," \u2014 "),Vk=n(s2e,"A",{href:!0});var oBr=s(Vk);Yvo=r(oBr,"FunnelForMultipleChoice"),oBr.forEach(t),Zvo=r(s2e," (Funnel Transformer model)"),s2e.forEach(t),ebo=i(J),I_=n(J,"LI",{});var l2e=s(I_);eZ=n(l2e,"STRONG",{});var rBr=s(eZ);obo=r(rBr,"ibert"),rBr.forEach(t),rbo=r(l2e," \u2014 "),Wk=n(l2e,"A",{href:!0});var tBr=s(Wk);tbo=r(tBr,"IBertForMultipleChoice"),tBr.forEach(t),abo=r(l2e," (I-BERT model)"),l2e.forEach(t),nbo=i(J),D_=n(J,"LI",{});var i2e=s(D_);oZ=n(i2e,"STRONG",{});var aBr=s(oZ);sbo=r(aBr,"longformer"),aBr.forEach(t),lbo=r(i2e," \u2014 "),Hk=n(i2e,"A",{href:!0});var nBr=s(Hk);ibo=r(nBr,"LongformerForMultipleChoice"),nBr.forEach(t),dbo=r(i2e," (Longformer model)"),i2e.forEach(t),cbo=i(J),N_=n(J,"LI",{});var d2e=s(N_);rZ=n(d2e,"STRONG",{});var sBr=s(rZ);fbo=r(sBr,"megatron-bert"),sBr.forEach(t),mbo=r(d2e," \u2014 "),Uk=n(d2e,"A",{href:!0});var lBr=s(Uk);hbo=r(lBr,"MegatronBertForMultipleChoice"),lBr.forEach(t),gbo=r(d2e," (MegatronBert model)"),d2e.forEach(t),ubo=i(J),j_=n(J,"LI",{});var c2e=s(j_);tZ=n(c2e,"STRONG",{});var iBr=s(tZ);pbo=r(iBr,"mobilebert"),iBr.forEach(t),_bo=r(c2e," \u2014 "),Jk=n(c2e,"A",{href:!0});var dBr=s(Jk);vbo=r(dBr,"MobileBertForMultipleChoice"),dBr.forEach(t),bbo=r(c2e," (MobileBERT model)"),c2e.forEach(t),Tbo=i(J),O_=n(J,"LI",{});var f2e=s(O_);aZ=n(f2e,"STRONG",{});var cBr=s(aZ);Fbo=r(cBr,"mpnet"),cBr.forEach(t),Ebo=r(f2e," \u2014 "),Kk=n(f2e,"A",{href:!0});var fBr=s(Kk);Cbo=r(fBr,"MPNetForMultipleChoice"),fBr.forEach(t),Mbo=r(f2e," (MPNet model)"),f2e.forEach(t),ybo=i(J),G_=n(J,"LI",{});var m2e=s(G_);nZ=n(m2e,"STRONG",{});var mBr=s(nZ);wbo=r(mBr,"qdqbert"),mBr.forEach(t),Abo=r(m2e," \u2014 "),Yk=n(m2e,"A",{href:!0});var hBr=s(Yk);Lbo=r(hBr,"QDQBertForMultipleChoice"),hBr.forEach(t),Bbo=r(m2e," (QDQBert model)"),m2e.forEach(t),xbo=i(J),q_=n(J,"LI",{});var h2e=s(q_);sZ=n(h2e,"STRONG",{});var gBr=s(sZ);kbo=r(gBr,"rembert"),gBr.forEach(t),Rbo=r(h2e," \u2014 "),Zk=n(h2e,"A",{href:!0});var uBr=s(Zk);Pbo=r(uBr,"RemBertForMultipleChoice"),uBr.forEach(t),Sbo=r(h2e," (RemBERT model)"),h2e.forEach(t),$bo=i(J),z_=n(J,"LI",{});var g2e=s(z_);lZ=n(g2e,"STRONG",{});var pBr=s(lZ);Ibo=r(pBr,"roberta"),pBr.forEach(t),Dbo=r(g2e," \u2014 "),eR=n(g2e,"A",{href:!0});var _Br=s(eR);Nbo=r(_Br,"RobertaForMultipleChoice"),_Br.forEach(t),jbo=r(g2e," (RoBERTa model)"),g2e.forEach(t),Obo=i(J),X_=n(J,"LI",{});var u2e=s(X_);iZ=n(u2e,"STRONG",{});var vBr=s(iZ);Gbo=r(vBr,"roformer"),vBr.forEach(t),qbo=r(u2e," \u2014 "),oR=n(u2e,"A",{href:!0});var bBr=s(oR);zbo=r(bBr,"RoFormerForMultipleChoice"),bBr.forEach(t),Xbo=r(u2e," (RoFormer model)"),u2e.forEach(t),Qbo=i(J),Q_=n(J,"LI",{});var p2e=s(Q_);dZ=n(p2e,"STRONG",{});var TBr=s(dZ);Vbo=r(TBr,"squeezebert"),TBr.forEach(t),Wbo=r(p2e," \u2014 "),rR=n(p2e,"A",{href:!0});var FBr=s(rR);Hbo=r(FBr,"SqueezeBertForMultipleChoice"),FBr.forEach(t),Ubo=r(p2e," (SqueezeBERT model)"),p2e.forEach(t),Jbo=i(J),V_=n(J,"LI",{});var _2e=s(V_);cZ=n(_2e,"STRONG",{});var EBr=s(cZ);Kbo=r(EBr,"xlm"),EBr.forEach(t),Ybo=r(_2e," \u2014 "),tR=n(_2e,"A",{href:!0});var CBr=s(tR);Zbo=r(CBr,"XLMForMultipleChoice"),CBr.forEach(t),e2o=r(_2e," (XLM model)"),_2e.forEach(t),o2o=i(J),W_=n(J,"LI",{});var v2e=s(W_);fZ=n(v2e,"STRONG",{});var MBr=s(fZ);r2o=r(MBr,"xlm-roberta"),MBr.forEach(t),t2o=r(v2e," \u2014 "),aR=n(v2e,"A",{href:!0});var yBr=s(aR);a2o=r(yBr,"XLMRobertaForMultipleChoice"),yBr.forEach(t),n2o=r(v2e," (XLM-RoBERTa model)"),v2e.forEach(t),s2o=i(J),H_=n(J,"LI",{});var b2e=s(H_);mZ=n(b2e,"STRONG",{});var wBr=s(mZ);l2o=r(wBr,"xlnet"),wBr.forEach(t),i2o=r(b2e," \u2014 "),nR=n(b2e,"A",{href:!0});var ABr=s(nR);d2o=r(ABr,"XLNetForMultipleChoice"),ABr.forEach(t),c2o=r(b2e," (XLNet model)"),b2e.forEach(t),J.forEach(t),f2o=i(yt),U_=n(yt,"P",{});var T2e=s(U_);m2o=r(T2e,"The model is set in evaluation mode by default using "),hZ=n(T2e,"CODE",{});var LBr=s(hZ);h2o=r(LBr,"model.eval()"),LBr.forEach(t),g2o=r(T2e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gZ=n(T2e,"CODE",{});var BBr=s(gZ);u2o=r(BBr,"model.train()"),BBr.forEach(t),T2e.forEach(t),p2o=i(yt),uZ=n(yt,"P",{});var xBr=s(uZ);_2o=r(xBr,"Examples:"),xBr.forEach(t),v2o=i(yt),m(aM.$$.fragment,yt),yt.forEach(t),ys.forEach(t),j5e=i(c),Ri=n(c,"H2",{class:!0});var Vwe=s(Ri);J_=n(Vwe,"A",{id:!0,class:!0,href:!0});var kBr=s(J_);pZ=n(kBr,"SPAN",{});var RBr=s(pZ);m(nM.$$.fragment,RBr),RBr.forEach(t),kBr.forEach(t),b2o=i(Vwe),_Z=n(Vwe,"SPAN",{});var PBr=s(_Z);T2o=r(PBr,"AutoModelForNextSentencePrediction"),PBr.forEach(t),Vwe.forEach(t),O5e=i(c),Vo=n(c,"DIV",{class:!0});var As=s(Vo);m(sM.$$.fragment,As),F2o=i(As),Pi=n(As,"P",{});var vj=s(Pi);E2o=r(vj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vZ=n(vj,"CODE",{});var SBr=s(vZ);C2o=r(SBr,"from_pretrained()"),SBr.forEach(t),M2o=r(vj,` class method or the
`),bZ=n(vj,"CODE",{});var $Br=s(bZ);y2o=r($Br,"from_config()"),$Br.forEach(t),w2o=r(vj," class method."),vj.forEach(t),A2o=i(As),lM=n(As,"P",{});var Wwe=s(lM);L2o=r(Wwe,"This class cannot be instantiated directly using "),TZ=n(Wwe,"CODE",{});var IBr=s(TZ);B2o=r(IBr,"__init__()"),IBr.forEach(t),x2o=r(Wwe," (throws an error)."),Wwe.forEach(t),k2o=i(As),Nr=n(As,"DIV",{class:!0});var Ls=s(Nr);m(iM.$$.fragment,Ls),R2o=i(Ls),FZ=n(Ls,"P",{});var DBr=s(FZ);P2o=r(DBr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),DBr.forEach(t),S2o=i(Ls),Si=n(Ls,"P",{});var bj=s(Si);$2o=r(bj,`Note:
Loading a model from its configuration file does `),EZ=n(bj,"STRONG",{});var NBr=s(EZ);I2o=r(NBr,"not"),NBr.forEach(t),D2o=r(bj,` load the model weights. It only affects the
model\u2019s configuration. Use `),CZ=n(bj,"CODE",{});var jBr=s(CZ);N2o=r(jBr,"from_pretrained()"),jBr.forEach(t),j2o=r(bj,` to load the model
weights.`),bj.forEach(t),O2o=i(Ls),MZ=n(Ls,"P",{});var OBr=s(MZ);G2o=r(OBr,"Examples:"),OBr.forEach(t),q2o=i(Ls),m(dM.$$.fragment,Ls),Ls.forEach(t),z2o=i(As),qe=n(As,"DIV",{class:!0});var wt=s(qe);m(cM.$$.fragment,wt),X2o=i(wt),yZ=n(wt,"P",{});var GBr=s(yZ);Q2o=r(GBr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),GBr.forEach(t),V2o=i(wt),Pa=n(wt,"P",{});var NE=s(Pa);W2o=r(NE,"The model class to instantiate is selected based on the "),wZ=n(NE,"CODE",{});var qBr=s(wZ);H2o=r(qBr,"model_type"),qBr.forEach(t),U2o=r(NE,` property of the config object (either
passed as an argument or loaded from `),AZ=n(NE,"CODE",{});var zBr=s(AZ);J2o=r(zBr,"pretrained_model_name_or_path"),zBr.forEach(t),K2o=r(NE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),LZ=n(NE,"CODE",{});var XBr=s(LZ);Y2o=r(XBr,"pretrained_model_name_or_path"),XBr.forEach(t),Z2o=r(NE,":"),NE.forEach(t),eTo=i(wt),jt=n(wt,"UL",{});var Bs=s(jt);K_=n(Bs,"LI",{});var F2e=s(K_);BZ=n(F2e,"STRONG",{});var QBr=s(BZ);oTo=r(QBr,"bert"),QBr.forEach(t),rTo=r(F2e," \u2014 "),sR=n(F2e,"A",{href:!0});var VBr=s(sR);tTo=r(VBr,"BertForNextSentencePrediction"),VBr.forEach(t),aTo=r(F2e," (BERT model)"),F2e.forEach(t),nTo=i(Bs),Y_=n(Bs,"LI",{});var E2e=s(Y_);xZ=n(E2e,"STRONG",{});var WBr=s(xZ);sTo=r(WBr,"fnet"),WBr.forEach(t),lTo=r(E2e," \u2014 "),lR=n(E2e,"A",{href:!0});var HBr=s(lR);iTo=r(HBr,"FNetForNextSentencePrediction"),HBr.forEach(t),dTo=r(E2e," (FNet model)"),E2e.forEach(t),cTo=i(Bs),Z_=n(Bs,"LI",{});var C2e=s(Z_);kZ=n(C2e,"STRONG",{});var UBr=s(kZ);fTo=r(UBr,"megatron-bert"),UBr.forEach(t),mTo=r(C2e," \u2014 "),iR=n(C2e,"A",{href:!0});var JBr=s(iR);hTo=r(JBr,"MegatronBertForNextSentencePrediction"),JBr.forEach(t),gTo=r(C2e," (MegatronBert model)"),C2e.forEach(t),uTo=i(Bs),e1=n(Bs,"LI",{});var M2e=s(e1);RZ=n(M2e,"STRONG",{});var KBr=s(RZ);pTo=r(KBr,"mobilebert"),KBr.forEach(t),_To=r(M2e," \u2014 "),dR=n(M2e,"A",{href:!0});var YBr=s(dR);vTo=r(YBr,"MobileBertForNextSentencePrediction"),YBr.forEach(t),bTo=r(M2e," (MobileBERT model)"),M2e.forEach(t),TTo=i(Bs),o1=n(Bs,"LI",{});var y2e=s(o1);PZ=n(y2e,"STRONG",{});var ZBr=s(PZ);FTo=r(ZBr,"qdqbert"),ZBr.forEach(t),ETo=r(y2e," \u2014 "),cR=n(y2e,"A",{href:!0});var e9r=s(cR);CTo=r(e9r,"QDQBertForNextSentencePrediction"),e9r.forEach(t),MTo=r(y2e," (QDQBert model)"),y2e.forEach(t),Bs.forEach(t),yTo=i(wt),r1=n(wt,"P",{});var w2e=s(r1);wTo=r(w2e,"The model is set in evaluation mode by default using "),SZ=n(w2e,"CODE",{});var o9r=s(SZ);ATo=r(o9r,"model.eval()"),o9r.forEach(t),LTo=r(w2e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$Z=n(w2e,"CODE",{});var r9r=s($Z);BTo=r(r9r,"model.train()"),r9r.forEach(t),w2e.forEach(t),xTo=i(wt),IZ=n(wt,"P",{});var t9r=s(IZ);kTo=r(t9r,"Examples:"),t9r.forEach(t),RTo=i(wt),m(fM.$$.fragment,wt),wt.forEach(t),As.forEach(t),G5e=i(c),$i=n(c,"H2",{class:!0});var Hwe=s($i);t1=n(Hwe,"A",{id:!0,class:!0,href:!0});var a9r=s(t1);DZ=n(a9r,"SPAN",{});var n9r=s(DZ);m(mM.$$.fragment,n9r),n9r.forEach(t),a9r.forEach(t),PTo=i(Hwe),NZ=n(Hwe,"SPAN",{});var s9r=s(NZ);STo=r(s9r,"AutoModelForTokenClassification"),s9r.forEach(t),Hwe.forEach(t),q5e=i(c),Wo=n(c,"DIV",{class:!0});var xs=s(Wo);m(hM.$$.fragment,xs),$To=i(xs),Ii=n(xs,"P",{});var Tj=s(Ii);ITo=r(Tj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),jZ=n(Tj,"CODE",{});var l9r=s(jZ);DTo=r(l9r,"from_pretrained()"),l9r.forEach(t),NTo=r(Tj,` class method or the
`),OZ=n(Tj,"CODE",{});var i9r=s(OZ);jTo=r(i9r,"from_config()"),i9r.forEach(t),OTo=r(Tj," class method."),Tj.forEach(t),GTo=i(xs),gM=n(xs,"P",{});var Uwe=s(gM);qTo=r(Uwe,"This class cannot be instantiated directly using "),GZ=n(Uwe,"CODE",{});var d9r=s(GZ);zTo=r(d9r,"__init__()"),d9r.forEach(t),XTo=r(Uwe," (throws an error)."),Uwe.forEach(t),QTo=i(xs),jr=n(xs,"DIV",{class:!0});var ks=s(jr);m(uM.$$.fragment,ks),VTo=i(ks),qZ=n(ks,"P",{});var c9r=s(qZ);WTo=r(c9r,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),c9r.forEach(t),HTo=i(ks),Di=n(ks,"P",{});var Fj=s(Di);UTo=r(Fj,`Note:
Loading a model from its configuration file does `),zZ=n(Fj,"STRONG",{});var f9r=s(zZ);JTo=r(f9r,"not"),f9r.forEach(t),KTo=r(Fj,` load the model weights. It only affects the
model\u2019s configuration. Use `),XZ=n(Fj,"CODE",{});var m9r=s(XZ);YTo=r(m9r,"from_pretrained()"),m9r.forEach(t),ZTo=r(Fj,` to load the model
weights.`),Fj.forEach(t),eFo=i(ks),QZ=n(ks,"P",{});var h9r=s(QZ);oFo=r(h9r,"Examples:"),h9r.forEach(t),rFo=i(ks),m(pM.$$.fragment,ks),ks.forEach(t),tFo=i(xs),ze=n(xs,"DIV",{class:!0});var At=s(ze);m(_M.$$.fragment,At),aFo=i(At),VZ=n(At,"P",{});var g9r=s(VZ);nFo=r(g9r,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),g9r.forEach(t),sFo=i(At),Sa=n(At,"P",{});var jE=s(Sa);lFo=r(jE,"The model class to instantiate is selected based on the "),WZ=n(jE,"CODE",{});var u9r=s(WZ);iFo=r(u9r,"model_type"),u9r.forEach(t),dFo=r(jE,` property of the config object (either
passed as an argument or loaded from `),HZ=n(jE,"CODE",{});var p9r=s(HZ);cFo=r(p9r,"pretrained_model_name_or_path"),p9r.forEach(t),fFo=r(jE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),UZ=n(jE,"CODE",{});var _9r=s(UZ);mFo=r(_9r,"pretrained_model_name_or_path"),_9r.forEach(t),hFo=r(jE,":"),jE.forEach(t),gFo=i(At),X=n(At,"UL",{});var V=s(X);a1=n(V,"LI",{});var A2e=s(a1);JZ=n(A2e,"STRONG",{});var v9r=s(JZ);uFo=r(v9r,"albert"),v9r.forEach(t),pFo=r(A2e," \u2014 "),fR=n(A2e,"A",{href:!0});var b9r=s(fR);_Fo=r(b9r,"AlbertForTokenClassification"),b9r.forEach(t),vFo=r(A2e," (ALBERT model)"),A2e.forEach(t),bFo=i(V),n1=n(V,"LI",{});var L2e=s(n1);KZ=n(L2e,"STRONG",{});var T9r=s(KZ);TFo=r(T9r,"bert"),T9r.forEach(t),FFo=r(L2e," \u2014 "),mR=n(L2e,"A",{href:!0});var F9r=s(mR);EFo=r(F9r,"BertForTokenClassification"),F9r.forEach(t),CFo=r(L2e," (BERT model)"),L2e.forEach(t),MFo=i(V),s1=n(V,"LI",{});var B2e=s(s1);YZ=n(B2e,"STRONG",{});var E9r=s(YZ);yFo=r(E9r,"big_bird"),E9r.forEach(t),wFo=r(B2e," \u2014 "),hR=n(B2e,"A",{href:!0});var C9r=s(hR);AFo=r(C9r,"BigBirdForTokenClassification"),C9r.forEach(t),LFo=r(B2e," (BigBird model)"),B2e.forEach(t),BFo=i(V),l1=n(V,"LI",{});var x2e=s(l1);ZZ=n(x2e,"STRONG",{});var M9r=s(ZZ);xFo=r(M9r,"camembert"),M9r.forEach(t),kFo=r(x2e," \u2014 "),gR=n(x2e,"A",{href:!0});var y9r=s(gR);RFo=r(y9r,"CamembertForTokenClassification"),y9r.forEach(t),PFo=r(x2e," (CamemBERT model)"),x2e.forEach(t),SFo=i(V),i1=n(V,"LI",{});var k2e=s(i1);eee=n(k2e,"STRONG",{});var w9r=s(eee);$Fo=r(w9r,"canine"),w9r.forEach(t),IFo=r(k2e," \u2014 "),uR=n(k2e,"A",{href:!0});var A9r=s(uR);DFo=r(A9r,"CanineForTokenClassification"),A9r.forEach(t),NFo=r(k2e," (Canine model)"),k2e.forEach(t),jFo=i(V),d1=n(V,"LI",{});var R2e=s(d1);oee=n(R2e,"STRONG",{});var L9r=s(oee);OFo=r(L9r,"convbert"),L9r.forEach(t),GFo=r(R2e," \u2014 "),pR=n(R2e,"A",{href:!0});var B9r=s(pR);qFo=r(B9r,"ConvBertForTokenClassification"),B9r.forEach(t),zFo=r(R2e," (ConvBERT model)"),R2e.forEach(t),XFo=i(V),c1=n(V,"LI",{});var P2e=s(c1);ree=n(P2e,"STRONG",{});var x9r=s(ree);QFo=r(x9r,"deberta"),x9r.forEach(t),VFo=r(P2e," \u2014 "),_R=n(P2e,"A",{href:!0});var k9r=s(_R);WFo=r(k9r,"DebertaForTokenClassification"),k9r.forEach(t),HFo=r(P2e," (DeBERTa model)"),P2e.forEach(t),UFo=i(V),f1=n(V,"LI",{});var S2e=s(f1);tee=n(S2e,"STRONG",{});var R9r=s(tee);JFo=r(R9r,"deberta-v2"),R9r.forEach(t),KFo=r(S2e," \u2014 "),vR=n(S2e,"A",{href:!0});var P9r=s(vR);YFo=r(P9r,"DebertaV2ForTokenClassification"),P9r.forEach(t),ZFo=r(S2e," (DeBERTa-v2 model)"),S2e.forEach(t),eEo=i(V),m1=n(V,"LI",{});var $2e=s(m1);aee=n($2e,"STRONG",{});var S9r=s(aee);oEo=r(S9r,"distilbert"),S9r.forEach(t),rEo=r($2e," \u2014 "),bR=n($2e,"A",{href:!0});var $9r=s(bR);tEo=r($9r,"DistilBertForTokenClassification"),$9r.forEach(t),aEo=r($2e," (DistilBERT model)"),$2e.forEach(t),nEo=i(V),h1=n(V,"LI",{});var I2e=s(h1);nee=n(I2e,"STRONG",{});var I9r=s(nee);sEo=r(I9r,"electra"),I9r.forEach(t),lEo=r(I2e," \u2014 "),TR=n(I2e,"A",{href:!0});var D9r=s(TR);iEo=r(D9r,"ElectraForTokenClassification"),D9r.forEach(t),dEo=r(I2e," (ELECTRA model)"),I2e.forEach(t),cEo=i(V),g1=n(V,"LI",{});var D2e=s(g1);see=n(D2e,"STRONG",{});var N9r=s(see);fEo=r(N9r,"flaubert"),N9r.forEach(t),mEo=r(D2e," \u2014 "),FR=n(D2e,"A",{href:!0});var j9r=s(FR);hEo=r(j9r,"FlaubertForTokenClassification"),j9r.forEach(t),gEo=r(D2e," (FlauBERT model)"),D2e.forEach(t),uEo=i(V),u1=n(V,"LI",{});var N2e=s(u1);lee=n(N2e,"STRONG",{});var O9r=s(lee);pEo=r(O9r,"fnet"),O9r.forEach(t),_Eo=r(N2e," \u2014 "),ER=n(N2e,"A",{href:!0});var G9r=s(ER);vEo=r(G9r,"FNetForTokenClassification"),G9r.forEach(t),bEo=r(N2e," (FNet model)"),N2e.forEach(t),TEo=i(V),p1=n(V,"LI",{});var j2e=s(p1);iee=n(j2e,"STRONG",{});var q9r=s(iee);FEo=r(q9r,"funnel"),q9r.forEach(t),EEo=r(j2e," \u2014 "),CR=n(j2e,"A",{href:!0});var z9r=s(CR);CEo=r(z9r,"FunnelForTokenClassification"),z9r.forEach(t),MEo=r(j2e," (Funnel Transformer model)"),j2e.forEach(t),yEo=i(V),_1=n(V,"LI",{});var O2e=s(_1);dee=n(O2e,"STRONG",{});var X9r=s(dee);wEo=r(X9r,"gpt2"),X9r.forEach(t),AEo=r(O2e," \u2014 "),MR=n(O2e,"A",{href:!0});var Q9r=s(MR);LEo=r(Q9r,"GPT2ForTokenClassification"),Q9r.forEach(t),BEo=r(O2e," (OpenAI GPT-2 model)"),O2e.forEach(t),xEo=i(V),v1=n(V,"LI",{});var G2e=s(v1);cee=n(G2e,"STRONG",{});var V9r=s(cee);kEo=r(V9r,"ibert"),V9r.forEach(t),REo=r(G2e," \u2014 "),yR=n(G2e,"A",{href:!0});var W9r=s(yR);PEo=r(W9r,"IBertForTokenClassification"),W9r.forEach(t),SEo=r(G2e," (I-BERT model)"),G2e.forEach(t),$Eo=i(V),b1=n(V,"LI",{});var q2e=s(b1);fee=n(q2e,"STRONG",{});var H9r=s(fee);IEo=r(H9r,"layoutlm"),H9r.forEach(t),DEo=r(q2e," \u2014 "),wR=n(q2e,"A",{href:!0});var U9r=s(wR);NEo=r(U9r,"LayoutLMForTokenClassification"),U9r.forEach(t),jEo=r(q2e," (LayoutLM model)"),q2e.forEach(t),OEo=i(V),T1=n(V,"LI",{});var z2e=s(T1);mee=n(z2e,"STRONG",{});var J9r=s(mee);GEo=r(J9r,"layoutlmv2"),J9r.forEach(t),qEo=r(z2e," \u2014 "),AR=n(z2e,"A",{href:!0});var K9r=s(AR);zEo=r(K9r,"LayoutLMv2ForTokenClassification"),K9r.forEach(t),XEo=r(z2e," (LayoutLMv2 model)"),z2e.forEach(t),QEo=i(V),F1=n(V,"LI",{});var X2e=s(F1);hee=n(X2e,"STRONG",{});var Y9r=s(hee);VEo=r(Y9r,"longformer"),Y9r.forEach(t),WEo=r(X2e," \u2014 "),LR=n(X2e,"A",{href:!0});var Z9r=s(LR);HEo=r(Z9r,"LongformerForTokenClassification"),Z9r.forEach(t),UEo=r(X2e," (Longformer model)"),X2e.forEach(t),JEo=i(V),E1=n(V,"LI",{});var Q2e=s(E1);gee=n(Q2e,"STRONG",{});var exr=s(gee);KEo=r(exr,"megatron-bert"),exr.forEach(t),YEo=r(Q2e," \u2014 "),BR=n(Q2e,"A",{href:!0});var oxr=s(BR);ZEo=r(oxr,"MegatronBertForTokenClassification"),oxr.forEach(t),eCo=r(Q2e," (MegatronBert model)"),Q2e.forEach(t),oCo=i(V),C1=n(V,"LI",{});var V2e=s(C1);uee=n(V2e,"STRONG",{});var rxr=s(uee);rCo=r(rxr,"mobilebert"),rxr.forEach(t),tCo=r(V2e," \u2014 "),xR=n(V2e,"A",{href:!0});var txr=s(xR);aCo=r(txr,"MobileBertForTokenClassification"),txr.forEach(t),nCo=r(V2e," (MobileBERT model)"),V2e.forEach(t),sCo=i(V),M1=n(V,"LI",{});var W2e=s(M1);pee=n(W2e,"STRONG",{});var axr=s(pee);lCo=r(axr,"mpnet"),axr.forEach(t),iCo=r(W2e," \u2014 "),kR=n(W2e,"A",{href:!0});var nxr=s(kR);dCo=r(nxr,"MPNetForTokenClassification"),nxr.forEach(t),cCo=r(W2e," (MPNet model)"),W2e.forEach(t),fCo=i(V),y1=n(V,"LI",{});var H2e=s(y1);_ee=n(H2e,"STRONG",{});var sxr=s(_ee);mCo=r(sxr,"qdqbert"),sxr.forEach(t),hCo=r(H2e," \u2014 "),RR=n(H2e,"A",{href:!0});var lxr=s(RR);gCo=r(lxr,"QDQBertForTokenClassification"),lxr.forEach(t),uCo=r(H2e," (QDQBert model)"),H2e.forEach(t),pCo=i(V),w1=n(V,"LI",{});var U2e=s(w1);vee=n(U2e,"STRONG",{});var ixr=s(vee);_Co=r(ixr,"rembert"),ixr.forEach(t),vCo=r(U2e," \u2014 "),PR=n(U2e,"A",{href:!0});var dxr=s(PR);bCo=r(dxr,"RemBertForTokenClassification"),dxr.forEach(t),TCo=r(U2e," (RemBERT model)"),U2e.forEach(t),FCo=i(V),A1=n(V,"LI",{});var J2e=s(A1);bee=n(J2e,"STRONG",{});var cxr=s(bee);ECo=r(cxr,"roberta"),cxr.forEach(t),CCo=r(J2e," \u2014 "),SR=n(J2e,"A",{href:!0});var fxr=s(SR);MCo=r(fxr,"RobertaForTokenClassification"),fxr.forEach(t),yCo=r(J2e," (RoBERTa model)"),J2e.forEach(t),wCo=i(V),L1=n(V,"LI",{});var K2e=s(L1);Tee=n(K2e,"STRONG",{});var mxr=s(Tee);ACo=r(mxr,"roformer"),mxr.forEach(t),LCo=r(K2e," \u2014 "),$R=n(K2e,"A",{href:!0});var hxr=s($R);BCo=r(hxr,"RoFormerForTokenClassification"),hxr.forEach(t),xCo=r(K2e," (RoFormer model)"),K2e.forEach(t),kCo=i(V),B1=n(V,"LI",{});var Y2e=s(B1);Fee=n(Y2e,"STRONG",{});var gxr=s(Fee);RCo=r(gxr,"squeezebert"),gxr.forEach(t),PCo=r(Y2e," \u2014 "),IR=n(Y2e,"A",{href:!0});var uxr=s(IR);SCo=r(uxr,"SqueezeBertForTokenClassification"),uxr.forEach(t),$Co=r(Y2e," (SqueezeBERT model)"),Y2e.forEach(t),ICo=i(V),x1=n(V,"LI",{});var Z2e=s(x1);Eee=n(Z2e,"STRONG",{});var pxr=s(Eee);DCo=r(pxr,"xlm"),pxr.forEach(t),NCo=r(Z2e," \u2014 "),DR=n(Z2e,"A",{href:!0});var _xr=s(DR);jCo=r(_xr,"XLMForTokenClassification"),_xr.forEach(t),OCo=r(Z2e," (XLM model)"),Z2e.forEach(t),GCo=i(V),k1=n(V,"LI",{});var eTe=s(k1);Cee=n(eTe,"STRONG",{});var vxr=s(Cee);qCo=r(vxr,"xlm-roberta"),vxr.forEach(t),zCo=r(eTe," \u2014 "),NR=n(eTe,"A",{href:!0});var bxr=s(NR);XCo=r(bxr,"XLMRobertaForTokenClassification"),bxr.forEach(t),QCo=r(eTe," (XLM-RoBERTa model)"),eTe.forEach(t),VCo=i(V),R1=n(V,"LI",{});var oTe=s(R1);Mee=n(oTe,"STRONG",{});var Txr=s(Mee);WCo=r(Txr,"xlnet"),Txr.forEach(t),HCo=r(oTe," \u2014 "),jR=n(oTe,"A",{href:!0});var Fxr=s(jR);UCo=r(Fxr,"XLNetForTokenClassification"),Fxr.forEach(t),JCo=r(oTe," (XLNet model)"),oTe.forEach(t),V.forEach(t),KCo=i(At),P1=n(At,"P",{});var rTe=s(P1);YCo=r(rTe,"The model is set in evaluation mode by default using "),yee=n(rTe,"CODE",{});var Exr=s(yee);ZCo=r(Exr,"model.eval()"),Exr.forEach(t),e3o=r(rTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wee=n(rTe,"CODE",{});var Cxr=s(wee);o3o=r(Cxr,"model.train()"),Cxr.forEach(t),rTe.forEach(t),r3o=i(At),Aee=n(At,"P",{});var Mxr=s(Aee);t3o=r(Mxr,"Examples:"),Mxr.forEach(t),a3o=i(At),m(vM.$$.fragment,At),At.forEach(t),xs.forEach(t),z5e=i(c),Ni=n(c,"H2",{class:!0});var Jwe=s(Ni);S1=n(Jwe,"A",{id:!0,class:!0,href:!0});var yxr=s(S1);Lee=n(yxr,"SPAN",{});var wxr=s(Lee);m(bM.$$.fragment,wxr),wxr.forEach(t),yxr.forEach(t),n3o=i(Jwe),Bee=n(Jwe,"SPAN",{});var Axr=s(Bee);s3o=r(Axr,"AutoModelForQuestionAnswering"),Axr.forEach(t),Jwe.forEach(t),X5e=i(c),Ho=n(c,"DIV",{class:!0});var Rs=s(Ho);m(TM.$$.fragment,Rs),l3o=i(Rs),ji=n(Rs,"P",{});var Ej=s(ji);i3o=r(Ej,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),xee=n(Ej,"CODE",{});var Lxr=s(xee);d3o=r(Lxr,"from_pretrained()"),Lxr.forEach(t),c3o=r(Ej,` class method or the
`),kee=n(Ej,"CODE",{});var Bxr=s(kee);f3o=r(Bxr,"from_config()"),Bxr.forEach(t),m3o=r(Ej," class method."),Ej.forEach(t),h3o=i(Rs),FM=n(Rs,"P",{});var Kwe=s(FM);g3o=r(Kwe,"This class cannot be instantiated directly using "),Ree=n(Kwe,"CODE",{});var xxr=s(Ree);u3o=r(xxr,"__init__()"),xxr.forEach(t),p3o=r(Kwe," (throws an error)."),Kwe.forEach(t),_3o=i(Rs),Or=n(Rs,"DIV",{class:!0});var Ps=s(Or);m(EM.$$.fragment,Ps),v3o=i(Ps),Pee=n(Ps,"P",{});var kxr=s(Pee);b3o=r(kxr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),kxr.forEach(t),T3o=i(Ps),Oi=n(Ps,"P",{});var Cj=s(Oi);F3o=r(Cj,`Note:
Loading a model from its configuration file does `),See=n(Cj,"STRONG",{});var Rxr=s(See);E3o=r(Rxr,"not"),Rxr.forEach(t),C3o=r(Cj,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ee=n(Cj,"CODE",{});var Pxr=s($ee);M3o=r(Pxr,"from_pretrained()"),Pxr.forEach(t),y3o=r(Cj,` to load the model
weights.`),Cj.forEach(t),w3o=i(Ps),Iee=n(Ps,"P",{});var Sxr=s(Iee);A3o=r(Sxr,"Examples:"),Sxr.forEach(t),L3o=i(Ps),m(CM.$$.fragment,Ps),Ps.forEach(t),B3o=i(Rs),Xe=n(Rs,"DIV",{class:!0});var Lt=s(Xe);m(MM.$$.fragment,Lt),x3o=i(Lt),Dee=n(Lt,"P",{});var $xr=s(Dee);k3o=r($xr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),$xr.forEach(t),R3o=i(Lt),$a=n(Lt,"P",{});var OE=s($a);P3o=r(OE,"The model class to instantiate is selected based on the "),Nee=n(OE,"CODE",{});var Ixr=s(Nee);S3o=r(Ixr,"model_type"),Ixr.forEach(t),$3o=r(OE,` property of the config object (either
passed as an argument or loaded from `),jee=n(OE,"CODE",{});var Dxr=s(jee);I3o=r(Dxr,"pretrained_model_name_or_path"),Dxr.forEach(t),D3o=r(OE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Oee=n(OE,"CODE",{});var Nxr=s(Oee);N3o=r(Nxr,"pretrained_model_name_or_path"),Nxr.forEach(t),j3o=r(OE,":"),OE.forEach(t),O3o=i(Lt),S=n(Lt,"UL",{});var I=s(S);$1=n(I,"LI",{});var tTe=s($1);Gee=n(tTe,"STRONG",{});var jxr=s(Gee);G3o=r(jxr,"albert"),jxr.forEach(t),q3o=r(tTe," \u2014 "),OR=n(tTe,"A",{href:!0});var Oxr=s(OR);z3o=r(Oxr,"AlbertForQuestionAnswering"),Oxr.forEach(t),X3o=r(tTe," (ALBERT model)"),tTe.forEach(t),Q3o=i(I),I1=n(I,"LI",{});var aTe=s(I1);qee=n(aTe,"STRONG",{});var Gxr=s(qee);V3o=r(Gxr,"bart"),Gxr.forEach(t),W3o=r(aTe," \u2014 "),GR=n(aTe,"A",{href:!0});var qxr=s(GR);H3o=r(qxr,"BartForQuestionAnswering"),qxr.forEach(t),U3o=r(aTe," (BART model)"),aTe.forEach(t),J3o=i(I),D1=n(I,"LI",{});var nTe=s(D1);zee=n(nTe,"STRONG",{});var zxr=s(zee);K3o=r(zxr,"bert"),zxr.forEach(t),Y3o=r(nTe," \u2014 "),qR=n(nTe,"A",{href:!0});var Xxr=s(qR);Z3o=r(Xxr,"BertForQuestionAnswering"),Xxr.forEach(t),eMo=r(nTe," (BERT model)"),nTe.forEach(t),oMo=i(I),N1=n(I,"LI",{});var sTe=s(N1);Xee=n(sTe,"STRONG",{});var Qxr=s(Xee);rMo=r(Qxr,"big_bird"),Qxr.forEach(t),tMo=r(sTe," \u2014 "),zR=n(sTe,"A",{href:!0});var Vxr=s(zR);aMo=r(Vxr,"BigBirdForQuestionAnswering"),Vxr.forEach(t),nMo=r(sTe," (BigBird model)"),sTe.forEach(t),sMo=i(I),j1=n(I,"LI",{});var lTe=s(j1);Qee=n(lTe,"STRONG",{});var Wxr=s(Qee);lMo=r(Wxr,"bigbird_pegasus"),Wxr.forEach(t),iMo=r(lTe," \u2014 "),XR=n(lTe,"A",{href:!0});var Hxr=s(XR);dMo=r(Hxr,"BigBirdPegasusForQuestionAnswering"),Hxr.forEach(t),cMo=r(lTe," (BigBirdPegasus model)"),lTe.forEach(t),fMo=i(I),O1=n(I,"LI",{});var iTe=s(O1);Vee=n(iTe,"STRONG",{});var Uxr=s(Vee);mMo=r(Uxr,"camembert"),Uxr.forEach(t),hMo=r(iTe," \u2014 "),QR=n(iTe,"A",{href:!0});var Jxr=s(QR);gMo=r(Jxr,"CamembertForQuestionAnswering"),Jxr.forEach(t),uMo=r(iTe," (CamemBERT model)"),iTe.forEach(t),pMo=i(I),G1=n(I,"LI",{});var dTe=s(G1);Wee=n(dTe,"STRONG",{});var Kxr=s(Wee);_Mo=r(Kxr,"canine"),Kxr.forEach(t),vMo=r(dTe," \u2014 "),VR=n(dTe,"A",{href:!0});var Yxr=s(VR);bMo=r(Yxr,"CanineForQuestionAnswering"),Yxr.forEach(t),TMo=r(dTe," (Canine model)"),dTe.forEach(t),FMo=i(I),q1=n(I,"LI",{});var cTe=s(q1);Hee=n(cTe,"STRONG",{});var Zxr=s(Hee);EMo=r(Zxr,"convbert"),Zxr.forEach(t),CMo=r(cTe," \u2014 "),WR=n(cTe,"A",{href:!0});var ekr=s(WR);MMo=r(ekr,"ConvBertForQuestionAnswering"),ekr.forEach(t),yMo=r(cTe," (ConvBERT model)"),cTe.forEach(t),wMo=i(I),z1=n(I,"LI",{});var fTe=s(z1);Uee=n(fTe,"STRONG",{});var okr=s(Uee);AMo=r(okr,"deberta"),okr.forEach(t),LMo=r(fTe," \u2014 "),HR=n(fTe,"A",{href:!0});var rkr=s(HR);BMo=r(rkr,"DebertaForQuestionAnswering"),rkr.forEach(t),xMo=r(fTe," (DeBERTa model)"),fTe.forEach(t),kMo=i(I),X1=n(I,"LI",{});var mTe=s(X1);Jee=n(mTe,"STRONG",{});var tkr=s(Jee);RMo=r(tkr,"deberta-v2"),tkr.forEach(t),PMo=r(mTe," \u2014 "),UR=n(mTe,"A",{href:!0});var akr=s(UR);SMo=r(akr,"DebertaV2ForQuestionAnswering"),akr.forEach(t),$Mo=r(mTe," (DeBERTa-v2 model)"),mTe.forEach(t),IMo=i(I),Q1=n(I,"LI",{});var hTe=s(Q1);Kee=n(hTe,"STRONG",{});var nkr=s(Kee);DMo=r(nkr,"distilbert"),nkr.forEach(t),NMo=r(hTe," \u2014 "),JR=n(hTe,"A",{href:!0});var skr=s(JR);jMo=r(skr,"DistilBertForQuestionAnswering"),skr.forEach(t),OMo=r(hTe," (DistilBERT model)"),hTe.forEach(t),GMo=i(I),V1=n(I,"LI",{});var gTe=s(V1);Yee=n(gTe,"STRONG",{});var lkr=s(Yee);qMo=r(lkr,"electra"),lkr.forEach(t),zMo=r(gTe," \u2014 "),KR=n(gTe,"A",{href:!0});var ikr=s(KR);XMo=r(ikr,"ElectraForQuestionAnswering"),ikr.forEach(t),QMo=r(gTe," (ELECTRA model)"),gTe.forEach(t),VMo=i(I),W1=n(I,"LI",{});var uTe=s(W1);Zee=n(uTe,"STRONG",{});var dkr=s(Zee);WMo=r(dkr,"flaubert"),dkr.forEach(t),HMo=r(uTe," \u2014 "),YR=n(uTe,"A",{href:!0});var ckr=s(YR);UMo=r(ckr,"FlaubertForQuestionAnsweringSimple"),ckr.forEach(t),JMo=r(uTe," (FlauBERT model)"),uTe.forEach(t),KMo=i(I),H1=n(I,"LI",{});var pTe=s(H1);eoe=n(pTe,"STRONG",{});var fkr=s(eoe);YMo=r(fkr,"fnet"),fkr.forEach(t),ZMo=r(pTe," \u2014 "),ZR=n(pTe,"A",{href:!0});var mkr=s(ZR);e5o=r(mkr,"FNetForQuestionAnswering"),mkr.forEach(t),o5o=r(pTe," (FNet model)"),pTe.forEach(t),r5o=i(I),U1=n(I,"LI",{});var _Te=s(U1);ooe=n(_Te,"STRONG",{});var hkr=s(ooe);t5o=r(hkr,"funnel"),hkr.forEach(t),a5o=r(_Te," \u2014 "),eP=n(_Te,"A",{href:!0});var gkr=s(eP);n5o=r(gkr,"FunnelForQuestionAnswering"),gkr.forEach(t),s5o=r(_Te," (Funnel Transformer model)"),_Te.forEach(t),l5o=i(I),J1=n(I,"LI",{});var vTe=s(J1);roe=n(vTe,"STRONG",{});var ukr=s(roe);i5o=r(ukr,"gptj"),ukr.forEach(t),d5o=r(vTe," \u2014 "),oP=n(vTe,"A",{href:!0});var pkr=s(oP);c5o=r(pkr,"GPTJForQuestionAnswering"),pkr.forEach(t),f5o=r(vTe," (GPT-J model)"),vTe.forEach(t),m5o=i(I),K1=n(I,"LI",{});var bTe=s(K1);toe=n(bTe,"STRONG",{});var _kr=s(toe);h5o=r(_kr,"ibert"),_kr.forEach(t),g5o=r(bTe," \u2014 "),rP=n(bTe,"A",{href:!0});var vkr=s(rP);u5o=r(vkr,"IBertForQuestionAnswering"),vkr.forEach(t),p5o=r(bTe," (I-BERT model)"),bTe.forEach(t),_5o=i(I),Y1=n(I,"LI",{});var TTe=s(Y1);aoe=n(TTe,"STRONG",{});var bkr=s(aoe);v5o=r(bkr,"layoutlmv2"),bkr.forEach(t),b5o=r(TTe," \u2014 "),tP=n(TTe,"A",{href:!0});var Tkr=s(tP);T5o=r(Tkr,"LayoutLMv2ForQuestionAnswering"),Tkr.forEach(t),F5o=r(TTe," (LayoutLMv2 model)"),TTe.forEach(t),E5o=i(I),Z1=n(I,"LI",{});var FTe=s(Z1);noe=n(FTe,"STRONG",{});var Fkr=s(noe);C5o=r(Fkr,"led"),Fkr.forEach(t),M5o=r(FTe," \u2014 "),aP=n(FTe,"A",{href:!0});var Ekr=s(aP);y5o=r(Ekr,"LEDForQuestionAnswering"),Ekr.forEach(t),w5o=r(FTe," (LED model)"),FTe.forEach(t),A5o=i(I),e4=n(I,"LI",{});var ETe=s(e4);soe=n(ETe,"STRONG",{});var Ckr=s(soe);L5o=r(Ckr,"longformer"),Ckr.forEach(t),B5o=r(ETe," \u2014 "),nP=n(ETe,"A",{href:!0});var Mkr=s(nP);x5o=r(Mkr,"LongformerForQuestionAnswering"),Mkr.forEach(t),k5o=r(ETe," (Longformer model)"),ETe.forEach(t),R5o=i(I),o4=n(I,"LI",{});var CTe=s(o4);loe=n(CTe,"STRONG",{});var ykr=s(loe);P5o=r(ykr,"lxmert"),ykr.forEach(t),S5o=r(CTe," \u2014 "),sP=n(CTe,"A",{href:!0});var wkr=s(sP);$5o=r(wkr,"LxmertForQuestionAnswering"),wkr.forEach(t),I5o=r(CTe," (LXMERT model)"),CTe.forEach(t),D5o=i(I),r4=n(I,"LI",{});var MTe=s(r4);ioe=n(MTe,"STRONG",{});var Akr=s(ioe);N5o=r(Akr,"mbart"),Akr.forEach(t),j5o=r(MTe," \u2014 "),lP=n(MTe,"A",{href:!0});var Lkr=s(lP);O5o=r(Lkr,"MBartForQuestionAnswering"),Lkr.forEach(t),G5o=r(MTe," (mBART model)"),MTe.forEach(t),q5o=i(I),t4=n(I,"LI",{});var yTe=s(t4);doe=n(yTe,"STRONG",{});var Bkr=s(doe);z5o=r(Bkr,"megatron-bert"),Bkr.forEach(t),X5o=r(yTe," \u2014 "),iP=n(yTe,"A",{href:!0});var xkr=s(iP);Q5o=r(xkr,"MegatronBertForQuestionAnswering"),xkr.forEach(t),V5o=r(yTe," (MegatronBert model)"),yTe.forEach(t),W5o=i(I),a4=n(I,"LI",{});var wTe=s(a4);coe=n(wTe,"STRONG",{});var kkr=s(coe);H5o=r(kkr,"mobilebert"),kkr.forEach(t),U5o=r(wTe," \u2014 "),dP=n(wTe,"A",{href:!0});var Rkr=s(dP);J5o=r(Rkr,"MobileBertForQuestionAnswering"),Rkr.forEach(t),K5o=r(wTe," (MobileBERT model)"),wTe.forEach(t),Y5o=i(I),n4=n(I,"LI",{});var ATe=s(n4);foe=n(ATe,"STRONG",{});var Pkr=s(foe);Z5o=r(Pkr,"mpnet"),Pkr.forEach(t),eyo=r(ATe," \u2014 "),cP=n(ATe,"A",{href:!0});var Skr=s(cP);oyo=r(Skr,"MPNetForQuestionAnswering"),Skr.forEach(t),ryo=r(ATe," (MPNet model)"),ATe.forEach(t),tyo=i(I),s4=n(I,"LI",{});var LTe=s(s4);moe=n(LTe,"STRONG",{});var $kr=s(moe);ayo=r($kr,"qdqbert"),$kr.forEach(t),nyo=r(LTe," \u2014 "),fP=n(LTe,"A",{href:!0});var Ikr=s(fP);syo=r(Ikr,"QDQBertForQuestionAnswering"),Ikr.forEach(t),lyo=r(LTe," (QDQBert model)"),LTe.forEach(t),iyo=i(I),l4=n(I,"LI",{});var BTe=s(l4);hoe=n(BTe,"STRONG",{});var Dkr=s(hoe);dyo=r(Dkr,"reformer"),Dkr.forEach(t),cyo=r(BTe," \u2014 "),mP=n(BTe,"A",{href:!0});var Nkr=s(mP);fyo=r(Nkr,"ReformerForQuestionAnswering"),Nkr.forEach(t),myo=r(BTe," (Reformer model)"),BTe.forEach(t),hyo=i(I),i4=n(I,"LI",{});var xTe=s(i4);goe=n(xTe,"STRONG",{});var jkr=s(goe);gyo=r(jkr,"rembert"),jkr.forEach(t),uyo=r(xTe," \u2014 "),hP=n(xTe,"A",{href:!0});var Okr=s(hP);pyo=r(Okr,"RemBertForQuestionAnswering"),Okr.forEach(t),_yo=r(xTe," (RemBERT model)"),xTe.forEach(t),vyo=i(I),d4=n(I,"LI",{});var kTe=s(d4);uoe=n(kTe,"STRONG",{});var Gkr=s(uoe);byo=r(Gkr,"roberta"),Gkr.forEach(t),Tyo=r(kTe," \u2014 "),gP=n(kTe,"A",{href:!0});var qkr=s(gP);Fyo=r(qkr,"RobertaForQuestionAnswering"),qkr.forEach(t),Eyo=r(kTe," (RoBERTa model)"),kTe.forEach(t),Cyo=i(I),c4=n(I,"LI",{});var RTe=s(c4);poe=n(RTe,"STRONG",{});var zkr=s(poe);Myo=r(zkr,"roformer"),zkr.forEach(t),yyo=r(RTe," \u2014 "),uP=n(RTe,"A",{href:!0});var Xkr=s(uP);wyo=r(Xkr,"RoFormerForQuestionAnswering"),Xkr.forEach(t),Ayo=r(RTe," (RoFormer model)"),RTe.forEach(t),Lyo=i(I),f4=n(I,"LI",{});var PTe=s(f4);_oe=n(PTe,"STRONG",{});var Qkr=s(_oe);Byo=r(Qkr,"splinter"),Qkr.forEach(t),xyo=r(PTe," \u2014 "),pP=n(PTe,"A",{href:!0});var Vkr=s(pP);kyo=r(Vkr,"SplinterForQuestionAnswering"),Vkr.forEach(t),Ryo=r(PTe," (Splinter model)"),PTe.forEach(t),Pyo=i(I),m4=n(I,"LI",{});var STe=s(m4);voe=n(STe,"STRONG",{});var Wkr=s(voe);Syo=r(Wkr,"squeezebert"),Wkr.forEach(t),$yo=r(STe," \u2014 "),_P=n(STe,"A",{href:!0});var Hkr=s(_P);Iyo=r(Hkr,"SqueezeBertForQuestionAnswering"),Hkr.forEach(t),Dyo=r(STe," (SqueezeBERT model)"),STe.forEach(t),Nyo=i(I),h4=n(I,"LI",{});var $Te=s(h4);boe=n($Te,"STRONG",{});var Ukr=s(boe);jyo=r(Ukr,"xlm"),Ukr.forEach(t),Oyo=r($Te," \u2014 "),vP=n($Te,"A",{href:!0});var Jkr=s(vP);Gyo=r(Jkr,"XLMForQuestionAnsweringSimple"),Jkr.forEach(t),qyo=r($Te," (XLM model)"),$Te.forEach(t),zyo=i(I),g4=n(I,"LI",{});var ITe=s(g4);Toe=n(ITe,"STRONG",{});var Kkr=s(Toe);Xyo=r(Kkr,"xlm-roberta"),Kkr.forEach(t),Qyo=r(ITe," \u2014 "),bP=n(ITe,"A",{href:!0});var Ykr=s(bP);Vyo=r(Ykr,"XLMRobertaForQuestionAnswering"),Ykr.forEach(t),Wyo=r(ITe," (XLM-RoBERTa model)"),ITe.forEach(t),Hyo=i(I),u4=n(I,"LI",{});var DTe=s(u4);Foe=n(DTe,"STRONG",{});var Zkr=s(Foe);Uyo=r(Zkr,"xlnet"),Zkr.forEach(t),Jyo=r(DTe," \u2014 "),TP=n(DTe,"A",{href:!0});var eRr=s(TP);Kyo=r(eRr,"XLNetForQuestionAnsweringSimple"),eRr.forEach(t),Yyo=r(DTe," (XLNet model)"),DTe.forEach(t),I.forEach(t),Zyo=i(Lt),p4=n(Lt,"P",{});var NTe=s(p4);ewo=r(NTe,"The model is set in evaluation mode by default using "),Eoe=n(NTe,"CODE",{});var oRr=s(Eoe);owo=r(oRr,"model.eval()"),oRr.forEach(t),rwo=r(NTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Coe=n(NTe,"CODE",{});var rRr=s(Coe);two=r(rRr,"model.train()"),rRr.forEach(t),NTe.forEach(t),awo=i(Lt),Moe=n(Lt,"P",{});var tRr=s(Moe);nwo=r(tRr,"Examples:"),tRr.forEach(t),swo=i(Lt),m(yM.$$.fragment,Lt),Lt.forEach(t),Rs.forEach(t),Q5e=i(c),Gi=n(c,"H2",{class:!0});var Ywe=s(Gi);_4=n(Ywe,"A",{id:!0,class:!0,href:!0});var aRr=s(_4);yoe=n(aRr,"SPAN",{});var nRr=s(yoe);m(wM.$$.fragment,nRr),nRr.forEach(t),aRr.forEach(t),lwo=i(Ywe),woe=n(Ywe,"SPAN",{});var sRr=s(woe);iwo=r(sRr,"AutoModelForTableQuestionAnswering"),sRr.forEach(t),Ywe.forEach(t),V5e=i(c),Uo=n(c,"DIV",{class:!0});var Ss=s(Uo);m(AM.$$.fragment,Ss),dwo=i(Ss),qi=n(Ss,"P",{});var Mj=s(qi);cwo=r(Mj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Aoe=n(Mj,"CODE",{});var lRr=s(Aoe);fwo=r(lRr,"from_pretrained()"),lRr.forEach(t),mwo=r(Mj,` class method or the
`),Loe=n(Mj,"CODE",{});var iRr=s(Loe);hwo=r(iRr,"from_config()"),iRr.forEach(t),gwo=r(Mj," class method."),Mj.forEach(t),uwo=i(Ss),LM=n(Ss,"P",{});var Zwe=s(LM);pwo=r(Zwe,"This class cannot be instantiated directly using "),Boe=n(Zwe,"CODE",{});var dRr=s(Boe);_wo=r(dRr,"__init__()"),dRr.forEach(t),vwo=r(Zwe," (throws an error)."),Zwe.forEach(t),bwo=i(Ss),Gr=n(Ss,"DIV",{class:!0});var $s=s(Gr);m(BM.$$.fragment,$s),Two=i($s),xoe=n($s,"P",{});var cRr=s(xoe);Fwo=r(cRr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),cRr.forEach(t),Ewo=i($s),zi=n($s,"P",{});var yj=s(zi);Cwo=r(yj,`Note:
Loading a model from its configuration file does `),koe=n(yj,"STRONG",{});var fRr=s(koe);Mwo=r(fRr,"not"),fRr.forEach(t),ywo=r(yj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Roe=n(yj,"CODE",{});var mRr=s(Roe);wwo=r(mRr,"from_pretrained()"),mRr.forEach(t),Awo=r(yj,` to load the model
weights.`),yj.forEach(t),Lwo=i($s),Poe=n($s,"P",{});var hRr=s(Poe);Bwo=r(hRr,"Examples:"),hRr.forEach(t),xwo=i($s),m(xM.$$.fragment,$s),$s.forEach(t),kwo=i(Ss),Qe=n(Ss,"DIV",{class:!0});var Bt=s(Qe);m(kM.$$.fragment,Bt),Rwo=i(Bt),Soe=n(Bt,"P",{});var gRr=s(Soe);Pwo=r(gRr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),gRr.forEach(t),Swo=i(Bt),Ia=n(Bt,"P",{});var GE=s(Ia);$wo=r(GE,"The model class to instantiate is selected based on the "),$oe=n(GE,"CODE",{});var uRr=s($oe);Iwo=r(uRr,"model_type"),uRr.forEach(t),Dwo=r(GE,` property of the config object (either
passed as an argument or loaded from `),Ioe=n(GE,"CODE",{});var pRr=s(Ioe);Nwo=r(pRr,"pretrained_model_name_or_path"),pRr.forEach(t),jwo=r(GE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Doe=n(GE,"CODE",{});var _Rr=s(Doe);Owo=r(_Rr,"pretrained_model_name_or_path"),_Rr.forEach(t),Gwo=r(GE,":"),GE.forEach(t),qwo=i(Bt),Noe=n(Bt,"UL",{});var vRr=s(Noe);v4=n(vRr,"LI",{});var jTe=s(v4);joe=n(jTe,"STRONG",{});var bRr=s(joe);zwo=r(bRr,"tapas"),bRr.forEach(t),Xwo=r(jTe," \u2014 "),FP=n(jTe,"A",{href:!0});var TRr=s(FP);Qwo=r(TRr,"TapasForQuestionAnswering"),TRr.forEach(t),Vwo=r(jTe," (TAPAS model)"),jTe.forEach(t),vRr.forEach(t),Wwo=i(Bt),b4=n(Bt,"P",{});var OTe=s(b4);Hwo=r(OTe,"The model is set in evaluation mode by default using "),Ooe=n(OTe,"CODE",{});var FRr=s(Ooe);Uwo=r(FRr,"model.eval()"),FRr.forEach(t),Jwo=r(OTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Goe=n(OTe,"CODE",{});var ERr=s(Goe);Kwo=r(ERr,"model.train()"),ERr.forEach(t),OTe.forEach(t),Ywo=i(Bt),qoe=n(Bt,"P",{});var CRr=s(qoe);Zwo=r(CRr,"Examples:"),CRr.forEach(t),e7o=i(Bt),m(RM.$$.fragment,Bt),Bt.forEach(t),Ss.forEach(t),W5e=i(c),Xi=n(c,"H2",{class:!0});var e7e=s(Xi);T4=n(e7e,"A",{id:!0,class:!0,href:!0});var MRr=s(T4);zoe=n(MRr,"SPAN",{});var yRr=s(zoe);m(PM.$$.fragment,yRr),yRr.forEach(t),MRr.forEach(t),o7o=i(e7e),Xoe=n(e7e,"SPAN",{});var wRr=s(Xoe);r7o=r(wRr,"AutoModelForImageClassification"),wRr.forEach(t),e7e.forEach(t),H5e=i(c),Jo=n(c,"DIV",{class:!0});var Is=s(Jo);m(SM.$$.fragment,Is),t7o=i(Is),Qi=n(Is,"P",{});var wj=s(Qi);a7o=r(wj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qoe=n(wj,"CODE",{});var ARr=s(Qoe);n7o=r(ARr,"from_pretrained()"),ARr.forEach(t),s7o=r(wj,` class method or the
`),Voe=n(wj,"CODE",{});var LRr=s(Voe);l7o=r(LRr,"from_config()"),LRr.forEach(t),i7o=r(wj," class method."),wj.forEach(t),d7o=i(Is),$M=n(Is,"P",{});var o7e=s($M);c7o=r(o7e,"This class cannot be instantiated directly using "),Woe=n(o7e,"CODE",{});var BRr=s(Woe);f7o=r(BRr,"__init__()"),BRr.forEach(t),m7o=r(o7e," (throws an error)."),o7e.forEach(t),h7o=i(Is),qr=n(Is,"DIV",{class:!0});var Ds=s(qr);m(IM.$$.fragment,Ds),g7o=i(Ds),Hoe=n(Ds,"P",{});var xRr=s(Hoe);u7o=r(xRr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),xRr.forEach(t),p7o=i(Ds),Vi=n(Ds,"P",{});var Aj=s(Vi);_7o=r(Aj,`Note:
Loading a model from its configuration file does `),Uoe=n(Aj,"STRONG",{});var kRr=s(Uoe);v7o=r(kRr,"not"),kRr.forEach(t),b7o=r(Aj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Joe=n(Aj,"CODE",{});var RRr=s(Joe);T7o=r(RRr,"from_pretrained()"),RRr.forEach(t),F7o=r(Aj,` to load the model
weights.`),Aj.forEach(t),E7o=i(Ds),Koe=n(Ds,"P",{});var PRr=s(Koe);C7o=r(PRr,"Examples:"),PRr.forEach(t),M7o=i(Ds),m(DM.$$.fragment,Ds),Ds.forEach(t),y7o=i(Is),Ve=n(Is,"DIV",{class:!0});var xt=s(Ve);m(NM.$$.fragment,xt),w7o=i(xt),Yoe=n(xt,"P",{});var SRr=s(Yoe);A7o=r(SRr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),SRr.forEach(t),L7o=i(xt),Da=n(xt,"P",{});var qE=s(Da);B7o=r(qE,"The model class to instantiate is selected based on the "),Zoe=n(qE,"CODE",{});var $Rr=s(Zoe);x7o=r($Rr,"model_type"),$Rr.forEach(t),k7o=r(qE,` property of the config object (either
passed as an argument or loaded from `),ere=n(qE,"CODE",{});var IRr=s(ere);R7o=r(IRr,"pretrained_model_name_or_path"),IRr.forEach(t),P7o=r(qE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ore=n(qE,"CODE",{});var DRr=s(ore);S7o=r(DRr,"pretrained_model_name_or_path"),DRr.forEach(t),$7o=r(qE,":"),qE.forEach(t),I7o=i(xt),Ko=n(xt,"UL",{});var zt=s(Ko);F4=n(zt,"LI",{});var GTe=s(F4);rre=n(GTe,"STRONG",{});var NRr=s(rre);D7o=r(NRr,"beit"),NRr.forEach(t),N7o=r(GTe," \u2014 "),EP=n(GTe,"A",{href:!0});var jRr=s(EP);j7o=r(jRr,"BeitForImageClassification"),jRr.forEach(t),O7o=r(GTe," (BEiT model)"),GTe.forEach(t),G7o=i(zt),ds=n(zt,"LI",{});var T0=s(ds);tre=n(T0,"STRONG",{});var ORr=s(tre);q7o=r(ORr,"deit"),ORr.forEach(t),z7o=r(T0," \u2014 "),CP=n(T0,"A",{href:!0});var GRr=s(CP);X7o=r(GRr,"DeiTForImageClassification"),GRr.forEach(t),Q7o=r(T0," or "),MP=n(T0,"A",{href:!0});var qRr=s(MP);V7o=r(qRr,"DeiTForImageClassificationWithTeacher"),qRr.forEach(t),W7o=r(T0," (DeiT model)"),T0.forEach(t),H7o=i(zt),E4=n(zt,"LI",{});var qTe=s(E4);are=n(qTe,"STRONG",{});var zRr=s(are);U7o=r(zRr,"imagegpt"),zRr.forEach(t),J7o=r(qTe," \u2014 "),yP=n(qTe,"A",{href:!0});var XRr=s(yP);K7o=r(XRr,"ImageGPTForImageClassification"),XRr.forEach(t),Y7o=r(qTe," (ImageGPT model)"),qTe.forEach(t),Z7o=i(zt),Gt=n(zt,"LI",{});var Ac=s(Gt);nre=n(Ac,"STRONG",{});var QRr=s(nre);e0o=r(QRr,"perceiver"),QRr.forEach(t),o0o=r(Ac," \u2014 "),wP=n(Ac,"A",{href:!0});var VRr=s(wP);r0o=r(VRr,"PerceiverForImageClassificationLearned"),VRr.forEach(t),t0o=r(Ac," or "),AP=n(Ac,"A",{href:!0});var WRr=s(AP);a0o=r(WRr,"PerceiverForImageClassificationFourier"),WRr.forEach(t),n0o=r(Ac," or "),LP=n(Ac,"A",{href:!0});var HRr=s(LP);s0o=r(HRr,"PerceiverForImageClassificationConvProcessing"),HRr.forEach(t),l0o=r(Ac," (Perceiver model)"),Ac.forEach(t),i0o=i(zt),C4=n(zt,"LI",{});var zTe=s(C4);sre=n(zTe,"STRONG",{});var URr=s(sre);d0o=r(URr,"segformer"),URr.forEach(t),c0o=r(zTe," \u2014 "),BP=n(zTe,"A",{href:!0});var JRr=s(BP);f0o=r(JRr,"SegformerForImageClassification"),JRr.forEach(t),m0o=r(zTe," (SegFormer model)"),zTe.forEach(t),h0o=i(zt),M4=n(zt,"LI",{});var XTe=s(M4);lre=n(XTe,"STRONG",{});var KRr=s(lre);g0o=r(KRr,"vit"),KRr.forEach(t),u0o=r(XTe," \u2014 "),xP=n(XTe,"A",{href:!0});var YRr=s(xP);p0o=r(YRr,"ViTForImageClassification"),YRr.forEach(t),_0o=r(XTe," (ViT model)"),XTe.forEach(t),zt.forEach(t),v0o=i(xt),y4=n(xt,"P",{});var QTe=s(y4);b0o=r(QTe,"The model is set in evaluation mode by default using "),ire=n(QTe,"CODE",{});var ZRr=s(ire);T0o=r(ZRr,"model.eval()"),ZRr.forEach(t),F0o=r(QTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dre=n(QTe,"CODE",{});var ePr=s(dre);E0o=r(ePr,"model.train()"),ePr.forEach(t),QTe.forEach(t),C0o=i(xt),cre=n(xt,"P",{});var oPr=s(cre);M0o=r(oPr,"Examples:"),oPr.forEach(t),y0o=i(xt),m(jM.$$.fragment,xt),xt.forEach(t),Is.forEach(t),U5e=i(c),Wi=n(c,"H2",{class:!0});var r7e=s(Wi);w4=n(r7e,"A",{id:!0,class:!0,href:!0});var rPr=s(w4);fre=n(rPr,"SPAN",{});var tPr=s(fre);m(OM.$$.fragment,tPr),tPr.forEach(t),rPr.forEach(t),w0o=i(r7e),mre=n(r7e,"SPAN",{});var aPr=s(mre);A0o=r(aPr,"AutoModelForVision2Seq"),aPr.forEach(t),r7e.forEach(t),J5e=i(c),Yo=n(c,"DIV",{class:!0});var Ns=s(Yo);m(GM.$$.fragment,Ns),L0o=i(Ns),Hi=n(Ns,"P",{});var Lj=s(Hi);B0o=r(Lj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),hre=n(Lj,"CODE",{});var nPr=s(hre);x0o=r(nPr,"from_pretrained()"),nPr.forEach(t),k0o=r(Lj,` class method or the
`),gre=n(Lj,"CODE",{});var sPr=s(gre);R0o=r(sPr,"from_config()"),sPr.forEach(t),P0o=r(Lj," class method."),Lj.forEach(t),S0o=i(Ns),qM=n(Ns,"P",{});var t7e=s(qM);$0o=r(t7e,"This class cannot be instantiated directly using "),ure=n(t7e,"CODE",{});var lPr=s(ure);I0o=r(lPr,"__init__()"),lPr.forEach(t),D0o=r(t7e," (throws an error)."),t7e.forEach(t),N0o=i(Ns),zr=n(Ns,"DIV",{class:!0});var js=s(zr);m(zM.$$.fragment,js),j0o=i(js),pre=n(js,"P",{});var iPr=s(pre);O0o=r(iPr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),iPr.forEach(t),G0o=i(js),Ui=n(js,"P",{});var Bj=s(Ui);q0o=r(Bj,`Note:
Loading a model from its configuration file does `),_re=n(Bj,"STRONG",{});var dPr=s(_re);z0o=r(dPr,"not"),dPr.forEach(t),X0o=r(Bj,` load the model weights. It only affects the
model\u2019s configuration. Use `),vre=n(Bj,"CODE",{});var cPr=s(vre);Q0o=r(cPr,"from_pretrained()"),cPr.forEach(t),V0o=r(Bj,` to load the model
weights.`),Bj.forEach(t),W0o=i(js),bre=n(js,"P",{});var fPr=s(bre);H0o=r(fPr,"Examples:"),fPr.forEach(t),U0o=i(js),m(XM.$$.fragment,js),js.forEach(t),J0o=i(Ns),We=n(Ns,"DIV",{class:!0});var kt=s(We);m(QM.$$.fragment,kt),K0o=i(kt),Tre=n(kt,"P",{});var mPr=s(Tre);Y0o=r(mPr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),mPr.forEach(t),Z0o=i(kt),Na=n(kt,"P",{});var zE=s(Na);eAo=r(zE,"The model class to instantiate is selected based on the "),Fre=n(zE,"CODE",{});var hPr=s(Fre);oAo=r(hPr,"model_type"),hPr.forEach(t),rAo=r(zE,` property of the config object (either
passed as an argument or loaded from `),Ere=n(zE,"CODE",{});var gPr=s(Ere);tAo=r(gPr,"pretrained_model_name_or_path"),gPr.forEach(t),aAo=r(zE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Cre=n(zE,"CODE",{});var uPr=s(Cre);nAo=r(uPr,"pretrained_model_name_or_path"),uPr.forEach(t),sAo=r(zE,":"),zE.forEach(t),lAo=i(kt),Mre=n(kt,"UL",{});var pPr=s(Mre);A4=n(pPr,"LI",{});var VTe=s(A4);yre=n(VTe,"STRONG",{});var _Pr=s(yre);iAo=r(_Pr,"vision-encoder-decoder"),_Pr.forEach(t),dAo=r(VTe," \u2014 "),kP=n(VTe,"A",{href:!0});var vPr=s(kP);cAo=r(vPr,"VisionEncoderDecoderModel"),vPr.forEach(t),fAo=r(VTe," (Vision Encoder decoder model)"),VTe.forEach(t),pPr.forEach(t),mAo=i(kt),L4=n(kt,"P",{});var WTe=s(L4);hAo=r(WTe,"The model is set in evaluation mode by default using "),wre=n(WTe,"CODE",{});var bPr=s(wre);gAo=r(bPr,"model.eval()"),bPr.forEach(t),uAo=r(WTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Are=n(WTe,"CODE",{});var TPr=s(Are);pAo=r(TPr,"model.train()"),TPr.forEach(t),WTe.forEach(t),_Ao=i(kt),Lre=n(kt,"P",{});var FPr=s(Lre);vAo=r(FPr,"Examples:"),FPr.forEach(t),bAo=i(kt),m(VM.$$.fragment,kt),kt.forEach(t),Ns.forEach(t),K5e=i(c),Ji=n(c,"H2",{class:!0});var a7e=s(Ji);B4=n(a7e,"A",{id:!0,class:!0,href:!0});var EPr=s(B4);Bre=n(EPr,"SPAN",{});var CPr=s(Bre);m(WM.$$.fragment,CPr),CPr.forEach(t),EPr.forEach(t),TAo=i(a7e),xre=n(a7e,"SPAN",{});var MPr=s(xre);FAo=r(MPr,"AutoModelForAudioClassification"),MPr.forEach(t),a7e.forEach(t),Y5e=i(c),Zo=n(c,"DIV",{class:!0});var Os=s(Zo);m(HM.$$.fragment,Os),EAo=i(Os),Ki=n(Os,"P",{});var xj=s(Ki);CAo=r(xj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kre=n(xj,"CODE",{});var yPr=s(kre);MAo=r(yPr,"from_pretrained()"),yPr.forEach(t),yAo=r(xj,` class method or the
`),Rre=n(xj,"CODE",{});var wPr=s(Rre);wAo=r(wPr,"from_config()"),wPr.forEach(t),AAo=r(xj," class method."),xj.forEach(t),LAo=i(Os),UM=n(Os,"P",{});var n7e=s(UM);BAo=r(n7e,"This class cannot be instantiated directly using "),Pre=n(n7e,"CODE",{});var APr=s(Pre);xAo=r(APr,"__init__()"),APr.forEach(t),kAo=r(n7e," (throws an error)."),n7e.forEach(t),RAo=i(Os),Xr=n(Os,"DIV",{class:!0});var Gs=s(Xr);m(JM.$$.fragment,Gs),PAo=i(Gs),Sre=n(Gs,"P",{});var LPr=s(Sre);SAo=r(LPr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),LPr.forEach(t),$Ao=i(Gs),Yi=n(Gs,"P",{});var kj=s(Yi);IAo=r(kj,`Note:
Loading a model from its configuration file does `),$re=n(kj,"STRONG",{});var BPr=s($re);DAo=r(BPr,"not"),BPr.forEach(t),NAo=r(kj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ire=n(kj,"CODE",{});var xPr=s(Ire);jAo=r(xPr,"from_pretrained()"),xPr.forEach(t),OAo=r(kj,` to load the model
weights.`),kj.forEach(t),GAo=i(Gs),Dre=n(Gs,"P",{});var kPr=s(Dre);qAo=r(kPr,"Examples:"),kPr.forEach(t),zAo=i(Gs),m(KM.$$.fragment,Gs),Gs.forEach(t),XAo=i(Os),He=n(Os,"DIV",{class:!0});var Rt=s(He);m(YM.$$.fragment,Rt),QAo=i(Rt),Nre=n(Rt,"P",{});var RPr=s(Nre);VAo=r(RPr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),RPr.forEach(t),WAo=i(Rt),ja=n(Rt,"P",{});var XE=s(ja);HAo=r(XE,"The model class to instantiate is selected based on the "),jre=n(XE,"CODE",{});var PPr=s(jre);UAo=r(PPr,"model_type"),PPr.forEach(t),JAo=r(XE,` property of the config object (either
passed as an argument or loaded from `),Ore=n(XE,"CODE",{});var SPr=s(Ore);KAo=r(SPr,"pretrained_model_name_or_path"),SPr.forEach(t),YAo=r(XE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gre=n(XE,"CODE",{});var $Pr=s(Gre);ZAo=r($Pr,"pretrained_model_name_or_path"),$Pr.forEach(t),e6o=r(XE,":"),XE.forEach(t),o6o=i(Rt),er=n(Rt,"UL",{});var Xt=s(er);x4=n(Xt,"LI",{});var HTe=s(x4);qre=n(HTe,"STRONG",{});var IPr=s(qre);r6o=r(IPr,"hubert"),IPr.forEach(t),t6o=r(HTe," \u2014 "),RP=n(HTe,"A",{href:!0});var DPr=s(RP);a6o=r(DPr,"HubertForSequenceClassification"),DPr.forEach(t),n6o=r(HTe," (Hubert model)"),HTe.forEach(t),s6o=i(Xt),k4=n(Xt,"LI",{});var UTe=s(k4);zre=n(UTe,"STRONG",{});var NPr=s(zre);l6o=r(NPr,"sew"),NPr.forEach(t),i6o=r(UTe," \u2014 "),PP=n(UTe,"A",{href:!0});var jPr=s(PP);d6o=r(jPr,"SEWForSequenceClassification"),jPr.forEach(t),c6o=r(UTe," (SEW model)"),UTe.forEach(t),f6o=i(Xt),R4=n(Xt,"LI",{});var JTe=s(R4);Xre=n(JTe,"STRONG",{});var OPr=s(Xre);m6o=r(OPr,"sew-d"),OPr.forEach(t),h6o=r(JTe," \u2014 "),SP=n(JTe,"A",{href:!0});var GPr=s(SP);g6o=r(GPr,"SEWDForSequenceClassification"),GPr.forEach(t),u6o=r(JTe," (SEW-D model)"),JTe.forEach(t),p6o=i(Xt),P4=n(Xt,"LI",{});var KTe=s(P4);Qre=n(KTe,"STRONG",{});var qPr=s(Qre);_6o=r(qPr,"unispeech"),qPr.forEach(t),v6o=r(KTe," \u2014 "),$P=n(KTe,"A",{href:!0});var zPr=s($P);b6o=r(zPr,"UniSpeechForSequenceClassification"),zPr.forEach(t),T6o=r(KTe," (UniSpeech model)"),KTe.forEach(t),F6o=i(Xt),S4=n(Xt,"LI",{});var YTe=s(S4);Vre=n(YTe,"STRONG",{});var XPr=s(Vre);E6o=r(XPr,"unispeech-sat"),XPr.forEach(t),C6o=r(YTe," \u2014 "),IP=n(YTe,"A",{href:!0});var QPr=s(IP);M6o=r(QPr,"UniSpeechSatForSequenceClassification"),QPr.forEach(t),y6o=r(YTe," (UniSpeechSat model)"),YTe.forEach(t),w6o=i(Xt),$4=n(Xt,"LI",{});var ZTe=s($4);Wre=n(ZTe,"STRONG",{});var VPr=s(Wre);A6o=r(VPr,"wav2vec2"),VPr.forEach(t),L6o=r(ZTe," \u2014 "),DP=n(ZTe,"A",{href:!0});var WPr=s(DP);B6o=r(WPr,"Wav2Vec2ForSequenceClassification"),WPr.forEach(t),x6o=r(ZTe," (Wav2Vec2 model)"),ZTe.forEach(t),Xt.forEach(t),k6o=i(Rt),I4=n(Rt,"P",{});var eFe=s(I4);R6o=r(eFe,"The model is set in evaluation mode by default using "),Hre=n(eFe,"CODE",{});var HPr=s(Hre);P6o=r(HPr,"model.eval()"),HPr.forEach(t),S6o=r(eFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ure=n(eFe,"CODE",{});var UPr=s(Ure);$6o=r(UPr,"model.train()"),UPr.forEach(t),eFe.forEach(t),I6o=i(Rt),Jre=n(Rt,"P",{});var JPr=s(Jre);D6o=r(JPr,"Examples:"),JPr.forEach(t),N6o=i(Rt),m(ZM.$$.fragment,Rt),Rt.forEach(t),Os.forEach(t),Z5e=i(c),Zi=n(c,"H2",{class:!0});var s7e=s(Zi);D4=n(s7e,"A",{id:!0,class:!0,href:!0});var KPr=s(D4);Kre=n(KPr,"SPAN",{});var YPr=s(Kre);m(e5.$$.fragment,YPr),YPr.forEach(t),KPr.forEach(t),j6o=i(s7e),Yre=n(s7e,"SPAN",{});var ZPr=s(Yre);O6o=r(ZPr,"AutoModelForCTC"),ZPr.forEach(t),s7e.forEach(t),eye=i(c),or=n(c,"DIV",{class:!0});var qs=s(or);m(o5.$$.fragment,qs),G6o=i(qs),ed=n(qs,"P",{});var Rj=s(ed);q6o=r(Rj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Zre=n(Rj,"CODE",{});var eSr=s(Zre);z6o=r(eSr,"from_pretrained()"),eSr.forEach(t),X6o=r(Rj,` class method or the
`),ete=n(Rj,"CODE",{});var oSr=s(ete);Q6o=r(oSr,"from_config()"),oSr.forEach(t),V6o=r(Rj," class method."),Rj.forEach(t),W6o=i(qs),r5=n(qs,"P",{});var l7e=s(r5);H6o=r(l7e,"This class cannot be instantiated directly using "),ote=n(l7e,"CODE",{});var rSr=s(ote);U6o=r(rSr,"__init__()"),rSr.forEach(t),J6o=r(l7e," (throws an error)."),l7e.forEach(t),K6o=i(qs),Qr=n(qs,"DIV",{class:!0});var zs=s(Qr);m(t5.$$.fragment,zs),Y6o=i(zs),rte=n(zs,"P",{});var tSr=s(rte);Z6o=r(tSr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),tSr.forEach(t),eLo=i(zs),od=n(zs,"P",{});var Pj=s(od);oLo=r(Pj,`Note:
Loading a model from its configuration file does `),tte=n(Pj,"STRONG",{});var aSr=s(tte);rLo=r(aSr,"not"),aSr.forEach(t),tLo=r(Pj,` load the model weights. It only affects the
model\u2019s configuration. Use `),ate=n(Pj,"CODE",{});var nSr=s(ate);aLo=r(nSr,"from_pretrained()"),nSr.forEach(t),nLo=r(Pj,` to load the model
weights.`),Pj.forEach(t),sLo=i(zs),nte=n(zs,"P",{});var sSr=s(nte);lLo=r(sSr,"Examples:"),sSr.forEach(t),iLo=i(zs),m(a5.$$.fragment,zs),zs.forEach(t),dLo=i(qs),Ue=n(qs,"DIV",{class:!0});var Pt=s(Ue);m(n5.$$.fragment,Pt),cLo=i(Pt),ste=n(Pt,"P",{});var lSr=s(ste);fLo=r(lSr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),lSr.forEach(t),mLo=i(Pt),Oa=n(Pt,"P",{});var QE=s(Oa);hLo=r(QE,"The model class to instantiate is selected based on the "),lte=n(QE,"CODE",{});var iSr=s(lte);gLo=r(iSr,"model_type"),iSr.forEach(t),uLo=r(QE,` property of the config object (either
passed as an argument or loaded from `),ite=n(QE,"CODE",{});var dSr=s(ite);pLo=r(dSr,"pretrained_model_name_or_path"),dSr.forEach(t),_Lo=r(QE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),dte=n(QE,"CODE",{});var cSr=s(dte);vLo=r(cSr,"pretrained_model_name_or_path"),cSr.forEach(t),bLo=r(QE,":"),QE.forEach(t),TLo=i(Pt),rr=n(Pt,"UL",{});var Qt=s(rr);N4=n(Qt,"LI",{});var oFe=s(N4);cte=n(oFe,"STRONG",{});var fSr=s(cte);FLo=r(fSr,"hubert"),fSr.forEach(t),ELo=r(oFe," \u2014 "),NP=n(oFe,"A",{href:!0});var mSr=s(NP);CLo=r(mSr,"HubertForCTC"),mSr.forEach(t),MLo=r(oFe," (Hubert model)"),oFe.forEach(t),yLo=i(Qt),j4=n(Qt,"LI",{});var rFe=s(j4);fte=n(rFe,"STRONG",{});var hSr=s(fte);wLo=r(hSr,"sew"),hSr.forEach(t),ALo=r(rFe," \u2014 "),jP=n(rFe,"A",{href:!0});var gSr=s(jP);LLo=r(gSr,"SEWForCTC"),gSr.forEach(t),BLo=r(rFe," (SEW model)"),rFe.forEach(t),xLo=i(Qt),O4=n(Qt,"LI",{});var tFe=s(O4);mte=n(tFe,"STRONG",{});var uSr=s(mte);kLo=r(uSr,"sew-d"),uSr.forEach(t),RLo=r(tFe," \u2014 "),OP=n(tFe,"A",{href:!0});var pSr=s(OP);PLo=r(pSr,"SEWDForCTC"),pSr.forEach(t),SLo=r(tFe," (SEW-D model)"),tFe.forEach(t),$Lo=i(Qt),G4=n(Qt,"LI",{});var aFe=s(G4);hte=n(aFe,"STRONG",{});var _Sr=s(hte);ILo=r(_Sr,"unispeech"),_Sr.forEach(t),DLo=r(aFe," \u2014 "),GP=n(aFe,"A",{href:!0});var vSr=s(GP);NLo=r(vSr,"UniSpeechForCTC"),vSr.forEach(t),jLo=r(aFe," (UniSpeech model)"),aFe.forEach(t),OLo=i(Qt),q4=n(Qt,"LI",{});var nFe=s(q4);gte=n(nFe,"STRONG",{});var bSr=s(gte);GLo=r(bSr,"unispeech-sat"),bSr.forEach(t),qLo=r(nFe," \u2014 "),qP=n(nFe,"A",{href:!0});var TSr=s(qP);zLo=r(TSr,"UniSpeechSatForCTC"),TSr.forEach(t),XLo=r(nFe," (UniSpeechSat model)"),nFe.forEach(t),QLo=i(Qt),z4=n(Qt,"LI",{});var sFe=s(z4);ute=n(sFe,"STRONG",{});var FSr=s(ute);VLo=r(FSr,"wav2vec2"),FSr.forEach(t),WLo=r(sFe," \u2014 "),zP=n(sFe,"A",{href:!0});var ESr=s(zP);HLo=r(ESr,"Wav2Vec2ForCTC"),ESr.forEach(t),ULo=r(sFe," (Wav2Vec2 model)"),sFe.forEach(t),Qt.forEach(t),JLo=i(Pt),X4=n(Pt,"P",{});var lFe=s(X4);KLo=r(lFe,"The model is set in evaluation mode by default using "),pte=n(lFe,"CODE",{});var CSr=s(pte);YLo=r(CSr,"model.eval()"),CSr.forEach(t),ZLo=r(lFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_te=n(lFe,"CODE",{});var MSr=s(_te);e8o=r(MSr,"model.train()"),MSr.forEach(t),lFe.forEach(t),o8o=i(Pt),vte=n(Pt,"P",{});var ySr=s(vte);r8o=r(ySr,"Examples:"),ySr.forEach(t),t8o=i(Pt),m(s5.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),oye=i(c),rd=n(c,"H2",{class:!0});var i7e=s(rd);Q4=n(i7e,"A",{id:!0,class:!0,href:!0});var wSr=s(Q4);bte=n(wSr,"SPAN",{});var ASr=s(bte);m(l5.$$.fragment,ASr),ASr.forEach(t),wSr.forEach(t),a8o=i(i7e),Tte=n(i7e,"SPAN",{});var LSr=s(Tte);n8o=r(LSr,"AutoModelForSpeechSeq2Seq"),LSr.forEach(t),i7e.forEach(t),rye=i(c),tr=n(c,"DIV",{class:!0});var Xs=s(tr);m(i5.$$.fragment,Xs),s8o=i(Xs),td=n(Xs,"P",{});var Sj=s(td);l8o=r(Sj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) when created
with the `),Fte=n(Sj,"CODE",{});var BSr=s(Fte);i8o=r(BSr,"from_pretrained()"),BSr.forEach(t),d8o=r(Sj,` class method or the
`),Ete=n(Sj,"CODE",{});var xSr=s(Ete);c8o=r(xSr,"from_config()"),xSr.forEach(t),f8o=r(Sj," class method."),Sj.forEach(t),m8o=i(Xs),d5=n(Xs,"P",{});var d7e=s(d5);h8o=r(d7e,"This class cannot be instantiated directly using "),Cte=n(d7e,"CODE",{});var kSr=s(Cte);g8o=r(kSr,"__init__()"),kSr.forEach(t),u8o=r(d7e," (throws an error)."),d7e.forEach(t),p8o=i(Xs),Vr=n(Xs,"DIV",{class:!0});var Qs=s(Vr);m(c5.$$.fragment,Qs),_8o=i(Qs),Mte=n(Qs,"P",{});var RSr=s(Mte);v8o=r(RSr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a configuration."),RSr.forEach(t),b8o=i(Qs),ad=n(Qs,"P",{});var $j=s(ad);T8o=r($j,`Note:
Loading a model from its configuration file does `),yte=n($j,"STRONG",{});var PSr=s(yte);F8o=r(PSr,"not"),PSr.forEach(t),E8o=r($j,` load the model weights. It only affects the
model\u2019s configuration. Use `),wte=n($j,"CODE",{});var SSr=s(wte);C8o=r(SSr,"from_pretrained()"),SSr.forEach(t),M8o=r($j,` to load the model
weights.`),$j.forEach(t),y8o=i(Qs),Ate=n(Qs,"P",{});var $Sr=s(Ate);w8o=r($Sr,"Examples:"),$Sr.forEach(t),A8o=i(Qs),m(f5.$$.fragment,Qs),Qs.forEach(t),L8o=i(Xs),Je=n(Xs,"DIV",{class:!0});var St=s(Je);m(m5.$$.fragment,St),B8o=i(St),Lte=n(St,"P",{});var ISr=s(Lte);x8o=r(ISr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a pretrained model."),ISr.forEach(t),k8o=i(St),Ga=n(St,"P",{});var VE=s(Ga);R8o=r(VE,"The model class to instantiate is selected based on the "),Bte=n(VE,"CODE",{});var DSr=s(Bte);P8o=r(DSr,"model_type"),DSr.forEach(t),S8o=r(VE,` property of the config object (either
passed as an argument or loaded from `),xte=n(VE,"CODE",{});var NSr=s(xte);$8o=r(NSr,"pretrained_model_name_or_path"),NSr.forEach(t),I8o=r(VE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kte=n(VE,"CODE",{});var jSr=s(kte);D8o=r(jSr,"pretrained_model_name_or_path"),jSr.forEach(t),N8o=r(VE,":"),VE.forEach(t),j8o=i(St),h5=n(St,"UL",{});var c7e=s(h5);V4=n(c7e,"LI",{});var iFe=s(V4);Rte=n(iFe,"STRONG",{});var OSr=s(Rte);O8o=r(OSr,"speech-encoder-decoder"),OSr.forEach(t),G8o=r(iFe," \u2014 "),XP=n(iFe,"A",{href:!0});var GSr=s(XP);q8o=r(GSr,"SpeechEncoderDecoderModel"),GSr.forEach(t),z8o=r(iFe," (Speech Encoder decoder model)"),iFe.forEach(t),X8o=i(c7e),W4=n(c7e,"LI",{});var dFe=s(W4);Pte=n(dFe,"STRONG",{});var qSr=s(Pte);Q8o=r(qSr,"speech_to_text"),qSr.forEach(t),V8o=r(dFe," \u2014 "),QP=n(dFe,"A",{href:!0});var zSr=s(QP);W8o=r(zSr,"Speech2TextForConditionalGeneration"),zSr.forEach(t),H8o=r(dFe," (Speech2Text model)"),dFe.forEach(t),c7e.forEach(t),U8o=i(St),H4=n(St,"P",{});var cFe=s(H4);J8o=r(cFe,"The model is set in evaluation mode by default using "),Ste=n(cFe,"CODE",{});var XSr=s(Ste);K8o=r(XSr,"model.eval()"),XSr.forEach(t),Y8o=r(cFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$te=n(cFe,"CODE",{});var QSr=s($te);Z8o=r(QSr,"model.train()"),QSr.forEach(t),cFe.forEach(t),eBo=i(St),Ite=n(St,"P",{});var VSr=s(Ite);oBo=r(VSr,"Examples:"),VSr.forEach(t),rBo=i(St),m(g5.$$.fragment,St),St.forEach(t),Xs.forEach(t),tye=i(c),nd=n(c,"H2",{class:!0});var f7e=s(nd);U4=n(f7e,"A",{id:!0,class:!0,href:!0});var WSr=s(U4);Dte=n(WSr,"SPAN",{});var HSr=s(Dte);m(u5.$$.fragment,HSr),HSr.forEach(t),WSr.forEach(t),tBo=i(f7e),Nte=n(f7e,"SPAN",{});var USr=s(Nte);aBo=r(USr,"AutoModelForObjectDetection"),USr.forEach(t),f7e.forEach(t),aye=i(c),ar=n(c,"DIV",{class:!0});var Vs=s(ar);m(p5.$$.fragment,Vs),nBo=i(Vs),sd=n(Vs,"P",{});var Ij=s(sd);sBo=r(Ij,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),jte=n(Ij,"CODE",{});var JSr=s(jte);lBo=r(JSr,"from_pretrained()"),JSr.forEach(t),iBo=r(Ij,` class method or the
`),Ote=n(Ij,"CODE",{});var KSr=s(Ote);dBo=r(KSr,"from_config()"),KSr.forEach(t),cBo=r(Ij," class method."),Ij.forEach(t),fBo=i(Vs),_5=n(Vs,"P",{});var m7e=s(_5);mBo=r(m7e,"This class cannot be instantiated directly using "),Gte=n(m7e,"CODE",{});var YSr=s(Gte);hBo=r(YSr,"__init__()"),YSr.forEach(t),gBo=r(m7e," (throws an error)."),m7e.forEach(t),uBo=i(Vs),Wr=n(Vs,"DIV",{class:!0});var Ws=s(Wr);m(v5.$$.fragment,Ws),pBo=i(Ws),qte=n(Ws,"P",{});var ZSr=s(qte);_Bo=r(ZSr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),ZSr.forEach(t),vBo=i(Ws),ld=n(Ws,"P",{});var Dj=s(ld);bBo=r(Dj,`Note:
Loading a model from its configuration file does `),zte=n(Dj,"STRONG",{});var e$r=s(zte);TBo=r(e$r,"not"),e$r.forEach(t),FBo=r(Dj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xte=n(Dj,"CODE",{});var o$r=s(Xte);EBo=r(o$r,"from_pretrained()"),o$r.forEach(t),CBo=r(Dj,` to load the model
weights.`),Dj.forEach(t),MBo=i(Ws),Qte=n(Ws,"P",{});var r$r=s(Qte);yBo=r(r$r,"Examples:"),r$r.forEach(t),wBo=i(Ws),m(b5.$$.fragment,Ws),Ws.forEach(t),ABo=i(Vs),Ke=n(Vs,"DIV",{class:!0});var $t=s(Ke);m(T5.$$.fragment,$t),LBo=i($t),Vte=n($t,"P",{});var t$r=s(Vte);BBo=r(t$r,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),t$r.forEach(t),xBo=i($t),qa=n($t,"P",{});var WE=s(qa);kBo=r(WE,"The model class to instantiate is selected based on the "),Wte=n(WE,"CODE",{});var a$r=s(Wte);RBo=r(a$r,"model_type"),a$r.forEach(t),PBo=r(WE,` property of the config object (either
passed as an argument or loaded from `),Hte=n(WE,"CODE",{});var n$r=s(Hte);SBo=r(n$r,"pretrained_model_name_or_path"),n$r.forEach(t),$Bo=r(WE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ute=n(WE,"CODE",{});var s$r=s(Ute);IBo=r(s$r,"pretrained_model_name_or_path"),s$r.forEach(t),DBo=r(WE,":"),WE.forEach(t),NBo=i($t),Jte=n($t,"UL",{});var l$r=s(Jte);J4=n(l$r,"LI",{});var fFe=s(J4);Kte=n(fFe,"STRONG",{});var i$r=s(Kte);jBo=r(i$r,"detr"),i$r.forEach(t),OBo=r(fFe," \u2014 "),VP=n(fFe,"A",{href:!0});var d$r=s(VP);GBo=r(d$r,"DetrForObjectDetection"),d$r.forEach(t),qBo=r(fFe," (DETR model)"),fFe.forEach(t),l$r.forEach(t),zBo=i($t),K4=n($t,"P",{});var mFe=s(K4);XBo=r(mFe,"The model is set in evaluation mode by default using "),Yte=n(mFe,"CODE",{});var c$r=s(Yte);QBo=r(c$r,"model.eval()"),c$r.forEach(t),VBo=r(mFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zte=n(mFe,"CODE",{});var f$r=s(Zte);WBo=r(f$r,"model.train()"),f$r.forEach(t),mFe.forEach(t),HBo=i($t),eae=n($t,"P",{});var m$r=s(eae);UBo=r(m$r,"Examples:"),m$r.forEach(t),JBo=i($t),m(F5.$$.fragment,$t),$t.forEach(t),Vs.forEach(t),nye=i(c),id=n(c,"H2",{class:!0});var h7e=s(id);Y4=n(h7e,"A",{id:!0,class:!0,href:!0});var h$r=s(Y4);oae=n(h$r,"SPAN",{});var g$r=s(oae);m(E5.$$.fragment,g$r),g$r.forEach(t),h$r.forEach(t),KBo=i(h7e),rae=n(h7e,"SPAN",{});var u$r=s(rae);YBo=r(u$r,"AutoModelForImageSegmentation"),u$r.forEach(t),h7e.forEach(t),sye=i(c),nr=n(c,"DIV",{class:!0});var Hs=s(nr);m(C5.$$.fragment,Hs),ZBo=i(Hs),dd=n(Hs,"P",{});var Nj=s(dd);e9o=r(Nj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),tae=n(Nj,"CODE",{});var p$r=s(tae);o9o=r(p$r,"from_pretrained()"),p$r.forEach(t),r9o=r(Nj,` class method or the
`),aae=n(Nj,"CODE",{});var _$r=s(aae);t9o=r(_$r,"from_config()"),_$r.forEach(t),a9o=r(Nj," class method."),Nj.forEach(t),n9o=i(Hs),M5=n(Hs,"P",{});var g7e=s(M5);s9o=r(g7e,"This class cannot be instantiated directly using "),nae=n(g7e,"CODE",{});var v$r=s(nae);l9o=r(v$r,"__init__()"),v$r.forEach(t),i9o=r(g7e," (throws an error)."),g7e.forEach(t),d9o=i(Hs),Hr=n(Hs,"DIV",{class:!0});var Us=s(Hr);m(y5.$$.fragment,Us),c9o=i(Us),sae=n(Us,"P",{});var b$r=s(sae);f9o=r(b$r,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),b$r.forEach(t),m9o=i(Us),cd=n(Us,"P",{});var jj=s(cd);h9o=r(jj,`Note:
Loading a model from its configuration file does `),lae=n(jj,"STRONG",{});var T$r=s(lae);g9o=r(T$r,"not"),T$r.forEach(t),u9o=r(jj,` load the model weights. It only affects the
model\u2019s configuration. Use `),iae=n(jj,"CODE",{});var F$r=s(iae);p9o=r(F$r,"from_pretrained()"),F$r.forEach(t),_9o=r(jj,` to load the model
weights.`),jj.forEach(t),v9o=i(Us),dae=n(Us,"P",{});var E$r=s(dae);b9o=r(E$r,"Examples:"),E$r.forEach(t),T9o=i(Us),m(w5.$$.fragment,Us),Us.forEach(t),F9o=i(Hs),Ye=n(Hs,"DIV",{class:!0});var It=s(Ye);m(A5.$$.fragment,It),E9o=i(It),cae=n(It,"P",{});var C$r=s(cae);C9o=r(C$r,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),C$r.forEach(t),M9o=i(It),za=n(It,"P",{});var HE=s(za);y9o=r(HE,"The model class to instantiate is selected based on the "),fae=n(HE,"CODE",{});var M$r=s(fae);w9o=r(M$r,"model_type"),M$r.forEach(t),A9o=r(HE,` property of the config object (either
passed as an argument or loaded from `),mae=n(HE,"CODE",{});var y$r=s(mae);L9o=r(y$r,"pretrained_model_name_or_path"),y$r.forEach(t),B9o=r(HE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),hae=n(HE,"CODE",{});var w$r=s(hae);x9o=r(w$r,"pretrained_model_name_or_path"),w$r.forEach(t),k9o=r(HE,":"),HE.forEach(t),R9o=i(It),gae=n(It,"UL",{});var A$r=s(gae);Z4=n(A$r,"LI",{});var hFe=s(Z4);uae=n(hFe,"STRONG",{});var L$r=s(uae);P9o=r(L$r,"detr"),L$r.forEach(t),S9o=r(hFe," \u2014 "),WP=n(hFe,"A",{href:!0});var B$r=s(WP);$9o=r(B$r,"DetrForSegmentation"),B$r.forEach(t),I9o=r(hFe," (DETR model)"),hFe.forEach(t),A$r.forEach(t),D9o=i(It),ev=n(It,"P",{});var gFe=s(ev);N9o=r(gFe,"The model is set in evaluation mode by default using "),pae=n(gFe,"CODE",{});var x$r=s(pae);j9o=r(x$r,"model.eval()"),x$r.forEach(t),O9o=r(gFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_ae=n(gFe,"CODE",{});var k$r=s(_ae);G9o=r(k$r,"model.train()"),k$r.forEach(t),gFe.forEach(t),q9o=i(It),vae=n(It,"P",{});var R$r=s(vae);z9o=r(R$r,"Examples:"),R$r.forEach(t),X9o=i(It),m(L5.$$.fragment,It),It.forEach(t),Hs.forEach(t),lye=i(c),fd=n(c,"H2",{class:!0});var u7e=s(fd);ov=n(u7e,"A",{id:!0,class:!0,href:!0});var P$r=s(ov);bae=n(P$r,"SPAN",{});var S$r=s(bae);m(B5.$$.fragment,S$r),S$r.forEach(t),P$r.forEach(t),Q9o=i(u7e),Tae=n(u7e,"SPAN",{});var $$r=s(Tae);V9o=r($$r,"TFAutoModel"),$$r.forEach(t),u7e.forEach(t),iye=i(c),sr=n(c,"DIV",{class:!0});var Js=s(sr);m(x5.$$.fragment,Js),W9o=i(Js),md=n(Js,"P",{});var Oj=s(md);H9o=r(Oj,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Fae=n(Oj,"CODE",{});var I$r=s(Fae);U9o=r(I$r,"from_pretrained()"),I$r.forEach(t),J9o=r(Oj,` class method or the
`),Eae=n(Oj,"CODE",{});var D$r=s(Eae);K9o=r(D$r,"from_config()"),D$r.forEach(t),Y9o=r(Oj," class method."),Oj.forEach(t),Z9o=i(Js),k5=n(Js,"P",{});var p7e=s(k5);exo=r(p7e,"This class cannot be instantiated directly using "),Cae=n(p7e,"CODE",{});var N$r=s(Cae);oxo=r(N$r,"__init__()"),N$r.forEach(t),rxo=r(p7e," (throws an error)."),p7e.forEach(t),txo=i(Js),Ur=n(Js,"DIV",{class:!0});var Ks=s(Ur);m(R5.$$.fragment,Ks),axo=i(Ks),Mae=n(Ks,"P",{});var j$r=s(Mae);nxo=r(j$r,"Instantiates one of the base model classes of the library from a configuration."),j$r.forEach(t),sxo=i(Ks),hd=n(Ks,"P",{});var Gj=s(hd);lxo=r(Gj,`Note:
Loading a model from its configuration file does `),yae=n(Gj,"STRONG",{});var O$r=s(yae);ixo=r(O$r,"not"),O$r.forEach(t),dxo=r(Gj,` load the model weights. It only affects the
model\u2019s configuration. Use `),wae=n(Gj,"CODE",{});var G$r=s(wae);cxo=r(G$r,"from_pretrained()"),G$r.forEach(t),fxo=r(Gj,` to load the model
weights.`),Gj.forEach(t),mxo=i(Ks),Aae=n(Ks,"P",{});var q$r=s(Aae);hxo=r(q$r,"Examples:"),q$r.forEach(t),gxo=i(Ks),m(P5.$$.fragment,Ks),Ks.forEach(t),uxo=i(Js),lo=n(Js,"DIV",{class:!0});var Vt=s(lo);m(S5.$$.fragment,Vt),pxo=i(Vt),Lae=n(Vt,"P",{});var z$r=s(Lae);_xo=r(z$r,"Instantiate one of the base model classes of the library from a pretrained model."),z$r.forEach(t),vxo=i(Vt),Xa=n(Vt,"P",{});var UE=s(Xa);bxo=r(UE,"The model class to instantiate is selected based on the "),Bae=n(UE,"CODE",{});var X$r=s(Bae);Txo=r(X$r,"model_type"),X$r.forEach(t),Fxo=r(UE,` property of the config object (either
passed as an argument or loaded from `),xae=n(UE,"CODE",{});var Q$r=s(xae);Exo=r(Q$r,"pretrained_model_name_or_path"),Q$r.forEach(t),Cxo=r(UE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kae=n(UE,"CODE",{});var V$r=s(kae);Mxo=r(V$r,"pretrained_model_name_or_path"),V$r.forEach(t),yxo=r(UE,":"),UE.forEach(t),wxo=i(Vt),B=n(Vt,"UL",{});var x=s(B);rv=n(x,"LI",{});var uFe=s(rv);Rae=n(uFe,"STRONG",{});var W$r=s(Rae);Axo=r(W$r,"albert"),W$r.forEach(t),Lxo=r(uFe," \u2014 "),HP=n(uFe,"A",{href:!0});var H$r=s(HP);Bxo=r(H$r,"TFAlbertModel"),H$r.forEach(t),xxo=r(uFe," (ALBERT model)"),uFe.forEach(t),kxo=i(x),tv=n(x,"LI",{});var pFe=s(tv);Pae=n(pFe,"STRONG",{});var U$r=s(Pae);Rxo=r(U$r,"bart"),U$r.forEach(t),Pxo=r(pFe," \u2014 "),UP=n(pFe,"A",{href:!0});var J$r=s(UP);Sxo=r(J$r,"TFBartModel"),J$r.forEach(t),$xo=r(pFe," (BART model)"),pFe.forEach(t),Ixo=i(x),av=n(x,"LI",{});var _Fe=s(av);Sae=n(_Fe,"STRONG",{});var K$r=s(Sae);Dxo=r(K$r,"bert"),K$r.forEach(t),Nxo=r(_Fe," \u2014 "),JP=n(_Fe,"A",{href:!0});var Y$r=s(JP);jxo=r(Y$r,"TFBertModel"),Y$r.forEach(t),Oxo=r(_Fe," (BERT model)"),_Fe.forEach(t),Gxo=i(x),nv=n(x,"LI",{});var vFe=s(nv);$ae=n(vFe,"STRONG",{});var Z$r=s($ae);qxo=r(Z$r,"blenderbot"),Z$r.forEach(t),zxo=r(vFe," \u2014 "),KP=n(vFe,"A",{href:!0});var eIr=s(KP);Xxo=r(eIr,"TFBlenderbotModel"),eIr.forEach(t),Qxo=r(vFe," (Blenderbot model)"),vFe.forEach(t),Vxo=i(x),sv=n(x,"LI",{});var bFe=s(sv);Iae=n(bFe,"STRONG",{});var oIr=s(Iae);Wxo=r(oIr,"blenderbot-small"),oIr.forEach(t),Hxo=r(bFe," \u2014 "),YP=n(bFe,"A",{href:!0});var rIr=s(YP);Uxo=r(rIr,"TFBlenderbotSmallModel"),rIr.forEach(t),Jxo=r(bFe," (BlenderbotSmall model)"),bFe.forEach(t),Kxo=i(x),lv=n(x,"LI",{});var TFe=s(lv);Dae=n(TFe,"STRONG",{});var tIr=s(Dae);Yxo=r(tIr,"camembert"),tIr.forEach(t),Zxo=r(TFe," \u2014 "),ZP=n(TFe,"A",{href:!0});var aIr=s(ZP);eko=r(aIr,"TFCamembertModel"),aIr.forEach(t),oko=r(TFe," (CamemBERT model)"),TFe.forEach(t),rko=i(x),iv=n(x,"LI",{});var FFe=s(iv);Nae=n(FFe,"STRONG",{});var nIr=s(Nae);tko=r(nIr,"convbert"),nIr.forEach(t),ako=r(FFe," \u2014 "),eS=n(FFe,"A",{href:!0});var sIr=s(eS);nko=r(sIr,"TFConvBertModel"),sIr.forEach(t),sko=r(FFe," (ConvBERT model)"),FFe.forEach(t),lko=i(x),dv=n(x,"LI",{});var EFe=s(dv);jae=n(EFe,"STRONG",{});var lIr=s(jae);iko=r(lIr,"ctrl"),lIr.forEach(t),dko=r(EFe," \u2014 "),oS=n(EFe,"A",{href:!0});var iIr=s(oS);cko=r(iIr,"TFCTRLModel"),iIr.forEach(t),fko=r(EFe," (CTRL model)"),EFe.forEach(t),mko=i(x),cv=n(x,"LI",{});var CFe=s(cv);Oae=n(CFe,"STRONG",{});var dIr=s(Oae);hko=r(dIr,"deberta"),dIr.forEach(t),gko=r(CFe," \u2014 "),rS=n(CFe,"A",{href:!0});var cIr=s(rS);uko=r(cIr,"TFDebertaModel"),cIr.forEach(t),pko=r(CFe," (DeBERTa model)"),CFe.forEach(t),_ko=i(x),fv=n(x,"LI",{});var MFe=s(fv);Gae=n(MFe,"STRONG",{});var fIr=s(Gae);vko=r(fIr,"deberta-v2"),fIr.forEach(t),bko=r(MFe," \u2014 "),tS=n(MFe,"A",{href:!0});var mIr=s(tS);Tko=r(mIr,"TFDebertaV2Model"),mIr.forEach(t),Fko=r(MFe," (DeBERTa-v2 model)"),MFe.forEach(t),Eko=i(x),mv=n(x,"LI",{});var yFe=s(mv);qae=n(yFe,"STRONG",{});var hIr=s(qae);Cko=r(hIr,"distilbert"),hIr.forEach(t),Mko=r(yFe," \u2014 "),aS=n(yFe,"A",{href:!0});var gIr=s(aS);yko=r(gIr,"TFDistilBertModel"),gIr.forEach(t),wko=r(yFe," (DistilBERT model)"),yFe.forEach(t),Ako=i(x),hv=n(x,"LI",{});var wFe=s(hv);zae=n(wFe,"STRONG",{});var uIr=s(zae);Lko=r(uIr,"dpr"),uIr.forEach(t),Bko=r(wFe," \u2014 "),nS=n(wFe,"A",{href:!0});var pIr=s(nS);xko=r(pIr,"TFDPRQuestionEncoder"),pIr.forEach(t),kko=r(wFe," (DPR model)"),wFe.forEach(t),Rko=i(x),gv=n(x,"LI",{});var AFe=s(gv);Xae=n(AFe,"STRONG",{});var _Ir=s(Xae);Pko=r(_Ir,"electra"),_Ir.forEach(t),Sko=r(AFe," \u2014 "),sS=n(AFe,"A",{href:!0});var vIr=s(sS);$ko=r(vIr,"TFElectraModel"),vIr.forEach(t),Iko=r(AFe," (ELECTRA model)"),AFe.forEach(t),Dko=i(x),uv=n(x,"LI",{});var LFe=s(uv);Qae=n(LFe,"STRONG",{});var bIr=s(Qae);Nko=r(bIr,"flaubert"),bIr.forEach(t),jko=r(LFe," \u2014 "),lS=n(LFe,"A",{href:!0});var TIr=s(lS);Oko=r(TIr,"TFFlaubertModel"),TIr.forEach(t),Gko=r(LFe," (FlauBERT model)"),LFe.forEach(t),qko=i(x),cs=n(x,"LI",{});var F0=s(cs);Vae=n(F0,"STRONG",{});var FIr=s(Vae);zko=r(FIr,"funnel"),FIr.forEach(t),Xko=r(F0," \u2014 "),iS=n(F0,"A",{href:!0});var EIr=s(iS);Qko=r(EIr,"TFFunnelModel"),EIr.forEach(t),Vko=r(F0," or "),dS=n(F0,"A",{href:!0});var CIr=s(dS);Wko=r(CIr,"TFFunnelBaseModel"),CIr.forEach(t),Hko=r(F0," (Funnel Transformer model)"),F0.forEach(t),Uko=i(x),pv=n(x,"LI",{});var BFe=s(pv);Wae=n(BFe,"STRONG",{});var MIr=s(Wae);Jko=r(MIr,"gpt2"),MIr.forEach(t),Kko=r(BFe," \u2014 "),cS=n(BFe,"A",{href:!0});var yIr=s(cS);Yko=r(yIr,"TFGPT2Model"),yIr.forEach(t),Zko=r(BFe," (OpenAI GPT-2 model)"),BFe.forEach(t),eRo=i(x),_v=n(x,"LI",{});var xFe=s(_v);Hae=n(xFe,"STRONG",{});var wIr=s(Hae);oRo=r(wIr,"hubert"),wIr.forEach(t),rRo=r(xFe," \u2014 "),fS=n(xFe,"A",{href:!0});var AIr=s(fS);tRo=r(AIr,"TFHubertModel"),AIr.forEach(t),aRo=r(xFe," (Hubert model)"),xFe.forEach(t),nRo=i(x),vv=n(x,"LI",{});var kFe=s(vv);Uae=n(kFe,"STRONG",{});var LIr=s(Uae);sRo=r(LIr,"layoutlm"),LIr.forEach(t),lRo=r(kFe," \u2014 "),mS=n(kFe,"A",{href:!0});var BIr=s(mS);iRo=r(BIr,"TFLayoutLMModel"),BIr.forEach(t),dRo=r(kFe," (LayoutLM model)"),kFe.forEach(t),cRo=i(x),bv=n(x,"LI",{});var RFe=s(bv);Jae=n(RFe,"STRONG",{});var xIr=s(Jae);fRo=r(xIr,"led"),xIr.forEach(t),mRo=r(RFe," \u2014 "),hS=n(RFe,"A",{href:!0});var kIr=s(hS);hRo=r(kIr,"TFLEDModel"),kIr.forEach(t),gRo=r(RFe," (LED model)"),RFe.forEach(t),uRo=i(x),Tv=n(x,"LI",{});var PFe=s(Tv);Kae=n(PFe,"STRONG",{});var RIr=s(Kae);pRo=r(RIr,"longformer"),RIr.forEach(t),_Ro=r(PFe," \u2014 "),gS=n(PFe,"A",{href:!0});var PIr=s(gS);vRo=r(PIr,"TFLongformerModel"),PIr.forEach(t),bRo=r(PFe," (Longformer model)"),PFe.forEach(t),TRo=i(x),Fv=n(x,"LI",{});var SFe=s(Fv);Yae=n(SFe,"STRONG",{});var SIr=s(Yae);FRo=r(SIr,"lxmert"),SIr.forEach(t),ERo=r(SFe," \u2014 "),uS=n(SFe,"A",{href:!0});var $Ir=s(uS);CRo=r($Ir,"TFLxmertModel"),$Ir.forEach(t),MRo=r(SFe," (LXMERT model)"),SFe.forEach(t),yRo=i(x),Ev=n(x,"LI",{});var $Fe=s(Ev);Zae=n($Fe,"STRONG",{});var IIr=s(Zae);wRo=r(IIr,"marian"),IIr.forEach(t),ARo=r($Fe," \u2014 "),pS=n($Fe,"A",{href:!0});var DIr=s(pS);LRo=r(DIr,"TFMarianModel"),DIr.forEach(t),BRo=r($Fe," (Marian model)"),$Fe.forEach(t),xRo=i(x),Cv=n(x,"LI",{});var IFe=s(Cv);ene=n(IFe,"STRONG",{});var NIr=s(ene);kRo=r(NIr,"mbart"),NIr.forEach(t),RRo=r(IFe," \u2014 "),_S=n(IFe,"A",{href:!0});var jIr=s(_S);PRo=r(jIr,"TFMBartModel"),jIr.forEach(t),SRo=r(IFe," (mBART model)"),IFe.forEach(t),$Ro=i(x),Mv=n(x,"LI",{});var DFe=s(Mv);one=n(DFe,"STRONG",{});var OIr=s(one);IRo=r(OIr,"mobilebert"),OIr.forEach(t),DRo=r(DFe," \u2014 "),vS=n(DFe,"A",{href:!0});var GIr=s(vS);NRo=r(GIr,"TFMobileBertModel"),GIr.forEach(t),jRo=r(DFe," (MobileBERT model)"),DFe.forEach(t),ORo=i(x),yv=n(x,"LI",{});var NFe=s(yv);rne=n(NFe,"STRONG",{});var qIr=s(rne);GRo=r(qIr,"mpnet"),qIr.forEach(t),qRo=r(NFe," \u2014 "),bS=n(NFe,"A",{href:!0});var zIr=s(bS);zRo=r(zIr,"TFMPNetModel"),zIr.forEach(t),XRo=r(NFe," (MPNet model)"),NFe.forEach(t),QRo=i(x),wv=n(x,"LI",{});var jFe=s(wv);tne=n(jFe,"STRONG",{});var XIr=s(tne);VRo=r(XIr,"mt5"),XIr.forEach(t),WRo=r(jFe," \u2014 "),TS=n(jFe,"A",{href:!0});var QIr=s(TS);HRo=r(QIr,"TFMT5Model"),QIr.forEach(t),URo=r(jFe," (mT5 model)"),jFe.forEach(t),JRo=i(x),Av=n(x,"LI",{});var OFe=s(Av);ane=n(OFe,"STRONG",{});var VIr=s(ane);KRo=r(VIr,"openai-gpt"),VIr.forEach(t),YRo=r(OFe," \u2014 "),FS=n(OFe,"A",{href:!0});var WIr=s(FS);ZRo=r(WIr,"TFOpenAIGPTModel"),WIr.forEach(t),ePo=r(OFe," (OpenAI GPT model)"),OFe.forEach(t),oPo=i(x),Lv=n(x,"LI",{});var GFe=s(Lv);nne=n(GFe,"STRONG",{});var HIr=s(nne);rPo=r(HIr,"pegasus"),HIr.forEach(t),tPo=r(GFe," \u2014 "),ES=n(GFe,"A",{href:!0});var UIr=s(ES);aPo=r(UIr,"TFPegasusModel"),UIr.forEach(t),nPo=r(GFe," (Pegasus model)"),GFe.forEach(t),sPo=i(x),Bv=n(x,"LI",{});var qFe=s(Bv);sne=n(qFe,"STRONG",{});var JIr=s(sne);lPo=r(JIr,"rembert"),JIr.forEach(t),iPo=r(qFe," \u2014 "),CS=n(qFe,"A",{href:!0});var KIr=s(CS);dPo=r(KIr,"TFRemBertModel"),KIr.forEach(t),cPo=r(qFe," (RemBERT model)"),qFe.forEach(t),fPo=i(x),xv=n(x,"LI",{});var zFe=s(xv);lne=n(zFe,"STRONG",{});var YIr=s(lne);mPo=r(YIr,"roberta"),YIr.forEach(t),hPo=r(zFe," \u2014 "),MS=n(zFe,"A",{href:!0});var ZIr=s(MS);gPo=r(ZIr,"TFRobertaModel"),ZIr.forEach(t),uPo=r(zFe," (RoBERTa model)"),zFe.forEach(t),pPo=i(x),kv=n(x,"LI",{});var XFe=s(kv);ine=n(XFe,"STRONG",{});var eDr=s(ine);_Po=r(eDr,"roformer"),eDr.forEach(t),vPo=r(XFe," \u2014 "),yS=n(XFe,"A",{href:!0});var oDr=s(yS);bPo=r(oDr,"TFRoFormerModel"),oDr.forEach(t),TPo=r(XFe," (RoFormer model)"),XFe.forEach(t),FPo=i(x),Rv=n(x,"LI",{});var QFe=s(Rv);dne=n(QFe,"STRONG",{});var rDr=s(dne);EPo=r(rDr,"t5"),rDr.forEach(t),CPo=r(QFe," \u2014 "),wS=n(QFe,"A",{href:!0});var tDr=s(wS);MPo=r(tDr,"TFT5Model"),tDr.forEach(t),yPo=r(QFe," (T5 model)"),QFe.forEach(t),wPo=i(x),Pv=n(x,"LI",{});var VFe=s(Pv);cne=n(VFe,"STRONG",{});var aDr=s(cne);APo=r(aDr,"tapas"),aDr.forEach(t),LPo=r(VFe," \u2014 "),AS=n(VFe,"A",{href:!0});var nDr=s(AS);BPo=r(nDr,"TFTapasModel"),nDr.forEach(t),xPo=r(VFe," (TAPAS model)"),VFe.forEach(t),kPo=i(x),Sv=n(x,"LI",{});var WFe=s(Sv);fne=n(WFe,"STRONG",{});var sDr=s(fne);RPo=r(sDr,"transfo-xl"),sDr.forEach(t),PPo=r(WFe," \u2014 "),LS=n(WFe,"A",{href:!0});var lDr=s(LS);SPo=r(lDr,"TFTransfoXLModel"),lDr.forEach(t),$Po=r(WFe," (Transformer-XL model)"),WFe.forEach(t),IPo=i(x),$v=n(x,"LI",{});var HFe=s($v);mne=n(HFe,"STRONG",{});var iDr=s(mne);DPo=r(iDr,"vit"),iDr.forEach(t),NPo=r(HFe," \u2014 "),BS=n(HFe,"A",{href:!0});var dDr=s(BS);jPo=r(dDr,"TFViTModel"),dDr.forEach(t),OPo=r(HFe," (ViT model)"),HFe.forEach(t),GPo=i(x),Iv=n(x,"LI",{});var UFe=s(Iv);hne=n(UFe,"STRONG",{});var cDr=s(hne);qPo=r(cDr,"wav2vec2"),cDr.forEach(t),zPo=r(UFe," \u2014 "),xS=n(UFe,"A",{href:!0});var fDr=s(xS);XPo=r(fDr,"TFWav2Vec2Model"),fDr.forEach(t),QPo=r(UFe," (Wav2Vec2 model)"),UFe.forEach(t),VPo=i(x),Dv=n(x,"LI",{});var JFe=s(Dv);gne=n(JFe,"STRONG",{});var mDr=s(gne);WPo=r(mDr,"xlm"),mDr.forEach(t),HPo=r(JFe," \u2014 "),kS=n(JFe,"A",{href:!0});var hDr=s(kS);UPo=r(hDr,"TFXLMModel"),hDr.forEach(t),JPo=r(JFe," (XLM model)"),JFe.forEach(t),KPo=i(x),Nv=n(x,"LI",{});var KFe=s(Nv);une=n(KFe,"STRONG",{});var gDr=s(une);YPo=r(gDr,"xlm-roberta"),gDr.forEach(t),ZPo=r(KFe," \u2014 "),RS=n(KFe,"A",{href:!0});var uDr=s(RS);eSo=r(uDr,"TFXLMRobertaModel"),uDr.forEach(t),oSo=r(KFe," (XLM-RoBERTa model)"),KFe.forEach(t),rSo=i(x),jv=n(x,"LI",{});var YFe=s(jv);pne=n(YFe,"STRONG",{});var pDr=s(pne);tSo=r(pDr,"xlnet"),pDr.forEach(t),aSo=r(YFe," \u2014 "),PS=n(YFe,"A",{href:!0});var _Dr=s(PS);nSo=r(_Dr,"TFXLNetModel"),_Dr.forEach(t),sSo=r(YFe," (XLNet model)"),YFe.forEach(t),x.forEach(t),lSo=i(Vt),_ne=n(Vt,"P",{});var vDr=s(_ne);iSo=r(vDr,"Examples:"),vDr.forEach(t),dSo=i(Vt),m($5.$$.fragment,Vt),Vt.forEach(t),Js.forEach(t),dye=i(c),gd=n(c,"H2",{class:!0});var _7e=s(gd);Ov=n(_7e,"A",{id:!0,class:!0,href:!0});var bDr=s(Ov);vne=n(bDr,"SPAN",{});var TDr=s(vne);m(I5.$$.fragment,TDr),TDr.forEach(t),bDr.forEach(t),cSo=i(_7e),bne=n(_7e,"SPAN",{});var FDr=s(bne);fSo=r(FDr,"TFAutoModelForPreTraining"),FDr.forEach(t),_7e.forEach(t),cye=i(c),lr=n(c,"DIV",{class:!0});var Ys=s(lr);m(D5.$$.fragment,Ys),mSo=i(Ys),ud=n(Ys,"P",{});var qj=s(ud);hSo=r(qj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Tne=n(qj,"CODE",{});var EDr=s(Tne);gSo=r(EDr,"from_pretrained()"),EDr.forEach(t),uSo=r(qj,` class method or the
`),Fne=n(qj,"CODE",{});var CDr=s(Fne);pSo=r(CDr,"from_config()"),CDr.forEach(t),_So=r(qj," class method."),qj.forEach(t),vSo=i(Ys),N5=n(Ys,"P",{});var v7e=s(N5);bSo=r(v7e,"This class cannot be instantiated directly using "),Ene=n(v7e,"CODE",{});var MDr=s(Ene);TSo=r(MDr,"__init__()"),MDr.forEach(t),FSo=r(v7e," (throws an error)."),v7e.forEach(t),ESo=i(Ys),Jr=n(Ys,"DIV",{class:!0});var Zs=s(Jr);m(j5.$$.fragment,Zs),CSo=i(Zs),Cne=n(Zs,"P",{});var yDr=s(Cne);MSo=r(yDr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),yDr.forEach(t),ySo=i(Zs),pd=n(Zs,"P",{});var zj=s(pd);wSo=r(zj,`Note:
Loading a model from its configuration file does `),Mne=n(zj,"STRONG",{});var wDr=s(Mne);ASo=r(wDr,"not"),wDr.forEach(t),LSo=r(zj,` load the model weights. It only affects the
model\u2019s configuration. Use `),yne=n(zj,"CODE",{});var ADr=s(yne);BSo=r(ADr,"from_pretrained()"),ADr.forEach(t),xSo=r(zj,` to load the model
weights.`),zj.forEach(t),kSo=i(Zs),wne=n(Zs,"P",{});var LDr=s(wne);RSo=r(LDr,"Examples:"),LDr.forEach(t),PSo=i(Zs),m(O5.$$.fragment,Zs),Zs.forEach(t),SSo=i(Ys),io=n(Ys,"DIV",{class:!0});var Wt=s(io);m(G5.$$.fragment,Wt),$So=i(Wt),Ane=n(Wt,"P",{});var BDr=s(Ane);ISo=r(BDr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),BDr.forEach(t),DSo=i(Wt),Qa=n(Wt,"P",{});var JE=s(Qa);NSo=r(JE,"The model class to instantiate is selected based on the "),Lne=n(JE,"CODE",{});var xDr=s(Lne);jSo=r(xDr,"model_type"),xDr.forEach(t),OSo=r(JE,` property of the config object (either
passed as an argument or loaded from `),Bne=n(JE,"CODE",{});var kDr=s(Bne);GSo=r(kDr,"pretrained_model_name_or_path"),kDr.forEach(t),qSo=r(JE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),xne=n(JE,"CODE",{});var RDr=s(xne);zSo=r(RDr,"pretrained_model_name_or_path"),RDr.forEach(t),XSo=r(JE,":"),JE.forEach(t),QSo=i(Wt),K=n(Wt,"UL",{});var ee=s(K);Gv=n(ee,"LI",{});var ZFe=s(Gv);kne=n(ZFe,"STRONG",{});var PDr=s(kne);VSo=r(PDr,"albert"),PDr.forEach(t),WSo=r(ZFe," \u2014 "),SS=n(ZFe,"A",{href:!0});var SDr=s(SS);HSo=r(SDr,"TFAlbertForPreTraining"),SDr.forEach(t),USo=r(ZFe," (ALBERT model)"),ZFe.forEach(t),JSo=i(ee),qv=n(ee,"LI",{});var eEe=s(qv);Rne=n(eEe,"STRONG",{});var $Dr=s(Rne);KSo=r($Dr,"bart"),$Dr.forEach(t),YSo=r(eEe," \u2014 "),$S=n(eEe,"A",{href:!0});var IDr=s($S);ZSo=r(IDr,"TFBartForConditionalGeneration"),IDr.forEach(t),e$o=r(eEe," (BART model)"),eEe.forEach(t),o$o=i(ee),zv=n(ee,"LI",{});var oEe=s(zv);Pne=n(oEe,"STRONG",{});var DDr=s(Pne);r$o=r(DDr,"bert"),DDr.forEach(t),t$o=r(oEe," \u2014 "),IS=n(oEe,"A",{href:!0});var NDr=s(IS);a$o=r(NDr,"TFBertForPreTraining"),NDr.forEach(t),n$o=r(oEe," (BERT model)"),oEe.forEach(t),s$o=i(ee),Xv=n(ee,"LI",{});var rEe=s(Xv);Sne=n(rEe,"STRONG",{});var jDr=s(Sne);l$o=r(jDr,"camembert"),jDr.forEach(t),i$o=r(rEe," \u2014 "),DS=n(rEe,"A",{href:!0});var ODr=s(DS);d$o=r(ODr,"TFCamembertForMaskedLM"),ODr.forEach(t),c$o=r(rEe," (CamemBERT model)"),rEe.forEach(t),f$o=i(ee),Qv=n(ee,"LI",{});var tEe=s(Qv);$ne=n(tEe,"STRONG",{});var GDr=s($ne);m$o=r(GDr,"ctrl"),GDr.forEach(t),h$o=r(tEe," \u2014 "),NS=n(tEe,"A",{href:!0});var qDr=s(NS);g$o=r(qDr,"TFCTRLLMHeadModel"),qDr.forEach(t),u$o=r(tEe," (CTRL model)"),tEe.forEach(t),p$o=i(ee),Vv=n(ee,"LI",{});var aEe=s(Vv);Ine=n(aEe,"STRONG",{});var zDr=s(Ine);_$o=r(zDr,"distilbert"),zDr.forEach(t),v$o=r(aEe," \u2014 "),jS=n(aEe,"A",{href:!0});var XDr=s(jS);b$o=r(XDr,"TFDistilBertForMaskedLM"),XDr.forEach(t),T$o=r(aEe," (DistilBERT model)"),aEe.forEach(t),F$o=i(ee),Wv=n(ee,"LI",{});var nEe=s(Wv);Dne=n(nEe,"STRONG",{});var QDr=s(Dne);E$o=r(QDr,"electra"),QDr.forEach(t),C$o=r(nEe," \u2014 "),OS=n(nEe,"A",{href:!0});var VDr=s(OS);M$o=r(VDr,"TFElectraForPreTraining"),VDr.forEach(t),y$o=r(nEe," (ELECTRA model)"),nEe.forEach(t),w$o=i(ee),Hv=n(ee,"LI",{});var sEe=s(Hv);Nne=n(sEe,"STRONG",{});var WDr=s(Nne);A$o=r(WDr,"flaubert"),WDr.forEach(t),L$o=r(sEe," \u2014 "),GS=n(sEe,"A",{href:!0});var HDr=s(GS);B$o=r(HDr,"TFFlaubertWithLMHeadModel"),HDr.forEach(t),x$o=r(sEe," (FlauBERT model)"),sEe.forEach(t),k$o=i(ee),Uv=n(ee,"LI",{});var lEe=s(Uv);jne=n(lEe,"STRONG",{});var UDr=s(jne);R$o=r(UDr,"funnel"),UDr.forEach(t),P$o=r(lEe," \u2014 "),qS=n(lEe,"A",{href:!0});var JDr=s(qS);S$o=r(JDr,"TFFunnelForPreTraining"),JDr.forEach(t),$$o=r(lEe," (Funnel Transformer model)"),lEe.forEach(t),I$o=i(ee),Jv=n(ee,"LI",{});var iEe=s(Jv);One=n(iEe,"STRONG",{});var KDr=s(One);D$o=r(KDr,"gpt2"),KDr.forEach(t),N$o=r(iEe," \u2014 "),zS=n(iEe,"A",{href:!0});var YDr=s(zS);j$o=r(YDr,"TFGPT2LMHeadModel"),YDr.forEach(t),O$o=r(iEe," (OpenAI GPT-2 model)"),iEe.forEach(t),G$o=i(ee),Kv=n(ee,"LI",{});var dEe=s(Kv);Gne=n(dEe,"STRONG",{});var ZDr=s(Gne);q$o=r(ZDr,"layoutlm"),ZDr.forEach(t),z$o=r(dEe," \u2014 "),XS=n(dEe,"A",{href:!0});var eNr=s(XS);X$o=r(eNr,"TFLayoutLMForMaskedLM"),eNr.forEach(t),Q$o=r(dEe," (LayoutLM model)"),dEe.forEach(t),V$o=i(ee),Yv=n(ee,"LI",{});var cEe=s(Yv);qne=n(cEe,"STRONG",{});var oNr=s(qne);W$o=r(oNr,"lxmert"),oNr.forEach(t),H$o=r(cEe," \u2014 "),QS=n(cEe,"A",{href:!0});var rNr=s(QS);U$o=r(rNr,"TFLxmertForPreTraining"),rNr.forEach(t),J$o=r(cEe," (LXMERT model)"),cEe.forEach(t),K$o=i(ee),Zv=n(ee,"LI",{});var fEe=s(Zv);zne=n(fEe,"STRONG",{});var tNr=s(zne);Y$o=r(tNr,"mobilebert"),tNr.forEach(t),Z$o=r(fEe," \u2014 "),VS=n(fEe,"A",{href:!0});var aNr=s(VS);eIo=r(aNr,"TFMobileBertForPreTraining"),aNr.forEach(t),oIo=r(fEe," (MobileBERT model)"),fEe.forEach(t),rIo=i(ee),eb=n(ee,"LI",{});var mEe=s(eb);Xne=n(mEe,"STRONG",{});var nNr=s(Xne);tIo=r(nNr,"mpnet"),nNr.forEach(t),aIo=r(mEe," \u2014 "),WS=n(mEe,"A",{href:!0});var sNr=s(WS);nIo=r(sNr,"TFMPNetForMaskedLM"),sNr.forEach(t),sIo=r(mEe," (MPNet model)"),mEe.forEach(t),lIo=i(ee),ob=n(ee,"LI",{});var hEe=s(ob);Qne=n(hEe,"STRONG",{});var lNr=s(Qne);iIo=r(lNr,"openai-gpt"),lNr.forEach(t),dIo=r(hEe," \u2014 "),HS=n(hEe,"A",{href:!0});var iNr=s(HS);cIo=r(iNr,"TFOpenAIGPTLMHeadModel"),iNr.forEach(t),fIo=r(hEe," (OpenAI GPT model)"),hEe.forEach(t),mIo=i(ee),rb=n(ee,"LI",{});var gEe=s(rb);Vne=n(gEe,"STRONG",{});var dNr=s(Vne);hIo=r(dNr,"roberta"),dNr.forEach(t),gIo=r(gEe," \u2014 "),US=n(gEe,"A",{href:!0});var cNr=s(US);uIo=r(cNr,"TFRobertaForMaskedLM"),cNr.forEach(t),pIo=r(gEe," (RoBERTa model)"),gEe.forEach(t),_Io=i(ee),tb=n(ee,"LI",{});var uEe=s(tb);Wne=n(uEe,"STRONG",{});var fNr=s(Wne);vIo=r(fNr,"t5"),fNr.forEach(t),bIo=r(uEe," \u2014 "),JS=n(uEe,"A",{href:!0});var mNr=s(JS);TIo=r(mNr,"TFT5ForConditionalGeneration"),mNr.forEach(t),FIo=r(uEe," (T5 model)"),uEe.forEach(t),EIo=i(ee),ab=n(ee,"LI",{});var pEe=s(ab);Hne=n(pEe,"STRONG",{});var hNr=s(Hne);CIo=r(hNr,"tapas"),hNr.forEach(t),MIo=r(pEe," \u2014 "),KS=n(pEe,"A",{href:!0});var gNr=s(KS);yIo=r(gNr,"TFTapasForMaskedLM"),gNr.forEach(t),wIo=r(pEe," (TAPAS model)"),pEe.forEach(t),AIo=i(ee),nb=n(ee,"LI",{});var _Ee=s(nb);Une=n(_Ee,"STRONG",{});var uNr=s(Une);LIo=r(uNr,"transfo-xl"),uNr.forEach(t),BIo=r(_Ee," \u2014 "),YS=n(_Ee,"A",{href:!0});var pNr=s(YS);xIo=r(pNr,"TFTransfoXLLMHeadModel"),pNr.forEach(t),kIo=r(_Ee," (Transformer-XL model)"),_Ee.forEach(t),RIo=i(ee),sb=n(ee,"LI",{});var vEe=s(sb);Jne=n(vEe,"STRONG",{});var _Nr=s(Jne);PIo=r(_Nr,"xlm"),_Nr.forEach(t),SIo=r(vEe," \u2014 "),ZS=n(vEe,"A",{href:!0});var vNr=s(ZS);$Io=r(vNr,"TFXLMWithLMHeadModel"),vNr.forEach(t),IIo=r(vEe," (XLM model)"),vEe.forEach(t),DIo=i(ee),lb=n(ee,"LI",{});var bEe=s(lb);Kne=n(bEe,"STRONG",{});var bNr=s(Kne);NIo=r(bNr,"xlm-roberta"),bNr.forEach(t),jIo=r(bEe," \u2014 "),e$=n(bEe,"A",{href:!0});var TNr=s(e$);OIo=r(TNr,"TFXLMRobertaForMaskedLM"),TNr.forEach(t),GIo=r(bEe," (XLM-RoBERTa model)"),bEe.forEach(t),qIo=i(ee),ib=n(ee,"LI",{});var TEe=s(ib);Yne=n(TEe,"STRONG",{});var FNr=s(Yne);zIo=r(FNr,"xlnet"),FNr.forEach(t),XIo=r(TEe," \u2014 "),o$=n(TEe,"A",{href:!0});var ENr=s(o$);QIo=r(ENr,"TFXLNetLMHeadModel"),ENr.forEach(t),VIo=r(TEe," (XLNet model)"),TEe.forEach(t),ee.forEach(t),WIo=i(Wt),Zne=n(Wt,"P",{});var CNr=s(Zne);HIo=r(CNr,"Examples:"),CNr.forEach(t),UIo=i(Wt),m(q5.$$.fragment,Wt),Wt.forEach(t),Ys.forEach(t),fye=i(c),_d=n(c,"H2",{class:!0});var b7e=s(_d);db=n(b7e,"A",{id:!0,class:!0,href:!0});var MNr=s(db);ese=n(MNr,"SPAN",{});var yNr=s(ese);m(z5.$$.fragment,yNr),yNr.forEach(t),MNr.forEach(t),JIo=i(b7e),ose=n(b7e,"SPAN",{});var wNr=s(ose);KIo=r(wNr,"TFAutoModelForCausalLM"),wNr.forEach(t),b7e.forEach(t),mye=i(c),ir=n(c,"DIV",{class:!0});var el=s(ir);m(X5.$$.fragment,el),YIo=i(el),vd=n(el,"P",{});var Xj=s(vd);ZIo=r(Xj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),rse=n(Xj,"CODE",{});var ANr=s(rse);eDo=r(ANr,"from_pretrained()"),ANr.forEach(t),oDo=r(Xj,` class method or the
`),tse=n(Xj,"CODE",{});var LNr=s(tse);rDo=r(LNr,"from_config()"),LNr.forEach(t),tDo=r(Xj," class method."),Xj.forEach(t),aDo=i(el),Q5=n(el,"P",{});var T7e=s(Q5);nDo=r(T7e,"This class cannot be instantiated directly using "),ase=n(T7e,"CODE",{});var BNr=s(ase);sDo=r(BNr,"__init__()"),BNr.forEach(t),lDo=r(T7e," (throws an error)."),T7e.forEach(t),iDo=i(el),Kr=n(el,"DIV",{class:!0});var ol=s(Kr);m(V5.$$.fragment,ol),dDo=i(ol),nse=n(ol,"P",{});var xNr=s(nse);cDo=r(xNr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),xNr.forEach(t),fDo=i(ol),bd=n(ol,"P",{});var Qj=s(bd);mDo=r(Qj,`Note:
Loading a model from its configuration file does `),sse=n(Qj,"STRONG",{});var kNr=s(sse);hDo=r(kNr,"not"),kNr.forEach(t),gDo=r(Qj,` load the model weights. It only affects the
model\u2019s configuration. Use `),lse=n(Qj,"CODE",{});var RNr=s(lse);uDo=r(RNr,"from_pretrained()"),RNr.forEach(t),pDo=r(Qj,` to load the model
weights.`),Qj.forEach(t),_Do=i(ol),ise=n(ol,"P",{});var PNr=s(ise);vDo=r(PNr,"Examples:"),PNr.forEach(t),bDo=i(ol),m(W5.$$.fragment,ol),ol.forEach(t),TDo=i(el),co=n(el,"DIV",{class:!0});var Ht=s(co);m(H5.$$.fragment,Ht),FDo=i(Ht),dse=n(Ht,"P",{});var SNr=s(dse);EDo=r(SNr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),SNr.forEach(t),CDo=i(Ht),Va=n(Ht,"P",{});var KE=s(Va);MDo=r(KE,"The model class to instantiate is selected based on the "),cse=n(KE,"CODE",{});var $Nr=s(cse);yDo=r($Nr,"model_type"),$Nr.forEach(t),wDo=r(KE,` property of the config object (either
passed as an argument or loaded from `),fse=n(KE,"CODE",{});var INr=s(fse);ADo=r(INr,"pretrained_model_name_or_path"),INr.forEach(t),LDo=r(KE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),mse=n(KE,"CODE",{});var DNr=s(mse);BDo=r(DNr,"pretrained_model_name_or_path"),DNr.forEach(t),xDo=r(KE,":"),KE.forEach(t),kDo=i(Ht),ve=n(Ht,"UL",{});var Ae=s(ve);cb=n(Ae,"LI",{});var FEe=s(cb);hse=n(FEe,"STRONG",{});var NNr=s(hse);RDo=r(NNr,"bert"),NNr.forEach(t),PDo=r(FEe," \u2014 "),r$=n(FEe,"A",{href:!0});var jNr=s(r$);SDo=r(jNr,"TFBertLMHeadModel"),jNr.forEach(t),$Do=r(FEe," (BERT model)"),FEe.forEach(t),IDo=i(Ae),fb=n(Ae,"LI",{});var EEe=s(fb);gse=n(EEe,"STRONG",{});var ONr=s(gse);DDo=r(ONr,"ctrl"),ONr.forEach(t),NDo=r(EEe," \u2014 "),t$=n(EEe,"A",{href:!0});var GNr=s(t$);jDo=r(GNr,"TFCTRLLMHeadModel"),GNr.forEach(t),ODo=r(EEe," (CTRL model)"),EEe.forEach(t),GDo=i(Ae),mb=n(Ae,"LI",{});var CEe=s(mb);use=n(CEe,"STRONG",{});var qNr=s(use);qDo=r(qNr,"gpt2"),qNr.forEach(t),zDo=r(CEe," \u2014 "),a$=n(CEe,"A",{href:!0});var zNr=s(a$);XDo=r(zNr,"TFGPT2LMHeadModel"),zNr.forEach(t),QDo=r(CEe," (OpenAI GPT-2 model)"),CEe.forEach(t),VDo=i(Ae),hb=n(Ae,"LI",{});var MEe=s(hb);pse=n(MEe,"STRONG",{});var XNr=s(pse);WDo=r(XNr,"openai-gpt"),XNr.forEach(t),HDo=r(MEe," \u2014 "),n$=n(MEe,"A",{href:!0});var QNr=s(n$);UDo=r(QNr,"TFOpenAIGPTLMHeadModel"),QNr.forEach(t),JDo=r(MEe," (OpenAI GPT model)"),MEe.forEach(t),KDo=i(Ae),gb=n(Ae,"LI",{});var yEe=s(gb);_se=n(yEe,"STRONG",{});var VNr=s(_se);YDo=r(VNr,"rembert"),VNr.forEach(t),ZDo=r(yEe," \u2014 "),s$=n(yEe,"A",{href:!0});var WNr=s(s$);eNo=r(WNr,"TFRemBertForCausalLM"),WNr.forEach(t),oNo=r(yEe," (RemBERT model)"),yEe.forEach(t),rNo=i(Ae),ub=n(Ae,"LI",{});var wEe=s(ub);vse=n(wEe,"STRONG",{});var HNr=s(vse);tNo=r(HNr,"roberta"),HNr.forEach(t),aNo=r(wEe," \u2014 "),l$=n(wEe,"A",{href:!0});var UNr=s(l$);nNo=r(UNr,"TFRobertaForCausalLM"),UNr.forEach(t),sNo=r(wEe," (RoBERTa model)"),wEe.forEach(t),lNo=i(Ae),pb=n(Ae,"LI",{});var AEe=s(pb);bse=n(AEe,"STRONG",{});var JNr=s(bse);iNo=r(JNr,"roformer"),JNr.forEach(t),dNo=r(AEe," \u2014 "),i$=n(AEe,"A",{href:!0});var KNr=s(i$);cNo=r(KNr,"TFRoFormerForCausalLM"),KNr.forEach(t),fNo=r(AEe," (RoFormer model)"),AEe.forEach(t),mNo=i(Ae),_b=n(Ae,"LI",{});var LEe=s(_b);Tse=n(LEe,"STRONG",{});var YNr=s(Tse);hNo=r(YNr,"transfo-xl"),YNr.forEach(t),gNo=r(LEe," \u2014 "),d$=n(LEe,"A",{href:!0});var ZNr=s(d$);uNo=r(ZNr,"TFTransfoXLLMHeadModel"),ZNr.forEach(t),pNo=r(LEe," (Transformer-XL model)"),LEe.forEach(t),_No=i(Ae),vb=n(Ae,"LI",{});var BEe=s(vb);Fse=n(BEe,"STRONG",{});var ejr=s(Fse);vNo=r(ejr,"xlm"),ejr.forEach(t),bNo=r(BEe," \u2014 "),c$=n(BEe,"A",{href:!0});var ojr=s(c$);TNo=r(ojr,"TFXLMWithLMHeadModel"),ojr.forEach(t),FNo=r(BEe," (XLM model)"),BEe.forEach(t),ENo=i(Ae),bb=n(Ae,"LI",{});var xEe=s(bb);Ese=n(xEe,"STRONG",{});var rjr=s(Ese);CNo=r(rjr,"xlnet"),rjr.forEach(t),MNo=r(xEe," \u2014 "),f$=n(xEe,"A",{href:!0});var tjr=s(f$);yNo=r(tjr,"TFXLNetLMHeadModel"),tjr.forEach(t),wNo=r(xEe," (XLNet model)"),xEe.forEach(t),Ae.forEach(t),ANo=i(Ht),Cse=n(Ht,"P",{});var ajr=s(Cse);LNo=r(ajr,"Examples:"),ajr.forEach(t),BNo=i(Ht),m(U5.$$.fragment,Ht),Ht.forEach(t),el.forEach(t),hye=i(c),Td=n(c,"H2",{class:!0});var F7e=s(Td);Tb=n(F7e,"A",{id:!0,class:!0,href:!0});var njr=s(Tb);Mse=n(njr,"SPAN",{});var sjr=s(Mse);m(J5.$$.fragment,sjr),sjr.forEach(t),njr.forEach(t),xNo=i(F7e),yse=n(F7e,"SPAN",{});var ljr=s(yse);kNo=r(ljr,"TFAutoModelForImageClassification"),ljr.forEach(t),F7e.forEach(t),gye=i(c),dr=n(c,"DIV",{class:!0});var rl=s(dr);m(K5.$$.fragment,rl),RNo=i(rl),Fd=n(rl,"P",{});var Vj=s(Fd);PNo=r(Vj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),wse=n(Vj,"CODE",{});var ijr=s(wse);SNo=r(ijr,"from_pretrained()"),ijr.forEach(t),$No=r(Vj,` class method or the
`),Ase=n(Vj,"CODE",{});var djr=s(Ase);INo=r(djr,"from_config()"),djr.forEach(t),DNo=r(Vj," class method."),Vj.forEach(t),NNo=i(rl),Y5=n(rl,"P",{});var E7e=s(Y5);jNo=r(E7e,"This class cannot be instantiated directly using "),Lse=n(E7e,"CODE",{});var cjr=s(Lse);ONo=r(cjr,"__init__()"),cjr.forEach(t),GNo=r(E7e," (throws an error)."),E7e.forEach(t),qNo=i(rl),Yr=n(rl,"DIV",{class:!0});var tl=s(Yr);m(Z5.$$.fragment,tl),zNo=i(tl),Bse=n(tl,"P",{});var fjr=s(Bse);XNo=r(fjr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),fjr.forEach(t),QNo=i(tl),Ed=n(tl,"P",{});var Wj=s(Ed);VNo=r(Wj,`Note:
Loading a model from its configuration file does `),xse=n(Wj,"STRONG",{});var mjr=s(xse);WNo=r(mjr,"not"),mjr.forEach(t),HNo=r(Wj,` load the model weights. It only affects the
model\u2019s configuration. Use `),kse=n(Wj,"CODE",{});var hjr=s(kse);UNo=r(hjr,"from_pretrained()"),hjr.forEach(t),JNo=r(Wj,` to load the model
weights.`),Wj.forEach(t),KNo=i(tl),Rse=n(tl,"P",{});var gjr=s(Rse);YNo=r(gjr,"Examples:"),gjr.forEach(t),ZNo=i(tl),m(ey.$$.fragment,tl),tl.forEach(t),ejo=i(rl),fo=n(rl,"DIV",{class:!0});var Ut=s(fo);m(oy.$$.fragment,Ut),ojo=i(Ut),Pse=n(Ut,"P",{});var ujr=s(Pse);rjo=r(ujr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ujr.forEach(t),tjo=i(Ut),Wa=n(Ut,"P",{});var YE=s(Wa);ajo=r(YE,"The model class to instantiate is selected based on the "),Sse=n(YE,"CODE",{});var pjr=s(Sse);njo=r(pjr,"model_type"),pjr.forEach(t),sjo=r(YE,` property of the config object (either
passed as an argument or loaded from `),$se=n(YE,"CODE",{});var _jr=s($se);ljo=r(_jr,"pretrained_model_name_or_path"),_jr.forEach(t),ijo=r(YE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ise=n(YE,"CODE",{});var vjr=s(Ise);djo=r(vjr,"pretrained_model_name_or_path"),vjr.forEach(t),cjo=r(YE,":"),YE.forEach(t),fjo=i(Ut),Dse=n(Ut,"UL",{});var bjr=s(Dse);Fb=n(bjr,"LI",{});var kEe=s(Fb);Nse=n(kEe,"STRONG",{});var Tjr=s(Nse);mjo=r(Tjr,"vit"),Tjr.forEach(t),hjo=r(kEe," \u2014 "),m$=n(kEe,"A",{href:!0});var Fjr=s(m$);gjo=r(Fjr,"TFViTForImageClassification"),Fjr.forEach(t),ujo=r(kEe," (ViT model)"),kEe.forEach(t),bjr.forEach(t),pjo=i(Ut),jse=n(Ut,"P",{});var Ejr=s(jse);_jo=r(Ejr,"Examples:"),Ejr.forEach(t),vjo=i(Ut),m(ry.$$.fragment,Ut),Ut.forEach(t),rl.forEach(t),uye=i(c),Cd=n(c,"H2",{class:!0});var C7e=s(Cd);Eb=n(C7e,"A",{id:!0,class:!0,href:!0});var Cjr=s(Eb);Ose=n(Cjr,"SPAN",{});var Mjr=s(Ose);m(ty.$$.fragment,Mjr),Mjr.forEach(t),Cjr.forEach(t),bjo=i(C7e),Gse=n(C7e,"SPAN",{});var yjr=s(Gse);Tjo=r(yjr,"TFAutoModelForMaskedLM"),yjr.forEach(t),C7e.forEach(t),pye=i(c),cr=n(c,"DIV",{class:!0});var al=s(cr);m(ay.$$.fragment,al),Fjo=i(al),Md=n(al,"P",{});var Hj=s(Md);Ejo=r(Hj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),qse=n(Hj,"CODE",{});var wjr=s(qse);Cjo=r(wjr,"from_pretrained()"),wjr.forEach(t),Mjo=r(Hj,` class method or the
`),zse=n(Hj,"CODE",{});var Ajr=s(zse);yjo=r(Ajr,"from_config()"),Ajr.forEach(t),wjo=r(Hj," class method."),Hj.forEach(t),Ajo=i(al),ny=n(al,"P",{});var M7e=s(ny);Ljo=r(M7e,"This class cannot be instantiated directly using "),Xse=n(M7e,"CODE",{});var Ljr=s(Xse);Bjo=r(Ljr,"__init__()"),Ljr.forEach(t),xjo=r(M7e," (throws an error)."),M7e.forEach(t),kjo=i(al),Zr=n(al,"DIV",{class:!0});var nl=s(Zr);m(sy.$$.fragment,nl),Rjo=i(nl),Qse=n(nl,"P",{});var Bjr=s(Qse);Pjo=r(Bjr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Bjr.forEach(t),Sjo=i(nl),yd=n(nl,"P",{});var Uj=s(yd);$jo=r(Uj,`Note:
Loading a model from its configuration file does `),Vse=n(Uj,"STRONG",{});var xjr=s(Vse);Ijo=r(xjr,"not"),xjr.forEach(t),Djo=r(Uj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=n(Uj,"CODE",{});var kjr=s(Wse);Njo=r(kjr,"from_pretrained()"),kjr.forEach(t),jjo=r(Uj,` to load the model
weights.`),Uj.forEach(t),Ojo=i(nl),Hse=n(nl,"P",{});var Rjr=s(Hse);Gjo=r(Rjr,"Examples:"),Rjr.forEach(t),qjo=i(nl),m(ly.$$.fragment,nl),nl.forEach(t),zjo=i(al),mo=n(al,"DIV",{class:!0});var Jt=s(mo);m(iy.$$.fragment,Jt),Xjo=i(Jt),Use=n(Jt,"P",{});var Pjr=s(Use);Qjo=r(Pjr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Pjr.forEach(t),Vjo=i(Jt),Ha=n(Jt,"P",{});var ZE=s(Ha);Wjo=r(ZE,"The model class to instantiate is selected based on the "),Jse=n(ZE,"CODE",{});var Sjr=s(Jse);Hjo=r(Sjr,"model_type"),Sjr.forEach(t),Ujo=r(ZE,` property of the config object (either
passed as an argument or loaded from `),Kse=n(ZE,"CODE",{});var $jr=s(Kse);Jjo=r($jr,"pretrained_model_name_or_path"),$jr.forEach(t),Kjo=r(ZE,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Yse=n(ZE,"CODE",{});var Ijr=s(Yse);Yjo=r(Ijr,"pretrained_model_name_or_path"),Ijr.forEach(t),Zjo=r(ZE,":"),ZE.forEach(t),eOo=i(Jt),te=n(Jt,"UL",{});var se=s(te);Cb=n(se,"LI",{});var REe=s(Cb);Zse=n(REe,"STRONG",{});var Djr=s(Zse);oOo=r(Djr,"albert"),Djr.forEach(t),rOo=r(REe," \u2014 "),h$=n(REe,"A",{href:!0});var Njr=s(h$);tOo=r(Njr,"TFAlbertForMaskedLM"),Njr.forEach(t),aOo=r(REe," (ALBERT model)"),REe.forEach(t),nOo=i(se),Mb=n(se,"LI",{});var PEe=s(Mb);ele=n(PEe,"STRONG",{});var jjr=s(ele);sOo=r(jjr,"bert"),jjr.forEach(t),lOo=r(PEe," \u2014 "),g$=n(PEe,"A",{href:!0});var Ojr=s(g$);iOo=r(Ojr,"TFBertForMaskedLM"),Ojr.forEach(t),dOo=r(PEe," (BERT model)"),PEe.forEach(t),cOo=i(se),yb=n(se,"LI",{});var SEe=s(yb);ole=n(SEe,"STRONG",{});var Gjr=s(ole);fOo=r(Gjr,"camembert"),Gjr.forEach(t),mOo=r(SEe," \u2014 "),u$=n(SEe,"A",{href:!0});var qjr=s(u$);hOo=r(qjr,"TFCamembertForMaskedLM"),qjr.forEach(t),gOo=r(SEe," (CamemBERT model)"),SEe.forEach(t),uOo=i(se),wb=n(se,"LI",{});var $Ee=s(wb);rle=n($Ee,"STRONG",{});var zjr=s(rle);pOo=r(zjr,"convbert"),zjr.forEach(t),_Oo=r($Ee," \u2014 "),p$=n($Ee,"A",{href:!0});var Xjr=s(p$);vOo=r(Xjr,"TFConvBertForMaskedLM"),Xjr.forEach(t),bOo=r($Ee," (ConvBERT model)"),$Ee.forEach(t),TOo=i(se),Ab=n(se,"LI",{});var IEe=s(Ab);tle=n(IEe,"STRONG",{});var Qjr=s(tle);FOo=r(Qjr,"deberta"),Qjr.forEach(t),EOo=r(IEe," \u2014 "),_$=n(IEe,"A",{href:!0});var Vjr=s(_$);COo=r(Vjr,"TFDebertaForMaskedLM"),Vjr.forEach(t),MOo=r(IEe," (DeBERTa model)"),IEe.forEach(t),yOo=i(se),Lb=n(se,"LI",{});var DEe=s(Lb);ale=n(DEe,"STRONG",{});var Wjr=s(ale);wOo=r(Wjr,"deberta-v2"),Wjr.forEach(t),AOo=r(DEe," \u2014 "),v$=n(DEe,"A",{href:!0});var Hjr=s(v$);LOo=r(Hjr,"TFDebertaV2ForMaskedLM"),Hjr.forEach(t),BOo=r(DEe," (DeBERTa-v2 model)"),DEe.forEach(t),xOo=i(se),Bb=n(se,"LI",{});var NEe=s(Bb);nle=n(NEe,"STRONG",{});var Ujr=s(nle);kOo=r(Ujr,"distilbert"),Ujr.forEach(t),ROo=r(NEe," \u2014 "),b$=n(NEe,"A",{href:!0});var Jjr=s(b$);POo=r(Jjr,"TFDistilBertForMaskedLM"),Jjr.forEach(t),SOo=r(NEe," (DistilBERT model)"),NEe.forEach(t),$Oo=i(se),xb=n(se,"LI",{});var jEe=s(xb);sle=n(jEe,"STRONG",{});var Kjr=s(sle);IOo=r(Kjr,"electra"),Kjr.forEach(t),DOo=r(jEe," \u2014 "),T$=n(jEe,"A",{href:!0});var Yjr=s(T$);NOo=r(Yjr,"TFElectraForMaskedLM"),Yjr.forEach(t),jOo=r(jEe," (ELECTRA model)"),jEe.forEach(t),OOo=i(se),kb=n(se,"LI",{});var OEe=s(kb);lle=n(OEe,"STRONG",{});var Zjr=s(lle);GOo=r(Zjr,"flaubert"),Zjr.forEach(t),qOo=r(OEe," \u2014 "),F$=n(OEe,"A",{href:!0});var eOr=s(F$);zOo=r(eOr,"TFFlaubertWithLMHeadModel"),eOr.forEach(t),XOo=r(OEe," (FlauBERT model)"),OEe.forEach(t),QOo=i(se),Rb=n(se,"LI",{});var GEe=s(Rb);ile=n(GEe,"STRONG",{});var oOr=s(ile);VOo=r(oOr,"funnel"),oOr.forEach(t),WOo=r(GEe," \u2014 "),E$=n(GEe,"A",{href:!0});var rOr=s(E$);HOo=r(rOr,"TFFunnelForMaskedLM"),rOr.forEach(t),UOo=r(GEe," (Funnel Transformer model)"),GEe.forEach(t),JOo=i(se),Pb=n(se,"LI",{});var qEe=s(Pb);dle=n(qEe,"STRONG",{});var tOr=s(dle);KOo=r(tOr,"layoutlm"),tOr.forEach(t),YOo=r(qEe," \u2014 "),C$=n(qEe,"A",{href:!0});var aOr=s(C$);ZOo=r(aOr,"TFLayoutLMForMaskedLM"),aOr.forEach(t),eGo=r(qEe," (LayoutLM model)"),qEe.forEach(t),oGo=i(se),Sb=n(se,"LI",{});var zEe=s(Sb);cle=n(zEe,"STRONG",{});var nOr=s(cle);rGo=r(nOr,"longformer"),nOr.forEach(t),tGo=r(zEe," \u2014 "),M$=n(zEe,"A",{href:!0});var sOr=s(M$);aGo=r(sOr,"TFLongformerForMaskedLM"),sOr.forEach(t),nGo=r(zEe," (Longformer model)"),zEe.forEach(t),sGo=i(se),$b=n(se,"LI",{});var XEe=s($b);fle=n(XEe,"STRONG",{});var lOr=s(fle);lGo=r(lOr,"mobilebert"),lOr.forEach(t),iGo=r(XEe," \u2014 "),y$=n(XEe,"A",{href:!0});var iOr=s(y$);dGo=r(iOr,"TFMobileBertForMaskedLM"),iOr.forEach(t),cGo=r(XEe," (MobileBERT model)"),XEe.forEach(t),fGo=i(se),Ib=n(se,"LI",{});var QEe=s(Ib);mle=n(QEe,"STRONG",{});var dOr=s(mle);mGo=r(dOr,"mpnet"),dOr.forEach(t),hGo=r(QEe," \u2014 "),w$=n(QEe,"A",{href:!0});var cOr=s(w$);gGo=r(cOr,"TFMPNetForMaskedLM"),cOr.forEach(t),uGo=r(QEe," (MPNet model)"),QEe.forEach(t),pGo=i(se),Db=n(se,"LI",{});var VEe=s(Db);hle=n(VEe,"STRONG",{});var fOr=s(hle);_Go=r(fOr,"rembert"),fOr.forEach(t),vGo=r(VEe," \u2014 "),A$=n(VEe,"A",{href:!0});var mOr=s(A$);bGo=r(mOr,"TFRemBertForMaskedLM"),mOr.forEach(t),TGo=r(VEe," (RemBERT model)"),VEe.forEach(t),FGo=i(se),Nb=n(se,"LI",{});var WEe=s(Nb);gle=n(WEe,"STRONG",{});var hOr=s(gle);EGo=r(hOr,"roberta"),hOr.forEach(t),CGo=r(WEe," \u2014 "),L$=n(WEe,"A",{href:!0});var gOr=s(L$);MGo=r(gOr,"TFRobertaForMaskedLM"),gOr.forEach(t),yGo=r(WEe," (RoBERTa model)"),WEe.forEach(t),wGo=i(se),jb=n(se,"LI",{});var HEe=s(jb);ule=n(HEe,"STRONG",{});var uOr=s(ule);AGo=r(uOr,"roformer"),uOr.forEach(t),LGo=r(HEe," \u2014 "),B$=n(HEe,"A",{href:!0});var pOr=s(B$);BGo=r(pOr,"TFRoFormerForMaskedLM"),pOr.forEach(t),xGo=r(HEe," (RoFormer model)"),HEe.forEach(t),kGo=i(se),Ob=n(se,"LI",{});var UEe=s(Ob);ple=n(UEe,"STRONG",{});var _Or=s(ple);RGo=r(_Or,"tapas"),_Or.forEach(t),PGo=r(UEe," \u2014 "),x$=n(UEe,"A",{href:!0});var vOr=s(x$);SGo=r(vOr,"TFTapasForMaskedLM"),vOr.forEach(t),$Go=r(UEe," (TAPAS model)"),UEe.forEach(t),IGo=i(se),Gb=n(se,"LI",{});var JEe=s(Gb);_le=n(JEe,"STRONG",{});var bOr=s(_le);DGo=r(bOr,"xlm"),bOr.forEach(t),NGo=r(JEe," \u2014 "),k$=n(JEe,"A",{href:!0});var TOr=s(k$);jGo=r(TOr,"TFXLMWithLMHeadModel"),TOr.forEach(t),OGo=r(JEe," (XLM model)"),JEe.forEach(t),GGo=i(se),qb=n(se,"LI",{});var KEe=s(qb);vle=n(KEe,"STRONG",{});var FOr=s(vle);qGo=r(FOr,"xlm-roberta"),FOr.forEach(t),zGo=r(KEe," \u2014 "),R$=n(KEe,"A",{href:!0});var EOr=s(R$);XGo=r(EOr,"TFXLMRobertaForMaskedLM"),EOr.forEach(t),QGo=r(KEe," (XLM-RoBERTa model)"),KEe.forEach(t),se.forEach(t),VGo=i(Jt),ble=n(Jt,"P",{});var COr=s(ble);WGo=r(COr,"Examples:"),COr.forEach(t),HGo=i(Jt),m(dy.$$.fragment,Jt),Jt.forEach(t),al.forEach(t),_ye=i(c),wd=n(c,"H2",{class:!0});var y7e=s(wd);zb=n(y7e,"A",{id:!0,class:!0,href:!0});var MOr=s(zb);Tle=n(MOr,"SPAN",{});var yOr=s(Tle);m(cy.$$.fragment,yOr),yOr.forEach(t),MOr.forEach(t),UGo=i(y7e),Fle=n(y7e,"SPAN",{});var wOr=s(Fle);JGo=r(wOr,"TFAutoModelForSeq2SeqLM"),wOr.forEach(t),y7e.forEach(t),vye=i(c),fr=n(c,"DIV",{class:!0});var sl=s(fr);m(fy.$$.fragment,sl),KGo=i(sl),Ad=n(sl,"P",{});var Jj=s(Ad);YGo=r(Jj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Ele=n(Jj,"CODE",{});var AOr=s(Ele);ZGo=r(AOr,"from_pretrained()"),AOr.forEach(t),eqo=r(Jj,` class method or the
`),Cle=n(Jj,"CODE",{});var LOr=s(Cle);oqo=r(LOr,"from_config()"),LOr.forEach(t),rqo=r(Jj," class method."),Jj.forEach(t),tqo=i(sl),my=n(sl,"P",{});var w7e=s(my);aqo=r(w7e,"This class cannot be instantiated directly using "),Mle=n(w7e,"CODE",{});var BOr=s(Mle);nqo=r(BOr,"__init__()"),BOr.forEach(t),sqo=r(w7e," (throws an error)."),w7e.forEach(t),lqo=i(sl),et=n(sl,"DIV",{class:!0});var ll=s(et);m(hy.$$.fragment,ll),iqo=i(ll),yle=n(ll,"P",{});var xOr=s(yle);dqo=r(xOr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),xOr.forEach(t),cqo=i(ll),Ld=n(ll,"P",{});var Kj=s(Ld);fqo=r(Kj,`Note:
Loading a model from its configuration file does `),wle=n(Kj,"STRONG",{});var kOr=s(wle);mqo=r(kOr,"not"),kOr.forEach(t),hqo=r(Kj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ale=n(Kj,"CODE",{});var ROr=s(Ale);gqo=r(ROr,"from_pretrained()"),ROr.forEach(t),uqo=r(Kj,` to load the model
weights.`),Kj.forEach(t),pqo=i(ll),Lle=n(ll,"P",{});var POr=s(Lle);_qo=r(POr,"Examples:"),POr.forEach(t),vqo=i(ll),m(gy.$$.fragment,ll),ll.forEach(t),bqo=i(sl),ho=n(sl,"DIV",{class:!0});var Kt=s(ho);m(uy.$$.fragment,Kt),Tqo=i(Kt),Ble=n(Kt,"P",{});var SOr=s(Ble);Fqo=r(SOr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),SOr.forEach(t),Eqo=i(Kt),Ua=n(Kt,"P",{});var eC=s(Ua);Cqo=r(eC,"The model class to instantiate is selected based on the "),xle=n(eC,"CODE",{});var $Or=s(xle);Mqo=r($Or,"model_type"),$Or.forEach(t),yqo=r(eC,` property of the config object (either
passed as an argument or loaded from `),kle=n(eC,"CODE",{});var IOr=s(kle);wqo=r(IOr,"pretrained_model_name_or_path"),IOr.forEach(t),Aqo=r(eC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Rle=n(eC,"CODE",{});var DOr=s(Rle);Lqo=r(DOr,"pretrained_model_name_or_path"),DOr.forEach(t),Bqo=r(eC,":"),eC.forEach(t),xqo=i(Kt),be=n(Kt,"UL",{});var Le=s(be);Xb=n(Le,"LI",{});var YEe=s(Xb);Ple=n(YEe,"STRONG",{});var NOr=s(Ple);kqo=r(NOr,"bart"),NOr.forEach(t),Rqo=r(YEe," \u2014 "),P$=n(YEe,"A",{href:!0});var jOr=s(P$);Pqo=r(jOr,"TFBartForConditionalGeneration"),jOr.forEach(t),Sqo=r(YEe," (BART model)"),YEe.forEach(t),$qo=i(Le),Qb=n(Le,"LI",{});var ZEe=s(Qb);Sle=n(ZEe,"STRONG",{});var OOr=s(Sle);Iqo=r(OOr,"blenderbot"),OOr.forEach(t),Dqo=r(ZEe," \u2014 "),S$=n(ZEe,"A",{href:!0});var GOr=s(S$);Nqo=r(GOr,"TFBlenderbotForConditionalGeneration"),GOr.forEach(t),jqo=r(ZEe," (Blenderbot model)"),ZEe.forEach(t),Oqo=i(Le),Vb=n(Le,"LI",{});var eCe=s(Vb);$le=n(eCe,"STRONG",{});var qOr=s($le);Gqo=r(qOr,"blenderbot-small"),qOr.forEach(t),qqo=r(eCe," \u2014 "),$$=n(eCe,"A",{href:!0});var zOr=s($$);zqo=r(zOr,"TFBlenderbotSmallForConditionalGeneration"),zOr.forEach(t),Xqo=r(eCe," (BlenderbotSmall model)"),eCe.forEach(t),Qqo=i(Le),Wb=n(Le,"LI",{});var oCe=s(Wb);Ile=n(oCe,"STRONG",{});var XOr=s(Ile);Vqo=r(XOr,"encoder-decoder"),XOr.forEach(t),Wqo=r(oCe," \u2014 "),I$=n(oCe,"A",{href:!0});var QOr=s(I$);Hqo=r(QOr,"TFEncoderDecoderModel"),QOr.forEach(t),Uqo=r(oCe," (Encoder decoder model)"),oCe.forEach(t),Jqo=i(Le),Hb=n(Le,"LI",{});var rCe=s(Hb);Dle=n(rCe,"STRONG",{});var VOr=s(Dle);Kqo=r(VOr,"led"),VOr.forEach(t),Yqo=r(rCe," \u2014 "),D$=n(rCe,"A",{href:!0});var WOr=s(D$);Zqo=r(WOr,"TFLEDForConditionalGeneration"),WOr.forEach(t),ezo=r(rCe," (LED model)"),rCe.forEach(t),ozo=i(Le),Ub=n(Le,"LI",{});var tCe=s(Ub);Nle=n(tCe,"STRONG",{});var HOr=s(Nle);rzo=r(HOr,"marian"),HOr.forEach(t),tzo=r(tCe," \u2014 "),N$=n(tCe,"A",{href:!0});var UOr=s(N$);azo=r(UOr,"TFMarianMTModel"),UOr.forEach(t),nzo=r(tCe," (Marian model)"),tCe.forEach(t),szo=i(Le),Jb=n(Le,"LI",{});var aCe=s(Jb);jle=n(aCe,"STRONG",{});var JOr=s(jle);lzo=r(JOr,"mbart"),JOr.forEach(t),izo=r(aCe," \u2014 "),j$=n(aCe,"A",{href:!0});var KOr=s(j$);dzo=r(KOr,"TFMBartForConditionalGeneration"),KOr.forEach(t),czo=r(aCe," (mBART model)"),aCe.forEach(t),fzo=i(Le),Kb=n(Le,"LI",{});var nCe=s(Kb);Ole=n(nCe,"STRONG",{});var YOr=s(Ole);mzo=r(YOr,"mt5"),YOr.forEach(t),hzo=r(nCe," \u2014 "),O$=n(nCe,"A",{href:!0});var ZOr=s(O$);gzo=r(ZOr,"TFMT5ForConditionalGeneration"),ZOr.forEach(t),uzo=r(nCe," (mT5 model)"),nCe.forEach(t),pzo=i(Le),Yb=n(Le,"LI",{});var sCe=s(Yb);Gle=n(sCe,"STRONG",{});var eGr=s(Gle);_zo=r(eGr,"pegasus"),eGr.forEach(t),vzo=r(sCe," \u2014 "),G$=n(sCe,"A",{href:!0});var oGr=s(G$);bzo=r(oGr,"TFPegasusForConditionalGeneration"),oGr.forEach(t),Tzo=r(sCe," (Pegasus model)"),sCe.forEach(t),Fzo=i(Le),Zb=n(Le,"LI",{});var lCe=s(Zb);qle=n(lCe,"STRONG",{});var rGr=s(qle);Ezo=r(rGr,"t5"),rGr.forEach(t),Czo=r(lCe," \u2014 "),q$=n(lCe,"A",{href:!0});var tGr=s(q$);Mzo=r(tGr,"TFT5ForConditionalGeneration"),tGr.forEach(t),yzo=r(lCe," (T5 model)"),lCe.forEach(t),Le.forEach(t),wzo=i(Kt),zle=n(Kt,"P",{});var aGr=s(zle);Azo=r(aGr,"Examples:"),aGr.forEach(t),Lzo=i(Kt),m(py.$$.fragment,Kt),Kt.forEach(t),sl.forEach(t),bye=i(c),Bd=n(c,"H2",{class:!0});var A7e=s(Bd);e2=n(A7e,"A",{id:!0,class:!0,href:!0});var nGr=s(e2);Xle=n(nGr,"SPAN",{});var sGr=s(Xle);m(_y.$$.fragment,sGr),sGr.forEach(t),nGr.forEach(t),Bzo=i(A7e),Qle=n(A7e,"SPAN",{});var lGr=s(Qle);xzo=r(lGr,"TFAutoModelForSequenceClassification"),lGr.forEach(t),A7e.forEach(t),Tye=i(c),mr=n(c,"DIV",{class:!0});var il=s(mr);m(vy.$$.fragment,il),kzo=i(il),xd=n(il,"P",{});var Yj=s(xd);Rzo=r(Yj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Vle=n(Yj,"CODE",{});var iGr=s(Vle);Pzo=r(iGr,"from_pretrained()"),iGr.forEach(t),Szo=r(Yj,` class method or the
`),Wle=n(Yj,"CODE",{});var dGr=s(Wle);$zo=r(dGr,"from_config()"),dGr.forEach(t),Izo=r(Yj," class method."),Yj.forEach(t),Dzo=i(il),by=n(il,"P",{});var L7e=s(by);Nzo=r(L7e,"This class cannot be instantiated directly using "),Hle=n(L7e,"CODE",{});var cGr=s(Hle);jzo=r(cGr,"__init__()"),cGr.forEach(t),Ozo=r(L7e," (throws an error)."),L7e.forEach(t),Gzo=i(il),ot=n(il,"DIV",{class:!0});var dl=s(ot);m(Ty.$$.fragment,dl),qzo=i(dl),Ule=n(dl,"P",{});var fGr=s(Ule);zzo=r(fGr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),fGr.forEach(t),Xzo=i(dl),kd=n(dl,"P",{});var Zj=s(kd);Qzo=r(Zj,`Note:
Loading a model from its configuration file does `),Jle=n(Zj,"STRONG",{});var mGr=s(Jle);Vzo=r(mGr,"not"),mGr.forEach(t),Wzo=r(Zj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=n(Zj,"CODE",{});var hGr=s(Kle);Hzo=r(hGr,"from_pretrained()"),hGr.forEach(t),Uzo=r(Zj,` to load the model
weights.`),Zj.forEach(t),Jzo=i(dl),Yle=n(dl,"P",{});var gGr=s(Yle);Kzo=r(gGr,"Examples:"),gGr.forEach(t),Yzo=i(dl),m(Fy.$$.fragment,dl),dl.forEach(t),Zzo=i(il),go=n(il,"DIV",{class:!0});var Yt=s(go);m(Ey.$$.fragment,Yt),eXo=i(Yt),Zle=n(Yt,"P",{});var uGr=s(Zle);oXo=r(uGr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),uGr.forEach(t),rXo=i(Yt),Ja=n(Yt,"P",{});var oC=s(Ja);tXo=r(oC,"The model class to instantiate is selected based on the "),eie=n(oC,"CODE",{});var pGr=s(eie);aXo=r(pGr,"model_type"),pGr.forEach(t),nXo=r(oC,` property of the config object (either
passed as an argument or loaded from `),oie=n(oC,"CODE",{});var _Gr=s(oie);sXo=r(_Gr,"pretrained_model_name_or_path"),_Gr.forEach(t),lXo=r(oC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),rie=n(oC,"CODE",{});var vGr=s(rie);iXo=r(vGr,"pretrained_model_name_or_path"),vGr.forEach(t),dXo=r(oC,":"),oC.forEach(t),cXo=i(Yt),W=n(Yt,"UL",{});var U=s(W);o2=n(U,"LI",{});var iCe=s(o2);tie=n(iCe,"STRONG",{});var bGr=s(tie);fXo=r(bGr,"albert"),bGr.forEach(t),mXo=r(iCe," \u2014 "),z$=n(iCe,"A",{href:!0});var TGr=s(z$);hXo=r(TGr,"TFAlbertForSequenceClassification"),TGr.forEach(t),gXo=r(iCe," (ALBERT model)"),iCe.forEach(t),uXo=i(U),r2=n(U,"LI",{});var dCe=s(r2);aie=n(dCe,"STRONG",{});var FGr=s(aie);pXo=r(FGr,"bert"),FGr.forEach(t),_Xo=r(dCe," \u2014 "),X$=n(dCe,"A",{href:!0});var EGr=s(X$);vXo=r(EGr,"TFBertForSequenceClassification"),EGr.forEach(t),bXo=r(dCe," (BERT model)"),dCe.forEach(t),TXo=i(U),t2=n(U,"LI",{});var cCe=s(t2);nie=n(cCe,"STRONG",{});var CGr=s(nie);FXo=r(CGr,"camembert"),CGr.forEach(t),EXo=r(cCe," \u2014 "),Q$=n(cCe,"A",{href:!0});var MGr=s(Q$);CXo=r(MGr,"TFCamembertForSequenceClassification"),MGr.forEach(t),MXo=r(cCe," (CamemBERT model)"),cCe.forEach(t),yXo=i(U),a2=n(U,"LI",{});var fCe=s(a2);sie=n(fCe,"STRONG",{});var yGr=s(sie);wXo=r(yGr,"convbert"),yGr.forEach(t),AXo=r(fCe," \u2014 "),V$=n(fCe,"A",{href:!0});var wGr=s(V$);LXo=r(wGr,"TFConvBertForSequenceClassification"),wGr.forEach(t),BXo=r(fCe," (ConvBERT model)"),fCe.forEach(t),xXo=i(U),n2=n(U,"LI",{});var mCe=s(n2);lie=n(mCe,"STRONG",{});var AGr=s(lie);kXo=r(AGr,"ctrl"),AGr.forEach(t),RXo=r(mCe," \u2014 "),W$=n(mCe,"A",{href:!0});var LGr=s(W$);PXo=r(LGr,"TFCTRLForSequenceClassification"),LGr.forEach(t),SXo=r(mCe," (CTRL model)"),mCe.forEach(t),$Xo=i(U),s2=n(U,"LI",{});var hCe=s(s2);iie=n(hCe,"STRONG",{});var BGr=s(iie);IXo=r(BGr,"deberta"),BGr.forEach(t),DXo=r(hCe," \u2014 "),H$=n(hCe,"A",{href:!0});var xGr=s(H$);NXo=r(xGr,"TFDebertaForSequenceClassification"),xGr.forEach(t),jXo=r(hCe," (DeBERTa model)"),hCe.forEach(t),OXo=i(U),l2=n(U,"LI",{});var gCe=s(l2);die=n(gCe,"STRONG",{});var kGr=s(die);GXo=r(kGr,"deberta-v2"),kGr.forEach(t),qXo=r(gCe," \u2014 "),U$=n(gCe,"A",{href:!0});var RGr=s(U$);zXo=r(RGr,"TFDebertaV2ForSequenceClassification"),RGr.forEach(t),XXo=r(gCe," (DeBERTa-v2 model)"),gCe.forEach(t),QXo=i(U),i2=n(U,"LI",{});var uCe=s(i2);cie=n(uCe,"STRONG",{});var PGr=s(cie);VXo=r(PGr,"distilbert"),PGr.forEach(t),WXo=r(uCe," \u2014 "),J$=n(uCe,"A",{href:!0});var SGr=s(J$);HXo=r(SGr,"TFDistilBertForSequenceClassification"),SGr.forEach(t),UXo=r(uCe," (DistilBERT model)"),uCe.forEach(t),JXo=i(U),d2=n(U,"LI",{});var pCe=s(d2);fie=n(pCe,"STRONG",{});var $Gr=s(fie);KXo=r($Gr,"electra"),$Gr.forEach(t),YXo=r(pCe," \u2014 "),K$=n(pCe,"A",{href:!0});var IGr=s(K$);ZXo=r(IGr,"TFElectraForSequenceClassification"),IGr.forEach(t),eQo=r(pCe," (ELECTRA model)"),pCe.forEach(t),oQo=i(U),c2=n(U,"LI",{});var _Ce=s(c2);mie=n(_Ce,"STRONG",{});var DGr=s(mie);rQo=r(DGr,"flaubert"),DGr.forEach(t),tQo=r(_Ce," \u2014 "),Y$=n(_Ce,"A",{href:!0});var NGr=s(Y$);aQo=r(NGr,"TFFlaubertForSequenceClassification"),NGr.forEach(t),nQo=r(_Ce," (FlauBERT model)"),_Ce.forEach(t),sQo=i(U),f2=n(U,"LI",{});var vCe=s(f2);hie=n(vCe,"STRONG",{});var jGr=s(hie);lQo=r(jGr,"funnel"),jGr.forEach(t),iQo=r(vCe," \u2014 "),Z$=n(vCe,"A",{href:!0});var OGr=s(Z$);dQo=r(OGr,"TFFunnelForSequenceClassification"),OGr.forEach(t),cQo=r(vCe," (Funnel Transformer model)"),vCe.forEach(t),fQo=i(U),m2=n(U,"LI",{});var bCe=s(m2);gie=n(bCe,"STRONG",{});var GGr=s(gie);mQo=r(GGr,"gpt2"),GGr.forEach(t),hQo=r(bCe," \u2014 "),eI=n(bCe,"A",{href:!0});var qGr=s(eI);gQo=r(qGr,"TFGPT2ForSequenceClassification"),qGr.forEach(t),uQo=r(bCe," (OpenAI GPT-2 model)"),bCe.forEach(t),pQo=i(U),h2=n(U,"LI",{});var TCe=s(h2);uie=n(TCe,"STRONG",{});var zGr=s(uie);_Qo=r(zGr,"layoutlm"),zGr.forEach(t),vQo=r(TCe," \u2014 "),oI=n(TCe,"A",{href:!0});var XGr=s(oI);bQo=r(XGr,"TFLayoutLMForSequenceClassification"),XGr.forEach(t),TQo=r(TCe," (LayoutLM model)"),TCe.forEach(t),FQo=i(U),g2=n(U,"LI",{});var FCe=s(g2);pie=n(FCe,"STRONG",{});var QGr=s(pie);EQo=r(QGr,"longformer"),QGr.forEach(t),CQo=r(FCe," \u2014 "),rI=n(FCe,"A",{href:!0});var VGr=s(rI);MQo=r(VGr,"TFLongformerForSequenceClassification"),VGr.forEach(t),yQo=r(FCe," (Longformer model)"),FCe.forEach(t),wQo=i(U),u2=n(U,"LI",{});var ECe=s(u2);_ie=n(ECe,"STRONG",{});var WGr=s(_ie);AQo=r(WGr,"mobilebert"),WGr.forEach(t),LQo=r(ECe," \u2014 "),tI=n(ECe,"A",{href:!0});var HGr=s(tI);BQo=r(HGr,"TFMobileBertForSequenceClassification"),HGr.forEach(t),xQo=r(ECe," (MobileBERT model)"),ECe.forEach(t),kQo=i(U),p2=n(U,"LI",{});var CCe=s(p2);vie=n(CCe,"STRONG",{});var UGr=s(vie);RQo=r(UGr,"mpnet"),UGr.forEach(t),PQo=r(CCe," \u2014 "),aI=n(CCe,"A",{href:!0});var JGr=s(aI);SQo=r(JGr,"TFMPNetForSequenceClassification"),JGr.forEach(t),$Qo=r(CCe," (MPNet model)"),CCe.forEach(t),IQo=i(U),_2=n(U,"LI",{});var MCe=s(_2);bie=n(MCe,"STRONG",{});var KGr=s(bie);DQo=r(KGr,"openai-gpt"),KGr.forEach(t),NQo=r(MCe," \u2014 "),nI=n(MCe,"A",{href:!0});var YGr=s(nI);jQo=r(YGr,"TFOpenAIGPTForSequenceClassification"),YGr.forEach(t),OQo=r(MCe," (OpenAI GPT model)"),MCe.forEach(t),GQo=i(U),v2=n(U,"LI",{});var yCe=s(v2);Tie=n(yCe,"STRONG",{});var ZGr=s(Tie);qQo=r(ZGr,"rembert"),ZGr.forEach(t),zQo=r(yCe," \u2014 "),sI=n(yCe,"A",{href:!0});var eqr=s(sI);XQo=r(eqr,"TFRemBertForSequenceClassification"),eqr.forEach(t),QQo=r(yCe," (RemBERT model)"),yCe.forEach(t),VQo=i(U),b2=n(U,"LI",{});var wCe=s(b2);Fie=n(wCe,"STRONG",{});var oqr=s(Fie);WQo=r(oqr,"roberta"),oqr.forEach(t),HQo=r(wCe," \u2014 "),lI=n(wCe,"A",{href:!0});var rqr=s(lI);UQo=r(rqr,"TFRobertaForSequenceClassification"),rqr.forEach(t),JQo=r(wCe," (RoBERTa model)"),wCe.forEach(t),KQo=i(U),T2=n(U,"LI",{});var ACe=s(T2);Eie=n(ACe,"STRONG",{});var tqr=s(Eie);YQo=r(tqr,"roformer"),tqr.forEach(t),ZQo=r(ACe," \u2014 "),iI=n(ACe,"A",{href:!0});var aqr=s(iI);eVo=r(aqr,"TFRoFormerForSequenceClassification"),aqr.forEach(t),oVo=r(ACe," (RoFormer model)"),ACe.forEach(t),rVo=i(U),F2=n(U,"LI",{});var LCe=s(F2);Cie=n(LCe,"STRONG",{});var nqr=s(Cie);tVo=r(nqr,"tapas"),nqr.forEach(t),aVo=r(LCe," \u2014 "),dI=n(LCe,"A",{href:!0});var sqr=s(dI);nVo=r(sqr,"TFTapasForSequenceClassification"),sqr.forEach(t),sVo=r(LCe," (TAPAS model)"),LCe.forEach(t),lVo=i(U),E2=n(U,"LI",{});var BCe=s(E2);Mie=n(BCe,"STRONG",{});var lqr=s(Mie);iVo=r(lqr,"transfo-xl"),lqr.forEach(t),dVo=r(BCe," \u2014 "),cI=n(BCe,"A",{href:!0});var iqr=s(cI);cVo=r(iqr,"TFTransfoXLForSequenceClassification"),iqr.forEach(t),fVo=r(BCe," (Transformer-XL model)"),BCe.forEach(t),mVo=i(U),C2=n(U,"LI",{});var xCe=s(C2);yie=n(xCe,"STRONG",{});var dqr=s(yie);hVo=r(dqr,"xlm"),dqr.forEach(t),gVo=r(xCe," \u2014 "),fI=n(xCe,"A",{href:!0});var cqr=s(fI);uVo=r(cqr,"TFXLMForSequenceClassification"),cqr.forEach(t),pVo=r(xCe," (XLM model)"),xCe.forEach(t),_Vo=i(U),M2=n(U,"LI",{});var kCe=s(M2);wie=n(kCe,"STRONG",{});var fqr=s(wie);vVo=r(fqr,"xlm-roberta"),fqr.forEach(t),bVo=r(kCe," \u2014 "),mI=n(kCe,"A",{href:!0});var mqr=s(mI);TVo=r(mqr,"TFXLMRobertaForSequenceClassification"),mqr.forEach(t),FVo=r(kCe," (XLM-RoBERTa model)"),kCe.forEach(t),EVo=i(U),y2=n(U,"LI",{});var RCe=s(y2);Aie=n(RCe,"STRONG",{});var hqr=s(Aie);CVo=r(hqr,"xlnet"),hqr.forEach(t),MVo=r(RCe," \u2014 "),hI=n(RCe,"A",{href:!0});var gqr=s(hI);yVo=r(gqr,"TFXLNetForSequenceClassification"),gqr.forEach(t),wVo=r(RCe," (XLNet model)"),RCe.forEach(t),U.forEach(t),AVo=i(Yt),Lie=n(Yt,"P",{});var uqr=s(Lie);LVo=r(uqr,"Examples:"),uqr.forEach(t),BVo=i(Yt),m(Cy.$$.fragment,Yt),Yt.forEach(t),il.forEach(t),Fye=i(c),Rd=n(c,"H2",{class:!0});var B7e=s(Rd);w2=n(B7e,"A",{id:!0,class:!0,href:!0});var pqr=s(w2);Bie=n(pqr,"SPAN",{});var _qr=s(Bie);m(My.$$.fragment,_qr),_qr.forEach(t),pqr.forEach(t),xVo=i(B7e),xie=n(B7e,"SPAN",{});var vqr=s(xie);kVo=r(vqr,"TFAutoModelForMultipleChoice"),vqr.forEach(t),B7e.forEach(t),Eye=i(c),hr=n(c,"DIV",{class:!0});var cl=s(hr);m(yy.$$.fragment,cl),RVo=i(cl),Pd=n(cl,"P",{});var eO=s(Pd);PVo=r(eO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kie=n(eO,"CODE",{});var bqr=s(kie);SVo=r(bqr,"from_pretrained()"),bqr.forEach(t),$Vo=r(eO,` class method or the
`),Rie=n(eO,"CODE",{});var Tqr=s(Rie);IVo=r(Tqr,"from_config()"),Tqr.forEach(t),DVo=r(eO," class method."),eO.forEach(t),NVo=i(cl),wy=n(cl,"P",{});var x7e=s(wy);jVo=r(x7e,"This class cannot be instantiated directly using "),Pie=n(x7e,"CODE",{});var Fqr=s(Pie);OVo=r(Fqr,"__init__()"),Fqr.forEach(t),GVo=r(x7e," (throws an error)."),x7e.forEach(t),qVo=i(cl),rt=n(cl,"DIV",{class:!0});var fl=s(rt);m(Ay.$$.fragment,fl),zVo=i(fl),Sie=n(fl,"P",{});var Eqr=s(Sie);XVo=r(Eqr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Eqr.forEach(t),QVo=i(fl),Sd=n(fl,"P",{});var oO=s(Sd);VVo=r(oO,`Note:
Loading a model from its configuration file does `),$ie=n(oO,"STRONG",{});var Cqr=s($ie);WVo=r(Cqr,"not"),Cqr.forEach(t),HVo=r(oO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Iie=n(oO,"CODE",{});var Mqr=s(Iie);UVo=r(Mqr,"from_pretrained()"),Mqr.forEach(t),JVo=r(oO,` to load the model
weights.`),oO.forEach(t),KVo=i(fl),Die=n(fl,"P",{});var yqr=s(Die);YVo=r(yqr,"Examples:"),yqr.forEach(t),ZVo=i(fl),m(Ly.$$.fragment,fl),fl.forEach(t),eWo=i(cl),uo=n(cl,"DIV",{class:!0});var Zt=s(uo);m(By.$$.fragment,Zt),oWo=i(Zt),Nie=n(Zt,"P",{});var wqr=s(Nie);rWo=r(wqr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),wqr.forEach(t),tWo=i(Zt),Ka=n(Zt,"P",{});var rC=s(Ka);aWo=r(rC,"The model class to instantiate is selected based on the "),jie=n(rC,"CODE",{});var Aqr=s(jie);nWo=r(Aqr,"model_type"),Aqr.forEach(t),sWo=r(rC,` property of the config object (either
passed as an argument or loaded from `),Oie=n(rC,"CODE",{});var Lqr=s(Oie);lWo=r(Lqr,"pretrained_model_name_or_path"),Lqr.forEach(t),iWo=r(rC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gie=n(rC,"CODE",{});var Bqr=s(Gie);dWo=r(Bqr,"pretrained_model_name_or_path"),Bqr.forEach(t),cWo=r(rC,":"),rC.forEach(t),fWo=i(Zt),de=n(Zt,"UL",{});var ce=s(de);A2=n(ce,"LI",{});var PCe=s(A2);qie=n(PCe,"STRONG",{});var xqr=s(qie);mWo=r(xqr,"albert"),xqr.forEach(t),hWo=r(PCe," \u2014 "),gI=n(PCe,"A",{href:!0});var kqr=s(gI);gWo=r(kqr,"TFAlbertForMultipleChoice"),kqr.forEach(t),uWo=r(PCe," (ALBERT model)"),PCe.forEach(t),pWo=i(ce),L2=n(ce,"LI",{});var SCe=s(L2);zie=n(SCe,"STRONG",{});var Rqr=s(zie);_Wo=r(Rqr,"bert"),Rqr.forEach(t),vWo=r(SCe," \u2014 "),uI=n(SCe,"A",{href:!0});var Pqr=s(uI);bWo=r(Pqr,"TFBertForMultipleChoice"),Pqr.forEach(t),TWo=r(SCe," (BERT model)"),SCe.forEach(t),FWo=i(ce),B2=n(ce,"LI",{});var $Ce=s(B2);Xie=n($Ce,"STRONG",{});var Sqr=s(Xie);EWo=r(Sqr,"camembert"),Sqr.forEach(t),CWo=r($Ce," \u2014 "),pI=n($Ce,"A",{href:!0});var $qr=s(pI);MWo=r($qr,"TFCamembertForMultipleChoice"),$qr.forEach(t),yWo=r($Ce," (CamemBERT model)"),$Ce.forEach(t),wWo=i(ce),x2=n(ce,"LI",{});var ICe=s(x2);Qie=n(ICe,"STRONG",{});var Iqr=s(Qie);AWo=r(Iqr,"convbert"),Iqr.forEach(t),LWo=r(ICe," \u2014 "),_I=n(ICe,"A",{href:!0});var Dqr=s(_I);BWo=r(Dqr,"TFConvBertForMultipleChoice"),Dqr.forEach(t),xWo=r(ICe," (ConvBERT model)"),ICe.forEach(t),kWo=i(ce),k2=n(ce,"LI",{});var DCe=s(k2);Vie=n(DCe,"STRONG",{});var Nqr=s(Vie);RWo=r(Nqr,"distilbert"),Nqr.forEach(t),PWo=r(DCe," \u2014 "),vI=n(DCe,"A",{href:!0});var jqr=s(vI);SWo=r(jqr,"TFDistilBertForMultipleChoice"),jqr.forEach(t),$Wo=r(DCe," (DistilBERT model)"),DCe.forEach(t),IWo=i(ce),R2=n(ce,"LI",{});var NCe=s(R2);Wie=n(NCe,"STRONG",{});var Oqr=s(Wie);DWo=r(Oqr,"electra"),Oqr.forEach(t),NWo=r(NCe," \u2014 "),bI=n(NCe,"A",{href:!0});var Gqr=s(bI);jWo=r(Gqr,"TFElectraForMultipleChoice"),Gqr.forEach(t),OWo=r(NCe," (ELECTRA model)"),NCe.forEach(t),GWo=i(ce),P2=n(ce,"LI",{});var jCe=s(P2);Hie=n(jCe,"STRONG",{});var qqr=s(Hie);qWo=r(qqr,"flaubert"),qqr.forEach(t),zWo=r(jCe," \u2014 "),TI=n(jCe,"A",{href:!0});var zqr=s(TI);XWo=r(zqr,"TFFlaubertForMultipleChoice"),zqr.forEach(t),QWo=r(jCe," (FlauBERT model)"),jCe.forEach(t),VWo=i(ce),S2=n(ce,"LI",{});var OCe=s(S2);Uie=n(OCe,"STRONG",{});var Xqr=s(Uie);WWo=r(Xqr,"funnel"),Xqr.forEach(t),HWo=r(OCe," \u2014 "),FI=n(OCe,"A",{href:!0});var Qqr=s(FI);UWo=r(Qqr,"TFFunnelForMultipleChoice"),Qqr.forEach(t),JWo=r(OCe," (Funnel Transformer model)"),OCe.forEach(t),KWo=i(ce),$2=n(ce,"LI",{});var GCe=s($2);Jie=n(GCe,"STRONG",{});var Vqr=s(Jie);YWo=r(Vqr,"longformer"),Vqr.forEach(t),ZWo=r(GCe," \u2014 "),EI=n(GCe,"A",{href:!0});var Wqr=s(EI);eHo=r(Wqr,"TFLongformerForMultipleChoice"),Wqr.forEach(t),oHo=r(GCe," (Longformer model)"),GCe.forEach(t),rHo=i(ce),I2=n(ce,"LI",{});var qCe=s(I2);Kie=n(qCe,"STRONG",{});var Hqr=s(Kie);tHo=r(Hqr,"mobilebert"),Hqr.forEach(t),aHo=r(qCe," \u2014 "),CI=n(qCe,"A",{href:!0});var Uqr=s(CI);nHo=r(Uqr,"TFMobileBertForMultipleChoice"),Uqr.forEach(t),sHo=r(qCe," (MobileBERT model)"),qCe.forEach(t),lHo=i(ce),D2=n(ce,"LI",{});var zCe=s(D2);Yie=n(zCe,"STRONG",{});var Jqr=s(Yie);iHo=r(Jqr,"mpnet"),Jqr.forEach(t),dHo=r(zCe," \u2014 "),MI=n(zCe,"A",{href:!0});var Kqr=s(MI);cHo=r(Kqr,"TFMPNetForMultipleChoice"),Kqr.forEach(t),fHo=r(zCe," (MPNet model)"),zCe.forEach(t),mHo=i(ce),N2=n(ce,"LI",{});var XCe=s(N2);Zie=n(XCe,"STRONG",{});var Yqr=s(Zie);hHo=r(Yqr,"rembert"),Yqr.forEach(t),gHo=r(XCe," \u2014 "),yI=n(XCe,"A",{href:!0});var Zqr=s(yI);uHo=r(Zqr,"TFRemBertForMultipleChoice"),Zqr.forEach(t),pHo=r(XCe," (RemBERT model)"),XCe.forEach(t),_Ho=i(ce),j2=n(ce,"LI",{});var QCe=s(j2);ede=n(QCe,"STRONG",{});var ezr=s(ede);vHo=r(ezr,"roberta"),ezr.forEach(t),bHo=r(QCe," \u2014 "),wI=n(QCe,"A",{href:!0});var ozr=s(wI);THo=r(ozr,"TFRobertaForMultipleChoice"),ozr.forEach(t),FHo=r(QCe," (RoBERTa model)"),QCe.forEach(t),EHo=i(ce),O2=n(ce,"LI",{});var VCe=s(O2);ode=n(VCe,"STRONG",{});var rzr=s(ode);CHo=r(rzr,"roformer"),rzr.forEach(t),MHo=r(VCe," \u2014 "),AI=n(VCe,"A",{href:!0});var tzr=s(AI);yHo=r(tzr,"TFRoFormerForMultipleChoice"),tzr.forEach(t),wHo=r(VCe," (RoFormer model)"),VCe.forEach(t),AHo=i(ce),G2=n(ce,"LI",{});var WCe=s(G2);rde=n(WCe,"STRONG",{});var azr=s(rde);LHo=r(azr,"xlm"),azr.forEach(t),BHo=r(WCe," \u2014 "),LI=n(WCe,"A",{href:!0});var nzr=s(LI);xHo=r(nzr,"TFXLMForMultipleChoice"),nzr.forEach(t),kHo=r(WCe," (XLM model)"),WCe.forEach(t),RHo=i(ce),q2=n(ce,"LI",{});var HCe=s(q2);tde=n(HCe,"STRONG",{});var szr=s(tde);PHo=r(szr,"xlm-roberta"),szr.forEach(t),SHo=r(HCe," \u2014 "),BI=n(HCe,"A",{href:!0});var lzr=s(BI);$Ho=r(lzr,"TFXLMRobertaForMultipleChoice"),lzr.forEach(t),IHo=r(HCe," (XLM-RoBERTa model)"),HCe.forEach(t),DHo=i(ce),z2=n(ce,"LI",{});var UCe=s(z2);ade=n(UCe,"STRONG",{});var izr=s(ade);NHo=r(izr,"xlnet"),izr.forEach(t),jHo=r(UCe," \u2014 "),xI=n(UCe,"A",{href:!0});var dzr=s(xI);OHo=r(dzr,"TFXLNetForMultipleChoice"),dzr.forEach(t),GHo=r(UCe," (XLNet model)"),UCe.forEach(t),ce.forEach(t),qHo=i(Zt),nde=n(Zt,"P",{});var czr=s(nde);zHo=r(czr,"Examples:"),czr.forEach(t),XHo=i(Zt),m(xy.$$.fragment,Zt),Zt.forEach(t),cl.forEach(t),Cye=i(c),$d=n(c,"H2",{class:!0});var k7e=s($d);X2=n(k7e,"A",{id:!0,class:!0,href:!0});var fzr=s(X2);sde=n(fzr,"SPAN",{});var mzr=s(sde);m(ky.$$.fragment,mzr),mzr.forEach(t),fzr.forEach(t),QHo=i(k7e),lde=n(k7e,"SPAN",{});var hzr=s(lde);VHo=r(hzr,"TFAutoModelForTableQuestionAnswering"),hzr.forEach(t),k7e.forEach(t),Mye=i(c),gr=n(c,"DIV",{class:!0});var ml=s(gr);m(Ry.$$.fragment,ml),WHo=i(ml),Id=n(ml,"P",{});var rO=s(Id);HHo=r(rO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),ide=n(rO,"CODE",{});var gzr=s(ide);UHo=r(gzr,"from_pretrained()"),gzr.forEach(t),JHo=r(rO,` class method or the
`),dde=n(rO,"CODE",{});var uzr=s(dde);KHo=r(uzr,"from_config()"),uzr.forEach(t),YHo=r(rO," class method."),rO.forEach(t),ZHo=i(ml),Py=n(ml,"P",{});var R7e=s(Py);eUo=r(R7e,"This class cannot be instantiated directly using "),cde=n(R7e,"CODE",{});var pzr=s(cde);oUo=r(pzr,"__init__()"),pzr.forEach(t),rUo=r(R7e," (throws an error)."),R7e.forEach(t),tUo=i(ml),tt=n(ml,"DIV",{class:!0});var hl=s(tt);m(Sy.$$.fragment,hl),aUo=i(hl),fde=n(hl,"P",{});var _zr=s(fde);nUo=r(_zr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),_zr.forEach(t),sUo=i(hl),Dd=n(hl,"P",{});var tO=s(Dd);lUo=r(tO,`Note:
Loading a model from its configuration file does `),mde=n(tO,"STRONG",{});var vzr=s(mde);iUo=r(vzr,"not"),vzr.forEach(t),dUo=r(tO,` load the model weights. It only affects the
model\u2019s configuration. Use `),hde=n(tO,"CODE",{});var bzr=s(hde);cUo=r(bzr,"from_pretrained()"),bzr.forEach(t),fUo=r(tO,` to load the model
weights.`),tO.forEach(t),mUo=i(hl),gde=n(hl,"P",{});var Tzr=s(gde);hUo=r(Tzr,"Examples:"),Tzr.forEach(t),gUo=i(hl),m($y.$$.fragment,hl),hl.forEach(t),uUo=i(ml),po=n(ml,"DIV",{class:!0});var ea=s(po);m(Iy.$$.fragment,ea),pUo=i(ea),ude=n(ea,"P",{});var Fzr=s(ude);_Uo=r(Fzr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Fzr.forEach(t),vUo=i(ea),Ya=n(ea,"P",{});var tC=s(Ya);bUo=r(tC,"The model class to instantiate is selected based on the "),pde=n(tC,"CODE",{});var Ezr=s(pde);TUo=r(Ezr,"model_type"),Ezr.forEach(t),FUo=r(tC,` property of the config object (either
passed as an argument or loaded from `),_de=n(tC,"CODE",{});var Czr=s(_de);EUo=r(Czr,"pretrained_model_name_or_path"),Czr.forEach(t),CUo=r(tC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),vde=n(tC,"CODE",{});var Mzr=s(vde);MUo=r(Mzr,"pretrained_model_name_or_path"),Mzr.forEach(t),yUo=r(tC,":"),tC.forEach(t),wUo=i(ea),bde=n(ea,"UL",{});var yzr=s(bde);Q2=n(yzr,"LI",{});var JCe=s(Q2);Tde=n(JCe,"STRONG",{});var wzr=s(Tde);AUo=r(wzr,"tapas"),wzr.forEach(t),LUo=r(JCe," \u2014 "),kI=n(JCe,"A",{href:!0});var Azr=s(kI);BUo=r(Azr,"TFTapasForQuestionAnswering"),Azr.forEach(t),xUo=r(JCe," (TAPAS model)"),JCe.forEach(t),yzr.forEach(t),kUo=i(ea),Fde=n(ea,"P",{});var Lzr=s(Fde);RUo=r(Lzr,"Examples:"),Lzr.forEach(t),PUo=i(ea),m(Dy.$$.fragment,ea),ea.forEach(t),ml.forEach(t),yye=i(c),Nd=n(c,"H2",{class:!0});var P7e=s(Nd);V2=n(P7e,"A",{id:!0,class:!0,href:!0});var Bzr=s(V2);Ede=n(Bzr,"SPAN",{});var xzr=s(Ede);m(Ny.$$.fragment,xzr),xzr.forEach(t),Bzr.forEach(t),SUo=i(P7e),Cde=n(P7e,"SPAN",{});var kzr=s(Cde);$Uo=r(kzr,"TFAutoModelForTokenClassification"),kzr.forEach(t),P7e.forEach(t),wye=i(c),ur=n(c,"DIV",{class:!0});var gl=s(ur);m(jy.$$.fragment,gl),IUo=i(gl),jd=n(gl,"P",{});var aO=s(jd);DUo=r(aO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Mde=n(aO,"CODE",{});var Rzr=s(Mde);NUo=r(Rzr,"from_pretrained()"),Rzr.forEach(t),jUo=r(aO,` class method or the
`),yde=n(aO,"CODE",{});var Pzr=s(yde);OUo=r(Pzr,"from_config()"),Pzr.forEach(t),GUo=r(aO," class method."),aO.forEach(t),qUo=i(gl),Oy=n(gl,"P",{});var S7e=s(Oy);zUo=r(S7e,"This class cannot be instantiated directly using "),wde=n(S7e,"CODE",{});var Szr=s(wde);XUo=r(Szr,"__init__()"),Szr.forEach(t),QUo=r(S7e," (throws an error)."),S7e.forEach(t),VUo=i(gl),at=n(gl,"DIV",{class:!0});var ul=s(at);m(Gy.$$.fragment,ul),WUo=i(ul),Ade=n(ul,"P",{});var $zr=s(Ade);HUo=r($zr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),$zr.forEach(t),UUo=i(ul),Od=n(ul,"P",{});var nO=s(Od);JUo=r(nO,`Note:
Loading a model from its configuration file does `),Lde=n(nO,"STRONG",{});var Izr=s(Lde);KUo=r(Izr,"not"),Izr.forEach(t),YUo=r(nO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bde=n(nO,"CODE",{});var Dzr=s(Bde);ZUo=r(Dzr,"from_pretrained()"),Dzr.forEach(t),eJo=r(nO,` to load the model
weights.`),nO.forEach(t),oJo=i(ul),xde=n(ul,"P",{});var Nzr=s(xde);rJo=r(Nzr,"Examples:"),Nzr.forEach(t),tJo=i(ul),m(qy.$$.fragment,ul),ul.forEach(t),aJo=i(gl),_o=n(gl,"DIV",{class:!0});var oa=s(_o);m(zy.$$.fragment,oa),nJo=i(oa),kde=n(oa,"P",{});var jzr=s(kde);sJo=r(jzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),jzr.forEach(t),lJo=i(oa),Za=n(oa,"P",{});var aC=s(Za);iJo=r(aC,"The model class to instantiate is selected based on the "),Rde=n(aC,"CODE",{});var Ozr=s(Rde);dJo=r(Ozr,"model_type"),Ozr.forEach(t),cJo=r(aC,` property of the config object (either
passed as an argument or loaded from `),Pde=n(aC,"CODE",{});var Gzr=s(Pde);fJo=r(Gzr,"pretrained_model_name_or_path"),Gzr.forEach(t),mJo=r(aC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Sde=n(aC,"CODE",{});var qzr=s(Sde);hJo=r(qzr,"pretrained_model_name_or_path"),qzr.forEach(t),gJo=r(aC,":"),aC.forEach(t),uJo=i(oa),ae=n(oa,"UL",{});var le=s(ae);W2=n(le,"LI",{});var KCe=s(W2);$de=n(KCe,"STRONG",{});var zzr=s($de);pJo=r(zzr,"albert"),zzr.forEach(t),_Jo=r(KCe," \u2014 "),RI=n(KCe,"A",{href:!0});var Xzr=s(RI);vJo=r(Xzr,"TFAlbertForTokenClassification"),Xzr.forEach(t),bJo=r(KCe," (ALBERT model)"),KCe.forEach(t),TJo=i(le),H2=n(le,"LI",{});var YCe=s(H2);Ide=n(YCe,"STRONG",{});var Qzr=s(Ide);FJo=r(Qzr,"bert"),Qzr.forEach(t),EJo=r(YCe," \u2014 "),PI=n(YCe,"A",{href:!0});var Vzr=s(PI);CJo=r(Vzr,"TFBertForTokenClassification"),Vzr.forEach(t),MJo=r(YCe," (BERT model)"),YCe.forEach(t),yJo=i(le),U2=n(le,"LI",{});var ZCe=s(U2);Dde=n(ZCe,"STRONG",{});var Wzr=s(Dde);wJo=r(Wzr,"camembert"),Wzr.forEach(t),AJo=r(ZCe," \u2014 "),SI=n(ZCe,"A",{href:!0});var Hzr=s(SI);LJo=r(Hzr,"TFCamembertForTokenClassification"),Hzr.forEach(t),BJo=r(ZCe," (CamemBERT model)"),ZCe.forEach(t),xJo=i(le),J2=n(le,"LI",{});var e3e=s(J2);Nde=n(e3e,"STRONG",{});var Uzr=s(Nde);kJo=r(Uzr,"convbert"),Uzr.forEach(t),RJo=r(e3e," \u2014 "),$I=n(e3e,"A",{href:!0});var Jzr=s($I);PJo=r(Jzr,"TFConvBertForTokenClassification"),Jzr.forEach(t),SJo=r(e3e," (ConvBERT model)"),e3e.forEach(t),$Jo=i(le),K2=n(le,"LI",{});var o3e=s(K2);jde=n(o3e,"STRONG",{});var Kzr=s(jde);IJo=r(Kzr,"deberta"),Kzr.forEach(t),DJo=r(o3e," \u2014 "),II=n(o3e,"A",{href:!0});var Yzr=s(II);NJo=r(Yzr,"TFDebertaForTokenClassification"),Yzr.forEach(t),jJo=r(o3e," (DeBERTa model)"),o3e.forEach(t),OJo=i(le),Y2=n(le,"LI",{});var r3e=s(Y2);Ode=n(r3e,"STRONG",{});var Zzr=s(Ode);GJo=r(Zzr,"deberta-v2"),Zzr.forEach(t),qJo=r(r3e," \u2014 "),DI=n(r3e,"A",{href:!0});var eXr=s(DI);zJo=r(eXr,"TFDebertaV2ForTokenClassification"),eXr.forEach(t),XJo=r(r3e," (DeBERTa-v2 model)"),r3e.forEach(t),QJo=i(le),Z2=n(le,"LI",{});var t3e=s(Z2);Gde=n(t3e,"STRONG",{});var oXr=s(Gde);VJo=r(oXr,"distilbert"),oXr.forEach(t),WJo=r(t3e," \u2014 "),NI=n(t3e,"A",{href:!0});var rXr=s(NI);HJo=r(rXr,"TFDistilBertForTokenClassification"),rXr.forEach(t),UJo=r(t3e," (DistilBERT model)"),t3e.forEach(t),JJo=i(le),eT=n(le,"LI",{});var a3e=s(eT);qde=n(a3e,"STRONG",{});var tXr=s(qde);KJo=r(tXr,"electra"),tXr.forEach(t),YJo=r(a3e," \u2014 "),jI=n(a3e,"A",{href:!0});var aXr=s(jI);ZJo=r(aXr,"TFElectraForTokenClassification"),aXr.forEach(t),eKo=r(a3e," (ELECTRA model)"),a3e.forEach(t),oKo=i(le),oT=n(le,"LI",{});var n3e=s(oT);zde=n(n3e,"STRONG",{});var nXr=s(zde);rKo=r(nXr,"flaubert"),nXr.forEach(t),tKo=r(n3e," \u2014 "),OI=n(n3e,"A",{href:!0});var sXr=s(OI);aKo=r(sXr,"TFFlaubertForTokenClassification"),sXr.forEach(t),nKo=r(n3e," (FlauBERT model)"),n3e.forEach(t),sKo=i(le),rT=n(le,"LI",{});var s3e=s(rT);Xde=n(s3e,"STRONG",{});var lXr=s(Xde);lKo=r(lXr,"funnel"),lXr.forEach(t),iKo=r(s3e," \u2014 "),GI=n(s3e,"A",{href:!0});var iXr=s(GI);dKo=r(iXr,"TFFunnelForTokenClassification"),iXr.forEach(t),cKo=r(s3e," (Funnel Transformer model)"),s3e.forEach(t),fKo=i(le),tT=n(le,"LI",{});var l3e=s(tT);Qde=n(l3e,"STRONG",{});var dXr=s(Qde);mKo=r(dXr,"layoutlm"),dXr.forEach(t),hKo=r(l3e," \u2014 "),qI=n(l3e,"A",{href:!0});var cXr=s(qI);gKo=r(cXr,"TFLayoutLMForTokenClassification"),cXr.forEach(t),uKo=r(l3e," (LayoutLM model)"),l3e.forEach(t),pKo=i(le),aT=n(le,"LI",{});var i3e=s(aT);Vde=n(i3e,"STRONG",{});var fXr=s(Vde);_Ko=r(fXr,"longformer"),fXr.forEach(t),vKo=r(i3e," \u2014 "),zI=n(i3e,"A",{href:!0});var mXr=s(zI);bKo=r(mXr,"TFLongformerForTokenClassification"),mXr.forEach(t),TKo=r(i3e," (Longformer model)"),i3e.forEach(t),FKo=i(le),nT=n(le,"LI",{});var d3e=s(nT);Wde=n(d3e,"STRONG",{});var hXr=s(Wde);EKo=r(hXr,"mobilebert"),hXr.forEach(t),CKo=r(d3e," \u2014 "),XI=n(d3e,"A",{href:!0});var gXr=s(XI);MKo=r(gXr,"TFMobileBertForTokenClassification"),gXr.forEach(t),yKo=r(d3e," (MobileBERT model)"),d3e.forEach(t),wKo=i(le),sT=n(le,"LI",{});var c3e=s(sT);Hde=n(c3e,"STRONG",{});var uXr=s(Hde);AKo=r(uXr,"mpnet"),uXr.forEach(t),LKo=r(c3e," \u2014 "),QI=n(c3e,"A",{href:!0});var pXr=s(QI);BKo=r(pXr,"TFMPNetForTokenClassification"),pXr.forEach(t),xKo=r(c3e," (MPNet model)"),c3e.forEach(t),kKo=i(le),lT=n(le,"LI",{});var f3e=s(lT);Ude=n(f3e,"STRONG",{});var _Xr=s(Ude);RKo=r(_Xr,"rembert"),_Xr.forEach(t),PKo=r(f3e," \u2014 "),VI=n(f3e,"A",{href:!0});var vXr=s(VI);SKo=r(vXr,"TFRemBertForTokenClassification"),vXr.forEach(t),$Ko=r(f3e," (RemBERT model)"),f3e.forEach(t),IKo=i(le),iT=n(le,"LI",{});var m3e=s(iT);Jde=n(m3e,"STRONG",{});var bXr=s(Jde);DKo=r(bXr,"roberta"),bXr.forEach(t),NKo=r(m3e," \u2014 "),WI=n(m3e,"A",{href:!0});var TXr=s(WI);jKo=r(TXr,"TFRobertaForTokenClassification"),TXr.forEach(t),OKo=r(m3e," (RoBERTa model)"),m3e.forEach(t),GKo=i(le),dT=n(le,"LI",{});var h3e=s(dT);Kde=n(h3e,"STRONG",{});var FXr=s(Kde);qKo=r(FXr,"roformer"),FXr.forEach(t),zKo=r(h3e," \u2014 "),HI=n(h3e,"A",{href:!0});var EXr=s(HI);XKo=r(EXr,"TFRoFormerForTokenClassification"),EXr.forEach(t),QKo=r(h3e," (RoFormer model)"),h3e.forEach(t),VKo=i(le),cT=n(le,"LI",{});var g3e=s(cT);Yde=n(g3e,"STRONG",{});var CXr=s(Yde);WKo=r(CXr,"xlm"),CXr.forEach(t),HKo=r(g3e," \u2014 "),UI=n(g3e,"A",{href:!0});var MXr=s(UI);UKo=r(MXr,"TFXLMForTokenClassification"),MXr.forEach(t),JKo=r(g3e," (XLM model)"),g3e.forEach(t),KKo=i(le),fT=n(le,"LI",{});var u3e=s(fT);Zde=n(u3e,"STRONG",{});var yXr=s(Zde);YKo=r(yXr,"xlm-roberta"),yXr.forEach(t),ZKo=r(u3e," \u2014 "),JI=n(u3e,"A",{href:!0});var wXr=s(JI);eYo=r(wXr,"TFXLMRobertaForTokenClassification"),wXr.forEach(t),oYo=r(u3e," (XLM-RoBERTa model)"),u3e.forEach(t),rYo=i(le),mT=n(le,"LI",{});var p3e=s(mT);ece=n(p3e,"STRONG",{});var AXr=s(ece);tYo=r(AXr,"xlnet"),AXr.forEach(t),aYo=r(p3e," \u2014 "),KI=n(p3e,"A",{href:!0});var LXr=s(KI);nYo=r(LXr,"TFXLNetForTokenClassification"),LXr.forEach(t),sYo=r(p3e," (XLNet model)"),p3e.forEach(t),le.forEach(t),lYo=i(oa),oce=n(oa,"P",{});var BXr=s(oce);iYo=r(BXr,"Examples:"),BXr.forEach(t),dYo=i(oa),m(Xy.$$.fragment,oa),oa.forEach(t),gl.forEach(t),Aye=i(c),Gd=n(c,"H2",{class:!0});var $7e=s(Gd);hT=n($7e,"A",{id:!0,class:!0,href:!0});var xXr=s(hT);rce=n(xXr,"SPAN",{});var kXr=s(rce);m(Qy.$$.fragment,kXr),kXr.forEach(t),xXr.forEach(t),cYo=i($7e),tce=n($7e,"SPAN",{});var RXr=s(tce);fYo=r(RXr,"TFAutoModelForQuestionAnswering"),RXr.forEach(t),$7e.forEach(t),Lye=i(c),pr=n(c,"DIV",{class:!0});var pl=s(pr);m(Vy.$$.fragment,pl),mYo=i(pl),qd=n(pl,"P",{});var sO=s(qd);hYo=r(sO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),ace=n(sO,"CODE",{});var PXr=s(ace);gYo=r(PXr,"from_pretrained()"),PXr.forEach(t),uYo=r(sO,` class method or the
`),nce=n(sO,"CODE",{});var SXr=s(nce);pYo=r(SXr,"from_config()"),SXr.forEach(t),_Yo=r(sO," class method."),sO.forEach(t),vYo=i(pl),Wy=n(pl,"P",{});var I7e=s(Wy);bYo=r(I7e,"This class cannot be instantiated directly using "),sce=n(I7e,"CODE",{});var $Xr=s(sce);TYo=r($Xr,"__init__()"),$Xr.forEach(t),FYo=r(I7e," (throws an error)."),I7e.forEach(t),EYo=i(pl),nt=n(pl,"DIV",{class:!0});var _l=s(nt);m(Hy.$$.fragment,_l),CYo=i(_l),lce=n(_l,"P",{});var IXr=s(lce);MYo=r(IXr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),IXr.forEach(t),yYo=i(_l),zd=n(_l,"P",{});var lO=s(zd);wYo=r(lO,`Note:
Loading a model from its configuration file does `),ice=n(lO,"STRONG",{});var DXr=s(ice);AYo=r(DXr,"not"),DXr.forEach(t),LYo=r(lO,` load the model weights. It only affects the
model\u2019s configuration. Use `),dce=n(lO,"CODE",{});var NXr=s(dce);BYo=r(NXr,"from_pretrained()"),NXr.forEach(t),xYo=r(lO,` to load the model
weights.`),lO.forEach(t),kYo=i(_l),cce=n(_l,"P",{});var jXr=s(cce);RYo=r(jXr,"Examples:"),jXr.forEach(t),PYo=i(_l),m(Uy.$$.fragment,_l),_l.forEach(t),SYo=i(pl),vo=n(pl,"DIV",{class:!0});var ra=s(vo);m(Jy.$$.fragment,ra),$Yo=i(ra),fce=n(ra,"P",{});var OXr=s(fce);IYo=r(OXr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),OXr.forEach(t),DYo=i(ra),en=n(ra,"P",{});var nC=s(en);NYo=r(nC,"The model class to instantiate is selected based on the "),mce=n(nC,"CODE",{});var GXr=s(mce);jYo=r(GXr,"model_type"),GXr.forEach(t),OYo=r(nC,` property of the config object (either
passed as an argument or loaded from `),hce=n(nC,"CODE",{});var qXr=s(hce);GYo=r(qXr,"pretrained_model_name_or_path"),qXr.forEach(t),qYo=r(nC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),gce=n(nC,"CODE",{});var zXr=s(gce);zYo=r(zXr,"pretrained_model_name_or_path"),zXr.forEach(t),XYo=r(nC,":"),nC.forEach(t),QYo=i(ra),ne=n(ra,"UL",{});var ie=s(ne);gT=n(ie,"LI",{});var _3e=s(gT);uce=n(_3e,"STRONG",{});var XXr=s(uce);VYo=r(XXr,"albert"),XXr.forEach(t),WYo=r(_3e," \u2014 "),YI=n(_3e,"A",{href:!0});var QXr=s(YI);HYo=r(QXr,"TFAlbertForQuestionAnswering"),QXr.forEach(t),UYo=r(_3e," (ALBERT model)"),_3e.forEach(t),JYo=i(ie),uT=n(ie,"LI",{});var v3e=s(uT);pce=n(v3e,"STRONG",{});var VXr=s(pce);KYo=r(VXr,"bert"),VXr.forEach(t),YYo=r(v3e," \u2014 "),ZI=n(v3e,"A",{href:!0});var WXr=s(ZI);ZYo=r(WXr,"TFBertForQuestionAnswering"),WXr.forEach(t),eZo=r(v3e," (BERT model)"),v3e.forEach(t),oZo=i(ie),pT=n(ie,"LI",{});var b3e=s(pT);_ce=n(b3e,"STRONG",{});var HXr=s(_ce);rZo=r(HXr,"camembert"),HXr.forEach(t),tZo=r(b3e," \u2014 "),eD=n(b3e,"A",{href:!0});var UXr=s(eD);aZo=r(UXr,"TFCamembertForQuestionAnswering"),UXr.forEach(t),nZo=r(b3e," (CamemBERT model)"),b3e.forEach(t),sZo=i(ie),_T=n(ie,"LI",{});var T3e=s(_T);vce=n(T3e,"STRONG",{});var JXr=s(vce);lZo=r(JXr,"convbert"),JXr.forEach(t),iZo=r(T3e," \u2014 "),oD=n(T3e,"A",{href:!0});var KXr=s(oD);dZo=r(KXr,"TFConvBertForQuestionAnswering"),KXr.forEach(t),cZo=r(T3e," (ConvBERT model)"),T3e.forEach(t),fZo=i(ie),vT=n(ie,"LI",{});var F3e=s(vT);bce=n(F3e,"STRONG",{});var YXr=s(bce);mZo=r(YXr,"deberta"),YXr.forEach(t),hZo=r(F3e," \u2014 "),rD=n(F3e,"A",{href:!0});var ZXr=s(rD);gZo=r(ZXr,"TFDebertaForQuestionAnswering"),ZXr.forEach(t),uZo=r(F3e," (DeBERTa model)"),F3e.forEach(t),pZo=i(ie),bT=n(ie,"LI",{});var E3e=s(bT);Tce=n(E3e,"STRONG",{});var eQr=s(Tce);_Zo=r(eQr,"deberta-v2"),eQr.forEach(t),vZo=r(E3e," \u2014 "),tD=n(E3e,"A",{href:!0});var oQr=s(tD);bZo=r(oQr,"TFDebertaV2ForQuestionAnswering"),oQr.forEach(t),TZo=r(E3e," (DeBERTa-v2 model)"),E3e.forEach(t),FZo=i(ie),TT=n(ie,"LI",{});var C3e=s(TT);Fce=n(C3e,"STRONG",{});var rQr=s(Fce);EZo=r(rQr,"distilbert"),rQr.forEach(t),CZo=r(C3e," \u2014 "),aD=n(C3e,"A",{href:!0});var tQr=s(aD);MZo=r(tQr,"TFDistilBertForQuestionAnswering"),tQr.forEach(t),yZo=r(C3e," (DistilBERT model)"),C3e.forEach(t),wZo=i(ie),FT=n(ie,"LI",{});var M3e=s(FT);Ece=n(M3e,"STRONG",{});var aQr=s(Ece);AZo=r(aQr,"electra"),aQr.forEach(t),LZo=r(M3e," \u2014 "),nD=n(M3e,"A",{href:!0});var nQr=s(nD);BZo=r(nQr,"TFElectraForQuestionAnswering"),nQr.forEach(t),xZo=r(M3e," (ELECTRA model)"),M3e.forEach(t),kZo=i(ie),ET=n(ie,"LI",{});var y3e=s(ET);Cce=n(y3e,"STRONG",{});var sQr=s(Cce);RZo=r(sQr,"flaubert"),sQr.forEach(t),PZo=r(y3e," \u2014 "),sD=n(y3e,"A",{href:!0});var lQr=s(sD);SZo=r(lQr,"TFFlaubertForQuestionAnsweringSimple"),lQr.forEach(t),$Zo=r(y3e," (FlauBERT model)"),y3e.forEach(t),IZo=i(ie),CT=n(ie,"LI",{});var w3e=s(CT);Mce=n(w3e,"STRONG",{});var iQr=s(Mce);DZo=r(iQr,"funnel"),iQr.forEach(t),NZo=r(w3e," \u2014 "),lD=n(w3e,"A",{href:!0});var dQr=s(lD);jZo=r(dQr,"TFFunnelForQuestionAnswering"),dQr.forEach(t),OZo=r(w3e," (Funnel Transformer model)"),w3e.forEach(t),GZo=i(ie),MT=n(ie,"LI",{});var A3e=s(MT);yce=n(A3e,"STRONG",{});var cQr=s(yce);qZo=r(cQr,"longformer"),cQr.forEach(t),zZo=r(A3e," \u2014 "),iD=n(A3e,"A",{href:!0});var fQr=s(iD);XZo=r(fQr,"TFLongformerForQuestionAnswering"),fQr.forEach(t),QZo=r(A3e," (Longformer model)"),A3e.forEach(t),VZo=i(ie),yT=n(ie,"LI",{});var L3e=s(yT);wce=n(L3e,"STRONG",{});var mQr=s(wce);WZo=r(mQr,"mobilebert"),mQr.forEach(t),HZo=r(L3e," \u2014 "),dD=n(L3e,"A",{href:!0});var hQr=s(dD);UZo=r(hQr,"TFMobileBertForQuestionAnswering"),hQr.forEach(t),JZo=r(L3e," (MobileBERT model)"),L3e.forEach(t),KZo=i(ie),wT=n(ie,"LI",{});var B3e=s(wT);Ace=n(B3e,"STRONG",{});var gQr=s(Ace);YZo=r(gQr,"mpnet"),gQr.forEach(t),ZZo=r(B3e," \u2014 "),cD=n(B3e,"A",{href:!0});var uQr=s(cD);eer=r(uQr,"TFMPNetForQuestionAnswering"),uQr.forEach(t),oer=r(B3e," (MPNet model)"),B3e.forEach(t),rer=i(ie),AT=n(ie,"LI",{});var x3e=s(AT);Lce=n(x3e,"STRONG",{});var pQr=s(Lce);ter=r(pQr,"rembert"),pQr.forEach(t),aer=r(x3e," \u2014 "),fD=n(x3e,"A",{href:!0});var _Qr=s(fD);ner=r(_Qr,"TFRemBertForQuestionAnswering"),_Qr.forEach(t),ser=r(x3e," (RemBERT model)"),x3e.forEach(t),ler=i(ie),LT=n(ie,"LI",{});var k3e=s(LT);Bce=n(k3e,"STRONG",{});var vQr=s(Bce);ier=r(vQr,"roberta"),vQr.forEach(t),der=r(k3e," \u2014 "),mD=n(k3e,"A",{href:!0});var bQr=s(mD);cer=r(bQr,"TFRobertaForQuestionAnswering"),bQr.forEach(t),fer=r(k3e," (RoBERTa model)"),k3e.forEach(t),mer=i(ie),BT=n(ie,"LI",{});var R3e=s(BT);xce=n(R3e,"STRONG",{});var TQr=s(xce);her=r(TQr,"roformer"),TQr.forEach(t),ger=r(R3e," \u2014 "),hD=n(R3e,"A",{href:!0});var FQr=s(hD);uer=r(FQr,"TFRoFormerForQuestionAnswering"),FQr.forEach(t),per=r(R3e," (RoFormer model)"),R3e.forEach(t),_er=i(ie),xT=n(ie,"LI",{});var P3e=s(xT);kce=n(P3e,"STRONG",{});var EQr=s(kce);ver=r(EQr,"xlm"),EQr.forEach(t),ber=r(P3e," \u2014 "),gD=n(P3e,"A",{href:!0});var CQr=s(gD);Ter=r(CQr,"TFXLMForQuestionAnsweringSimple"),CQr.forEach(t),Fer=r(P3e," (XLM model)"),P3e.forEach(t),Eer=i(ie),kT=n(ie,"LI",{});var S3e=s(kT);Rce=n(S3e,"STRONG",{});var MQr=s(Rce);Cer=r(MQr,"xlm-roberta"),MQr.forEach(t),Mer=r(S3e," \u2014 "),uD=n(S3e,"A",{href:!0});var yQr=s(uD);yer=r(yQr,"TFXLMRobertaForQuestionAnswering"),yQr.forEach(t),wer=r(S3e," (XLM-RoBERTa model)"),S3e.forEach(t),Aer=i(ie),RT=n(ie,"LI",{});var $3e=s(RT);Pce=n($3e,"STRONG",{});var wQr=s(Pce);Ler=r(wQr,"xlnet"),wQr.forEach(t),Ber=r($3e," \u2014 "),pD=n($3e,"A",{href:!0});var AQr=s(pD);xer=r(AQr,"TFXLNetForQuestionAnsweringSimple"),AQr.forEach(t),ker=r($3e," (XLNet model)"),$3e.forEach(t),ie.forEach(t),Rer=i(ra),Sce=n(ra,"P",{});var LQr=s(Sce);Per=r(LQr,"Examples:"),LQr.forEach(t),Ser=i(ra),m(Ky.$$.fragment,ra),ra.forEach(t),pl.forEach(t),Bye=i(c),Xd=n(c,"H2",{class:!0});var D7e=s(Xd);PT=n(D7e,"A",{id:!0,class:!0,href:!0});var BQr=s(PT);$ce=n(BQr,"SPAN",{});var xQr=s($ce);m(Yy.$$.fragment,xQr),xQr.forEach(t),BQr.forEach(t),$er=i(D7e),Ice=n(D7e,"SPAN",{});var kQr=s(Ice);Ier=r(kQr,"FlaxAutoModel"),kQr.forEach(t),D7e.forEach(t),xye=i(c),_r=n(c,"DIV",{class:!0});var vl=s(_r);m(Zy.$$.fragment,vl),Der=i(vl),Qd=n(vl,"P",{});var iO=s(Qd);Ner=r(iO,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dce=n(iO,"CODE",{});var RQr=s(Dce);jer=r(RQr,"from_pretrained()"),RQr.forEach(t),Oer=r(iO,` class method or the
`),Nce=n(iO,"CODE",{});var PQr=s(Nce);Ger=r(PQr,"from_config()"),PQr.forEach(t),qer=r(iO," class method."),iO.forEach(t),zer=i(vl),ew=n(vl,"P",{});var N7e=s(ew);Xer=r(N7e,"This class cannot be instantiated directly using "),jce=n(N7e,"CODE",{});var SQr=s(jce);Qer=r(SQr,"__init__()"),SQr.forEach(t),Ver=r(N7e," (throws an error)."),N7e.forEach(t),Wer=i(vl),st=n(vl,"DIV",{class:!0});var bl=s(st);m(ow.$$.fragment,bl),Her=i(bl),Oce=n(bl,"P",{});var $Qr=s(Oce);Uer=r($Qr,"Instantiates one of the base model classes of the library from a configuration."),$Qr.forEach(t),Jer=i(bl),Vd=n(bl,"P",{});var dO=s(Vd);Ker=r(dO,`Note:
Loading a model from its configuration file does `),Gce=n(dO,"STRONG",{});var IQr=s(Gce);Yer=r(IQr,"not"),IQr.forEach(t),Zer=r(dO,` load the model weights. It only affects the
model\u2019s configuration. Use `),qce=n(dO,"CODE",{});var DQr=s(qce);eor=r(DQr,"from_pretrained()"),DQr.forEach(t),oor=r(dO,` to load the model
weights.`),dO.forEach(t),ror=i(bl),zce=n(bl,"P",{});var NQr=s(zce);tor=r(NQr,"Examples:"),NQr.forEach(t),aor=i(bl),m(rw.$$.fragment,bl),bl.forEach(t),nor=i(vl),bo=n(vl,"DIV",{class:!0});var ta=s(bo);m(tw.$$.fragment,ta),sor=i(ta),Xce=n(ta,"P",{});var jQr=s(Xce);lor=r(jQr,"Instantiate one of the base model classes of the library from a pretrained model."),jQr.forEach(t),ior=i(ta),on=n(ta,"P",{});var sC=s(on);dor=r(sC,"The model class to instantiate is selected based on the "),Qce=n(sC,"CODE",{});var OQr=s(Qce);cor=r(OQr,"model_type"),OQr.forEach(t),mor=r(sC,` property of the config object (either
passed as an argument or loaded from `),Vce=n(sC,"CODE",{});var GQr=s(Vce);hor=r(GQr,"pretrained_model_name_or_path"),GQr.forEach(t),gor=r(sC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Wce=n(sC,"CODE",{});var qQr=s(Wce);uor=r(qQr,"pretrained_model_name_or_path"),qQr.forEach(t),por=r(sC,":"),sC.forEach(t),_or=i(ta),Y=n(ta,"UL",{});var oe=s(Y);ST=n(oe,"LI",{});var I3e=s(ST);Hce=n(I3e,"STRONG",{});var zQr=s(Hce);vor=r(zQr,"albert"),zQr.forEach(t),bor=r(I3e," \u2014 "),_D=n(I3e,"A",{href:!0});var XQr=s(_D);Tor=r(XQr,"FlaxAlbertModel"),XQr.forEach(t),For=r(I3e," (ALBERT model)"),I3e.forEach(t),Eor=i(oe),$T=n(oe,"LI",{});var D3e=s($T);Uce=n(D3e,"STRONG",{});var QQr=s(Uce);Cor=r(QQr,"bart"),QQr.forEach(t),Mor=r(D3e," \u2014 "),vD=n(D3e,"A",{href:!0});var VQr=s(vD);yor=r(VQr,"FlaxBartModel"),VQr.forEach(t),wor=r(D3e," (BART model)"),D3e.forEach(t),Aor=i(oe),IT=n(oe,"LI",{});var N3e=s(IT);Jce=n(N3e,"STRONG",{});var WQr=s(Jce);Lor=r(WQr,"beit"),WQr.forEach(t),Bor=r(N3e," \u2014 "),bD=n(N3e,"A",{href:!0});var HQr=s(bD);xor=r(HQr,"FlaxBeitModel"),HQr.forEach(t),kor=r(N3e," (BEiT model)"),N3e.forEach(t),Ror=i(oe),DT=n(oe,"LI",{});var j3e=s(DT);Kce=n(j3e,"STRONG",{});var UQr=s(Kce);Por=r(UQr,"bert"),UQr.forEach(t),Sor=r(j3e," \u2014 "),TD=n(j3e,"A",{href:!0});var JQr=s(TD);$or=r(JQr,"FlaxBertModel"),JQr.forEach(t),Ior=r(j3e," (BERT model)"),j3e.forEach(t),Dor=i(oe),NT=n(oe,"LI",{});var O3e=s(NT);Yce=n(O3e,"STRONG",{});var KQr=s(Yce);Nor=r(KQr,"big_bird"),KQr.forEach(t),jor=r(O3e," \u2014 "),FD=n(O3e,"A",{href:!0});var YQr=s(FD);Oor=r(YQr,"FlaxBigBirdModel"),YQr.forEach(t),Gor=r(O3e," (BigBird model)"),O3e.forEach(t),qor=i(oe),jT=n(oe,"LI",{});var G3e=s(jT);Zce=n(G3e,"STRONG",{});var ZQr=s(Zce);zor=r(ZQr,"blenderbot"),ZQr.forEach(t),Xor=r(G3e," \u2014 "),ED=n(G3e,"A",{href:!0});var eVr=s(ED);Qor=r(eVr,"FlaxBlenderbotModel"),eVr.forEach(t),Vor=r(G3e," (Blenderbot model)"),G3e.forEach(t),Wor=i(oe),OT=n(oe,"LI",{});var q3e=s(OT);efe=n(q3e,"STRONG",{});var oVr=s(efe);Hor=r(oVr,"blenderbot-small"),oVr.forEach(t),Uor=r(q3e," \u2014 "),CD=n(q3e,"A",{href:!0});var rVr=s(CD);Jor=r(rVr,"FlaxBlenderbotSmallModel"),rVr.forEach(t),Kor=r(q3e," (BlenderbotSmall model)"),q3e.forEach(t),Yor=i(oe),GT=n(oe,"LI",{});var z3e=s(GT);ofe=n(z3e,"STRONG",{});var tVr=s(ofe);Zor=r(tVr,"clip"),tVr.forEach(t),err=r(z3e," \u2014 "),MD=n(z3e,"A",{href:!0});var aVr=s(MD);orr=r(aVr,"FlaxCLIPModel"),aVr.forEach(t),rrr=r(z3e," (CLIP model)"),z3e.forEach(t),trr=i(oe),qT=n(oe,"LI",{});var X3e=s(qT);rfe=n(X3e,"STRONG",{});var nVr=s(rfe);arr=r(nVr,"distilbert"),nVr.forEach(t),nrr=r(X3e," \u2014 "),yD=n(X3e,"A",{href:!0});var sVr=s(yD);srr=r(sVr,"FlaxDistilBertModel"),sVr.forEach(t),lrr=r(X3e," (DistilBERT model)"),X3e.forEach(t),irr=i(oe),zT=n(oe,"LI",{});var Q3e=s(zT);tfe=n(Q3e,"STRONG",{});var lVr=s(tfe);drr=r(lVr,"electra"),lVr.forEach(t),crr=r(Q3e," \u2014 "),wD=n(Q3e,"A",{href:!0});var iVr=s(wD);frr=r(iVr,"FlaxElectraModel"),iVr.forEach(t),mrr=r(Q3e," (ELECTRA model)"),Q3e.forEach(t),hrr=i(oe),XT=n(oe,"LI",{});var V3e=s(XT);afe=n(V3e,"STRONG",{});var dVr=s(afe);grr=r(dVr,"gpt2"),dVr.forEach(t),urr=r(V3e," \u2014 "),AD=n(V3e,"A",{href:!0});var cVr=s(AD);prr=r(cVr,"FlaxGPT2Model"),cVr.forEach(t),_rr=r(V3e," (OpenAI GPT-2 model)"),V3e.forEach(t),vrr=i(oe),QT=n(oe,"LI",{});var W3e=s(QT);nfe=n(W3e,"STRONG",{});var fVr=s(nfe);brr=r(fVr,"gpt_neo"),fVr.forEach(t),Trr=r(W3e," \u2014 "),LD=n(W3e,"A",{href:!0});var mVr=s(LD);Frr=r(mVr,"FlaxGPTNeoModel"),mVr.forEach(t),Err=r(W3e," (GPT Neo model)"),W3e.forEach(t),Crr=i(oe),VT=n(oe,"LI",{});var H3e=s(VT);sfe=n(H3e,"STRONG",{});var hVr=s(sfe);Mrr=r(hVr,"gptj"),hVr.forEach(t),yrr=r(H3e," \u2014 "),BD=n(H3e,"A",{href:!0});var gVr=s(BD);wrr=r(gVr,"FlaxGPTJModel"),gVr.forEach(t),Arr=r(H3e," (GPT-J model)"),H3e.forEach(t),Lrr=i(oe),WT=n(oe,"LI",{});var U3e=s(WT);lfe=n(U3e,"STRONG",{});var uVr=s(lfe);Brr=r(uVr,"marian"),uVr.forEach(t),xrr=r(U3e," \u2014 "),xD=n(U3e,"A",{href:!0});var pVr=s(xD);krr=r(pVr,"FlaxMarianModel"),pVr.forEach(t),Rrr=r(U3e," (Marian model)"),U3e.forEach(t),Prr=i(oe),HT=n(oe,"LI",{});var J3e=s(HT);ife=n(J3e,"STRONG",{});var _Vr=s(ife);Srr=r(_Vr,"mbart"),_Vr.forEach(t),$rr=r(J3e," \u2014 "),kD=n(J3e,"A",{href:!0});var vVr=s(kD);Irr=r(vVr,"FlaxMBartModel"),vVr.forEach(t),Drr=r(J3e," (mBART model)"),J3e.forEach(t),Nrr=i(oe),UT=n(oe,"LI",{});var K3e=s(UT);dfe=n(K3e,"STRONG",{});var bVr=s(dfe);jrr=r(bVr,"mt5"),bVr.forEach(t),Orr=r(K3e," \u2014 "),RD=n(K3e,"A",{href:!0});var TVr=s(RD);Grr=r(TVr,"FlaxMT5Model"),TVr.forEach(t),qrr=r(K3e," (mT5 model)"),K3e.forEach(t),zrr=i(oe),JT=n(oe,"LI",{});var Y3e=s(JT);cfe=n(Y3e,"STRONG",{});var FVr=s(cfe);Xrr=r(FVr,"pegasus"),FVr.forEach(t),Qrr=r(Y3e," \u2014 "),PD=n(Y3e,"A",{href:!0});var EVr=s(PD);Vrr=r(EVr,"FlaxPegasusModel"),EVr.forEach(t),Wrr=r(Y3e," (Pegasus model)"),Y3e.forEach(t),Hrr=i(oe),KT=n(oe,"LI",{});var Z3e=s(KT);ffe=n(Z3e,"STRONG",{});var CVr=s(ffe);Urr=r(CVr,"roberta"),CVr.forEach(t),Jrr=r(Z3e," \u2014 "),SD=n(Z3e,"A",{href:!0});var MVr=s(SD);Krr=r(MVr,"FlaxRobertaModel"),MVr.forEach(t),Yrr=r(Z3e," (RoBERTa model)"),Z3e.forEach(t),Zrr=i(oe),YT=n(oe,"LI",{});var eMe=s(YT);mfe=n(eMe,"STRONG",{});var yVr=s(mfe);etr=r(yVr,"t5"),yVr.forEach(t),otr=r(eMe," \u2014 "),$D=n(eMe,"A",{href:!0});var wVr=s($D);rtr=r(wVr,"FlaxT5Model"),wVr.forEach(t),ttr=r(eMe," (T5 model)"),eMe.forEach(t),atr=i(oe),ZT=n(oe,"LI",{});var oMe=s(ZT);hfe=n(oMe,"STRONG",{});var AVr=s(hfe);ntr=r(AVr,"vision-text-dual-encoder"),AVr.forEach(t),str=r(oMe," \u2014 "),ID=n(oMe,"A",{href:!0});var LVr=s(ID);ltr=r(LVr,"FlaxVisionTextDualEncoderModel"),LVr.forEach(t),itr=r(oMe," (VisionTextDualEncoder model)"),oMe.forEach(t),dtr=i(oe),eF=n(oe,"LI",{});var rMe=s(eF);gfe=n(rMe,"STRONG",{});var BVr=s(gfe);ctr=r(BVr,"vit"),BVr.forEach(t),ftr=r(rMe," \u2014 "),DD=n(rMe,"A",{href:!0});var xVr=s(DD);mtr=r(xVr,"FlaxViTModel"),xVr.forEach(t),htr=r(rMe," (ViT model)"),rMe.forEach(t),gtr=i(oe),oF=n(oe,"LI",{});var tMe=s(oF);ufe=n(tMe,"STRONG",{});var kVr=s(ufe);utr=r(kVr,"wav2vec2"),kVr.forEach(t),ptr=r(tMe," \u2014 "),ND=n(tMe,"A",{href:!0});var RVr=s(ND);_tr=r(RVr,"FlaxWav2Vec2Model"),RVr.forEach(t),vtr=r(tMe," (Wav2Vec2 model)"),tMe.forEach(t),oe.forEach(t),btr=i(ta),pfe=n(ta,"P",{});var PVr=s(pfe);Ttr=r(PVr,"Examples:"),PVr.forEach(t),Ftr=i(ta),m(aw.$$.fragment,ta),ta.forEach(t),vl.forEach(t),kye=i(c),Wd=n(c,"H2",{class:!0});var j7e=s(Wd);rF=n(j7e,"A",{id:!0,class:!0,href:!0});var SVr=s(rF);_fe=n(SVr,"SPAN",{});var $Vr=s(_fe);m(nw.$$.fragment,$Vr),$Vr.forEach(t),SVr.forEach(t),Etr=i(j7e),vfe=n(j7e,"SPAN",{});var IVr=s(vfe);Ctr=r(IVr,"FlaxAutoModelForCausalLM"),IVr.forEach(t),j7e.forEach(t),Rye=i(c),vr=n(c,"DIV",{class:!0});var Tl=s(vr);m(sw.$$.fragment,Tl),Mtr=i(Tl),Hd=n(Tl,"P",{});var cO=s(Hd);ytr=r(cO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),bfe=n(cO,"CODE",{});var DVr=s(bfe);wtr=r(DVr,"from_pretrained()"),DVr.forEach(t),Atr=r(cO,` class method or the
`),Tfe=n(cO,"CODE",{});var NVr=s(Tfe);Ltr=r(NVr,"from_config()"),NVr.forEach(t),Btr=r(cO," class method."),cO.forEach(t),xtr=i(Tl),lw=n(Tl,"P",{});var O7e=s(lw);ktr=r(O7e,"This class cannot be instantiated directly using "),Ffe=n(O7e,"CODE",{});var jVr=s(Ffe);Rtr=r(jVr,"__init__()"),jVr.forEach(t),Ptr=r(O7e," (throws an error)."),O7e.forEach(t),Str=i(Tl),lt=n(Tl,"DIV",{class:!0});var Fl=s(lt);m(iw.$$.fragment,Fl),$tr=i(Fl),Efe=n(Fl,"P",{});var OVr=s(Efe);Itr=r(OVr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),OVr.forEach(t),Dtr=i(Fl),Ud=n(Fl,"P",{});var fO=s(Ud);Ntr=r(fO,`Note:
Loading a model from its configuration file does `),Cfe=n(fO,"STRONG",{});var GVr=s(Cfe);jtr=r(GVr,"not"),GVr.forEach(t),Otr=r(fO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mfe=n(fO,"CODE",{});var qVr=s(Mfe);Gtr=r(qVr,"from_pretrained()"),qVr.forEach(t),qtr=r(fO,` to load the model
weights.`),fO.forEach(t),ztr=i(Fl),yfe=n(Fl,"P",{});var zVr=s(yfe);Xtr=r(zVr,"Examples:"),zVr.forEach(t),Qtr=i(Fl),m(dw.$$.fragment,Fl),Fl.forEach(t),Vtr=i(Tl),To=n(Tl,"DIV",{class:!0});var aa=s(To);m(cw.$$.fragment,aa),Wtr=i(aa),wfe=n(aa,"P",{});var XVr=s(wfe);Htr=r(XVr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),XVr.forEach(t),Utr=i(aa),rn=n(aa,"P",{});var lC=s(rn);Jtr=r(lC,"The model class to instantiate is selected based on the "),Afe=n(lC,"CODE",{});var QVr=s(Afe);Ktr=r(QVr,"model_type"),QVr.forEach(t),Ytr=r(lC,` property of the config object (either
passed as an argument or loaded from `),Lfe=n(lC,"CODE",{});var VVr=s(Lfe);Ztr=r(VVr,"pretrained_model_name_or_path"),VVr.forEach(t),ear=r(lC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Bfe=n(lC,"CODE",{});var WVr=s(Bfe);oar=r(WVr,"pretrained_model_name_or_path"),WVr.forEach(t),rar=r(lC,":"),lC.forEach(t),tar=i(aa),Jd=n(aa,"UL",{});var mO=s(Jd);tF=n(mO,"LI",{});var aMe=s(tF);xfe=n(aMe,"STRONG",{});var HVr=s(xfe);aar=r(HVr,"gpt2"),HVr.forEach(t),nar=r(aMe," \u2014 "),jD=n(aMe,"A",{href:!0});var UVr=s(jD);sar=r(UVr,"FlaxGPT2LMHeadModel"),UVr.forEach(t),lar=r(aMe," (OpenAI GPT-2 model)"),aMe.forEach(t),iar=i(mO),aF=n(mO,"LI",{});var nMe=s(aF);kfe=n(nMe,"STRONG",{});var JVr=s(kfe);dar=r(JVr,"gpt_neo"),JVr.forEach(t),car=r(nMe," \u2014 "),OD=n(nMe,"A",{href:!0});var KVr=s(OD);far=r(KVr,"FlaxGPTNeoForCausalLM"),KVr.forEach(t),mar=r(nMe," (GPT Neo model)"),nMe.forEach(t),har=i(mO),nF=n(mO,"LI",{});var sMe=s(nF);Rfe=n(sMe,"STRONG",{});var YVr=s(Rfe);gar=r(YVr,"gptj"),YVr.forEach(t),uar=r(sMe," \u2014 "),GD=n(sMe,"A",{href:!0});var ZVr=s(GD);par=r(ZVr,"FlaxGPTJForCausalLM"),ZVr.forEach(t),_ar=r(sMe," (GPT-J model)"),sMe.forEach(t),mO.forEach(t),bar=i(aa),Pfe=n(aa,"P",{});var eWr=s(Pfe);Tar=r(eWr,"Examples:"),eWr.forEach(t),Far=i(aa),m(fw.$$.fragment,aa),aa.forEach(t),Tl.forEach(t),Pye=i(c),Kd=n(c,"H2",{class:!0});var G7e=s(Kd);sF=n(G7e,"A",{id:!0,class:!0,href:!0});var oWr=s(sF);Sfe=n(oWr,"SPAN",{});var rWr=s(Sfe);m(mw.$$.fragment,rWr),rWr.forEach(t),oWr.forEach(t),Ear=i(G7e),$fe=n(G7e,"SPAN",{});var tWr=s($fe);Car=r(tWr,"FlaxAutoModelForPreTraining"),tWr.forEach(t),G7e.forEach(t),Sye=i(c),br=n(c,"DIV",{class:!0});var El=s(br);m(hw.$$.fragment,El),Mar=i(El),Yd=n(El,"P",{});var hO=s(Yd);yar=r(hO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Ife=n(hO,"CODE",{});var aWr=s(Ife);war=r(aWr,"from_pretrained()"),aWr.forEach(t),Aar=r(hO,` class method or the
`),Dfe=n(hO,"CODE",{});var nWr=s(Dfe);Lar=r(nWr,"from_config()"),nWr.forEach(t),Bar=r(hO," class method."),hO.forEach(t),xar=i(El),gw=n(El,"P",{});var q7e=s(gw);kar=r(q7e,"This class cannot be instantiated directly using "),Nfe=n(q7e,"CODE",{});var sWr=s(Nfe);Rar=r(sWr,"__init__()"),sWr.forEach(t),Par=r(q7e," (throws an error)."),q7e.forEach(t),Sar=i(El),it=n(El,"DIV",{class:!0});var Cl=s(it);m(uw.$$.fragment,Cl),$ar=i(Cl),jfe=n(Cl,"P",{});var lWr=s(jfe);Iar=r(lWr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),lWr.forEach(t),Dar=i(Cl),Zd=n(Cl,"P",{});var gO=s(Zd);Nar=r(gO,`Note:
Loading a model from its configuration file does `),Ofe=n(gO,"STRONG",{});var iWr=s(Ofe);jar=r(iWr,"not"),iWr.forEach(t),Oar=r(gO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gfe=n(gO,"CODE",{});var dWr=s(Gfe);Gar=r(dWr,"from_pretrained()"),dWr.forEach(t),qar=r(gO,` to load the model
weights.`),gO.forEach(t),zar=i(Cl),qfe=n(Cl,"P",{});var cWr=s(qfe);Xar=r(cWr,"Examples:"),cWr.forEach(t),Qar=i(Cl),m(pw.$$.fragment,Cl),Cl.forEach(t),Var=i(El),Fo=n(El,"DIV",{class:!0});var na=s(Fo);m(_w.$$.fragment,na),War=i(na),zfe=n(na,"P",{});var fWr=s(zfe);Har=r(fWr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),fWr.forEach(t),Uar=i(na),tn=n(na,"P",{});var iC=s(tn);Jar=r(iC,"The model class to instantiate is selected based on the "),Xfe=n(iC,"CODE",{});var mWr=s(Xfe);Kar=r(mWr,"model_type"),mWr.forEach(t),Yar=r(iC,` property of the config object (either
passed as an argument or loaded from `),Qfe=n(iC,"CODE",{});var hWr=s(Qfe);Zar=r(hWr,"pretrained_model_name_or_path"),hWr.forEach(t),enr=r(iC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Vfe=n(iC,"CODE",{});var gWr=s(Vfe);onr=r(gWr,"pretrained_model_name_or_path"),gWr.forEach(t),rnr=r(iC,":"),iC.forEach(t),tnr=i(na),Te=n(na,"UL",{});var Be=s(Te);lF=n(Be,"LI",{});var lMe=s(lF);Wfe=n(lMe,"STRONG",{});var uWr=s(Wfe);anr=r(uWr,"albert"),uWr.forEach(t),nnr=r(lMe," \u2014 "),qD=n(lMe,"A",{href:!0});var pWr=s(qD);snr=r(pWr,"FlaxAlbertForPreTraining"),pWr.forEach(t),lnr=r(lMe," (ALBERT model)"),lMe.forEach(t),inr=i(Be),iF=n(Be,"LI",{});var iMe=s(iF);Hfe=n(iMe,"STRONG",{});var _Wr=s(Hfe);dnr=r(_Wr,"bart"),_Wr.forEach(t),cnr=r(iMe," \u2014 "),zD=n(iMe,"A",{href:!0});var vWr=s(zD);fnr=r(vWr,"FlaxBartForConditionalGeneration"),vWr.forEach(t),mnr=r(iMe," (BART model)"),iMe.forEach(t),hnr=i(Be),dF=n(Be,"LI",{});var dMe=s(dF);Ufe=n(dMe,"STRONG",{});var bWr=s(Ufe);gnr=r(bWr,"bert"),bWr.forEach(t),unr=r(dMe," \u2014 "),XD=n(dMe,"A",{href:!0});var TWr=s(XD);pnr=r(TWr,"FlaxBertForPreTraining"),TWr.forEach(t),_nr=r(dMe," (BERT model)"),dMe.forEach(t),vnr=i(Be),cF=n(Be,"LI",{});var cMe=s(cF);Jfe=n(cMe,"STRONG",{});var FWr=s(Jfe);bnr=r(FWr,"big_bird"),FWr.forEach(t),Tnr=r(cMe," \u2014 "),QD=n(cMe,"A",{href:!0});var EWr=s(QD);Fnr=r(EWr,"FlaxBigBirdForPreTraining"),EWr.forEach(t),Enr=r(cMe," (BigBird model)"),cMe.forEach(t),Cnr=i(Be),fF=n(Be,"LI",{});var fMe=s(fF);Kfe=n(fMe,"STRONG",{});var CWr=s(Kfe);Mnr=r(CWr,"electra"),CWr.forEach(t),ynr=r(fMe," \u2014 "),VD=n(fMe,"A",{href:!0});var MWr=s(VD);wnr=r(MWr,"FlaxElectraForPreTraining"),MWr.forEach(t),Anr=r(fMe," (ELECTRA model)"),fMe.forEach(t),Lnr=i(Be),mF=n(Be,"LI",{});var mMe=s(mF);Yfe=n(mMe,"STRONG",{});var yWr=s(Yfe);Bnr=r(yWr,"mbart"),yWr.forEach(t),xnr=r(mMe," \u2014 "),WD=n(mMe,"A",{href:!0});var wWr=s(WD);knr=r(wWr,"FlaxMBartForConditionalGeneration"),wWr.forEach(t),Rnr=r(mMe," (mBART model)"),mMe.forEach(t),Pnr=i(Be),hF=n(Be,"LI",{});var hMe=s(hF);Zfe=n(hMe,"STRONG",{});var AWr=s(Zfe);Snr=r(AWr,"mt5"),AWr.forEach(t),$nr=r(hMe," \u2014 "),HD=n(hMe,"A",{href:!0});var LWr=s(HD);Inr=r(LWr,"FlaxMT5ForConditionalGeneration"),LWr.forEach(t),Dnr=r(hMe," (mT5 model)"),hMe.forEach(t),Nnr=i(Be),gF=n(Be,"LI",{});var gMe=s(gF);eme=n(gMe,"STRONG",{});var BWr=s(eme);jnr=r(BWr,"roberta"),BWr.forEach(t),Onr=r(gMe," \u2014 "),UD=n(gMe,"A",{href:!0});var xWr=s(UD);Gnr=r(xWr,"FlaxRobertaForMaskedLM"),xWr.forEach(t),qnr=r(gMe," (RoBERTa model)"),gMe.forEach(t),znr=i(Be),uF=n(Be,"LI",{});var uMe=s(uF);ome=n(uMe,"STRONG",{});var kWr=s(ome);Xnr=r(kWr,"t5"),kWr.forEach(t),Qnr=r(uMe," \u2014 "),JD=n(uMe,"A",{href:!0});var RWr=s(JD);Vnr=r(RWr,"FlaxT5ForConditionalGeneration"),RWr.forEach(t),Wnr=r(uMe," (T5 model)"),uMe.forEach(t),Hnr=i(Be),pF=n(Be,"LI",{});var pMe=s(pF);rme=n(pMe,"STRONG",{});var PWr=s(rme);Unr=r(PWr,"wav2vec2"),PWr.forEach(t),Jnr=r(pMe," \u2014 "),KD=n(pMe,"A",{href:!0});var SWr=s(KD);Knr=r(SWr,"FlaxWav2Vec2ForPreTraining"),SWr.forEach(t),Ynr=r(pMe," (Wav2Vec2 model)"),pMe.forEach(t),Be.forEach(t),Znr=i(na),tme=n(na,"P",{});var $Wr=s(tme);esr=r($Wr,"Examples:"),$Wr.forEach(t),osr=i(na),m(vw.$$.fragment,na),na.forEach(t),El.forEach(t),$ye=i(c),ec=n(c,"H2",{class:!0});var z7e=s(ec);_F=n(z7e,"A",{id:!0,class:!0,href:!0});var IWr=s(_F);ame=n(IWr,"SPAN",{});var DWr=s(ame);m(bw.$$.fragment,DWr),DWr.forEach(t),IWr.forEach(t),rsr=i(z7e),nme=n(z7e,"SPAN",{});var NWr=s(nme);tsr=r(NWr,"FlaxAutoModelForMaskedLM"),NWr.forEach(t),z7e.forEach(t),Iye=i(c),Tr=n(c,"DIV",{class:!0});var Ml=s(Tr);m(Tw.$$.fragment,Ml),asr=i(Ml),oc=n(Ml,"P",{});var uO=s(oc);nsr=r(uO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),sme=n(uO,"CODE",{});var jWr=s(sme);ssr=r(jWr,"from_pretrained()"),jWr.forEach(t),lsr=r(uO,` class method or the
`),lme=n(uO,"CODE",{});var OWr=s(lme);isr=r(OWr,"from_config()"),OWr.forEach(t),dsr=r(uO," class method."),uO.forEach(t),csr=i(Ml),Fw=n(Ml,"P",{});var X7e=s(Fw);fsr=r(X7e,"This class cannot be instantiated directly using "),ime=n(X7e,"CODE",{});var GWr=s(ime);msr=r(GWr,"__init__()"),GWr.forEach(t),hsr=r(X7e," (throws an error)."),X7e.forEach(t),gsr=i(Ml),dt=n(Ml,"DIV",{class:!0});var yl=s(dt);m(Ew.$$.fragment,yl),usr=i(yl),dme=n(yl,"P",{});var qWr=s(dme);psr=r(qWr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),qWr.forEach(t),_sr=i(yl),rc=n(yl,"P",{});var pO=s(rc);vsr=r(pO,`Note:
Loading a model from its configuration file does `),cme=n(pO,"STRONG",{});var zWr=s(cme);bsr=r(zWr,"not"),zWr.forEach(t),Tsr=r(pO,` load the model weights. It only affects the
model\u2019s configuration. Use `),fme=n(pO,"CODE",{});var XWr=s(fme);Fsr=r(XWr,"from_pretrained()"),XWr.forEach(t),Esr=r(pO,` to load the model
weights.`),pO.forEach(t),Csr=i(yl),mme=n(yl,"P",{});var QWr=s(mme);Msr=r(QWr,"Examples:"),QWr.forEach(t),ysr=i(yl),m(Cw.$$.fragment,yl),yl.forEach(t),wsr=i(Ml),Eo=n(Ml,"DIV",{class:!0});var sa=s(Eo);m(Mw.$$.fragment,sa),Asr=i(sa),hme=n(sa,"P",{});var VWr=s(hme);Lsr=r(VWr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),VWr.forEach(t),Bsr=i(sa),an=n(sa,"P",{});var dC=s(an);xsr=r(dC,"The model class to instantiate is selected based on the "),gme=n(dC,"CODE",{});var WWr=s(gme);ksr=r(WWr,"model_type"),WWr.forEach(t),Rsr=r(dC,` property of the config object (either
passed as an argument or loaded from `),ume=n(dC,"CODE",{});var HWr=s(ume);Psr=r(HWr,"pretrained_model_name_or_path"),HWr.forEach(t),Ssr=r(dC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),pme=n(dC,"CODE",{});var UWr=s(pme);$sr=r(UWr,"pretrained_model_name_or_path"),UWr.forEach(t),Isr=r(dC,":"),dC.forEach(t),Dsr=i(sa),ke=n(sa,"UL",{});var Ro=s(ke);vF=n(Ro,"LI",{});var _Me=s(vF);_me=n(_Me,"STRONG",{});var JWr=s(_me);Nsr=r(JWr,"albert"),JWr.forEach(t),jsr=r(_Me," \u2014 "),YD=n(_Me,"A",{href:!0});var KWr=s(YD);Osr=r(KWr,"FlaxAlbertForMaskedLM"),KWr.forEach(t),Gsr=r(_Me," (ALBERT model)"),_Me.forEach(t),qsr=i(Ro),bF=n(Ro,"LI",{});var vMe=s(bF);vme=n(vMe,"STRONG",{});var YWr=s(vme);zsr=r(YWr,"bart"),YWr.forEach(t),Xsr=r(vMe," \u2014 "),ZD=n(vMe,"A",{href:!0});var ZWr=s(ZD);Qsr=r(ZWr,"FlaxBartForConditionalGeneration"),ZWr.forEach(t),Vsr=r(vMe," (BART model)"),vMe.forEach(t),Wsr=i(Ro),TF=n(Ro,"LI",{});var bMe=s(TF);bme=n(bMe,"STRONG",{});var eHr=s(bme);Hsr=r(eHr,"bert"),eHr.forEach(t),Usr=r(bMe," \u2014 "),eN=n(bMe,"A",{href:!0});var oHr=s(eN);Jsr=r(oHr,"FlaxBertForMaskedLM"),oHr.forEach(t),Ksr=r(bMe," (BERT model)"),bMe.forEach(t),Ysr=i(Ro),FF=n(Ro,"LI",{});var TMe=s(FF);Tme=n(TMe,"STRONG",{});var rHr=s(Tme);Zsr=r(rHr,"big_bird"),rHr.forEach(t),elr=r(TMe," \u2014 "),oN=n(TMe,"A",{href:!0});var tHr=s(oN);olr=r(tHr,"FlaxBigBirdForMaskedLM"),tHr.forEach(t),rlr=r(TMe," (BigBird model)"),TMe.forEach(t),tlr=i(Ro),EF=n(Ro,"LI",{});var FMe=s(EF);Fme=n(FMe,"STRONG",{});var aHr=s(Fme);alr=r(aHr,"distilbert"),aHr.forEach(t),nlr=r(FMe," \u2014 "),rN=n(FMe,"A",{href:!0});var nHr=s(rN);slr=r(nHr,"FlaxDistilBertForMaskedLM"),nHr.forEach(t),llr=r(FMe," (DistilBERT model)"),FMe.forEach(t),ilr=i(Ro),CF=n(Ro,"LI",{});var EMe=s(CF);Eme=n(EMe,"STRONG",{});var sHr=s(Eme);dlr=r(sHr,"electra"),sHr.forEach(t),clr=r(EMe," \u2014 "),tN=n(EMe,"A",{href:!0});var lHr=s(tN);flr=r(lHr,"FlaxElectraForMaskedLM"),lHr.forEach(t),mlr=r(EMe," (ELECTRA model)"),EMe.forEach(t),hlr=i(Ro),MF=n(Ro,"LI",{});var CMe=s(MF);Cme=n(CMe,"STRONG",{});var iHr=s(Cme);glr=r(iHr,"mbart"),iHr.forEach(t),ulr=r(CMe," \u2014 "),aN=n(CMe,"A",{href:!0});var dHr=s(aN);plr=r(dHr,"FlaxMBartForConditionalGeneration"),dHr.forEach(t),_lr=r(CMe," (mBART model)"),CMe.forEach(t),vlr=i(Ro),yF=n(Ro,"LI",{});var MMe=s(yF);Mme=n(MMe,"STRONG",{});var cHr=s(Mme);blr=r(cHr,"roberta"),cHr.forEach(t),Tlr=r(MMe," \u2014 "),nN=n(MMe,"A",{href:!0});var fHr=s(nN);Flr=r(fHr,"FlaxRobertaForMaskedLM"),fHr.forEach(t),Elr=r(MMe," (RoBERTa model)"),MMe.forEach(t),Ro.forEach(t),Clr=i(sa),yme=n(sa,"P",{});var mHr=s(yme);Mlr=r(mHr,"Examples:"),mHr.forEach(t),ylr=i(sa),m(yw.$$.fragment,sa),sa.forEach(t),Ml.forEach(t),Dye=i(c),tc=n(c,"H2",{class:!0});var Q7e=s(tc);wF=n(Q7e,"A",{id:!0,class:!0,href:!0});var hHr=s(wF);wme=n(hHr,"SPAN",{});var gHr=s(wme);m(ww.$$.fragment,gHr),gHr.forEach(t),hHr.forEach(t),wlr=i(Q7e),Ame=n(Q7e,"SPAN",{});var uHr=s(Ame);Alr=r(uHr,"FlaxAutoModelForSeq2SeqLM"),uHr.forEach(t),Q7e.forEach(t),Nye=i(c),Fr=n(c,"DIV",{class:!0});var wl=s(Fr);m(Aw.$$.fragment,wl),Llr=i(wl),ac=n(wl,"P",{});var _O=s(ac);Blr=r(_O,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Lme=n(_O,"CODE",{});var pHr=s(Lme);xlr=r(pHr,"from_pretrained()"),pHr.forEach(t),klr=r(_O,` class method or the
`),Bme=n(_O,"CODE",{});var _Hr=s(Bme);Rlr=r(_Hr,"from_config()"),_Hr.forEach(t),Plr=r(_O," class method."),_O.forEach(t),Slr=i(wl),Lw=n(wl,"P",{});var V7e=s(Lw);$lr=r(V7e,"This class cannot be instantiated directly using "),xme=n(V7e,"CODE",{});var vHr=s(xme);Ilr=r(vHr,"__init__()"),vHr.forEach(t),Dlr=r(V7e," (throws an error)."),V7e.forEach(t),Nlr=i(wl),ct=n(wl,"DIV",{class:!0});var Al=s(ct);m(Bw.$$.fragment,Al),jlr=i(Al),kme=n(Al,"P",{});var bHr=s(kme);Olr=r(bHr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),bHr.forEach(t),Glr=i(Al),nc=n(Al,"P",{});var vO=s(nc);qlr=r(vO,`Note:
Loading a model from its configuration file does `),Rme=n(vO,"STRONG",{});var THr=s(Rme);zlr=r(THr,"not"),THr.forEach(t),Xlr=r(vO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pme=n(vO,"CODE",{});var FHr=s(Pme);Qlr=r(FHr,"from_pretrained()"),FHr.forEach(t),Vlr=r(vO,` to load the model
weights.`),vO.forEach(t),Wlr=i(Al),Sme=n(Al,"P",{});var EHr=s(Sme);Hlr=r(EHr,"Examples:"),EHr.forEach(t),Ulr=i(Al),m(xw.$$.fragment,Al),Al.forEach(t),Jlr=i(wl),Co=n(wl,"DIV",{class:!0});var la=s(Co);m(kw.$$.fragment,la),Klr=i(la),$me=n(la,"P",{});var CHr=s($me);Ylr=r(CHr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),CHr.forEach(t),Zlr=i(la),nn=n(la,"P",{});var cC=s(nn);eir=r(cC,"The model class to instantiate is selected based on the "),Ime=n(cC,"CODE",{});var MHr=s(Ime);oir=r(MHr,"model_type"),MHr.forEach(t),rir=r(cC,` property of the config object (either
passed as an argument or loaded from `),Dme=n(cC,"CODE",{});var yHr=s(Dme);tir=r(yHr,"pretrained_model_name_or_path"),yHr.forEach(t),air=r(cC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Nme=n(cC,"CODE",{});var wHr=s(Nme);nir=r(wHr,"pretrained_model_name_or_path"),wHr.forEach(t),sir=r(cC,":"),cC.forEach(t),lir=i(la),Me=n(la,"UL",{});var oo=s(Me);AF=n(oo,"LI",{});var yMe=s(AF);jme=n(yMe,"STRONG",{});var AHr=s(jme);iir=r(AHr,"bart"),AHr.forEach(t),dir=r(yMe," \u2014 "),sN=n(yMe,"A",{href:!0});var LHr=s(sN);cir=r(LHr,"FlaxBartForConditionalGeneration"),LHr.forEach(t),fir=r(yMe," (BART model)"),yMe.forEach(t),mir=i(oo),LF=n(oo,"LI",{});var wMe=s(LF);Ome=n(wMe,"STRONG",{});var BHr=s(Ome);hir=r(BHr,"blenderbot"),BHr.forEach(t),gir=r(wMe," \u2014 "),lN=n(wMe,"A",{href:!0});var xHr=s(lN);uir=r(xHr,"FlaxBlenderbotForConditionalGeneration"),xHr.forEach(t),pir=r(wMe," (Blenderbot model)"),wMe.forEach(t),_ir=i(oo),BF=n(oo,"LI",{});var AMe=s(BF);Gme=n(AMe,"STRONG",{});var kHr=s(Gme);vir=r(kHr,"blenderbot-small"),kHr.forEach(t),bir=r(AMe," \u2014 "),iN=n(AMe,"A",{href:!0});var RHr=s(iN);Tir=r(RHr,"FlaxBlenderbotSmallForConditionalGeneration"),RHr.forEach(t),Fir=r(AMe," (BlenderbotSmall model)"),AMe.forEach(t),Eir=i(oo),xF=n(oo,"LI",{});var LMe=s(xF);qme=n(LMe,"STRONG",{});var PHr=s(qme);Cir=r(PHr,"encoder-decoder"),PHr.forEach(t),Mir=r(LMe," \u2014 "),dN=n(LMe,"A",{href:!0});var SHr=s(dN);yir=r(SHr,"FlaxEncoderDecoderModel"),SHr.forEach(t),wir=r(LMe," (Encoder decoder model)"),LMe.forEach(t),Air=i(oo),kF=n(oo,"LI",{});var BMe=s(kF);zme=n(BMe,"STRONG",{});var $Hr=s(zme);Lir=r($Hr,"marian"),$Hr.forEach(t),Bir=r(BMe," \u2014 "),cN=n(BMe,"A",{href:!0});var IHr=s(cN);xir=r(IHr,"FlaxMarianMTModel"),IHr.forEach(t),kir=r(BMe," (Marian model)"),BMe.forEach(t),Rir=i(oo),RF=n(oo,"LI",{});var xMe=s(RF);Xme=n(xMe,"STRONG",{});var DHr=s(Xme);Pir=r(DHr,"mbart"),DHr.forEach(t),Sir=r(xMe," \u2014 "),fN=n(xMe,"A",{href:!0});var NHr=s(fN);$ir=r(NHr,"FlaxMBartForConditionalGeneration"),NHr.forEach(t),Iir=r(xMe," (mBART model)"),xMe.forEach(t),Dir=i(oo),PF=n(oo,"LI",{});var kMe=s(PF);Qme=n(kMe,"STRONG",{});var jHr=s(Qme);Nir=r(jHr,"mt5"),jHr.forEach(t),jir=r(kMe," \u2014 "),mN=n(kMe,"A",{href:!0});var OHr=s(mN);Oir=r(OHr,"FlaxMT5ForConditionalGeneration"),OHr.forEach(t),Gir=r(kMe," (mT5 model)"),kMe.forEach(t),qir=i(oo),SF=n(oo,"LI",{});var RMe=s(SF);Vme=n(RMe,"STRONG",{});var GHr=s(Vme);zir=r(GHr,"pegasus"),GHr.forEach(t),Xir=r(RMe," \u2014 "),hN=n(RMe,"A",{href:!0});var qHr=s(hN);Qir=r(qHr,"FlaxPegasusForConditionalGeneration"),qHr.forEach(t),Vir=r(RMe," (Pegasus model)"),RMe.forEach(t),Wir=i(oo),$F=n(oo,"LI",{});var PMe=s($F);Wme=n(PMe,"STRONG",{});var zHr=s(Wme);Hir=r(zHr,"t5"),zHr.forEach(t),Uir=r(PMe," \u2014 "),gN=n(PMe,"A",{href:!0});var XHr=s(gN);Jir=r(XHr,"FlaxT5ForConditionalGeneration"),XHr.forEach(t),Kir=r(PMe," (T5 model)"),PMe.forEach(t),oo.forEach(t),Yir=i(la),Hme=n(la,"P",{});var QHr=s(Hme);Zir=r(QHr,"Examples:"),QHr.forEach(t),edr=i(la),m(Rw.$$.fragment,la),la.forEach(t),wl.forEach(t),jye=i(c),sc=n(c,"H2",{class:!0});var W7e=s(sc);IF=n(W7e,"A",{id:!0,class:!0,href:!0});var VHr=s(IF);Ume=n(VHr,"SPAN",{});var WHr=s(Ume);m(Pw.$$.fragment,WHr),WHr.forEach(t),VHr.forEach(t),odr=i(W7e),Jme=n(W7e,"SPAN",{});var HHr=s(Jme);rdr=r(HHr,"FlaxAutoModelForSequenceClassification"),HHr.forEach(t),W7e.forEach(t),Oye=i(c),Er=n(c,"DIV",{class:!0});var Ll=s(Er);m(Sw.$$.fragment,Ll),tdr=i(Ll),lc=n(Ll,"P",{});var bO=s(lc);adr=r(bO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Kme=n(bO,"CODE",{});var UHr=s(Kme);ndr=r(UHr,"from_pretrained()"),UHr.forEach(t),sdr=r(bO,` class method or the
`),Yme=n(bO,"CODE",{});var JHr=s(Yme);ldr=r(JHr,"from_config()"),JHr.forEach(t),idr=r(bO," class method."),bO.forEach(t),ddr=i(Ll),$w=n(Ll,"P",{});var H7e=s($w);cdr=r(H7e,"This class cannot be instantiated directly using "),Zme=n(H7e,"CODE",{});var KHr=s(Zme);fdr=r(KHr,"__init__()"),KHr.forEach(t),mdr=r(H7e," (throws an error)."),H7e.forEach(t),hdr=i(Ll),ft=n(Ll,"DIV",{class:!0});var Bl=s(ft);m(Iw.$$.fragment,Bl),gdr=i(Bl),ehe=n(Bl,"P",{});var YHr=s(ehe);udr=r(YHr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),YHr.forEach(t),pdr=i(Bl),ic=n(Bl,"P",{});var TO=s(ic);_dr=r(TO,`Note:
Loading a model from its configuration file does `),ohe=n(TO,"STRONG",{});var ZHr=s(ohe);vdr=r(ZHr,"not"),ZHr.forEach(t),bdr=r(TO,` load the model weights. It only affects the
model\u2019s configuration. Use `),rhe=n(TO,"CODE",{});var eUr=s(rhe);Tdr=r(eUr,"from_pretrained()"),eUr.forEach(t),Fdr=r(TO,` to load the model
weights.`),TO.forEach(t),Edr=i(Bl),the=n(Bl,"P",{});var oUr=s(the);Cdr=r(oUr,"Examples:"),oUr.forEach(t),Mdr=i(Bl),m(Dw.$$.fragment,Bl),Bl.forEach(t),ydr=i(Ll),Mo=n(Ll,"DIV",{class:!0});var ia=s(Mo);m(Nw.$$.fragment,ia),wdr=i(ia),ahe=n(ia,"P",{});var rUr=s(ahe);Adr=r(rUr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),rUr.forEach(t),Ldr=i(ia),sn=n(ia,"P",{});var fC=s(sn);Bdr=r(fC,"The model class to instantiate is selected based on the "),nhe=n(fC,"CODE",{});var tUr=s(nhe);xdr=r(tUr,"model_type"),tUr.forEach(t),kdr=r(fC,` property of the config object (either
passed as an argument or loaded from `),she=n(fC,"CODE",{});var aUr=s(she);Rdr=r(aUr,"pretrained_model_name_or_path"),aUr.forEach(t),Pdr=r(fC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),lhe=n(fC,"CODE",{});var nUr=s(lhe);Sdr=r(nUr,"pretrained_model_name_or_path"),nUr.forEach(t),$dr=r(fC,":"),fC.forEach(t),Idr=i(ia),Re=n(ia,"UL",{});var Po=s(Re);DF=n(Po,"LI",{});var SMe=s(DF);ihe=n(SMe,"STRONG",{});var sUr=s(ihe);Ddr=r(sUr,"albert"),sUr.forEach(t),Ndr=r(SMe," \u2014 "),uN=n(SMe,"A",{href:!0});var lUr=s(uN);jdr=r(lUr,"FlaxAlbertForSequenceClassification"),lUr.forEach(t),Odr=r(SMe," (ALBERT model)"),SMe.forEach(t),Gdr=i(Po),NF=n(Po,"LI",{});var $Me=s(NF);dhe=n($Me,"STRONG",{});var iUr=s(dhe);qdr=r(iUr,"bart"),iUr.forEach(t),zdr=r($Me," \u2014 "),pN=n($Me,"A",{href:!0});var dUr=s(pN);Xdr=r(dUr,"FlaxBartForSequenceClassification"),dUr.forEach(t),Qdr=r($Me," (BART model)"),$Me.forEach(t),Vdr=i(Po),jF=n(Po,"LI",{});var IMe=s(jF);che=n(IMe,"STRONG",{});var cUr=s(che);Wdr=r(cUr,"bert"),cUr.forEach(t),Hdr=r(IMe," \u2014 "),_N=n(IMe,"A",{href:!0});var fUr=s(_N);Udr=r(fUr,"FlaxBertForSequenceClassification"),fUr.forEach(t),Jdr=r(IMe," (BERT model)"),IMe.forEach(t),Kdr=i(Po),OF=n(Po,"LI",{});var DMe=s(OF);fhe=n(DMe,"STRONG",{});var mUr=s(fhe);Ydr=r(mUr,"big_bird"),mUr.forEach(t),Zdr=r(DMe," \u2014 "),vN=n(DMe,"A",{href:!0});var hUr=s(vN);ecr=r(hUr,"FlaxBigBirdForSequenceClassification"),hUr.forEach(t),ocr=r(DMe," (BigBird model)"),DMe.forEach(t),rcr=i(Po),GF=n(Po,"LI",{});var NMe=s(GF);mhe=n(NMe,"STRONG",{});var gUr=s(mhe);tcr=r(gUr,"distilbert"),gUr.forEach(t),acr=r(NMe," \u2014 "),bN=n(NMe,"A",{href:!0});var uUr=s(bN);ncr=r(uUr,"FlaxDistilBertForSequenceClassification"),uUr.forEach(t),scr=r(NMe," (DistilBERT model)"),NMe.forEach(t),lcr=i(Po),qF=n(Po,"LI",{});var jMe=s(qF);hhe=n(jMe,"STRONG",{});var pUr=s(hhe);icr=r(pUr,"electra"),pUr.forEach(t),dcr=r(jMe," \u2014 "),TN=n(jMe,"A",{href:!0});var _Ur=s(TN);ccr=r(_Ur,"FlaxElectraForSequenceClassification"),_Ur.forEach(t),fcr=r(jMe," (ELECTRA model)"),jMe.forEach(t),mcr=i(Po),zF=n(Po,"LI",{});var OMe=s(zF);ghe=n(OMe,"STRONG",{});var vUr=s(ghe);hcr=r(vUr,"mbart"),vUr.forEach(t),gcr=r(OMe," \u2014 "),FN=n(OMe,"A",{href:!0});var bUr=s(FN);ucr=r(bUr,"FlaxMBartForSequenceClassification"),bUr.forEach(t),pcr=r(OMe," (mBART model)"),OMe.forEach(t),_cr=i(Po),XF=n(Po,"LI",{});var GMe=s(XF);uhe=n(GMe,"STRONG",{});var TUr=s(uhe);vcr=r(TUr,"roberta"),TUr.forEach(t),bcr=r(GMe," \u2014 "),EN=n(GMe,"A",{href:!0});var FUr=s(EN);Tcr=r(FUr,"FlaxRobertaForSequenceClassification"),FUr.forEach(t),Fcr=r(GMe," (RoBERTa model)"),GMe.forEach(t),Po.forEach(t),Ecr=i(ia),phe=n(ia,"P",{});var EUr=s(phe);Ccr=r(EUr,"Examples:"),EUr.forEach(t),Mcr=i(ia),m(jw.$$.fragment,ia),ia.forEach(t),Ll.forEach(t),Gye=i(c),dc=n(c,"H2",{class:!0});var U7e=s(dc);QF=n(U7e,"A",{id:!0,class:!0,href:!0});var CUr=s(QF);_he=n(CUr,"SPAN",{});var MUr=s(_he);m(Ow.$$.fragment,MUr),MUr.forEach(t),CUr.forEach(t),ycr=i(U7e),vhe=n(U7e,"SPAN",{});var yUr=s(vhe);wcr=r(yUr,"FlaxAutoModelForQuestionAnswering"),yUr.forEach(t),U7e.forEach(t),qye=i(c),Cr=n(c,"DIV",{class:!0});var xl=s(Cr);m(Gw.$$.fragment,xl),Acr=i(xl),cc=n(xl,"P",{});var FO=s(cc);Lcr=r(FO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),bhe=n(FO,"CODE",{});var wUr=s(bhe);Bcr=r(wUr,"from_pretrained()"),wUr.forEach(t),xcr=r(FO,` class method or the
`),The=n(FO,"CODE",{});var AUr=s(The);kcr=r(AUr,"from_config()"),AUr.forEach(t),Rcr=r(FO," class method."),FO.forEach(t),Pcr=i(xl),qw=n(xl,"P",{});var J7e=s(qw);Scr=r(J7e,"This class cannot be instantiated directly using "),Fhe=n(J7e,"CODE",{});var LUr=s(Fhe);$cr=r(LUr,"__init__()"),LUr.forEach(t),Icr=r(J7e," (throws an error)."),J7e.forEach(t),Dcr=i(xl),mt=n(xl,"DIV",{class:!0});var kl=s(mt);m(zw.$$.fragment,kl),Ncr=i(kl),Ehe=n(kl,"P",{});var BUr=s(Ehe);jcr=r(BUr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),BUr.forEach(t),Ocr=i(kl),fc=n(kl,"P",{});var EO=s(fc);Gcr=r(EO,`Note:
Loading a model from its configuration file does `),Che=n(EO,"STRONG",{});var xUr=s(Che);qcr=r(xUr,"not"),xUr.forEach(t),zcr=r(EO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mhe=n(EO,"CODE",{});var kUr=s(Mhe);Xcr=r(kUr,"from_pretrained()"),kUr.forEach(t),Qcr=r(EO,` to load the model
weights.`),EO.forEach(t),Vcr=i(kl),yhe=n(kl,"P",{});var RUr=s(yhe);Wcr=r(RUr,"Examples:"),RUr.forEach(t),Hcr=i(kl),m(Xw.$$.fragment,kl),kl.forEach(t),Ucr=i(xl),yo=n(xl,"DIV",{class:!0});var da=s(yo);m(Qw.$$.fragment,da),Jcr=i(da),whe=n(da,"P",{});var PUr=s(whe);Kcr=r(PUr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),PUr.forEach(t),Ycr=i(da),ln=n(da,"P",{});var mC=s(ln);Zcr=r(mC,"The model class to instantiate is selected based on the "),Ahe=n(mC,"CODE",{});var SUr=s(Ahe);efr=r(SUr,"model_type"),SUr.forEach(t),ofr=r(mC,` property of the config object (either
passed as an argument or loaded from `),Lhe=n(mC,"CODE",{});var $Ur=s(Lhe);rfr=r($Ur,"pretrained_model_name_or_path"),$Ur.forEach(t),tfr=r(mC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Bhe=n(mC,"CODE",{});var IUr=s(Bhe);afr=r(IUr,"pretrained_model_name_or_path"),IUr.forEach(t),nfr=r(mC,":"),mC.forEach(t),sfr=i(da),Pe=n(da,"UL",{});var So=s(Pe);VF=n(So,"LI",{});var qMe=s(VF);xhe=n(qMe,"STRONG",{});var DUr=s(xhe);lfr=r(DUr,"albert"),DUr.forEach(t),ifr=r(qMe," \u2014 "),CN=n(qMe,"A",{href:!0});var NUr=s(CN);dfr=r(NUr,"FlaxAlbertForQuestionAnswering"),NUr.forEach(t),cfr=r(qMe," (ALBERT model)"),qMe.forEach(t),ffr=i(So),WF=n(So,"LI",{});var zMe=s(WF);khe=n(zMe,"STRONG",{});var jUr=s(khe);mfr=r(jUr,"bart"),jUr.forEach(t),hfr=r(zMe," \u2014 "),MN=n(zMe,"A",{href:!0});var OUr=s(MN);gfr=r(OUr,"FlaxBartForQuestionAnswering"),OUr.forEach(t),ufr=r(zMe," (BART model)"),zMe.forEach(t),pfr=i(So),HF=n(So,"LI",{});var XMe=s(HF);Rhe=n(XMe,"STRONG",{});var GUr=s(Rhe);_fr=r(GUr,"bert"),GUr.forEach(t),vfr=r(XMe," \u2014 "),yN=n(XMe,"A",{href:!0});var qUr=s(yN);bfr=r(qUr,"FlaxBertForQuestionAnswering"),qUr.forEach(t),Tfr=r(XMe," (BERT model)"),XMe.forEach(t),Ffr=i(So),UF=n(So,"LI",{});var QMe=s(UF);Phe=n(QMe,"STRONG",{});var zUr=s(Phe);Efr=r(zUr,"big_bird"),zUr.forEach(t),Cfr=r(QMe," \u2014 "),wN=n(QMe,"A",{href:!0});var XUr=s(wN);Mfr=r(XUr,"FlaxBigBirdForQuestionAnswering"),XUr.forEach(t),yfr=r(QMe," (BigBird model)"),QMe.forEach(t),wfr=i(So),JF=n(So,"LI",{});var VMe=s(JF);She=n(VMe,"STRONG",{});var QUr=s(She);Afr=r(QUr,"distilbert"),QUr.forEach(t),Lfr=r(VMe," \u2014 "),AN=n(VMe,"A",{href:!0});var VUr=s(AN);Bfr=r(VUr,"FlaxDistilBertForQuestionAnswering"),VUr.forEach(t),xfr=r(VMe," (DistilBERT model)"),VMe.forEach(t),kfr=i(So),KF=n(So,"LI",{});var WMe=s(KF);$he=n(WMe,"STRONG",{});var WUr=s($he);Rfr=r(WUr,"electra"),WUr.forEach(t),Pfr=r(WMe," \u2014 "),LN=n(WMe,"A",{href:!0});var HUr=s(LN);Sfr=r(HUr,"FlaxElectraForQuestionAnswering"),HUr.forEach(t),$fr=r(WMe," (ELECTRA model)"),WMe.forEach(t),Ifr=i(So),YF=n(So,"LI",{});var HMe=s(YF);Ihe=n(HMe,"STRONG",{});var UUr=s(Ihe);Dfr=r(UUr,"mbart"),UUr.forEach(t),Nfr=r(HMe," \u2014 "),BN=n(HMe,"A",{href:!0});var JUr=s(BN);jfr=r(JUr,"FlaxMBartForQuestionAnswering"),JUr.forEach(t),Ofr=r(HMe," (mBART model)"),HMe.forEach(t),Gfr=i(So),ZF=n(So,"LI",{});var UMe=s(ZF);Dhe=n(UMe,"STRONG",{});var KUr=s(Dhe);qfr=r(KUr,"roberta"),KUr.forEach(t),zfr=r(UMe," \u2014 "),xN=n(UMe,"A",{href:!0});var YUr=s(xN);Xfr=r(YUr,"FlaxRobertaForQuestionAnswering"),YUr.forEach(t),Qfr=r(UMe," (RoBERTa model)"),UMe.forEach(t),So.forEach(t),Vfr=i(da),Nhe=n(da,"P",{});var ZUr=s(Nhe);Wfr=r(ZUr,"Examples:"),ZUr.forEach(t),Hfr=i(da),m(Vw.$$.fragment,da),da.forEach(t),xl.forEach(t),zye=i(c),mc=n(c,"H2",{class:!0});var K7e=s(mc);eE=n(K7e,"A",{id:!0,class:!0,href:!0});var eJr=s(eE);jhe=n(eJr,"SPAN",{});var oJr=s(jhe);m(Ww.$$.fragment,oJr),oJr.forEach(t),eJr.forEach(t),Ufr=i(K7e),Ohe=n(K7e,"SPAN",{});var rJr=s(Ohe);Jfr=r(rJr,"FlaxAutoModelForTokenClassification"),rJr.forEach(t),K7e.forEach(t),Xye=i(c),Mr=n(c,"DIV",{class:!0});var Rl=s(Mr);m(Hw.$$.fragment,Rl),Kfr=i(Rl),hc=n(Rl,"P",{});var CO=s(hc);Yfr=r(CO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Ghe=n(CO,"CODE",{});var tJr=s(Ghe);Zfr=r(tJr,"from_pretrained()"),tJr.forEach(t),emr=r(CO,` class method or the
`),qhe=n(CO,"CODE",{});var aJr=s(qhe);omr=r(aJr,"from_config()"),aJr.forEach(t),rmr=r(CO," class method."),CO.forEach(t),tmr=i(Rl),Uw=n(Rl,"P",{});var Y7e=s(Uw);amr=r(Y7e,"This class cannot be instantiated directly using "),zhe=n(Y7e,"CODE",{});var nJr=s(zhe);nmr=r(nJr,"__init__()"),nJr.forEach(t),smr=r(Y7e," (throws an error)."),Y7e.forEach(t),lmr=i(Rl),ht=n(Rl,"DIV",{class:!0});var Pl=s(ht);m(Jw.$$.fragment,Pl),imr=i(Pl),Xhe=n(Pl,"P",{});var sJr=s(Xhe);dmr=r(sJr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),sJr.forEach(t),cmr=i(Pl),gc=n(Pl,"P",{});var MO=s(gc);fmr=r(MO,`Note:
Loading a model from its configuration file does `),Qhe=n(MO,"STRONG",{});var lJr=s(Qhe);mmr=r(lJr,"not"),lJr.forEach(t),hmr=r(MO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vhe=n(MO,"CODE",{});var iJr=s(Vhe);gmr=r(iJr,"from_pretrained()"),iJr.forEach(t),umr=r(MO,` to load the model
weights.`),MO.forEach(t),pmr=i(Pl),Whe=n(Pl,"P",{});var dJr=s(Whe);_mr=r(dJr,"Examples:"),dJr.forEach(t),vmr=i(Pl),m(Kw.$$.fragment,Pl),Pl.forEach(t),bmr=i(Rl),wo=n(Rl,"DIV",{class:!0});var ca=s(wo);m(Yw.$$.fragment,ca),Tmr=i(ca),Hhe=n(ca,"P",{});var cJr=s(Hhe);Fmr=r(cJr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),cJr.forEach(t),Emr=i(ca),dn=n(ca,"P",{});var hC=s(dn);Cmr=r(hC,"The model class to instantiate is selected based on the "),Uhe=n(hC,"CODE",{});var fJr=s(Uhe);Mmr=r(fJr,"model_type"),fJr.forEach(t),ymr=r(hC,` property of the config object (either
passed as an argument or loaded from `),Jhe=n(hC,"CODE",{});var mJr=s(Jhe);wmr=r(mJr,"pretrained_model_name_or_path"),mJr.forEach(t),Amr=r(hC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Khe=n(hC,"CODE",{});var hJr=s(Khe);Lmr=r(hJr,"pretrained_model_name_or_path"),hJr.forEach(t),Bmr=r(hC,":"),hC.forEach(t),xmr=i(ca),yr=n(ca,"UL",{});var fa=s(yr);oE=n(fa,"LI",{});var JMe=s(oE);Yhe=n(JMe,"STRONG",{});var gJr=s(Yhe);kmr=r(gJr,"albert"),gJr.forEach(t),Rmr=r(JMe," \u2014 "),kN=n(JMe,"A",{href:!0});var uJr=s(kN);Pmr=r(uJr,"FlaxAlbertForTokenClassification"),uJr.forEach(t),Smr=r(JMe," (ALBERT model)"),JMe.forEach(t),$mr=i(fa),rE=n(fa,"LI",{});var KMe=s(rE);Zhe=n(KMe,"STRONG",{});var pJr=s(Zhe);Imr=r(pJr,"bert"),pJr.forEach(t),Dmr=r(KMe," \u2014 "),RN=n(KMe,"A",{href:!0});var _Jr=s(RN);Nmr=r(_Jr,"FlaxBertForTokenClassification"),_Jr.forEach(t),jmr=r(KMe," (BERT model)"),KMe.forEach(t),Omr=i(fa),tE=n(fa,"LI",{});var YMe=s(tE);ege=n(YMe,"STRONG",{});var vJr=s(ege);Gmr=r(vJr,"big_bird"),vJr.forEach(t),qmr=r(YMe," \u2014 "),PN=n(YMe,"A",{href:!0});var bJr=s(PN);zmr=r(bJr,"FlaxBigBirdForTokenClassification"),bJr.forEach(t),Xmr=r(YMe," (BigBird model)"),YMe.forEach(t),Qmr=i(fa),aE=n(fa,"LI",{});var ZMe=s(aE);oge=n(ZMe,"STRONG",{});var TJr=s(oge);Vmr=r(TJr,"distilbert"),TJr.forEach(t),Wmr=r(ZMe," \u2014 "),SN=n(ZMe,"A",{href:!0});var FJr=s(SN);Hmr=r(FJr,"FlaxDistilBertForTokenClassification"),FJr.forEach(t),Umr=r(ZMe," (DistilBERT model)"),ZMe.forEach(t),Jmr=i(fa),nE=n(fa,"LI",{});var e5e=s(nE);rge=n(e5e,"STRONG",{});var EJr=s(rge);Kmr=r(EJr,"electra"),EJr.forEach(t),Ymr=r(e5e," \u2014 "),$N=n(e5e,"A",{href:!0});var CJr=s($N);Zmr=r(CJr,"FlaxElectraForTokenClassification"),CJr.forEach(t),ehr=r(e5e," (ELECTRA model)"),e5e.forEach(t),ohr=i(fa),sE=n(fa,"LI",{});var o5e=s(sE);tge=n(o5e,"STRONG",{});var MJr=s(tge);rhr=r(MJr,"roberta"),MJr.forEach(t),thr=r(o5e," \u2014 "),IN=n(o5e,"A",{href:!0});var yJr=s(IN);ahr=r(yJr,"FlaxRobertaForTokenClassification"),yJr.forEach(t),nhr=r(o5e," (RoBERTa model)"),o5e.forEach(t),fa.forEach(t),shr=i(ca),age=n(ca,"P",{});var wJr=s(age);lhr=r(wJr,"Examples:"),wJr.forEach(t),ihr=i(ca),m(Zw.$$.fragment,ca),ca.forEach(t),Rl.forEach(t),Qye=i(c),uc=n(c,"H2",{class:!0});var Z7e=s(uc);lE=n(Z7e,"A",{id:!0,class:!0,href:!0});var AJr=s(lE);nge=n(AJr,"SPAN",{});var LJr=s(nge);m(e7.$$.fragment,LJr),LJr.forEach(t),AJr.forEach(t),dhr=i(Z7e),sge=n(Z7e,"SPAN",{});var BJr=s(sge);chr=r(BJr,"FlaxAutoModelForMultipleChoice"),BJr.forEach(t),Z7e.forEach(t),Vye=i(c),wr=n(c,"DIV",{class:!0});var Sl=s(wr);m(o7.$$.fragment,Sl),fhr=i(Sl),pc=n(Sl,"P",{});var yO=s(pc);mhr=r(yO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),lge=n(yO,"CODE",{});var xJr=s(lge);hhr=r(xJr,"from_pretrained()"),xJr.forEach(t),ghr=r(yO,` class method or the
`),ige=n(yO,"CODE",{});var kJr=s(ige);uhr=r(kJr,"from_config()"),kJr.forEach(t),phr=r(yO," class method."),yO.forEach(t),_hr=i(Sl),r7=n(Sl,"P",{});var e0e=s(r7);vhr=r(e0e,"This class cannot be instantiated directly using "),dge=n(e0e,"CODE",{});var RJr=s(dge);bhr=r(RJr,"__init__()"),RJr.forEach(t),Thr=r(e0e," (throws an error)."),e0e.forEach(t),Fhr=i(Sl),gt=n(Sl,"DIV",{class:!0});var $l=s(gt);m(t7.$$.fragment,$l),Ehr=i($l),cge=n($l,"P",{});var PJr=s(cge);Chr=r(PJr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),PJr.forEach(t),Mhr=i($l),_c=n($l,"P",{});var wO=s(_c);yhr=r(wO,`Note:
Loading a model from its configuration file does `),fge=n(wO,"STRONG",{});var SJr=s(fge);whr=r(SJr,"not"),SJr.forEach(t),Ahr=r(wO,` load the model weights. It only affects the
model\u2019s configuration. Use `),mge=n(wO,"CODE",{});var $Jr=s(mge);Lhr=r($Jr,"from_pretrained()"),$Jr.forEach(t),Bhr=r(wO,` to load the model
weights.`),wO.forEach(t),xhr=i($l),hge=n($l,"P",{});var IJr=s(hge);khr=r(IJr,"Examples:"),IJr.forEach(t),Rhr=i($l),m(a7.$$.fragment,$l),$l.forEach(t),Phr=i(Sl),Ao=n(Sl,"DIV",{class:!0});var ma=s(Ao);m(n7.$$.fragment,ma),Shr=i(ma),gge=n(ma,"P",{});var DJr=s(gge);$hr=r(DJr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),DJr.forEach(t),Ihr=i(ma),cn=n(ma,"P",{});var gC=s(cn);Dhr=r(gC,"The model class to instantiate is selected based on the "),uge=n(gC,"CODE",{});var NJr=s(uge);Nhr=r(NJr,"model_type"),NJr.forEach(t),jhr=r(gC,` property of the config object (either
passed as an argument or loaded from `),pge=n(gC,"CODE",{});var jJr=s(pge);Ohr=r(jJr,"pretrained_model_name_or_path"),jJr.forEach(t),Ghr=r(gC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),_ge=n(gC,"CODE",{});var OJr=s(_ge);qhr=r(OJr,"pretrained_model_name_or_path"),OJr.forEach(t),zhr=r(gC,":"),gC.forEach(t),Xhr=i(ma),Ar=n(ma,"UL",{});var ha=s(Ar);iE=n(ha,"LI",{});var r5e=s(iE);vge=n(r5e,"STRONG",{});var GJr=s(vge);Qhr=r(GJr,"albert"),GJr.forEach(t),Vhr=r(r5e," \u2014 "),DN=n(r5e,"A",{href:!0});var qJr=s(DN);Whr=r(qJr,"FlaxAlbertForMultipleChoice"),qJr.forEach(t),Hhr=r(r5e," (ALBERT model)"),r5e.forEach(t),Uhr=i(ha),dE=n(ha,"LI",{});var t5e=s(dE);bge=n(t5e,"STRONG",{});var zJr=s(bge);Jhr=r(zJr,"bert"),zJr.forEach(t),Khr=r(t5e," \u2014 "),NN=n(t5e,"A",{href:!0});var XJr=s(NN);Yhr=r(XJr,"FlaxBertForMultipleChoice"),XJr.forEach(t),Zhr=r(t5e," (BERT model)"),t5e.forEach(t),egr=i(ha),cE=n(ha,"LI",{});var a5e=s(cE);Tge=n(a5e,"STRONG",{});var QJr=s(Tge);ogr=r(QJr,"big_bird"),QJr.forEach(t),rgr=r(a5e," \u2014 "),jN=n(a5e,"A",{href:!0});var VJr=s(jN);tgr=r(VJr,"FlaxBigBirdForMultipleChoice"),VJr.forEach(t),agr=r(a5e," (BigBird model)"),a5e.forEach(t),ngr=i(ha),fE=n(ha,"LI",{});var n5e=s(fE);Fge=n(n5e,"STRONG",{});var WJr=s(Fge);sgr=r(WJr,"distilbert"),WJr.forEach(t),lgr=r(n5e," \u2014 "),ON=n(n5e,"A",{href:!0});var HJr=s(ON);igr=r(HJr,"FlaxDistilBertForMultipleChoice"),HJr.forEach(t),dgr=r(n5e," (DistilBERT model)"),n5e.forEach(t),cgr=i(ha),mE=n(ha,"LI",{});var s5e=s(mE);Ege=n(s5e,"STRONG",{});var UJr=s(Ege);fgr=r(UJr,"electra"),UJr.forEach(t),mgr=r(s5e," \u2014 "),GN=n(s5e,"A",{href:!0});var JJr=s(GN);hgr=r(JJr,"FlaxElectraForMultipleChoice"),JJr.forEach(t),ggr=r(s5e," (ELECTRA model)"),s5e.forEach(t),ugr=i(ha),hE=n(ha,"LI",{});var l5e=s(hE);Cge=n(l5e,"STRONG",{});var KJr=s(Cge);pgr=r(KJr,"roberta"),KJr.forEach(t),_gr=r(l5e," \u2014 "),qN=n(l5e,"A",{href:!0});var YJr=s(qN);vgr=r(YJr,"FlaxRobertaForMultipleChoice"),YJr.forEach(t),bgr=r(l5e," (RoBERTa model)"),l5e.forEach(t),ha.forEach(t),Tgr=i(ma),Mge=n(ma,"P",{});var ZJr=s(Mge);Fgr=r(ZJr,"Examples:"),ZJr.forEach(t),Egr=i(ma),m(s7.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),Wye=i(c),vc=n(c,"H2",{class:!0});var o0e=s(vc);gE=n(o0e,"A",{id:!0,class:!0,href:!0});var eKr=s(gE);yge=n(eKr,"SPAN",{});var oKr=s(yge);m(l7.$$.fragment,oKr),oKr.forEach(t),eKr.forEach(t),Cgr=i(o0e),wge=n(o0e,"SPAN",{});var rKr=s(wge);Mgr=r(rKr,"FlaxAutoModelForNextSentencePrediction"),rKr.forEach(t),o0e.forEach(t),Hye=i(c),Lr=n(c,"DIV",{class:!0});var Il=s(Lr);m(i7.$$.fragment,Il),ygr=i(Il),bc=n(Il,"P",{});var AO=s(bc);wgr=r(AO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Age=n(AO,"CODE",{});var tKr=s(Age);Agr=r(tKr,"from_pretrained()"),tKr.forEach(t),Lgr=r(AO,` class method or the
`),Lge=n(AO,"CODE",{});var aKr=s(Lge);Bgr=r(aKr,"from_config()"),aKr.forEach(t),xgr=r(AO," class method."),AO.forEach(t),kgr=i(Il),d7=n(Il,"P",{});var r0e=s(d7);Rgr=r(r0e,"This class cannot be instantiated directly using "),Bge=n(r0e,"CODE",{});var nKr=s(Bge);Pgr=r(nKr,"__init__()"),nKr.forEach(t),Sgr=r(r0e," (throws an error)."),r0e.forEach(t),$gr=i(Il),ut=n(Il,"DIV",{class:!0});var Dl=s(ut);m(c7.$$.fragment,Dl),Igr=i(Dl),xge=n(Dl,"P",{});var sKr=s(xge);Dgr=r(sKr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),sKr.forEach(t),Ngr=i(Dl),Tc=n(Dl,"P",{});var LO=s(Tc);jgr=r(LO,`Note:
Loading a model from its configuration file does `),kge=n(LO,"STRONG",{});var lKr=s(kge);Ogr=r(lKr,"not"),lKr.forEach(t),Ggr=r(LO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Rge=n(LO,"CODE",{});var iKr=s(Rge);qgr=r(iKr,"from_pretrained()"),iKr.forEach(t),zgr=r(LO,` to load the model
weights.`),LO.forEach(t),Xgr=i(Dl),Pge=n(Dl,"P",{});var dKr=s(Pge);Qgr=r(dKr,"Examples:"),dKr.forEach(t),Vgr=i(Dl),m(f7.$$.fragment,Dl),Dl.forEach(t),Wgr=i(Il),Lo=n(Il,"DIV",{class:!0});var ga=s(Lo);m(m7.$$.fragment,ga),Hgr=i(ga),Sge=n(ga,"P",{});var cKr=s(Sge);Ugr=r(cKr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),cKr.forEach(t),Jgr=i(ga),fn=n(ga,"P",{});var uC=s(fn);Kgr=r(uC,"The model class to instantiate is selected based on the "),$ge=n(uC,"CODE",{});var fKr=s($ge);Ygr=r(fKr,"model_type"),fKr.forEach(t),Zgr=r(uC,` property of the config object (either
passed as an argument or loaded from `),Ige=n(uC,"CODE",{});var mKr=s(Ige);eur=r(mKr,"pretrained_model_name_or_path"),mKr.forEach(t),our=r(uC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Dge=n(uC,"CODE",{});var hKr=s(Dge);rur=r(hKr,"pretrained_model_name_or_path"),hKr.forEach(t),tur=r(uC,":"),uC.forEach(t),aur=i(ga),Nge=n(ga,"UL",{});var gKr=s(Nge);uE=n(gKr,"LI",{});var i5e=s(uE);jge=n(i5e,"STRONG",{});var uKr=s(jge);nur=r(uKr,"bert"),uKr.forEach(t),sur=r(i5e," \u2014 "),zN=n(i5e,"A",{href:!0});var pKr=s(zN);lur=r(pKr,"FlaxBertForNextSentencePrediction"),pKr.forEach(t),iur=r(i5e," (BERT model)"),i5e.forEach(t),gKr.forEach(t),dur=i(ga),Oge=n(ga,"P",{});var _Kr=s(Oge);cur=r(_Kr,"Examples:"),_Kr.forEach(t),fur=i(ga),m(h7.$$.fragment,ga),ga.forEach(t),Il.forEach(t),Uye=i(c),Fc=n(c,"H2",{class:!0});var t0e=s(Fc);pE=n(t0e,"A",{id:!0,class:!0,href:!0});var vKr=s(pE);Gge=n(vKr,"SPAN",{});var bKr=s(Gge);m(g7.$$.fragment,bKr),bKr.forEach(t),vKr.forEach(t),mur=i(t0e),qge=n(t0e,"SPAN",{});var TKr=s(qge);hur=r(TKr,"FlaxAutoModelForImageClassification"),TKr.forEach(t),t0e.forEach(t),Jye=i(c),Br=n(c,"DIV",{class:!0});var Nl=s(Br);m(u7.$$.fragment,Nl),gur=i(Nl),Ec=n(Nl,"P",{});var BO=s(Ec);uur=r(BO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),zge=n(BO,"CODE",{});var FKr=s(zge);pur=r(FKr,"from_pretrained()"),FKr.forEach(t),_ur=r(BO,` class method or the
`),Xge=n(BO,"CODE",{});var EKr=s(Xge);vur=r(EKr,"from_config()"),EKr.forEach(t),bur=r(BO," class method."),BO.forEach(t),Tur=i(Nl),p7=n(Nl,"P",{});var a0e=s(p7);Fur=r(a0e,"This class cannot be instantiated directly using "),Qge=n(a0e,"CODE",{});var CKr=s(Qge);Eur=r(CKr,"__init__()"),CKr.forEach(t),Cur=r(a0e," (throws an error)."),a0e.forEach(t),Mur=i(Nl),pt=n(Nl,"DIV",{class:!0});var jl=s(pt);m(_7.$$.fragment,jl),yur=i(jl),Vge=n(jl,"P",{});var MKr=s(Vge);wur=r(MKr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),MKr.forEach(t),Aur=i(jl),Cc=n(jl,"P",{});var xO=s(Cc);Lur=r(xO,`Note:
Loading a model from its configuration file does `),Wge=n(xO,"STRONG",{});var yKr=s(Wge);Bur=r(yKr,"not"),yKr.forEach(t),xur=r(xO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hge=n(xO,"CODE",{});var wKr=s(Hge);kur=r(wKr,"from_pretrained()"),wKr.forEach(t),Rur=r(xO,` to load the model
weights.`),xO.forEach(t),Pur=i(jl),Uge=n(jl,"P",{});var AKr=s(Uge);Sur=r(AKr,"Examples:"),AKr.forEach(t),$ur=i(jl),m(v7.$$.fragment,jl),jl.forEach(t),Iur=i(Nl),Bo=n(Nl,"DIV",{class:!0});var ua=s(Bo);m(b7.$$.fragment,ua),Dur=i(ua),Jge=n(ua,"P",{});var LKr=s(Jge);Nur=r(LKr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),LKr.forEach(t),jur=i(ua),mn=n(ua,"P",{});var pC=s(mn);Our=r(pC,"The model class to instantiate is selected based on the "),Kge=n(pC,"CODE",{});var BKr=s(Kge);Gur=r(BKr,"model_type"),BKr.forEach(t),qur=r(pC,` property of the config object (either
passed as an argument or loaded from `),Yge=n(pC,"CODE",{});var xKr=s(Yge);zur=r(xKr,"pretrained_model_name_or_path"),xKr.forEach(t),Xur=r(pC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Zge=n(pC,"CODE",{});var kKr=s(Zge);Qur=r(kKr,"pretrained_model_name_or_path"),kKr.forEach(t),Vur=r(pC,":"),pC.forEach(t),Wur=i(ua),T7=n(ua,"UL",{});var n0e=s(T7);_E=n(n0e,"LI",{});var d5e=s(_E);eue=n(d5e,"STRONG",{});var RKr=s(eue);Hur=r(RKr,"beit"),RKr.forEach(t),Uur=r(d5e," \u2014 "),XN=n(d5e,"A",{href:!0});var PKr=s(XN);Jur=r(PKr,"FlaxBeitForImageClassification"),PKr.forEach(t),Kur=r(d5e," (BEiT model)"),d5e.forEach(t),Yur=i(n0e),vE=n(n0e,"LI",{});var c5e=s(vE);oue=n(c5e,"STRONG",{});var SKr=s(oue);Zur=r(SKr,"vit"),SKr.forEach(t),epr=r(c5e," \u2014 "),QN=n(c5e,"A",{href:!0});var $Kr=s(QN);opr=r($Kr,"FlaxViTForImageClassification"),$Kr.forEach(t),rpr=r(c5e," (ViT model)"),c5e.forEach(t),n0e.forEach(t),tpr=i(ua),rue=n(ua,"P",{});var IKr=s(rue);apr=r(IKr,"Examples:"),IKr.forEach(t),npr=i(ua),m(F7.$$.fragment,ua),ua.forEach(t),Nl.forEach(t),Kye=i(c),Mc=n(c,"H2",{class:!0});var s0e=s(Mc);bE=n(s0e,"A",{id:!0,class:!0,href:!0});var DKr=s(bE);tue=n(DKr,"SPAN",{});var NKr=s(tue);m(E7.$$.fragment,NKr),NKr.forEach(t),DKr.forEach(t),spr=i(s0e),aue=n(s0e,"SPAN",{});var jKr=s(aue);lpr=r(jKr,"FlaxAutoModelForVision2Seq"),jKr.forEach(t),s0e.forEach(t),Yye=i(c),xr=n(c,"DIV",{class:!0});var Ol=s(xr);m(C7.$$.fragment,Ol),ipr=i(Ol),yc=n(Ol,"P",{});var kO=s(yc);dpr=r(kO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),nue=n(kO,"CODE",{});var OKr=s(nue);cpr=r(OKr,"from_pretrained()"),OKr.forEach(t),fpr=r(kO,` class method or the
`),sue=n(kO,"CODE",{});var GKr=s(sue);mpr=r(GKr,"from_config()"),GKr.forEach(t),hpr=r(kO," class method."),kO.forEach(t),gpr=i(Ol),M7=n(Ol,"P",{});var l0e=s(M7);upr=r(l0e,"This class cannot be instantiated directly using "),lue=n(l0e,"CODE",{});var qKr=s(lue);ppr=r(qKr,"__init__()"),qKr.forEach(t),_pr=r(l0e," (throws an error)."),l0e.forEach(t),vpr=i(Ol),_t=n(Ol,"DIV",{class:!0});var Gl=s(_t);m(y7.$$.fragment,Gl),bpr=i(Gl),iue=n(Gl,"P",{});var zKr=s(iue);Tpr=r(zKr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),zKr.forEach(t),Fpr=i(Gl),wc=n(Gl,"P",{});var RO=s(wc);Epr=r(RO,`Note:
Loading a model from its configuration file does `),due=n(RO,"STRONG",{});var XKr=s(due);Cpr=r(XKr,"not"),XKr.forEach(t),Mpr=r(RO,` load the model weights. It only affects the
model\u2019s configuration. Use `),cue=n(RO,"CODE",{});var QKr=s(cue);ypr=r(QKr,"from_pretrained()"),QKr.forEach(t),wpr=r(RO,` to load the model
weights.`),RO.forEach(t),Apr=i(Gl),fue=n(Gl,"P",{});var VKr=s(fue);Lpr=r(VKr,"Examples:"),VKr.forEach(t),Bpr=i(Gl),m(w7.$$.fragment,Gl),Gl.forEach(t),xpr=i(Ol),xo=n(Ol,"DIV",{class:!0});var pa=s(xo);m(A7.$$.fragment,pa),kpr=i(pa),mue=n(pa,"P",{});var WKr=s(mue);Rpr=r(WKr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),WKr.forEach(t),Ppr=i(pa),hn=n(pa,"P",{});var _C=s(hn);Spr=r(_C,"The model class to instantiate is selected based on the "),hue=n(_C,"CODE",{});var HKr=s(hue);$pr=r(HKr,"model_type"),HKr.forEach(t),Ipr=r(_C,` property of the config object (either
passed as an argument or loaded from `),gue=n(_C,"CODE",{});var UKr=s(gue);Dpr=r(UKr,"pretrained_model_name_or_path"),UKr.forEach(t),Npr=r(_C,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),uue=n(_C,"CODE",{});var JKr=s(uue);jpr=r(JKr,"pretrained_model_name_or_path"),JKr.forEach(t),Opr=r(_C,":"),_C.forEach(t),Gpr=i(pa),pue=n(pa,"UL",{});var KKr=s(pue);TE=n(KKr,"LI",{});var f5e=s(TE);_ue=n(f5e,"STRONG",{});var YKr=s(_ue);qpr=r(YKr,"vision-encoder-decoder"),YKr.forEach(t),zpr=r(f5e," \u2014 "),VN=n(f5e,"A",{href:!0});var ZKr=s(VN);Xpr=r(ZKr,"FlaxVisionEncoderDecoderModel"),ZKr.forEach(t),Qpr=r(f5e," (Vision Encoder decoder model)"),f5e.forEach(t),KKr.forEach(t),Vpr=i(pa),vue=n(pa,"P",{});var eYr=s(vue);Wpr=r(eYr,"Examples:"),eYr.forEach(t),Hpr=i(pa),m(L7.$$.fragment,pa),pa.forEach(t),Ol.forEach(t),this.h()},h(){d(re,"name","hf:doc:metadata"),d(re,"content",JSON.stringify(dYr)),d(ue,"id","auto-classes"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#auto-classes"),d(me,"class","relative group"),d(gn,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoConfig"),d(pn,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoModel"),d(_n,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoTokenizer"),d(Ul,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertModel"),d(Rc,"id","extending-the-auto-classes"),d(Rc,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Rc,"href","#extending-the-auto-classes"),d(Jl,"class","relative group"),d(Sc,"id","transformers.AutoConfig"),d(Sc,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Sc,"href","#transformers.AutoConfig"),d(Kl,"class","relative group"),d(L0,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(B0,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertConfig"),d(x0,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartConfig"),d(k0,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitConfig"),d(R0,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertConfig"),d(P0,"href","/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationConfig"),d(S0,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdConfig"),d($0,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(I0,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(D0,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig"),d(N0,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig"),d(j0,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineConfig"),d(O0,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPConfig"),d(G0,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertConfig"),d(q0,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLConfig"),d(z0,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaConfig"),d(X0,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Config"),d(Q0,"href","/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTConfig"),d(V0,"href","/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrConfig"),d(W0,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig"),d(H0,"href","/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRConfig"),d(U0,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraConfig"),d(J0,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),d(K0,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertConfig"),d(Y0,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetConfig"),d(Z0,"href","/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTConfig"),d(eA,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelConfig"),d(oA,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Config"),d(rA,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(tA,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJConfig"),d(aA,"href","/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertConfig"),d(nA,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertConfig"),d(sA,"href","/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(lA,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(iA,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(dA,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDConfig"),d(cA,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerConfig"),d(fA,"href","/docs/transformers/v4.14.1/en/model_doc/luke#transformers.LukeConfig"),d(mA,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertConfig"),d(hA,"href","/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Config"),d(gA,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianConfig"),d(uA,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartConfig"),d(pA,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertConfig"),d(_A,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(vA,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetConfig"),d(bA,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Config"),d(TA,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTConfig"),d(FA,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusConfig"),d(EA,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverConfig"),d(CA,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(MA,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(yA,"href","/docs/transformers/v4.14.1/en/model_doc/rag#transformers.RagConfig"),d(wA,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerConfig"),d(AA,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertConfig"),d(LA,"href","/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertConfig"),d(BA,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig"),d(xA,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerConfig"),d(kA,"href","/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerConfig"),d(RA,"href","/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWConfig"),d(PA,"href","/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDConfig"),d(SA,"href","/docs/transformers/v4.14.1/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderConfig"),d($A,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(IA,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(DA,"href","/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterConfig"),d(NA,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(jA,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Config"),d(OA,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasConfig"),d(GA,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLConfig"),d(qA,"href","/docs/transformers/v4.14.1/en/model_doc/trocr#transformers.TrOCRConfig"),d(zA,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechConfig"),d(XA,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig"),d(QA,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"),d(VA,"href","/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig"),d(WA,"href","/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(HA,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTConfig"),d(UA,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d(JA,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMConfig"),d(KA,"href","/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig"),d(YA,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaConfig"),d(ZA,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetConfig"),d(so,"class","docstring"),d(lm,"class","docstring"),d(Do,"class","docstring"),d(im,"id","transformers.AutoTokenizer"),d(im,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(im,"href","#transformers.AutoTokenizer"),d(Zl,"class","relative group"),d(e6,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(o6,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertTokenizer"),d(r6,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(t6,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartTokenizer"),d(a6,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartTokenizerFast"),d(n6,"href","/docs/transformers/v4.14.1/en/model_doc/barthez#transformers.BarthezTokenizer"),d(s6,"href","/docs/transformers/v4.14.1/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(l6,"href","/docs/transformers/v4.14.1/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(i6,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertTokenizer"),d(d6,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertTokenizerFast"),d(c6,"href","/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationTokenizer"),d(f6,"href","/docs/transformers/v4.14.1/en/model_doc/bert_japanese#transformers.BertJapaneseTokenizer"),d(m6,"href","/docs/transformers/v4.14.1/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(h6,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdTokenizer"),d(g6,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdTokenizerFast"),d(u6,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(p6,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(_6,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(v6,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(b6,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallTokenizer"),d(T6,"href","/docs/transformers/v4.14.1/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(F6,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertTokenizer"),d(E6,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(C6,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineTokenizer"),d(M6,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPTokenizer"),d(y6,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(w6,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(A6,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d(L6,"href","/docs/transformers/v4.14.1/en/model_doc/cpm#transformers.CpmTokenizer"),d(B6,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(x6,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaTokenizer"),d(k6,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(R6,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Tokenizer"),d(P6,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(S6,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d($6,"href","/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(I6,"href","/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(D6,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraTokenizer"),d(N6,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(j6,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(O6,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetTokenizer"),d(G6,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(q6,"href","/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(z6,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelTokenizer"),d(X6,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(Q6,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(V6,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(W6,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(H6,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(U6,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(J6,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizer"),d(K6,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Y6,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(Z6,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(eL,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(oL,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(rL,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDTokenizer"),d(tL,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDTokenizerFast"),d(aL,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerTokenizer"),d(nL,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(sL,"href","/docs/transformers/v4.14.1/en/model_doc/luke#transformers.LukeTokenizer"),d(lL,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(iL,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(dL,"href","/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(cL,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianTokenizer"),d(fL,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartTokenizer"),d(mL,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(hL,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(gL,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(uL,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(pL,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(_L,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(vL,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(bL,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Tokenizer"),d(TL,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5TokenizerFast"),d(FL,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTTokenizer"),d(EL,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTTokenizerFast"),d(CL,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(ML,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(yL,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(wL,"href","/docs/transformers/v4.14.1/en/model_doc/phobert#transformers.PhobertTokenizer"),d(AL,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(LL,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertTokenizer"),d(BL,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertTokenizerFast"),d(xL,"href","/docs/transformers/v4.14.1/en/model_doc/rag#transformers.RagTokenizer"),d(kL,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerTokenizer"),d(RL,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(PL,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertTokenizer"),d(SL,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d($L,"href","/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(IL,"href","/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(DL,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizer"),d(NL,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(jL,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(OL,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(GL,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(qL,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(zL,"href","/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterTokenizer"),d(XL,"href","/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(QL,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(VL,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(WL,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Tokenizer"),d(HL,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5TokenizerFast"),d(UL,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasTokenizer"),d(JL,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLTokenizer"),d(KL,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(YL,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMTokenizer"),d(ZL,"href","/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetTokenizer"),d(e8,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaTokenizer"),d(o8,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaTokenizerFast"),d(r8,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(t8,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(a8,"href","/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained"),d(n8,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(ye,"class","docstring"),d($m,"class","docstring"),d(No,"class","docstring"),d(Im,"id","transformers.AutoFeatureExtractor"),d(Im,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Im,"href","#transformers.AutoFeatureExtractor"),d(ti,"class","relative group"),d(s8,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(l8,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(i8,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(d8,"href","/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(c8,"href","/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(f8,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(m8,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(h8,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(g8,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(u8,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(p8,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(_8,"href","/docs/transformers/v4.14.1/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained"),d(Fe,"class","docstring"),d(Dt,"class","docstring"),d(Hm,"id","transformers.AutoProcessor"),d(Hm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Hm,"href","#transformers.AutoProcessor"),d(li,"class","relative group"),d(v8,"href","/docs/transformers/v4.14.1/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(b8,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPProcessor"),d(T8,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(F8,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(E8,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(C8,"href","/docs/transformers/v4.14.1/en/model_doc/trocr#transformers.TrOCRProcessor"),d(M8,"href","/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderProcessor"),d(y8,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(Ee,"class","docstring"),d(Nt,"class","docstring"),d(th,"id","transformers.AutoModel"),d(th,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(th,"href","#transformers.AutoModel"),d(fi,"class","relative group"),d(kr,"class","docstring"),d(w8,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertModel"),d(A8,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartModel"),d(L8,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitModel"),d(B8,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertModel"),d(x8,"href","/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationEncoder"),d(k8,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdModel"),d(R8,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(P8,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(S8,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallModel"),d($8,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertModel"),d(I8,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineModel"),d(D8,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.CLIPModel"),d(N8,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertModel"),d(j8,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLModel"),d(O8,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaModel"),d(G8,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2Model"),d(q8,"href","/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTModel"),d(z8,"href","/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrModel"),d(X8,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertModel"),d(Q8,"href","/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(V8,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraModel"),d(W8,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertModel"),d(H8,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetModel"),d(U8,"href","/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTModel"),d(J8,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelModel"),d(K8,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelBaseModel"),d(Y8,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2Model"),d(Z8,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(eB,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJModel"),d(oB,"href","/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertModel"),d(rB,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertModel"),d(tB,"href","/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(aB,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(nB,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(sB,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDModel"),d(lB,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerModel"),d(iB,"href","/docs/transformers/v4.14.1/en/model_doc/luke#transformers.LukeModel"),d(dB,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertModel"),d(cB,"href","/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100Model"),d(fB,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianModel"),d(mB,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartModel"),d(hB,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertModel"),d(gB,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertModel"),d(uB,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetModel"),d(pB,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5Model"),d(_B,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTModel"),d(vB,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusModel"),d(bB,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverModel"),d(TB,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(FB,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertModel"),d(EB,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerModel"),d(CB,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertModel"),d(MB,"href","/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertModel"),d(yB,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaModel"),d(wB,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerModel"),d(AB,"href","/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerModel"),d(LB,"href","/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWModel"),d(BB,"href","/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDModel"),d(xB,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(kB,"href","/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterModel"),d(RB,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(PB,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5Model"),d(SB,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasModel"),d($B,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLModel"),d(IB,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechModel"),d(DB,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatModel"),d(NB,"href","/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel"),d(jB,"href","/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertModel"),d(OB,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTModel"),d(GB,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(qB,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMModel"),d(zB,"href","/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetModel"),d(XB,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaModel"),d(QB,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetModel"),d($e,"class","docstring"),d(jo,"class","docstring"),d(Cg,"id","transformers.AutoModelForPreTraining"),d(Cg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Cg,"href","#transformers.AutoModelForPreTraining"),d(gi,"class","relative group"),d(Rr,"class","docstring"),d(VB,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForPreTraining"),d(WB,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(HB,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForPreTraining"),d(UB,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForPreTraining"),d(JB,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(KB,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(YB,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(ZB,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM"),d(e9,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(o9,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForPreTraining"),d(r9,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(t9,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForPreTraining"),d(a9,"href","/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(n9,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(s9,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(l9,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(i9,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(d9,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(c9,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(f9,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForPreTraining"),d(m9,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(h9,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(g9,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel"),d(u9,"href","/docs/transformers/v4.14.1/en/model_doc/retribert#transformers.RetriBertModel"),d(p9,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(_9,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(v9,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(b9,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(T9,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel"),d(F9,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(E9,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForPreTraining"),d(C9,"href","/docs/transformers/v4.14.1/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(M9,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(y9,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(w9,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM"),d(A9,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Ie,"class","docstring"),d(Oo,"class","docstring"),d(lu,"id","transformers.AutoModelForCausalLM"),d(lu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lu,"href","#transformers.AutoModelForCausalLM"),d(_i,"class","relative group"),d(Pr,"class","docstring"),d(L9,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForCausalLM"),d(B9,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertLMHeadModel"),d(x9,"href","/docs/transformers/v4.14.1/en/model_doc/bertgeneration#transformers.BertGenerationDecoder"),d(k9,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForCausalLM"),d(R9,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(P9,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(S9,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForCausalLM"),d($9,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(I9,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(D9,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(N9,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(j9,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(O9,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianForCausalLM"),d(G9,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForCausalLM"),d(q9,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForCausalLM"),d(z9,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel"),d(X9,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(Q9,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d(V9,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(W9,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(H9,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(U9,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForCausalLM"),d(J9,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(K9,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(Y9,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel"),d(Z9,"href","/docs/transformers/v4.14.1/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(ex,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(ox,"href","/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForCausalLM"),d(rx,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForCausalLM"),d(tx,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(De,"class","docstring"),d(Go,"class","docstring"),d(ju,"id","transformers.AutoModelForMaskedLM"),d(ju,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ju,"href","#transformers.AutoModelForMaskedLM"),d(Ti,"class","relative group"),d(Sr,"class","docstring"),d(ax,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForMaskedLM"),d(nx,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(sx,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForMaskedLM"),d(lx,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForMaskedLM"),d(ix,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(dx,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(cx,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(fx,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM"),d(mx,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(hx,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(gx,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(ux,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(px,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(_x,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(vx,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(bx,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(Tx,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(Fx,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForMaskedLM"),d(Ex,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(Cx,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(Mx,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(yx,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(wx,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(Ax,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(Lx,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(Bx,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(xx,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(kx,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(Rx,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(Px,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM"),d(Ne,"class","docstring"),d(qo,"class","docstring"),d(vp,"id","transformers.AutoModelForSeq2SeqLM"),d(vp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vp,"href","#transformers.AutoModelForSeq2SeqLM"),d(Ci,"class","relative group"),d($r,"class","docstring"),d(Sx,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForConditionalGeneration"),d($x,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(Ix,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(Dx,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForConditionalGeneration"),d(Nx,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),d(jx,"href","/docs/transformers/v4.14.1/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(Ox,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(Gx,"href","/docs/transformers/v4.14.1/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(qx,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.MarianMTModel"),d(zx,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(Xx,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(Qx,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(Vx,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(Wx,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(Hx,"href","/docs/transformers/v4.14.1/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(je,"class","docstring"),d(zo,"class","docstring"),d($p,"id","transformers.AutoModelForSequenceClassification"),d($p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($p,"href","#transformers.AutoModelForSequenceClassification"),d(wi,"class","relative group"),d(Ir,"class","docstring"),d(Ux,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(Jx,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForSequenceClassification"),d(Kx,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForSequenceClassification"),d(Yx,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForSequenceClassification"),d(Zx,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d(ek,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(ok,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(rk,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(tk,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(ak,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(nk,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForSequenceClassification"),d(sk,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(lk,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(ik,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(dk,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(ck,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(fk,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(mk,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(hk,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(gk,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(uk,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(pk,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(_k,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForSequenceClassification"),d(vk,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(bk,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(Tk,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForSequenceClassification"),d(Fk,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(Ek,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(Ck,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.OpenAIGPTForSequenceClassification"),d(Mk,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(yk,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(wk,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(Ak,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(Lk,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(Bk,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(xk,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(kk,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(Rk,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TransfoXLForSequenceClassification"),d(Pk,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(Sk,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForSequenceClassification"),d($k,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(Oe,"class","docstring"),d(Xo,"class","docstring"),d(M_,"id","transformers.AutoModelForMultipleChoice"),d(M_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(M_,"href","#transformers.AutoModelForMultipleChoice"),d(Bi,"class","relative group"),d(Dr,"class","docstring"),d(Ik,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(Dk,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForMultipleChoice"),d(Nk,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForMultipleChoice"),d(jk,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(Ok,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForMultipleChoice"),d(Gk,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(qk,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(zk,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(Xk,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(Qk,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(Vk,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(Wk,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d(Hk,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(Uk,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForMultipleChoice"),d(Jk,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(Kk,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(Yk,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(Zk,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(eR,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(oR,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(rR,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(tR,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(aR,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForMultipleChoice"),d(nR,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(Ge,"class","docstring"),d(Qo,"class","docstring"),d(J_,"id","transformers.AutoModelForNextSentencePrediction"),d(J_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J_,"href","#transformers.AutoModelForNextSentencePrediction"),d(Ri,"class","relative group"),d(Nr,"class","docstring"),d(sR,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(lR,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(iR,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForNextSentencePrediction"),d(dR,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(cR,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(qe,"class","docstring"),d(Vo,"class","docstring"),d(t1,"id","transformers.AutoModelForTokenClassification"),d(t1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(t1,"href","#transformers.AutoModelForTokenClassification"),d($i,"class","relative group"),d(jr,"class","docstring"),d(fR,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(mR,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForTokenClassification"),d(hR,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForTokenClassification"),d(gR,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(uR,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForTokenClassification"),d(pR,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(_R,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(vR,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForTokenClassification"),d(bR,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(TR,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(FR,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(ER,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(CR,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(MR,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(yR,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(wR,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(AR,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(LR,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d(BR,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForTokenClassification"),d(xR,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(kR,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(RR,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(PR,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(SR,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d($R,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(IR,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(DR,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(NR,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForTokenClassification"),d(jR,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(ze,"class","docstring"),d(Wo,"class","docstring"),d(S1,"id","transformers.AutoModelForQuestionAnswering"),d(S1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(S1,"href","#transformers.AutoModelForQuestionAnswering"),d(Ni,"class","relative group"),d(Or,"class","docstring"),d(OR,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d(GR,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(qR,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(zR,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.BigBirdForQuestionAnswering"),d(XR,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(QR,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(VR,"href","/docs/transformers/v4.14.1/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(WR,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(HR,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(UR,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.DebertaV2ForQuestionAnswering"),d(JR,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(KR,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(YR,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(ZR,"href","/docs/transformers/v4.14.1/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(eP,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(oP,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(rP,"href","/docs/transformers/v4.14.1/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(tP,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(aP,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(nP,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(sP,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(lP,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(iP,"href","/docs/transformers/v4.14.1/en/model_doc/megatron_bert#transformers.MegatronBertForQuestionAnswering"),d(dP,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(cP,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(fP,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(mP,"href","/docs/transformers/v4.14.1/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(hP,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(gP,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(uP,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(pP,"href","/docs/transformers/v4.14.1/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(_P,"href","/docs/transformers/v4.14.1/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(vP,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(bP,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.XLMRobertaForQuestionAnswering"),d(TP,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(Xe,"class","docstring"),d(Ho,"class","docstring"),d(_4,"id","transformers.AutoModelForTableQuestionAnswering"),d(_4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_4,"href","#transformers.AutoModelForTableQuestionAnswering"),d(Gi,"class","relative group"),d(Gr,"class","docstring"),d(FP,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(Qe,"class","docstring"),d(Uo,"class","docstring"),d(T4,"id","transformers.AutoModelForImageClassification"),d(T4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(T4,"href","#transformers.AutoModelForImageClassification"),d(Xi,"class","relative group"),d(qr,"class","docstring"),d(EP,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.BeitForImageClassification"),d(CP,"href","/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTForImageClassification"),d(MP,"href","/docs/transformers/v4.14.1/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d(yP,"href","/docs/transformers/v4.14.1/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(wP,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(AP,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(LP,"href","/docs/transformers/v4.14.1/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(BP,"href","/docs/transformers/v4.14.1/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(xP,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTForImageClassification"),d(Ve,"class","docstring"),d(Jo,"class","docstring"),d(w4,"id","transformers.AutoModelForVision2Seq"),d(w4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w4,"href","#transformers.AutoModelForVision2Seq"),d(Wi,"class","relative group"),d(zr,"class","docstring"),d(kP,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),d(We,"class","docstring"),d(Yo,"class","docstring"),d(B4,"id","transformers.AutoModelForAudioClassification"),d(B4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B4,"href","#transformers.AutoModelForAudioClassification"),d(Ji,"class","relative group"),d(Xr,"class","docstring"),d(RP,"href","/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d(PP,"href","/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(SP,"href","/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDForSequenceClassification"),d($P,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(IP,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForSequenceClassification"),d(DP,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(He,"class","docstring"),d(Zo,"class","docstring"),d(D4,"id","transformers.AutoModelForCTC"),d(D4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(D4,"href","#transformers.AutoModelForCTC"),d(Zi,"class","relative group"),d(Qr,"class","docstring"),d(NP,"href","/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.HubertForCTC"),d(jP,"href","/docs/transformers/v4.14.1/en/model_doc/sew#transformers.SEWForCTC"),d(OP,"href","/docs/transformers/v4.14.1/en/model_doc/sew_d#transformers.SEWDForCTC"),d(GP,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(qP,"href","/docs/transformers/v4.14.1/en/model_doc/unispeech_sat#transformers.UniSpeechSatForCTC"),d(zP,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(Ue,"class","docstring"),d(or,"class","docstring"),d(Q4,"id","transformers.AutoModelForSpeechSeq2Seq"),d(Q4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Q4,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(rd,"class","relative group"),d(Vr,"class","docstring"),d(XP,"href","/docs/transformers/v4.14.1/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderModel"),d(QP,"href","/docs/transformers/v4.14.1/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(Je,"class","docstring"),d(tr,"class","docstring"),d(U4,"id","transformers.AutoModelForObjectDetection"),d(U4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(U4,"href","#transformers.AutoModelForObjectDetection"),d(nd,"class","relative group"),d(Wr,"class","docstring"),d(VP,"href","/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrForObjectDetection"),d(Ke,"class","docstring"),d(ar,"class","docstring"),d(Y4,"id","transformers.AutoModelForImageSegmentation"),d(Y4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y4,"href","#transformers.AutoModelForImageSegmentation"),d(id,"class","relative group"),d(Hr,"class","docstring"),d(WP,"href","/docs/transformers/v4.14.1/en/model_doc/detr#transformers.DetrForSegmentation"),d(Ye,"class","docstring"),d(nr,"class","docstring"),d(ov,"id","transformers.TFAutoModel"),d(ov,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ov,"href","#transformers.TFAutoModel"),d(fd,"class","relative group"),d(Ur,"class","docstring"),d(HP,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertModel"),d(UP,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartModel"),d(JP,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertModel"),d(KP,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(YP,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallModel"),d(ZP,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertModel"),d(eS,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertModel"),d(oS,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLModel"),d(rS,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaModel"),d(tS,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2Model"),d(aS,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(nS,"href","/docs/transformers/v4.14.1/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(sS,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraModel"),d(lS,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(iS,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelModel"),d(dS,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(cS,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2Model"),d(fS,"href","/docs/transformers/v4.14.1/en/model_doc/hubert#transformers.TFHubertModel"),d(mS,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(hS,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.TFLEDModel"),d(gS,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerModel"),d(uS,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.TFLxmertModel"),d(pS,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.TFMarianModel"),d(_S,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.TFMBartModel"),d(vS,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(bS,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetModel"),d(TS,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.TFMT5Model"),d(FS,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTModel"),d(ES,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.TFPegasusModel"),d(CS,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertModel"),d(MS,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaModel"),d(yS,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerModel"),d(wS,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5Model"),d(AS,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasModel"),d(LS,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLModel"),d(BS,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.TFViTModel"),d(xS,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(kS,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMModel"),d(RS,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaModel"),d(PS,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetModel"),d(lo,"class","docstring"),d(sr,"class","docstring"),d(Ov,"id","transformers.TFAutoModelForPreTraining"),d(Ov,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ov,"href","#transformers.TFAutoModelForPreTraining"),d(gd,"class","relative group"),d(Jr,"class","docstring"),d(SS,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d($S,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(IS,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForPreTraining"),d(DS,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(NS,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(jS,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(OS,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(GS,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(qS,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(zS,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(XS,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(QS,"href","/docs/transformers/v4.14.1/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(VS,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(WS,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(HS,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel"),d(US,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(JS,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(KS,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(YS,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel"),d(ZS,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(e$,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM"),d(o$,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(io,"class","docstring"),d(lr,"class","docstring"),d(db,"id","transformers.TFAutoModelForCausalLM"),d(db,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(db,"href","#transformers.TFAutoModelForCausalLM"),d(_d,"class","relative group"),d(Kr,"class","docstring"),d(r$,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertLMHeadModel"),d(t$,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(a$,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(n$,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel"),d(s$,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(l$,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(i$,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(d$,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel"),d(c$,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(f$,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(co,"class","docstring"),d(ir,"class","docstring"),d(Tb,"id","transformers.TFAutoModelForImageClassification"),d(Tb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tb,"href","#transformers.TFAutoModelForImageClassification"),d(Td,"class","relative group"),d(Yr,"class","docstring"),d(m$,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.TFViTForImageClassification"),d(fo,"class","docstring"),d(dr,"class","docstring"),d(Eb,"id","transformers.TFAutoModelForMaskedLM"),d(Eb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Eb,"href","#transformers.TFAutoModelForMaskedLM"),d(Cd,"class","relative group"),d(Zr,"class","docstring"),d(h$,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(g$,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(u$,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(p$,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(_$,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(v$,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForMaskedLM"),d(b$,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(T$,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(F$,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(E$,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(C$,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(M$,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(y$,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(w$,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(A$,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(L$,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(B$,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(x$,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(k$,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(R$,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM"),d(mo,"class","docstring"),d(cr,"class","docstring"),d(zb,"id","transformers.TFAutoModelForSeq2SeqLM"),d(zb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(zb,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(wd,"class","relative group"),d(et,"class","docstring"),d(P$,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(S$,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d($$,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(I$,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel"),d(D$,"href","/docs/transformers/v4.14.1/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(N$,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.TFMarianMTModel"),d(j$,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(O$,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(G$,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(q$,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(ho,"class","docstring"),d(fr,"class","docstring"),d(e2,"id","transformers.TFAutoModelForSequenceClassification"),d(e2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(e2,"href","#transformers.TFAutoModelForSequenceClassification"),d(Bd,"class","relative group"),d(ot,"class","docstring"),d(z$,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(X$,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(Q$,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(V$,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(W$,"href","/docs/transformers/v4.14.1/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(H$,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(U$,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForSequenceClassification"),d(J$,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(K$,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d(Y$,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(Z$,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(eI,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(oI,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d(rI,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(tI,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(aI,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d(nI,"href","/docs/transformers/v4.14.1/en/model_doc/gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(sI,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(lI,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(iI,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d(dI,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(cI,"href","/docs/transformers/v4.14.1/en/model_doc/transformerxl#transformers.TFTransfoXLForSequenceClassification"),d(fI,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(mI,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForSequenceClassification"),d(hI,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(go,"class","docstring"),d(mr,"class","docstring"),d(w2,"id","transformers.TFAutoModelForMultipleChoice"),d(w2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w2,"href","#transformers.TFAutoModelForMultipleChoice"),d(Rd,"class","relative group"),d(rt,"class","docstring"),d(gI,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(uI,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(pI,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(_I,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(vI,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(bI,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(TI,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(FI,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(EI,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(CI,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(MI,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(yI,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(wI,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(AI,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(LI,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(BI,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMultipleChoice"),d(xI,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(uo,"class","docstring"),d(hr,"class","docstring"),d(X2,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(X2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(X2,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d($d,"class","relative group"),d(tt,"class","docstring"),d(kI,"href","/docs/transformers/v4.14.1/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(po,"class","docstring"),d(gr,"class","docstring"),d(V2,"id","transformers.TFAutoModelForTokenClassification"),d(V2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(V2,"href","#transformers.TFAutoModelForTokenClassification"),d(Nd,"class","relative group"),d(at,"class","docstring"),d(RI,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(PI,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(SI,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d($I,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(II,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(DI,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForTokenClassification"),d(NI,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(jI,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(OI,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(GI,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(qI,"href","/docs/transformers/v4.14.1/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(zI,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(XI,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(QI,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(VI,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(WI,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(HI,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(UI,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(JI,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForTokenClassification"),d(KI,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(_o,"class","docstring"),d(ur,"class","docstring"),d(hT,"id","transformers.TFAutoModelForQuestionAnswering"),d(hT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hT,"href","#transformers.TFAutoModelForQuestionAnswering"),d(Gd,"class","relative group"),d(nt,"class","docstring"),d(YI,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(ZI,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(eD,"href","/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(oD,"href","/docs/transformers/v4.14.1/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d(rD,"href","/docs/transformers/v4.14.1/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(tD,"href","/docs/transformers/v4.14.1/en/model_doc/deberta_v2#transformers.TFDebertaV2ForQuestionAnswering"),d(aD,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d(nD,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(sD,"href","/docs/transformers/v4.14.1/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(lD,"href","/docs/transformers/v4.14.1/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(iD,"href","/docs/transformers/v4.14.1/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d(dD,"href","/docs/transformers/v4.14.1/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(cD,"href","/docs/transformers/v4.14.1/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(fD,"href","/docs/transformers/v4.14.1/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(mD,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d(hD,"href","/docs/transformers/v4.14.1/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(gD,"href","/docs/transformers/v4.14.1/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(uD,"href","/docs/transformers/v4.14.1/en/model_doc/xlmroberta#transformers.TFXLMRobertaForQuestionAnswering"),d(pD,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(vo,"class","docstring"),d(pr,"class","docstring"),d(PT,"id","transformers.FlaxAutoModel"),d(PT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(PT,"href","#transformers.FlaxAutoModel"),d(Xd,"class","relative group"),d(st,"class","docstring"),d(_D,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertModel"),d(vD,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartModel"),d(bD,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.FlaxBeitModel"),d(TD,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertModel"),d(FD,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdModel"),d(ED,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(CD,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallModel"),d(MD,"href","/docs/transformers/v4.14.1/en/model_doc/clip#transformers.FlaxCLIPModel"),d(yD,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(wD,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraModel"),d(AD,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(LD,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(BD,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(xD,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.FlaxMarianModel"),d(kD,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartModel"),d(RD,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5Model"),d(PD,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(SD,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaModel"),d($D,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5Model"),d(ID,"href","/docs/transformers/v4.14.1/en/model_doc/vision_text_dual_encoder#transformers.FlaxVisionTextDualEncoderModel"),d(DD,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.FlaxViTModel"),d(ND,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(bo,"class","docstring"),d(_r,"class","docstring"),d(rF,"id","transformers.FlaxAutoModelForCausalLM"),d(rF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(rF,"href","#transformers.FlaxAutoModelForCausalLM"),d(Wd,"class","relative group"),d(lt,"class","docstring"),d(jD,"href","/docs/transformers/v4.14.1/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(OD,"href","/docs/transformers/v4.14.1/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(GD,"href","/docs/transformers/v4.14.1/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(To,"class","docstring"),d(vr,"class","docstring"),d(sF,"id","transformers.FlaxAutoModelForPreTraining"),d(sF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sF,"href","#transformers.FlaxAutoModelForPreTraining"),d(Kd,"class","relative group"),d(it,"class","docstring"),d(qD,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(zD,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(XD,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(QD,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForPreTraining"),d(VD,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(WD,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(HD,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(UD,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(JD,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(KD,"href","/docs/transformers/v4.14.1/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(Fo,"class","docstring"),d(br,"class","docstring"),d(_F,"id","transformers.FlaxAutoModelForMaskedLM"),d(_F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_F,"href","#transformers.FlaxAutoModelForMaskedLM"),d(ec,"class","relative group"),d(dt,"class","docstring"),d(YD,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d(ZD,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(eN,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(oN,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForMaskedLM"),d(rN,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(tN,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(aN,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(nN,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(Eo,"class","docstring"),d(Tr,"class","docstring"),d(wF,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(wF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wF,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(tc,"class","relative group"),d(ct,"class","docstring"),d(sN,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(lN,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(iN,"href","/docs/transformers/v4.14.1/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(dN,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),d(cN,"href","/docs/transformers/v4.14.1/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(fN,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(mN,"href","/docs/transformers/v4.14.1/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(hN,"href","/docs/transformers/v4.14.1/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(gN,"href","/docs/transformers/v4.14.1/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(Co,"class","docstring"),d(Fr,"class","docstring"),d(IF,"id","transformers.FlaxAutoModelForSequenceClassification"),d(IF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(IF,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(sc,"class","relative group"),d(ft,"class","docstring"),d(uN,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(pN,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(_N,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(vN,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForSequenceClassification"),d(bN,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(TN,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(FN,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(EN,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(Mo,"class","docstring"),d(Er,"class","docstring"),d(QF,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(QF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(QF,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(dc,"class","relative group"),d(mt,"class","docstring"),d(CN,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(MN,"href","/docs/transformers/v4.14.1/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(yN,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(wN,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForQuestionAnswering"),d(AN,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(LN,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(BN,"href","/docs/transformers/v4.14.1/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(xN,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(yo,"class","docstring"),d(Cr,"class","docstring"),d(eE,"id","transformers.FlaxAutoModelForTokenClassification"),d(eE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(eE,"href","#transformers.FlaxAutoModelForTokenClassification"),d(mc,"class","relative group"),d(ht,"class","docstring"),d(kN,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(RN,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(PN,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForTokenClassification"),d(SN,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d($N,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(IN,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(wo,"class","docstring"),d(Mr,"class","docstring"),d(lE,"id","transformers.FlaxAutoModelForMultipleChoice"),d(lE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lE,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(uc,"class","relative group"),d(gt,"class","docstring"),d(DN,"href","/docs/transformers/v4.14.1/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(NN,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(jN,"href","/docs/transformers/v4.14.1/en/model_doc/bigbird#transformers.FlaxBigBirdForMultipleChoice"),d(ON,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(GN,"href","/docs/transformers/v4.14.1/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(qN,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(Ao,"class","docstring"),d(wr,"class","docstring"),d(gE,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(gE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(gE,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(vc,"class","relative group"),d(ut,"class","docstring"),d(zN,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d(Lo,"class","docstring"),d(Lr,"class","docstring"),d(pE,"id","transformers.FlaxAutoModelForImageClassification"),d(pE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pE,"href","#transformers.FlaxAutoModelForImageClassification"),d(Fc,"class","relative group"),d(pt,"class","docstring"),d(XN,"href","/docs/transformers/v4.14.1/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d(QN,"href","/docs/transformers/v4.14.1/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(Bo,"class","docstring"),d(Br,"class","docstring"),d(bE,"id","transformers.FlaxAutoModelForVision2Seq"),d(bE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bE,"href","#transformers.FlaxAutoModelForVision2Seq"),d(Mc,"class","relative group"),d(_t,"class","docstring"),d(VN,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel"),d(xo,"class","docstring"),d(xr,"class","docstring")},m(c,_){e(document.head,re),v(c,Se,_),v(c,me,_),e(me,ue),e(ue,ro),h(ge,ro,null),e(me,Ce),e(me,$o),e($o,zl),v(c,Lc,_),v(c,Ot,_),e(Ot,Xl),e(Ot,Ql),e(Ql,vC),e(Ot,Bc),v(c,xe,_),v(c,ao,_),e(ao,Vl),e(ao,gn),e(gn,bC),e(ao,un),e(ao,pn),e(pn,TC),e(ao,Wl),e(ao,_n),e(_n,FC),e(ao,Hl),v(c,xc,_),h(_a,c,_),v(c,no,_),v(c,pe,_),e(pe,E0),e(pe,Ul),e(Ul,C0),e(pe,M0),v(c,Io,_),v(c,va,_),e(va,y0),e(va,kc),e(kc,w0),e(va,i0e),v(c,m5e,_),v(c,Jl,_),e(Jl,Rc),e(Rc,PO),h(EC,PO,null),e(Jl,d0e),e(Jl,SO),e(SO,c0e),v(c,h5e,_),v(c,vn,_),e(vn,f0e),e(vn,$O),e($O,m0e),e(vn,h0e),e(vn,IO),e(IO,g0e),e(vn,u0e),v(c,g5e,_),h(CC,c,_),v(c,u5e,_),v(c,A0,_),e(A0,p0e),v(c,p5e,_),h(Pc,c,_),v(c,_5e,_),v(c,Kl,_),e(Kl,Sc),e(Sc,DO),h(MC,DO,null),e(Kl,_0e),e(Kl,NO),e(NO,v0e),v(c,v5e,_),v(c,Do,_),h(yC,Do,null),e(Do,b0e),e(Do,wC),e(wC,T0e),e(wC,L0),e(L0,F0e),e(wC,E0e),e(Do,C0e),e(Do,AC),e(AC,M0e),e(AC,jO),e(jO,y0e),e(AC,w0e),e(Do,A0e),e(Do,so),h(LC,so,null),e(so,L0e),e(so,OO),e(OO,B0e),e(so,x0e),e(so,Yl),e(Yl,k0e),e(Yl,GO),e(GO,R0e),e(Yl,P0e),e(Yl,qO),e(qO,S0e),e(Yl,$0e),e(so,I0e),e(so,b),e(b,$c),e($c,zO),e(zO,D0e),e($c,N0e),e($c,B0),e(B0,j0e),e($c,O0e),e(b,G0e),e(b,Ic),e(Ic,XO),e(XO,q0e),e(Ic,z0e),e(Ic,x0),e(x0,X0e),e(Ic,Q0e),e(b,V0e),e(b,Dc),e(Dc,QO),e(QO,W0e),e(Dc,H0e),e(Dc,k0),e(k0,U0e),e(Dc,J0e),e(b,K0e),e(b,Nc),e(Nc,VO),e(VO,Y0e),e(Nc,Z0e),e(Nc,R0),e(R0,eAe),e(Nc,oAe),e(b,rAe),e(b,jc),e(jc,WO),e(WO,tAe),e(jc,aAe),e(jc,P0),e(P0,nAe),e(jc,sAe),e(b,lAe),e(b,Oc),e(Oc,HO),e(HO,iAe),e(Oc,dAe),e(Oc,S0),e(S0,cAe),e(Oc,fAe),e(b,mAe),e(b,Gc),e(Gc,UO),e(UO,hAe),e(Gc,gAe),e(Gc,$0),e($0,uAe),e(Gc,pAe),e(b,_Ae),e(b,qc),e(qc,JO),e(JO,vAe),e(qc,bAe),e(qc,I0),e(I0,TAe),e(qc,FAe),e(b,EAe),e(b,zc),e(zc,KO),e(KO,CAe),e(zc,MAe),e(zc,D0),e(D0,yAe),e(zc,wAe),e(b,AAe),e(b,Xc),e(Xc,YO),e(YO,LAe),e(Xc,BAe),e(Xc,N0),e(N0,xAe),e(Xc,kAe),e(b,RAe),e(b,Qc),e(Qc,ZO),e(ZO,PAe),e(Qc,SAe),e(Qc,j0),e(j0,$Ae),e(Qc,IAe),e(b,DAe),e(b,Vc),e(Vc,eG),e(eG,NAe),e(Vc,jAe),e(Vc,O0),e(O0,OAe),e(Vc,GAe),e(b,qAe),e(b,Wc),e(Wc,oG),e(oG,zAe),e(Wc,XAe),e(Wc,G0),e(G0,QAe),e(Wc,VAe),e(b,WAe),e(b,Hc),e(Hc,rG),e(rG,HAe),e(Hc,UAe),e(Hc,q0),e(q0,JAe),e(Hc,KAe),e(b,YAe),e(b,Uc),e(Uc,tG),e(tG,ZAe),e(Uc,e6e),e(Uc,z0),e(z0,o6e),e(Uc,r6e),e(b,t6e),e(b,Jc),e(Jc,aG),e(aG,a6e),e(Jc,n6e),e(Jc,X0),e(X0,s6e),e(Jc,l6e),e(b,i6e),e(b,Kc),e(Kc,nG),e(nG,d6e),e(Kc,c6e),e(Kc,Q0),e(Q0,f6e),e(Kc,m6e),e(b,h6e),e(b,Yc),e(Yc,sG),e(sG,g6e),e(Yc,u6e),e(Yc,V0),e(V0,p6e),e(Yc,_6e),e(b,v6e),e(b,Zc),e(Zc,lG),e(lG,b6e),e(Zc,T6e),e(Zc,W0),e(W0,F6e),e(Zc,E6e),e(b,C6e),e(b,ef),e(ef,iG),e(iG,M6e),e(ef,y6e),e(ef,H0),e(H0,w6e),e(ef,A6e),e(b,L6e),e(b,of),e(of,dG),e(dG,B6e),e(of,x6e),e(of,U0),e(U0,k6e),e(of,R6e),e(b,P6e),e(b,rf),e(rf,cG),e(cG,S6e),e(rf,$6e),e(rf,J0),e(J0,I6e),e(rf,D6e),e(b,N6e),e(b,tf),e(tf,fG),e(fG,j6e),e(tf,O6e),e(tf,K0),e(K0,G6e),e(tf,q6e),e(b,z6e),e(b,af),e(af,mG),e(mG,X6e),e(af,Q6e),e(af,Y0),e(Y0,V6e),e(af,W6e),e(b,H6e),e(b,nf),e(nf,hG),e(hG,U6e),e(nf,J6e),e(nf,Z0),e(Z0,K6e),e(nf,Y6e),e(b,Z6e),e(b,sf),e(sf,gG),e(gG,eLe),e(sf,oLe),e(sf,eA),e(eA,rLe),e(sf,tLe),e(b,aLe),e(b,lf),e(lf,uG),e(uG,nLe),e(lf,sLe),e(lf,oA),e(oA,lLe),e(lf,iLe),e(b,dLe),e(b,df),e(df,pG),e(pG,cLe),e(df,fLe),e(df,rA),e(rA,mLe),e(df,hLe),e(b,gLe),e(b,cf),e(cf,_G),e(_G,uLe),e(cf,pLe),e(cf,tA),e(tA,_Le),e(cf,vLe),e(b,bLe),e(b,ff),e(ff,vG),e(vG,TLe),e(ff,FLe),e(ff,aA),e(aA,ELe),e(ff,CLe),e(b,MLe),e(b,mf),e(mf,bG),e(bG,yLe),e(mf,wLe),e(mf,nA),e(nA,ALe),e(mf,LLe),e(b,BLe),e(b,hf),e(hf,TG),e(TG,xLe),e(hf,kLe),e(hf,sA),e(sA,RLe),e(hf,PLe),e(b,SLe),e(b,gf),e(gf,FG),e(FG,$Le),e(gf,ILe),e(gf,lA),e(lA,DLe),e(gf,NLe),e(b,jLe),e(b,uf),e(uf,EG),e(EG,OLe),e(uf,GLe),e(uf,iA),e(iA,qLe),e(uf,zLe),e(b,XLe),e(b,pf),e(pf,CG),e(CG,QLe),e(pf,VLe),e(pf,dA),e(dA,WLe),e(pf,HLe),e(b,ULe),e(b,_f),e(_f,MG),e(MG,JLe),e(_f,KLe),e(_f,cA),e(cA,YLe),e(_f,ZLe),e(b,e8e),e(b,vf),e(vf,yG),e(yG,o8e),e(vf,r8e),e(vf,fA),e(fA,t8e),e(vf,a8e),e(b,n8e),e(b,bf),e(bf,wG),e(wG,s8e),e(bf,l8e),e(bf,mA),e(mA,i8e),e(bf,d8e),e(b,c8e),e(b,Tf),e(Tf,AG),e(AG,f8e),e(Tf,m8e),e(Tf,hA),e(hA,h8e),e(Tf,g8e),e(b,u8e),e(b,Ff),e(Ff,LG),e(LG,p8e),e(Ff,_8e),e(Ff,gA),e(gA,v8e),e(Ff,b8e),e(b,T8e),e(b,Ef),e(Ef,BG),e(BG,F8e),e(Ef,E8e),e(Ef,uA),e(uA,C8e),e(Ef,M8e),e(b,y8e),e(b,Cf),e(Cf,xG),e(xG,w8e),e(Cf,A8e),e(Cf,pA),e(pA,L8e),e(Cf,B8e),e(b,x8e),e(b,Mf),e(Mf,kG),e(kG,k8e),e(Mf,R8e),e(Mf,_A),e(_A,P8e),e(Mf,S8e),e(b,$8e),e(b,yf),e(yf,RG),e(RG,I8e),e(yf,D8e),e(yf,vA),e(vA,N8e),e(yf,j8e),e(b,O8e),e(b,wf),e(wf,PG),e(PG,G8e),e(wf,q8e),e(wf,bA),e(bA,z8e),e(wf,X8e),e(b,Q8e),e(b,Af),e(Af,SG),e(SG,V8e),e(Af,W8e),e(Af,TA),e(TA,H8e),e(Af,U8e),e(b,J8e),e(b,Lf),e(Lf,$G),e($G,K8e),e(Lf,Y8e),e(Lf,FA),e(FA,Z8e),e(Lf,eBe),e(b,oBe),e(b,Bf),e(Bf,IG),e(IG,rBe),e(Bf,tBe),e(Bf,EA),e(EA,aBe),e(Bf,nBe),e(b,sBe),e(b,xf),e(xf,DG),e(DG,lBe),e(xf,iBe),e(xf,CA),e(CA,dBe),e(xf,cBe),e(b,fBe),e(b,kf),e(kf,NG),e(NG,mBe),e(kf,hBe),e(kf,MA),e(MA,gBe),e(kf,uBe),e(b,pBe),e(b,Rf),e(Rf,jG),e(jG,_Be),e(Rf,vBe),e(Rf,yA),e(yA,bBe),e(Rf,TBe),e(b,FBe),e(b,Pf),e(Pf,OG),e(OG,EBe),e(Pf,CBe),e(Pf,wA),e(wA,MBe),e(Pf,yBe),e(b,wBe),e(b,Sf),e(Sf,GG),e(GG,ABe),e(Sf,LBe),e(Sf,AA),e(AA,BBe),e(Sf,xBe),e(b,kBe),e(b,$f),e($f,qG),e(qG,RBe),e($f,PBe),e($f,LA),e(LA,SBe),e($f,$Be),e(b,IBe),e(b,If),e(If,zG),e(zG,DBe),e(If,NBe),e(If,BA),e(BA,jBe),e(If,OBe),e(b,GBe),e(b,Df),e(Df,XG),e(XG,qBe),e(Df,zBe),e(Df,xA),e(xA,XBe),e(Df,QBe),e(b,VBe),e(b,Nf),e(Nf,QG),e(QG,WBe),e(Nf,HBe),e(Nf,kA),e(kA,UBe),e(Nf,JBe),e(b,KBe),e(b,jf),e(jf,VG),e(VG,YBe),e(jf,ZBe),e(jf,RA),e(RA,e9e),e(jf,o9e),e(b,r9e),e(b,Of),e(Of,WG),e(WG,t9e),e(Of,a9e),e(Of,PA),e(PA,n9e),e(Of,s9e),e(b,l9e),e(b,Gf),e(Gf,HG),e(HG,i9e),e(Gf,d9e),e(Gf,SA),e(SA,c9e),e(Gf,f9e),e(b,m9e),e(b,qf),e(qf,UG),e(UG,h9e),e(qf,g9e),e(qf,$A),e($A,u9e),e(qf,p9e),e(b,_9e),e(b,zf),e(zf,JG),e(JG,v9e),e(zf,b9e),e(zf,IA),e(IA,T9e),e(zf,F9e),e(b,E9e),e(b,Xf),e(Xf,KG),e(KG,C9e),e(Xf,M9e),e(Xf,DA),e(DA,y9e),e(Xf,w9e),e(b,A9e),e(b,Qf),e(Qf,YG),e(YG,L9e),e(Qf,B9e),e(Qf,NA),e(NA,x9e),e(Qf,k9e),e(b,R9e),e(b,Vf),e(Vf,ZG),e(ZG,P9e),e(Vf,S9e),e(Vf,jA),e(jA,$9e),e(Vf,I9e),e(b,D9e),e(b,Wf),e(Wf,eq),e(eq,N9e),e(Wf,j9e),e(Wf,OA),e(OA,O9e),e(Wf,G9e),e(b,q9e),e(b,Hf),e(Hf,oq),e(oq,z9e),e(Hf,X9e),e(Hf,GA),e(GA,Q9e),e(Hf,V9e),e(b,W9e),e(b,Uf),e(Uf,rq),e(rq,H9e),e(Uf,U9e),e(Uf,qA),e(qA,J9e),e(Uf,K9e),e(b,Y9e),e(b,Jf),e(Jf,tq),e(tq,Z9e),e(Jf,exe),e(Jf,zA),e(zA,oxe),e(Jf,rxe),e(b,txe),e(b,Kf),e(Kf,aq),e(aq,axe),e(Kf,nxe),e(Kf,XA),e(XA,sxe),e(Kf,lxe),e(b,ixe),e(b,Yf),e(Yf,nq),e(nq,dxe),e(Yf,cxe),e(Yf,QA),e(QA,fxe),e(Yf,mxe),e(b,hxe),e(b,Zf),e(Zf,sq),e(sq,gxe),e(Zf,uxe),e(Zf,VA),e(VA,pxe),e(Zf,_xe),e(b,vxe),e(b,em),e(em,lq),e(lq,bxe),e(em,Txe),e(em,WA),e(WA,Fxe),e(em,Exe),e(b,Cxe),e(b,om),e(om,iq),e(iq,Mxe),e(om,yxe),e(om,HA),e(HA,wxe),e(om,Axe),e(b,Lxe),e(b,rm),e(rm,dq),e(dq,Bxe),e(rm,xxe),e(rm,UA),e(UA,kxe),e(rm,Rxe),e(b,Pxe),e(b,tm),e(tm,cq),e(cq,Sxe),e(tm,$xe),e(tm,JA),e(JA,Ixe),e(tm,Dxe),e(b,Nxe),e(b,am),e(am,fq),e(fq,jxe),e(am,Oxe),e(am,KA),e(KA,Gxe),e(am,qxe),e(b,zxe),e(b,nm),e(nm,mq),e(mq,Xxe),e(nm,Qxe),e(nm,YA),e(YA,Vxe),e(nm,Wxe),e(b,Hxe),e(b,sm),e(sm,hq),e(hq,Uxe),e(sm,Jxe),e(sm,ZA),e(ZA,Kxe),e(sm,Yxe),e(so,Zxe),e(so,gq),e(gq,eke),e(so,oke),h(BC,so,null),e(Do,rke),e(Do,lm),h(xC,lm,null),e(lm,tke),e(lm,uq),e(uq,ake),v(c,b5e,_),v(c,Zl,_),e(Zl,im),e(im,pq),h(kC,pq,null),e(Zl,nke),e(Zl,_q),e(_q,ske),v(c,T5e,_),v(c,No,_),h(RC,No,null),e(No,lke),e(No,PC),e(PC,ike),e(PC,e6),e(e6,dke),e(PC,cke),e(No,fke),e(No,SC),e(SC,mke),e(SC,vq),e(vq,hke),e(SC,gke),e(No,uke),e(No,ye),h($C,ye,null),e(ye,pke),e(ye,bq),e(bq,_ke),e(ye,vke),e(ye,ba),e(ba,bke),e(ba,Tq),e(Tq,Tke),e(ba,Fke),e(ba,Fq),e(Fq,Eke),e(ba,Cke),e(ba,Eq),e(Eq,Mke),e(ba,yke),e(ye,wke),e(ye,C),e(C,bn),e(bn,Cq),e(Cq,Ake),e(bn,Lke),e(bn,o6),e(o6,Bke),e(bn,xke),e(bn,r6),e(r6,kke),e(bn,Rke),e(C,Pke),e(C,Tn),e(Tn,Mq),e(Mq,Ske),e(Tn,$ke),e(Tn,t6),e(t6,Ike),e(Tn,Dke),e(Tn,a6),e(a6,Nke),e(Tn,jke),e(C,Oke),e(C,Fn),e(Fn,yq),e(yq,Gke),e(Fn,qke),e(Fn,n6),e(n6,zke),e(Fn,Xke),e(Fn,s6),e(s6,Qke),e(Fn,Vke),e(C,Wke),e(C,dm),e(dm,wq),e(wq,Hke),e(dm,Uke),e(dm,l6),e(l6,Jke),e(dm,Kke),e(C,Yke),e(C,En),e(En,Aq),e(Aq,Zke),e(En,eRe),e(En,i6),e(i6,oRe),e(En,rRe),e(En,d6),e(d6,tRe),e(En,aRe),e(C,nRe),e(C,cm),e(cm,Lq),e(Lq,sRe),e(cm,lRe),e(cm,c6),e(c6,iRe),e(cm,dRe),e(C,cRe),e(C,fm),e(fm,Bq),e(Bq,fRe),e(fm,mRe),e(fm,f6),e(f6,hRe),e(fm,gRe),e(C,uRe),e(C,mm),e(mm,xq),e(xq,pRe),e(mm,_Re),e(mm,m6),e(m6,vRe),e(mm,bRe),e(C,TRe),e(C,Cn),e(Cn,kq),e(kq,FRe),e(Cn,ERe),e(Cn,h6),e(h6,CRe),e(Cn,MRe),e(Cn,g6),e(g6,yRe),e(Cn,wRe),e(C,ARe),e(C,Mn),e(Mn,Rq),e(Rq,LRe),e(Mn,BRe),e(Mn,u6),e(u6,xRe),e(Mn,kRe),e(Mn,p6),e(p6,RRe),e(Mn,PRe),e(C,SRe),e(C,yn),e(yn,Pq),e(Pq,$Re),e(yn,IRe),e(yn,_6),e(_6,DRe),e(yn,NRe),e(yn,v6),e(v6,jRe),e(yn,ORe),e(C,GRe),e(C,hm),e(hm,Sq),e(Sq,qRe),e(hm,zRe),e(hm,b6),e(b6,XRe),e(hm,QRe),e(C,VRe),e(C,gm),e(gm,$q),e($q,WRe),e(gm,HRe),e(gm,T6),e(T6,URe),e(gm,JRe),e(C,KRe),e(C,wn),e(wn,Iq),e(Iq,YRe),e(wn,ZRe),e(wn,F6),e(F6,ePe),e(wn,oPe),e(wn,E6),e(E6,rPe),e(wn,tPe),e(C,aPe),e(C,um),e(um,Dq),e(Dq,nPe),e(um,sPe),e(um,C6),e(C6,lPe),e(um,iPe),e(C,dPe),e(C,An),e(An,Nq),e(Nq,cPe),e(An,fPe),e(An,M6),e(M6,mPe),e(An,hPe),e(An,y6),e(y6,gPe),e(An,uPe),e(C,pPe),e(C,Ln),e(Ln,jq),e(jq,_Pe),e(Ln,vPe),e(Ln,w6),e(w6,bPe),e(Ln,TPe),e(Ln,A6),e(A6,FPe),e(Ln,EPe),e(C,CPe),e(C,Bn),e(Bn,Oq),e(Oq,MPe),e(Bn,yPe),e(Bn,L6),e(L6,wPe),e(Bn,APe),e(Bn,Gq),e(Gq,LPe),e(Bn,BPe),e(C,xPe),e(C,pm),e(pm,qq),e(qq,kPe),e(pm,RPe),e(pm,B6),e(B6,PPe),e(pm,SPe),e(C,$Pe),e(C,xn),e(xn,zq),e(zq,IPe),e(xn,DPe),e(xn,x6),e(x6,NPe),e(xn,jPe),e(xn,k6),e(k6,OPe),e(xn,GPe),e(C,qPe),e(C,_m),e(_m,Xq),e(Xq,zPe),e(_m,XPe),e(_m,R6),e(R6,QPe),e(_m,VPe),e(C,WPe),e(C,kn),e(kn,Qq),e(Qq,HPe),e(kn,UPe),e(kn,P6),e(P6,JPe),e(kn,KPe),e(kn,S6),e(S6,YPe),e(kn,ZPe),e(C,eSe),e(C,Rn),e(Rn,Vq),e(Vq,oSe),e(Rn,rSe),e(Rn,$6),e($6,tSe),e(Rn,aSe),e(Rn,I6),e(I6,nSe),e(Rn,sSe),e(C,lSe),e(C,Pn),e(Pn,Wq),e(Wq,iSe),e(Pn,dSe),e(Pn,D6),e(D6,cSe),e(Pn,fSe),e(Pn,N6),e(N6,mSe),e(Pn,hSe),e(C,gSe),e(C,vm),e(vm,Hq),e(Hq,uSe),e(vm,pSe),e(vm,j6),e(j6,_Se),e(vm,vSe),e(C,bSe),e(C,Sn),e(Sn,Uq),e(Uq,TSe),e(Sn,FSe),e(Sn,O6),e(O6,ESe),e(Sn,CSe),e(Sn,G6),e(G6,MSe),e(Sn,ySe),e(C,wSe),e(C,bm),e(bm,Jq),e(Jq,ASe),e(bm,LSe),e(bm,q6),e(q6,BSe),e(bm,xSe),e(C,kSe),e(C,$n),e($n,Kq),e(Kq,RSe),e($n,PSe),e($n,z6),e(z6,SSe),e($n,$Se),e($n,X6),e(X6,ISe),e($n,DSe),e(C,NSe),e(C,In),e(In,Yq),e(Yq,jSe),e(In,OSe),e(In,Q6),e(Q6,GSe),e(In,qSe),e(In,V6),e(V6,zSe),e(In,XSe),e(C,QSe),e(C,Dn),e(Dn,Zq),e(Zq,VSe),e(Dn,WSe),e(Dn,W6),e(W6,HSe),e(Dn,USe),e(Dn,H6),e(H6,JSe),e(Dn,KSe),e(C,YSe),e(C,Tm),e(Tm,ez),e(ez,ZSe),e(Tm,e$e),e(Tm,U6),e(U6,o$e),e(Tm,r$e),e(C,t$e),e(C,Nn),e(Nn,oz),e(oz,a$e),e(Nn,n$e),e(Nn,J6),e(J6,s$e),e(Nn,l$e),e(Nn,K6),e(K6,i$e),e(Nn,d$e),e(C,c$e),e(C,jn),e(jn,rz),e(rz,f$e),e(jn,m$e),e(jn,Y6),e(Y6,h$e),e(jn,g$e),e(jn,Z6),e(Z6,u$e),e(jn,p$e),e(C,_$e),e(C,On),e(On,tz),e(tz,v$e),e(On,b$e),e(On,eL),e(eL,T$e),e(On,F$e),e(On,oL),e(oL,E$e),e(On,C$e),e(C,M$e),e(C,Gn),e(Gn,az),e(az,y$e),e(Gn,w$e),e(Gn,rL),e(rL,A$e),e(Gn,L$e),e(Gn,tL),e(tL,B$e),e(Gn,x$e),e(C,k$e),e(C,qn),e(qn,nz),e(nz,R$e),e(qn,P$e),e(qn,aL),e(aL,S$e),e(qn,$$e),e(qn,nL),e(nL,I$e),e(qn,D$e),e(C,N$e),e(C,Fm),e(Fm,sz),e(sz,j$e),e(Fm,O$e),e(Fm,sL),e(sL,G$e),e(Fm,q$e),e(C,z$e),e(C,zn),e(zn,lz),e(lz,X$e),e(zn,Q$e),e(zn,lL),e(lL,V$e),e(zn,W$e),e(zn,iL),e(iL,H$e),e(zn,U$e),e(C,J$e),e(C,Em),e(Em,iz),e(iz,K$e),e(Em,Y$e),e(Em,dL),e(dL,Z$e),e(Em,eIe),e(C,oIe),e(C,Cm),e(Cm,dz),e(dz,rIe),e(Cm,tIe),e(Cm,cL),e(cL,aIe),e(Cm,nIe),e(C,sIe),e(C,Xn),e(Xn,cz),e(cz,lIe),e(Xn,iIe),e(Xn,fL),e(fL,dIe),e(Xn,cIe),e(Xn,mL),e(mL,fIe),e(Xn,mIe),e(C,hIe),e(C,Qn),e(Qn,fz),e(fz,gIe),e(Qn,uIe),e(Qn,hL),e(hL,pIe),e(Qn,_Ie),e(Qn,gL),e(gL,vIe),e(Qn,bIe),e(C,TIe),e(C,Vn),e(Vn,mz),e(mz,FIe),e(Vn,EIe),e(Vn,uL),e(uL,CIe),e(Vn,MIe),e(Vn,pL),e(pL,yIe),e(Vn,wIe),e(C,AIe),e(C,Wn),e(Wn,hz),e(hz,LIe),e(Wn,BIe),e(Wn,_L),e(_L,xIe),e(Wn,kIe),e(Wn,vL),e(vL,RIe),e(Wn,PIe),e(C,SIe),e(C,Hn),e(Hn,gz),e(gz,$Ie),e(Hn,IIe),e(Hn,bL),e(bL,DIe),e(Hn,NIe),e(Hn,TL),e(TL,jIe),e(Hn,OIe),e(C,GIe),e(C,Un),e(Un,uz),e(uz,qIe),e(Un,zIe),e(Un,FL),e(FL,XIe),e(Un,QIe),e(Un,EL),e(EL,VIe),e(Un,WIe),e(C,HIe),e(C,Jn),e(Jn,pz),e(pz,UIe),e(Jn,JIe),e(Jn,CL),e(CL,KIe),e(Jn,YIe),e(Jn,ML),e(ML,ZIe),e(Jn,eDe),e(C,oDe),e(C,Mm),e(Mm,_z),e(_z,rDe),e(Mm,tDe),e(Mm,yL),e(yL,aDe),e(Mm,nDe),e(C,sDe),e(C,ym),e(ym,vz),e(vz,lDe),e(ym,iDe),e(ym,wL),e(wL,dDe),e(ym,cDe),e(C,fDe),e(C,wm),e(wm,bz),e(bz,mDe),e(wm,hDe),e(wm,AL),e(AL,gDe),e(wm,uDe),e(C,pDe),e(C,Kn),e(Kn,Tz),e(Tz,_De),e(Kn,vDe),e(Kn,LL),e(LL,bDe),e(Kn,TDe),e(Kn,BL),e(BL,FDe),e(Kn,EDe),e(C,CDe),e(C,Am),e(Am,Fz),e(Fz,MDe),e(Am,yDe),e(Am,xL),e(xL,wDe),e(Am,ADe),e(C,LDe),e(C,Yn),e(Yn,Ez),e(Ez,BDe),e(Yn,xDe),e(Yn,kL),e(kL,kDe),e(Yn,RDe),e(Yn,RL),e(RL,PDe),e(Yn,SDe),e(C,$De),e(C,Zn),e(Zn,Cz),e(Cz,IDe),e(Zn,DDe),e(Zn,PL),e(PL,NDe),e(Zn,jDe),e(Zn,SL),e(SL,ODe),e(Zn,GDe),e(C,qDe),e(C,es),e(es,Mz),e(Mz,zDe),e(es,XDe),e(es,$L),e($L,QDe),e(es,VDe),e(es,IL),e(IL,WDe),e(es,HDe),e(C,UDe),e(C,os),e(os,yz),e(yz,JDe),e(os,KDe),e(os,DL),e(DL,YDe),e(os,ZDe),e(os,NL),e(NL,eNe),e(os,oNe),e(C,rNe),e(C,rs),e(rs,wz),e(wz,tNe),e(rs,aNe),e(rs,jL),e(jL,nNe),e(rs,sNe),e(rs,OL),e(OL,lNe),e(rs,iNe),e(C,dNe),e(C,Lm),e(Lm,Az),e(Az,cNe),e(Lm,fNe),e(Lm,GL),e(GL,mNe),e(Lm,hNe),e(C,gNe),e(C,Bm),e(Bm,Lz),e(Lz,uNe),e(Bm,pNe),e(Bm,qL),e(qL,_Ne),e(Bm,vNe),e(C,bNe),e(C,ts),e(ts,Bz),e(Bz,TNe),e(ts,FNe),e(ts,zL),e(zL,ENe),e(ts,CNe),e(ts,XL),e(XL,MNe),e(ts,yNe),e(C,wNe),e(C,as),e(as,xz),e(xz,ANe),e(as,LNe),e(as,QL),e(QL,BNe),e(as,xNe),e(as,VL),e(VL,kNe),e(as,RNe),e(C,PNe),e(C,ns),e(ns,kz),e(kz,SNe),e(ns,$Ne),e(ns,WL),e(WL,INe),e(ns,DNe),e(ns,HL),e(HL,NNe),e(ns,jNe),e(C,ONe),e(C,xm),e(xm,Rz),e(Rz,GNe),e(xm,qNe),e(xm,UL),e(UL,zNe),e(xm,XNe),e(C,QNe),e(C,km),e(km,Pz),e(Pz,VNe),e(km,WNe),e(km,JL),e(JL,HNe),e(km,UNe),e(C,JNe),e(C,Rm),e(Rm,Sz),e(Sz,KNe),e(Rm,YNe),e(Rm,KL),e(KL,ZNe),e(Rm,eje),e(C,oje),e(C,Pm),e(Pm,$z),e($z,rje),e(Pm,tje),e(Pm,YL),e(YL,aje),e(Pm,nje),e(C,sje),e(C,Sm),e(Sm,Iz),e(Iz,lje),e(Sm,ije),e(Sm,ZL),e(ZL,dje),e(Sm,cje),e(C,fje),e(C,ss),e(ss,Dz),e(Dz,mje),e(ss,hje),e(ss,e8),e(e8,gje),e(ss,uje),e(ss,o8),e(o8,pje),e(ss,_je),e(C,vje),e(C,ls),e(ls,Nz),e(Nz,bje),e(ls,Tje),e(ls,r8),e(r8,Fje),e(ls,Eje),e(ls,t8),e(t8,Cje),e(ls,Mje),e(ye,yje),e(ye,ei),e(ei,wje),e(ei,jz),e(jz,Aje),e(ei,Lje),e(ei,Oz),e(Oz,Bje),e(ei,xje),e(ye,kje),e(ye,oi),e(oi,Ta),e(Ta,Rje),e(Ta,Gz),e(Gz,Pje),e(Ta,Sje),e(Ta,qz),e(qz,$je),e(Ta,Ije),e(Ta,zz),e(zz,Dje),e(Ta,Nje),e(oi,jje),e(oi,Fa),e(Fa,Oje),e(Fa,Xz),e(Xz,Gje),e(Fa,qje),e(Fa,a8),e(a8,zje),e(Fa,Xje),e(Fa,Qz),e(Qz,Qje),e(Fa,Vje),e(oi,Wje),e(oi,k),e(k,Hje),e(k,Vz),e(Vz,Uje),e(k,Jje),e(k,Wz),e(Wz,Kje),e(k,Yje),e(k,Hz),e(Hz,Zje),e(k,eOe),e(k,n8),e(n8,oOe),e(k,rOe),e(k,Uz),e(Uz,tOe),e(k,aOe),e(k,ri),e(ri,nOe),e(ri,Jz),e(Jz,sOe),e(ri,lOe),e(ri,Kz),e(Kz,iOe),e(ri,dOe),e(k,cOe),e(k,IC),e(IC,fOe),e(IC,Yz),e(Yz,mOe),e(IC,hOe),e(k,gOe),e(k,Zz),e(Zz,uOe),e(k,pOe),e(k,DC),e(DC,_Oe),e(DC,eX),e(eX,vOe),e(DC,bOe),e(k,TOe),e(k,oX),e(oX,FOe),e(k,EOe),e(k,rX),e(rX,COe),e(k,MOe),e(k,tX),e(tX,yOe),e(k,wOe),e(k,aX),e(aX,AOe),e(k,LOe),e(k,nX),e(nX,BOe),e(k,xOe),e(k,sX),e(sX,kOe),e(k,ROe),e(k,lX),e(lX,POe),e(k,SOe),e(k,iX),e(iX,$Oe),e(k,IOe),e(k,dX),e(dX,DOe),e(k,NOe),e(k,cX),e(cX,jOe),e(k,OOe),e(k,NC),e(NC,GOe),e(NC,fX),e(fX,qOe),e(NC,zOe),e(k,XOe),e(k,mX),e(mX,QOe),e(k,VOe),e(k,jC),e(jC,WOe),e(jC,hX),e(hX,HOe),e(jC,UOe),e(k,JOe),e(k,OC),e(OC,KOe),e(OC,gX),e(gX,YOe),e(OC,ZOe),e(k,eGe),e(k,uX),e(uX,oGe),e(k,rGe),e(k,pX),e(pX,tGe),e(k,aGe),e(k,_X),e(_X,nGe),e(k,sGe),e(k,vX),e(vX,lGe),e(k,iGe),e(k,bX),e(bX,dGe),e(k,cGe),e(k,TX),e(TX,fGe),e(k,mGe),e(k,FX),e(FX,hGe),e(k,gGe),e(k,EX),e(EX,uGe),e(k,pGe),e(k,CX),e(CX,_Ge),e(k,vGe),e(k,MX),e(MX,bGe),e(k,TGe),e(k,yX),e(yX,FGe),e(k,EGe),e(k,wX),e(wX,CGe),e(k,MGe),e(k,AX),e(AX,yGe),e(k,wGe),e(ye,AGe),e(ye,LX),e(LX,LGe),e(ye,BGe),h(GC,ye,null),e(No,xGe),e(No,$m),h(qC,$m,null),e($m,kGe),e($m,BX),e(BX,RGe),v(c,F5e,_),v(c,ti,_),e(ti,Im),e(Im,xX),h(zC,xX,null),e(ti,PGe),e(ti,kX),e(kX,SGe),v(c,E5e,_),v(c,Dt,_),h(XC,Dt,null),e(Dt,$Ge),e(Dt,QC),e(QC,IGe),e(QC,s8),e(s8,DGe),e(QC,NGe),e(Dt,jGe),e(Dt,VC),e(VC,OGe),e(VC,RX),e(RX,GGe),e(VC,qGe),e(Dt,zGe),e(Dt,Fe),h(WC,Fe,null),e(Fe,XGe),e(Fe,PX),e(PX,QGe),e(Fe,VGe),e(Fe,Ea),e(Ea,WGe),e(Ea,SX),e(SX,HGe),e(Ea,UGe),e(Ea,$X),e($X,JGe),e(Ea,KGe),e(Ea,IX),e(IX,YGe),e(Ea,ZGe),e(Fe,eqe),e(Fe,_e),e(_e,Dm),e(Dm,DX),e(DX,oqe),e(Dm,rqe),e(Dm,l8),e(l8,tqe),e(Dm,aqe),e(_e,nqe),e(_e,Nm),e(Nm,NX),e(NX,sqe),e(Nm,lqe),e(Nm,i8),e(i8,iqe),e(Nm,dqe),e(_e,cqe),e(_e,jm),e(jm,jX),e(jX,fqe),e(jm,mqe),e(jm,d8),e(d8,hqe),e(jm,gqe),e(_e,uqe),e(_e,Om),e(Om,OX),e(OX,pqe),e(Om,_qe),e(Om,c8),e(c8,vqe),e(Om,bqe),e(_e,Tqe),e(_e,Gm),e(Gm,GX),e(GX,Fqe),e(Gm,Eqe),e(Gm,f8),e(f8,Cqe),e(Gm,Mqe),e(_e,yqe),e(_e,qm),e(qm,qX),e(qX,wqe),e(qm,Aqe),e(qm,m8),e(m8,Lqe),e(qm,Bqe),e(_e,xqe),e(_e,zm),e(zm,zX),e(zX,kqe),e(zm,Rqe),e(zm,h8),e(h8,Pqe),e(zm,Sqe),e(_e,$qe),e(_e,Xm),e(Xm,XX),e(XX,Iqe),e(Xm,Dqe),e(Xm,g8),e(g8,Nqe),e(Xm,jqe),e(_e,Oqe),e(_e,Qm),e(Qm,QX),e(QX,Gqe),e(Qm,qqe),e(Qm,u8),e(u8,zqe),e(Qm,Xqe),e(_e,Qqe),e(_e,Vm),e(Vm,VX),e(VX,Vqe),e(Vm,Wqe),e(Vm,p8),e(p8,Hqe),e(Vm,Uqe),e(Fe,Jqe),e(Fe,ai),e(ai,Kqe),e(ai,WX),e(WX,Yqe),e(ai,Zqe),e(ai,HX),e(HX,eze),e(ai,oze),e(Fe,rze),e(Fe,ni),e(ni,Ca),e(Ca,tze),e(Ca,UX),e(UX,aze),e(Ca,nze),e(Ca,JX),e(JX,sze),e(Ca,lze),e(Ca,KX),e(KX,ize),e(Ca,dze),e(ni,cze),e(ni,Ma),e(Ma,fze),e(Ma,YX),e(YX,mze),e(Ma,hze),e(Ma,_8),e(_8,gze),e(Ma,uze),e(Ma,ZX),e(ZX,pze),e(Ma,_ze),e(ni,vze),e(ni,N),e(N,bze),e(N,eQ),e(eQ,Tze),e(N,Fze),e(N,oQ),e(oQ,Eze),e(N,Cze),e(N,si),e(si,Mze),e(si,rQ),e(rQ,yze),e(si,wze),e(si,tQ),e(tQ,Aze),e(si,Lze),e(N,Bze),e(N,HC),e(HC,xze),e(HC,aQ),e(aQ,kze),e(HC,Rze),e(N,Pze),e(N,nQ),e(nQ,Sze),e(N,$ze),e(N,UC),e(UC,Ize),e(UC,sQ),e(sQ,Dze),e(UC,Nze),e(N,jze),e(N,lQ),e(lQ,Oze),e(N,Gze),e(N,iQ),e(iQ,qze),e(N,zze),e(N,dQ),e(dQ,Xze),e(N,Qze),e(N,cQ),e(cQ,Vze),e(N,Wze),e(N,JC),e(JC,Hze),e(JC,fQ),e(fQ,Uze),e(JC,Jze),e(N,Kze),e(N,mQ),e(mQ,Yze),e(N,Zze),e(N,hQ),e(hQ,eXe),e(N,oXe),e(N,gQ),e(gQ,rXe),e(N,tXe),e(N,uQ),e(uQ,aXe),e(N,nXe),e(N,pQ),e(pQ,sXe),e(N,lXe),e(N,_Q),e(_Q,iXe),e(N,dXe),e(N,vQ),e(vQ,cXe),e(N,fXe),e(N,bQ),e(bQ,mXe),e(N,hXe),e(N,KC),e(KC,gXe),e(KC,TQ),e(TQ,uXe),e(KC,pXe),e(N,_Xe),e(N,FQ),e(FQ,vXe),e(N,bXe),e(N,EQ),e(EQ,TXe),e(N,FXe),e(N,CQ),e(CQ,EXe),e(N,CXe),e(N,MQ),e(MQ,MXe),e(N,yXe),e(N,yQ),e(yQ,wXe),e(N,AXe),e(N,wQ),e(wQ,LXe),e(N,BXe),e(N,AQ),e(AQ,xXe),e(N,kXe),e(N,LQ),e(LQ,RXe),e(N,PXe),e(N,BQ),e(BQ,SXe),e(N,$Xe),e(N,xQ),e(xQ,IXe),e(N,DXe),e(N,kQ),e(kQ,NXe),e(N,jXe),e(Fe,OXe),h(Wm,Fe,null),e(Fe,GXe),e(Fe,RQ),e(RQ,qXe),e(Fe,zXe),h(YC,Fe,null),v(c,C5e,_),v(c,li,_),e(li,Hm),e(Hm,PQ),h(ZC,PQ,null),e(li,XXe),e(li,SQ),e(SQ,QXe),v(c,M5e,_),v(c,Nt,_),h(e3,Nt,null),e(Nt,VXe),e(Nt,o3),e(o3,WXe),e(o3,v8),e(v8,HXe),e(o3,UXe),e(Nt,JXe),e(Nt,r3),e(r3,KXe),e(r3,$Q),e($Q,YXe),e(r3,ZXe),e(Nt,eQe),e(Nt,Ee),h(t3,Ee,null),e(Ee,oQe),e(Ee,IQ),e(IQ,rQe),e(Ee,tQe),e(Ee,ii),e(ii,aQe),e(ii,DQ),e(DQ,nQe),e(ii,sQe),e(ii,NQ),e(NQ,lQe),e(ii,iQe),e(Ee,dQe),e(Ee,to),e(to,Um),e(Um,jQ),e(jQ,cQe),e(Um,fQe),e(Um,b8),e(b8,mQe),e(Um,hQe),e(to,gQe),e(to,Jm),e(Jm,OQ),e(OQ,uQe),e(Jm,pQe),e(Jm,T8),e(T8,_Qe),e(Jm,vQe),e(to,bQe),e(to,Km),e(Km,GQ),e(GQ,TQe),e(Km,FQe),e(Km,F8),e(F8,EQe),e(Km,CQe),e(to,MQe),e(to,Ym),e(Ym,qQ),e(qQ,yQe),e(Ym,wQe),e(Ym,E8),e(E8,AQe),e(Ym,LQe),e(to,BQe),e(to,Zm),e(Zm,zQ),e(zQ,xQe),e(Zm,kQe),e(Zm,C8),e(C8,RQe),e(Zm,PQe),e(to,SQe),e(to,eh),e(eh,XQ),e(XQ,$Qe),e(eh,IQe),e(eh,M8),e(M8,DQe),e(eh,NQe),e(to,jQe),e(to,oh),e(oh,QQ),e(QQ,OQe),e(oh,GQe),e(oh,y8),e(y8,qQe),e(oh,zQe),e(Ee,XQe),e(Ee,di),e(di,QQe),e(di,VQ),e(VQ,VQe),e(di,WQe),e(di,WQ),e(WQ,HQe),e(di,UQe),e(Ee,JQe),e(Ee,a3),e(a3,ya),e(ya,KQe),e(ya,HQ),e(HQ,YQe),e(ya,ZQe),e(ya,UQ),e(UQ,eVe),e(ya,oVe),e(ya,JQ),e(JQ,rVe),e(ya,tVe),e(a3,aVe),e(a3,D),e(D,nVe),e(D,KQ),e(KQ,sVe),e(D,lVe),e(D,YQ),e(YQ,iVe),e(D,dVe),e(D,ZQ),e(ZQ,cVe),e(D,fVe),e(D,ci),e(ci,mVe),e(ci,eV),e(eV,hVe),e(ci,gVe),e(ci,oV),e(oV,uVe),e(ci,pVe),e(D,_Ve),e(D,n3),e(n3,vVe),e(n3,rV),e(rV,bVe),e(n3,TVe),e(D,FVe),e(D,tV),e(tV,EVe),e(D,CVe),e(D,s3),e(s3,MVe),e(s3,aV),e(aV,yVe),e(s3,wVe),e(D,AVe),e(D,nV),e(nV,LVe),e(D,BVe),e(D,sV),e(sV,xVe),e(D,kVe),e(D,lV),e(lV,RVe),e(D,PVe),e(D,iV),e(iV,SVe),e(D,$Ve),e(D,l3),e(l3,IVe),e(l3,dV),e(dV,DVe),e(l3,NVe),e(D,jVe),e(D,cV),e(cV,OVe),e(D,GVe),e(D,fV),e(fV,qVe),e(D,zVe),e(D,mV),e(mV,XVe),e(D,QVe),e(D,hV),e(hV,VVe),e(D,WVe),e(D,gV),e(gV,HVe),e(D,UVe),e(D,uV),e(uV,JVe),e(D,KVe),e(D,pV),e(pV,YVe),e(D,ZVe),e(D,_V),e(_V,eWe),e(D,oWe),e(D,i3),e(i3,rWe),e(i3,vV),e(vV,tWe),e(i3,aWe),e(D,nWe),e(D,bV),e(bV,sWe),e(D,lWe),e(D,TV),e(TV,iWe),e(D,dWe),e(D,FV),e(FV,cWe),e(D,fWe),e(D,EV),e(EV,mWe),e(D,hWe),e(D,CV),e(CV,gWe),e(D,uWe),e(D,MV),e(MV,pWe),e(D,_We),e(D,yV),e(yV,vWe),e(D,bWe),e(D,wV),e(wV,TWe),e(D,FWe),e(D,AV),e(AV,EWe),e(D,CWe),e(D,LV),e(LV,MWe),e(D,yWe),e(D,BV),e(BV,wWe),e(D,AWe),e(Ee,LWe),h(rh,Ee,null),e(Ee,BWe),e(Ee,xV),e(xV,xWe),e(Ee,kWe),h(d3,Ee,null),v(c,y5e,_),v(c,fi,_),e(fi,th),e(th,kV),h(c3,kV,null),e(fi,RWe),e(fi,RV),e(RV,PWe),v(c,w5e,_),v(c,jo,_),h(f3,jo,null),e(jo,SWe),e(jo,mi),e(mi,$We),e(mi,PV),e(PV,IWe),e(mi,DWe),e(mi,SV),e(SV,NWe),e(mi,jWe),e(jo,OWe),e(jo,m3),e(m3,GWe),e(m3,$V),e($V,qWe),e(m3,zWe),e(jo,XWe),e(jo,kr),h(h3,kr,null),e(kr,QWe),e(kr,IV),e(IV,VWe),e(kr,WWe),e(kr,hi),e(hi,HWe),e(hi,DV),e(DV,UWe),e(hi,JWe),e(hi,NV),e(NV,KWe),e(hi,YWe),e(kr,ZWe),e(kr,jV),e(jV,eHe),e(kr,oHe),h(g3,kr,null),e(jo,rHe),e(jo,$e),h(u3,$e,null),e($e,tHe),e($e,OV),e(OV,aHe),e($e,nHe),e($e,wa),e(wa,sHe),e(wa,GV),e(GV,lHe),e(wa,iHe),e(wa,qV),e(qV,dHe),e(wa,cHe),e(wa,zV),e(zV,fHe),e(wa,mHe),e($e,hHe),e($e,F),e(F,ah),e(ah,XV),e(XV,gHe),e(ah,uHe),e(ah,w8),e(w8,pHe),e(ah,_He),e(F,vHe),e(F,nh),e(nh,QV),e(QV,bHe),e(nh,THe),e(nh,A8),e(A8,FHe),e(nh,EHe),e(F,CHe),e(F,sh),e(sh,VV),e(VV,MHe),e(sh,yHe),e(sh,L8),e(L8,wHe),e(sh,AHe),e(F,LHe),e(F,lh),e(lh,WV),e(WV,BHe),e(lh,xHe),e(lh,B8),e(B8,kHe),e(lh,RHe),e(F,PHe),e(F,ih),e(ih,HV),e(HV,SHe),e(ih,$He),e(ih,x8),e(x8,IHe),e(ih,DHe),e(F,NHe),e(F,dh),e(dh,UV),e(UV,jHe),e(dh,OHe),e(dh,k8),e(k8,GHe),e(dh,qHe),e(F,zHe),e(F,ch),e(ch,JV),e(JV,XHe),e(ch,QHe),e(ch,R8),e(R8,VHe),e(ch,WHe),e(F,HHe),e(F,fh),e(fh,KV),e(KV,UHe),e(fh,JHe),e(fh,P8),e(P8,KHe),e(fh,YHe),e(F,ZHe),e(F,mh),e(mh,YV),e(YV,eUe),e(mh,oUe),e(mh,S8),e(S8,rUe),e(mh,tUe),e(F,aUe),e(F,hh),e(hh,ZV),e(ZV,nUe),e(hh,sUe),e(hh,$8),e($8,lUe),e(hh,iUe),e(F,dUe),e(F,gh),e(gh,eW),e(eW,cUe),e(gh,fUe),e(gh,I8),e(I8,mUe),e(gh,hUe),e(F,gUe),e(F,uh),e(uh,oW),e(oW,uUe),e(uh,pUe),e(uh,D8),e(D8,_Ue),e(uh,vUe),e(F,bUe),e(F,ph),e(ph,rW),e(rW,TUe),e(ph,FUe),e(ph,N8),e(N8,EUe),e(ph,CUe),e(F,MUe),e(F,_h),e(_h,tW),e(tW,yUe),e(_h,wUe),e(_h,j8),e(j8,AUe),e(_h,LUe),e(F,BUe),e(F,vh),e(vh,aW),e(aW,xUe),e(vh,kUe),e(vh,O8),e(O8,RUe),e(vh,PUe),e(F,SUe),e(F,bh),e(bh,nW),e(nW,$Ue),e(bh,IUe),e(bh,G8),e(G8,DUe),e(bh,NUe),e(F,jUe),e(F,Th),e(Th,sW),e(sW,OUe),e(Th,GUe),e(Th,q8),e(q8,qUe),e(Th,zUe),e(F,XUe),e(F,Fh),e(Fh,lW),e(lW,QUe),e(Fh,VUe),e(Fh,z8),e(z8,WUe),e(Fh,HUe),e(F,UUe),e(F,Eh),e(Eh,iW),e(iW,JUe),e(Eh,KUe),e(Eh,X8),e(X8,YUe),e(Eh,ZUe),e(F,eJe),e(F,Ch),e(Ch,dW),e(dW,oJe),e(Ch,rJe),e(Ch,Q8),e(Q8,tJe),e(Ch,aJe),e(F,nJe),e(F,Mh),e(Mh,cW),e(cW,sJe),e(Mh,lJe),e(Mh,V8),e(V8,iJe),e(Mh,dJe),e(F,cJe),e(F,yh),e(yh,fW),e(fW,fJe),e(yh,mJe),e(yh,W8),e(W8,hJe),e(yh,gJe),e(F,uJe),e(F,wh),e(wh,mW),e(mW,pJe),e(wh,_Je),e(wh,H8),e(H8,vJe),e(wh,bJe),e(F,TJe),e(F,Ah),e(Ah,hW),e(hW,FJe),e(Ah,EJe),e(Ah,U8),e(U8,CJe),e(Ah,MJe),e(F,yJe),e(F,is),e(is,gW),e(gW,wJe),e(is,AJe),e(is,J8),e(J8,LJe),e(is,BJe),e(is,K8),e(K8,xJe),e(is,kJe),e(F,RJe),e(F,Lh),e(Lh,uW),e(uW,PJe),e(Lh,SJe),e(Lh,Y8),e(Y8,$Je),e(Lh,IJe),e(F,DJe),e(F,Bh),e(Bh,pW),e(pW,NJe),e(Bh,jJe),e(Bh,Z8),e(Z8,OJe),e(Bh,GJe),e(F,qJe),e(F,xh),e(xh,_W),e(_W,zJe),e(xh,XJe),e(xh,eB),e(eB,QJe),e(xh,VJe),e(F,WJe),e(F,kh),e(kh,vW),e(vW,HJe),e(kh,UJe),e(kh,oB),e(oB,JJe),e(kh,KJe),e(F,YJe),e(F,Rh),e(Rh,bW),e(bW,ZJe),e(Rh,eKe),e(Rh,rB),e(rB,oKe),e(Rh,rKe),e(F,tKe),e(F,Ph),e(Ph,TW),e(TW,aKe),e(Ph,nKe),e(Ph,tB),e(tB,sKe),e(Ph,lKe),e(F,iKe),e(F,Sh),e(Sh,FW),e(FW,dKe),e(Sh,cKe),e(Sh,aB),e(aB,fKe),e(Sh,mKe),e(F,hKe),e(F,$h),e($h,EW),e(EW,gKe),e($h,uKe),e($h,nB),e(nB,pKe),e($h,_Ke),e(F,vKe),e(F,Ih),e(Ih,CW),e(CW,bKe),e(Ih,TKe),e(Ih,sB),e(sB,FKe),e(Ih,EKe),e(F,CKe),e(F,Dh),e(Dh,MW),e(MW,MKe),e(Dh,yKe),e(Dh,lB),e(lB,wKe),e(Dh,AKe),e(F,LKe),e(F,Nh),e(Nh,yW),e(yW,BKe),e(Nh,xKe),e(Nh,iB),e(iB,kKe),e(Nh,RKe),e(F,PKe),e(F,jh),e(jh,wW),e(wW,SKe),e(jh,$Ke),e(jh,dB),e(dB,IKe),e(jh,DKe),e(F,NKe),e(F,Oh),e(Oh,AW),e(AW,jKe),e(Oh,OKe),e(Oh,cB),e(cB,GKe),e(Oh,qKe),e(F,zKe),e(F,Gh),e(Gh,LW),e(LW,XKe),e(Gh,QKe),e(Gh,fB),e(fB,VKe),e(Gh,WKe),e(F,HKe),e(F,qh),e(qh,BW),e(BW,UKe),e(qh,JKe),e(qh,mB),e(mB,KKe),e(qh,YKe),e(F,ZKe),e(F,zh),e(zh,xW),e(xW,eYe),e(zh,oYe),e(zh,hB),e(hB,rYe),e(zh,tYe),e(F,aYe),e(F,Xh),e(Xh,kW),e(kW,nYe),e(Xh,sYe),e(Xh,gB),e(gB,lYe),e(Xh,iYe),e(F,dYe),e(F,Qh),e(Qh,RW),e(RW,cYe),e(Qh,fYe),e(Qh,uB),e(uB,mYe),e(Qh,hYe),e(F,gYe),e(F,Vh),e(Vh,PW),e(PW,uYe),e(Vh,pYe),e(Vh,pB),e(pB,_Ye),e(Vh,vYe),e(F,bYe),e(F,Wh),e(Wh,SW),e(SW,TYe),e(Wh,FYe),e(Wh,_B),e(_B,EYe),e(Wh,CYe),e(F,MYe),e(F,Hh),e(Hh,$W),e($W,yYe),e(Hh,wYe),e(Hh,vB),e(vB,AYe),e(Hh,LYe),e(F,BYe),e(F,Uh),e(Uh,IW),e(IW,xYe),e(Uh,kYe),e(Uh,bB),e(bB,RYe),e(Uh,PYe),e(F,SYe),e(F,Jh),e(Jh,DW),e(DW,$Ye),e(Jh,IYe),e(Jh,TB),e(TB,DYe),e(Jh,NYe),e(F,jYe),e(F,Kh),e(Kh,NW),e(NW,OYe),e(Kh,GYe),e(Kh,FB),e(FB,qYe),e(Kh,zYe),e(F,XYe),e(F,Yh),e(Yh,jW),e(jW,QYe),e(Yh,VYe),e(Yh,EB),e(EB,WYe),e(Yh,HYe),e(F,UYe),e(F,Zh),e(Zh,OW),e(OW,JYe),e(Zh,KYe),e(Zh,CB),e(CB,YYe),e(Zh,ZYe),e(F,eZe),e(F,eg),e(eg,GW),e(GW,oZe),e(eg,rZe),e(eg,MB),e(MB,tZe),e(eg,aZe),e(F,nZe),e(F,og),e(og,qW),e(qW,sZe),e(og,lZe),e(og,yB),e(yB,iZe),e(og,dZe),e(F,cZe),e(F,rg),e(rg,zW),e(zW,fZe),e(rg,mZe),e(rg,wB),e(wB,hZe),e(rg,gZe),e(F,uZe),e(F,tg),e(tg,XW),e(XW,pZe),e(tg,_Ze),e(tg,AB),e(AB,vZe),e(tg,bZe),e(F,TZe),e(F,ag),e(ag,QW),e(QW,FZe),e(ag,EZe),e(ag,LB),e(LB,CZe),e(ag,MZe),e(F,yZe),e(F,ng),e(ng,VW),e(VW,wZe),e(ng,AZe),e(ng,BB),e(BB,LZe),e(ng,BZe),e(F,xZe),e(F,sg),e(sg,WW),e(WW,kZe),e(sg,RZe),e(sg,xB),e(xB,PZe),e(sg,SZe),e(F,$Ze),e(F,lg),e(lg,HW),e(HW,IZe),e(lg,DZe),e(lg,kB),e(kB,NZe),e(lg,jZe),e(F,OZe),e(F,ig),e(ig,UW),e(UW,GZe),e(ig,qZe),e(ig,RB),e(RB,zZe),e(ig,XZe),e(F,QZe),e(F,dg),e(dg,JW),e(JW,VZe),e(dg,WZe),e(dg,PB),e(PB,HZe),e(dg,UZe),e(F,JZe),e(F,cg),e(cg,KW),e(KW,KZe),e(cg,YZe),e(cg,SB),e(SB,ZZe),e(cg,eeo),e(F,oeo),e(F,fg),e(fg,YW),e(YW,reo),e(fg,teo),e(fg,$B),e($B,aeo),e(fg,neo),e(F,seo),e(F,mg),e(mg,ZW),e(ZW,leo),e(mg,ieo),e(mg,IB),e(IB,deo),e(mg,ceo),e(F,feo),e(F,hg),e(hg,eH),e(eH,meo),e(hg,heo),e(hg,DB),e(DB,geo),e(hg,ueo),e(F,peo),e(F,gg),e(gg,oH),e(oH,_eo),e(gg,veo),e(gg,NB),e(NB,beo),e(gg,Teo),e(F,Feo),e(F,ug),e(ug,rH),e(rH,Eeo),e(ug,Ceo),e(ug,jB),e(jB,Meo),e(ug,yeo),e(F,weo),e(F,pg),e(pg,tH),e(tH,Aeo),e(pg,Leo),e(pg,OB),e(OB,Beo),e(pg,xeo),e(F,keo),e(F,_g),e(_g,aH),e(aH,Reo),e(_g,Peo),e(_g,GB),e(GB,Seo),e(_g,$eo),e(F,Ieo),e(F,vg),e(vg,nH),e(nH,Deo),e(vg,Neo),e(vg,qB),e(qB,jeo),e(vg,Oeo),e(F,Geo),e(F,bg),e(bg,sH),e(sH,qeo),e(bg,zeo),e(bg,zB),e(zB,Xeo),e(bg,Qeo),e(F,Veo),e(F,Tg),e(Tg,lH),e(lH,Weo),e(Tg,Heo),e(Tg,XB),e(XB,Ueo),e(Tg,Jeo),e(F,Keo),e(F,Fg),e(Fg,iH),e(iH,Yeo),e(Fg,Zeo),e(Fg,QB),e(QB,eoo),e(Fg,ooo),e($e,roo),e($e,Eg),e(Eg,too),e(Eg,dH),e(dH,aoo),e(Eg,noo),e(Eg,cH),e(cH,soo),e($e,loo),e($e,fH),e(fH,ioo),e($e,doo),h(p3,$e,null),v(c,A5e,_),v(c,gi,_),e(gi,Cg),e(Cg,mH),h(_3,mH,null),e(gi,coo),e(gi,hH),e(hH,foo),v(c,L5e,_),v(c,Oo,_),h(v3,Oo,null),e(Oo,moo),e(Oo,ui),e(ui,hoo),e(ui,gH),e(gH,goo),e(ui,uoo),e(ui,uH),e(uH,poo),e(ui,_oo),e(Oo,voo),e(Oo,b3),e(b3,boo),e(b3,pH),e(pH,Too),e(b3,Foo),e(Oo,Eoo),e(Oo,Rr),h(T3,Rr,null),e(Rr,Coo),e(Rr,_H),e(_H,Moo),e(Rr,yoo),e(Rr,pi),e(pi,woo),e(pi,vH),e(vH,Aoo),e(pi,Loo),e(pi,bH),e(bH,Boo),e(pi,xoo),e(Rr,koo),e(Rr,TH),e(TH,Roo),e(Rr,Poo),h(F3,Rr,null),e(Oo,Soo),e(Oo,Ie),h(E3,Ie,null),e(Ie,$oo),e(Ie,FH),e(FH,Ioo),e(Ie,Doo),e(Ie,Aa),e(Aa,Noo),e(Aa,EH),e(EH,joo),e(Aa,Ooo),e(Aa,CH),e(CH,Goo),e(Aa,qoo),e(Aa,MH),e(MH,zoo),e(Aa,Xoo),e(Ie,Qoo),e(Ie,R),e(R,Mg),e(Mg,yH),e(yH,Voo),e(Mg,Woo),e(Mg,VB),e(VB,Hoo),e(Mg,Uoo),e(R,Joo),e(R,yg),e(yg,wH),e(wH,Koo),e(yg,Yoo),e(yg,WB),e(WB,Zoo),e(yg,ero),e(R,oro),e(R,wg),e(wg,AH),e(AH,rro),e(wg,tro),e(wg,HB),e(HB,aro),e(wg,nro),e(R,sro),e(R,Ag),e(Ag,LH),e(LH,lro),e(Ag,iro),e(Ag,UB),e(UB,dro),e(Ag,cro),e(R,fro),e(R,Lg),e(Lg,BH),e(BH,mro),e(Lg,hro),e(Lg,JB),e(JB,gro),e(Lg,uro),e(R,pro),e(R,Bg),e(Bg,xH),e(xH,_ro),e(Bg,vro),e(Bg,KB),e(KB,bro),e(Bg,Tro),e(R,Fro),e(R,xg),e(xg,kH),e(kH,Ero),e(xg,Cro),e(xg,YB),e(YB,Mro),e(xg,yro),e(R,wro),e(R,kg),e(kg,RH),e(RH,Aro),e(kg,Lro),e(kg,ZB),e(ZB,Bro),e(kg,xro),e(R,kro),e(R,Rg),e(Rg,PH),e(PH,Rro),e(Rg,Pro),e(Rg,e9),e(e9,Sro),e(Rg,$ro),e(R,Iro),e(R,Pg),e(Pg,SH),e(SH,Dro),e(Pg,Nro),e(Pg,o9),e(o9,jro),e(Pg,Oro),e(R,Gro),e(R,Sg),e(Sg,$H),e($H,qro),e(Sg,zro),e(Sg,r9),e(r9,Xro),e(Sg,Qro),e(R,Vro),e(R,$g),e($g,IH),e(IH,Wro),e($g,Hro),e($g,t9),e(t9,Uro),e($g,Jro),e(R,Kro),e(R,Ig),e(Ig,DH),e(DH,Yro),e(Ig,Zro),e(Ig,a9),e(a9,eto),e(Ig,oto),e(R,rto),e(R,Dg),e(Dg,NH),e(NH,tto),e(Dg,ato),e(Dg,n9),e(n9,nto),e(Dg,sto),e(R,lto),e(R,Ng),e(Ng,jH),e(jH,ito),e(Ng,dto),e(Ng,s9),e(s9,cto),e(Ng,fto),e(R,mto),e(R,jg),e(jg,OH),e(OH,hto),e(jg,gto),e(jg,l9),e(l9,uto),e(jg,pto),e(R,_to),e(R,Og),e(Og,GH),e(GH,vto),e(Og,bto),e(Og,i9),e(i9,Tto),e(Og,Fto),e(R,Eto),e(R,Gg),e(Gg,qH),e(qH,Cto),e(Gg,Mto),e(Gg,d9),e(d9,yto),e(Gg,wto),e(R,Ato),e(R,qg),e(qg,zH),e(zH,Lto),e(qg,Bto),e(qg,c9),e(c9,xto),e(qg,kto),e(R,Rto),e(R,zg),e(zg,XH),e(XH,Pto),e(zg,Sto),e(zg,f9),e(f9,$to),e(zg,Ito),e(R,Dto),e(R,Xg),e(Xg,QH),e(QH,Nto),e(Xg,jto),e(Xg,m9),e(m9,Oto),e(Xg,Gto),e(R,qto),e(R,Qg),e(Qg,VH),e(VH,zto),e(Qg,Xto),e(Qg,h9),e(h9,Qto),e(Qg,Vto),e(R,Wto),e(R,Vg),e(Vg,WH),e(WH,Hto),e(Vg,Uto),e(Vg,g9),e(g9,Jto),e(Vg,Kto),e(R,Yto),e(R,Wg),e(Wg,HH),e(HH,Zto),e(Wg,eao),e(Wg,u9),e(u9,oao),e(Wg,rao),e(R,tao),e(R,Hg),e(Hg,UH),e(UH,aao),e(Hg,nao),e(Hg,p9),e(p9,sao),e(Hg,lao),e(R,iao),e(R,Ug),e(Ug,JH),e(JH,dao),e(Ug,cao),e(Ug,_9),e(_9,fao),e(Ug,mao),e(R,hao),e(R,Jg),e(Jg,KH),e(KH,gao),e(Jg,uao),e(Jg,v9),e(v9,pao),e(Jg,_ao),e(R,vao),e(R,Kg),e(Kg,YH),e(YH,bao),e(Kg,Tao),e(Kg,b9),e(b9,Fao),e(Kg,Eao),e(R,Cao),e(R,Yg),e(Yg,ZH),e(ZH,Mao),e(Yg,yao),e(Yg,T9),e(T9,wao),e(Yg,Aao),e(R,Lao),e(R,Zg),e(Zg,eU),e(eU,Bao),e(Zg,xao),e(Zg,F9),e(F9,kao),e(Zg,Rao),e(R,Pao),e(R,eu),e(eu,oU),e(oU,Sao),e(eu,$ao),e(eu,E9),e(E9,Iao),e(eu,Dao),e(R,Nao),e(R,ou),e(ou,rU),e(rU,jao),e(ou,Oao),e(ou,C9),e(C9,Gao),e(ou,qao),e(R,zao),e(R,ru),e(ru,tU),e(tU,Xao),e(ru,Qao),e(ru,M9),e(M9,Vao),e(ru,Wao),e(R,Hao),e(R,tu),e(tu,aU),e(aU,Uao),e(tu,Jao),e(tu,y9),e(y9,Kao),e(tu,Yao),e(R,Zao),e(R,au),e(au,nU),e(nU,eno),e(au,ono),e(au,w9),e(w9,rno),e(au,tno),e(R,ano),e(R,nu),e(nu,sU),e(sU,nno),e(nu,sno),e(nu,A9),e(A9,lno),e(nu,ino),e(Ie,dno),e(Ie,su),e(su,cno),e(su,lU),e(lU,fno),e(su,mno),e(su,iU),e(iU,hno),e(Ie,gno),e(Ie,dU),e(dU,uno),e(Ie,pno),h(C3,Ie,null),v(c,B5e,_),v(c,_i,_),e(_i,lu),e(lu,cU),h(M3,cU,null),e(_i,_no),e(_i,fU),e(fU,vno),v(c,x5e,_),v(c,Go,_),h(y3,Go,null),e(Go,bno),e(Go,vi),e(vi,Tno),e(vi,mU),e(mU,Fno),e(vi,Eno),e(vi,hU),e(hU,Cno),e(vi,Mno),e(Go,yno),e(Go,w3),e(w3,wno),e(w3,gU),e(gU,Ano),e(w3,Lno),e(Go,Bno),e(Go,Pr),h(A3,Pr,null),e(Pr,xno),e(Pr,uU),e(uU,kno),e(Pr,Rno),e(Pr,bi),e(bi,Pno),e(bi,pU),e(pU,Sno),e(bi,$no),e(bi,_U),e(_U,Ino),e(bi,Dno),e(Pr,Nno),e(Pr,vU),e(vU,jno),e(Pr,Ono),h(L3,Pr,null),e(Go,Gno),e(Go,De),h(B3,De,null),e(De,qno),e(De,bU),e(bU,zno),e(De,Xno),e(De,La),e(La,Qno),e(La,TU),e(TU,Vno),e(La,Wno),e(La,FU),e(FU,Hno),e(La,Uno),e(La,EU),e(EU,Jno),e(La,Kno),e(De,Yno),e(De,q),e(q,iu),e(iu,CU),e(CU,Zno),e(iu,eso),e(iu,L9),e(L9,oso),e(iu,rso),e(q,tso),e(q,du),e(du,MU),e(MU,aso),e(du,nso),e(du,B9),e(B9,sso),e(du,lso),e(q,iso),e(q,cu),e(cu,yU),e(yU,dso),e(cu,cso),e(cu,x9),e(x9,fso),e(cu,mso),e(q,hso),e(q,fu),e(fu,wU),e(wU,gso),e(fu,uso),e(fu,k9),e(k9,pso),e(fu,_so),e(q,vso),e(q,mu),e(mu,AU),e(AU,bso),e(mu,Tso),e(mu,R9),e(R9,Fso),e(mu,Eso),e(q,Cso),e(q,hu),e(hu,LU),e(LU,Mso),e(hu,yso),e(hu,P9),e(P9,wso),e(hu,Aso),e(q,Lso),e(q,gu),e(gu,BU),e(BU,Bso),e(gu,xso),e(gu,S9),e(S9,kso),e(gu,Rso),e(q,Pso),e(q,uu),e(uu,xU),e(xU,Sso),e(uu,$so),e(uu,$9),e($9,Iso),e(uu,Dso),e(q,Nso),e(q,pu),e(pu,kU),e(kU,jso),e(pu,Oso),e(pu,I9),e(I9,Gso),e(pu,qso),e(q,zso),e(q,_u),e(_u,RU),e(RU,Xso),e(_u,Qso),e(_u,D9),e(D9,Vso),e(_u,Wso),e(q,Hso),e(q,vu),e(vu,PU),e(PU,Uso),e(vu,Jso),e(vu,N9),e(N9,Kso),e(vu,Yso),e(q,Zso),e(q,bu),e(bu,SU),e(SU,elo),e(bu,olo),e(bu,j9),e(j9,rlo),e(bu,tlo),e(q,alo),e(q,Tu),e(Tu,$U),e($U,nlo),e(Tu,slo),e(Tu,O9),e(O9,llo),e(Tu,ilo),e(q,dlo),e(q,Fu),e(Fu,IU),e(IU,clo),e(Fu,flo),e(Fu,G9),e(G9,mlo),e(Fu,hlo),e(q,glo),e(q,Eu),e(Eu,DU),e(DU,ulo),e(Eu,plo),e(Eu,q9),e(q9,_lo),e(Eu,vlo),e(q,blo),e(q,Cu),e(Cu,NU),e(NU,Tlo),e(Cu,Flo),e(Cu,z9),e(z9,Elo),e(Cu,Clo),e(q,Mlo),e(q,Mu),e(Mu,jU),e(jU,ylo),e(Mu,wlo),e(Mu,X9),e(X9,Alo),e(Mu,Llo),e(q,Blo),e(q,yu),e(yu,OU),e(OU,xlo),e(yu,klo),e(yu,Q9),e(Q9,Rlo),e(yu,Plo),e(q,Slo),e(q,wu),e(wu,GU),e(GU,$lo),e(wu,Ilo),e(wu,V9),e(V9,Dlo),e(wu,Nlo),e(q,jlo),e(q,Au),e(Au,qU),e(qU,Olo),e(Au,Glo),e(Au,W9),e(W9,qlo),e(Au,zlo),e(q,Xlo),e(q,Lu),e(Lu,zU),e(zU,Qlo),e(Lu,Vlo),e(Lu,H9),e(H9,Wlo),e(Lu,Hlo),e(q,Ulo),e(q,Bu),e(Bu,XU),e(XU,Jlo),e(Bu,Klo),e(Bu,U9),e(U9,Ylo),e(Bu,Zlo),e(q,eio),e(q,xu),e(xu,QU),e(QU,oio),e(xu,rio),e(xu,J9),e(J9,tio),e(xu,aio),e(q,nio),e(q,ku),e(ku,VU),e(VU,sio),e(ku,lio),e(ku,K9),e(K9,iio),e(ku,dio),e(q,cio),e(q,Ru),e(Ru,WU),e(WU,fio),e(Ru,mio),e(Ru,Y9),e(Y9,hio),e(Ru,gio),e(q,uio),e(q,Pu),e(Pu,HU),e(HU,pio),e(Pu,_io),e(Pu,Z9),e(Z9,vio),e(Pu,bio),e(q,Tio),e(q,Su),e(Su,UU),e(UU,Fio),e(Su,Eio),e(Su,ex),e(ex,Cio),e(Su,Mio),e(q,yio),e(q,$u),e($u,JU),e(JU,wio),e($u,Aio),e($u,ox),e(ox,Lio),e($u,Bio),e(q,xio),e(q,Iu),e(Iu,KU),e(KU,kio),e(Iu,Rio),e(Iu,rx),e(rx,Pio),e(Iu,Sio),e(q,$io),e(q,Du),e(Du,YU),e(YU,Iio),e(Du,Dio),e(Du,tx),e(tx,Nio),e(Du,jio),e(De,Oio),e(De,Nu),e(Nu,Gio),e(Nu,ZU),e(ZU,qio),e(Nu,zio),e(Nu,eJ),e(eJ,Xio),e(De,Qio),e(De,oJ),e(oJ,Vio),e(De,Wio),h(x3,De,null),v(c,k5e,_),v(c,Ti,_),e(Ti,ju),e(ju,rJ),h(k3,rJ,null),e(Ti,Hio),e(Ti,tJ),e(tJ,Uio),v(c,R5e,_),v(c,qo,_),h(R3,qo,null),e(qo,Jio),e(qo,Fi),e(Fi,Kio),e(Fi,aJ),e(aJ,Yio),e(Fi,Zio),e(Fi,nJ),e(nJ,edo),e(Fi,odo),e(qo,rdo),e(qo,P3),e(P3,tdo),e(P3,sJ),e(sJ,ado),e(P3,ndo),e(qo,sdo),e(qo,Sr),h(S3,Sr,null),e(Sr,ldo),e(Sr,lJ),e(lJ,ido),e(Sr,ddo),e(Sr,Ei),e(Ei,cdo),e(Ei,iJ),e(iJ,fdo),e(Ei,mdo),e(Ei,dJ),e(dJ,hdo),e(Ei,gdo),e(Sr,udo),e(Sr,cJ),e(cJ,pdo),e(Sr,_do),h($3,Sr,null),e(qo,vdo),e(qo,Ne),h(I3,Ne,null),e(Ne,bdo),e(Ne,fJ),e(fJ,Tdo),e(Ne,Fdo),e(Ne,Ba),e(Ba,Edo),e(Ba,mJ),e(mJ,Cdo),e(Ba,Mdo),e(Ba,hJ),e(hJ,ydo),e(Ba,wdo),e(Ba,gJ),e(gJ,Ado),e(Ba,Ldo),e(Ne,Bdo),e(Ne,O),e(O,Ou),e(Ou,uJ),e(uJ,xdo),e(Ou,kdo),e(Ou,ax),e(ax,Rdo),e(Ou,Pdo),e(O,Sdo),e(O,Gu),e(Gu,pJ),e(pJ,$do),e(Gu,Ido),e(Gu,nx),e(nx,Ddo),e(Gu,Ndo),e(O,jdo),e(O,qu),e(qu,_J),e(_J,Odo),e(qu,Gdo),e(qu,sx),e(sx,qdo),e(qu,zdo),e(O,Xdo),e(O,zu),e(zu,vJ),e(vJ,Qdo),e(zu,Vdo),e(zu,lx),e(lx,Wdo),e(zu,Hdo),e(O,Udo),e(O,Xu),e(Xu,bJ),e(bJ,Jdo),e(Xu,Kdo),e(Xu,ix),e(ix,Ydo),e(Xu,Zdo),e(O,eco),e(O,Qu),e(Qu,TJ),e(TJ,oco),e(Qu,rco),e(Qu,dx),e(dx,tco),e(Qu,aco),e(O,nco),e(O,Vu),e(Vu,FJ),e(FJ,sco),e(Vu,lco),e(Vu,cx),e(cx,ico),e(Vu,dco),e(O,cco),e(O,Wu),e(Wu,EJ),e(EJ,fco),e(Wu,mco),e(Wu,fx),e(fx,hco),e(Wu,gco),e(O,uco),e(O,Hu),e(Hu,CJ),e(CJ,pco),e(Hu,_co),e(Hu,mx),e(mx,vco),e(Hu,bco),e(O,Tco),e(O,Uu),e(Uu,MJ),e(MJ,Fco),e(Uu,Eco),e(Uu,hx),e(hx,Cco),e(Uu,Mco),e(O,yco),e(O,Ju),e(Ju,yJ),e(yJ,wco),e(Ju,Aco),e(Ju,gx),e(gx,Lco),e(Ju,Bco),e(O,xco),e(O,Ku),e(Ku,wJ),e(wJ,kco),e(Ku,Rco),e(Ku,ux),e(ux,Pco),e(Ku,Sco),e(O,$co),e(O,Yu),e(Yu,AJ),e(AJ,Ico),e(Yu,Dco),e(Yu,px),e(px,Nco),e(Yu,jco),e(O,Oco),e(O,Zu),e(Zu,LJ),e(LJ,Gco),e(Zu,qco),e(Zu,_x),e(_x,zco),e(Zu,Xco),e(O,Qco),e(O,ep),e(ep,BJ),e(BJ,Vco),e(ep,Wco),e(ep,vx),e(vx,Hco),e(ep,Uco),e(O,Jco),e(O,op),e(op,xJ),e(xJ,Kco),e(op,Yco),e(op,bx),e(bx,Zco),e(op,efo),e(O,ofo),e(O,rp),e(rp,kJ),e(kJ,rfo),e(rp,tfo),e(rp,Tx),e(Tx,afo),e(rp,nfo),e(O,sfo),e(O,tp),e(tp,RJ),e(RJ,lfo),e(tp,ifo),e(tp,Fx),e(Fx,dfo),e(tp,cfo),e(O,ffo),e(O,ap),e(ap,PJ),e(PJ,mfo),e(ap,hfo),e(ap,Ex),e(Ex,gfo),e(ap,ufo),e(O,pfo),e(O,np),e(np,SJ),e(SJ,_fo),e(np,vfo),e(np,Cx),e(Cx,bfo),e(np,Tfo),e(O,Ffo),e(O,sp),e(sp,$J),e($J,Efo),e(sp,Cfo),e(sp,Mx),e(Mx,Mfo),e(sp,yfo),e(O,wfo),e(O,lp),e(lp,IJ),e(IJ,Afo),e(lp,Lfo),e(lp,yx),e(yx,Bfo),e(lp,xfo),e(O,kfo),e(O,ip),e(ip,DJ),e(DJ,Rfo),e(ip,Pfo),e(ip,wx),e(wx,Sfo),e(ip,$fo),e(O,Ifo),e(O,dp),e(dp,NJ),e(NJ,Dfo),e(dp,Nfo),e(dp,Ax),e(Ax,jfo),e(dp,Ofo),e(O,Gfo),e(O,cp),e(cp,jJ),e(jJ,qfo),e(cp,zfo),e(cp,Lx),e(Lx,Xfo),e(cp,Qfo),e(O,Vfo),e(O,fp),e(fp,OJ),e(OJ,Wfo),e(fp,Hfo),e(fp,Bx),e(Bx,Ufo),e(fp,Jfo),e(O,Kfo),e(O,mp),e(mp,GJ),e(GJ,Yfo),e(mp,Zfo),e(mp,xx),e(xx,emo),e(mp,omo),e(O,rmo),e(O,hp),e(hp,qJ),e(qJ,tmo),e(hp,amo),e(hp,kx),e(kx,nmo),e(hp,smo),e(O,lmo),e(O,gp),e(gp,zJ),e(zJ,imo),e(gp,dmo),e(gp,XJ),e(XJ,cmo),e(gp,fmo),e(O,mmo),e(O,up),e(up,QJ),e(QJ,hmo),e(up,gmo),e(up,Rx),e(Rx,umo),e(up,pmo),e(O,_mo),e(O,pp),e(pp,VJ),e(VJ,vmo),e(pp,bmo),e(pp,Px),e(Px,Tmo),e(pp,Fmo),e(Ne,Emo),e(Ne,_p),e(_p,Cmo),e(_p,WJ),e(WJ,Mmo),e(_p,ymo),e(_p,HJ),e(HJ,wmo),e(Ne,Amo),e(Ne,UJ),e(UJ,Lmo),e(Ne,Bmo),h(D3,Ne,null),v(c,P5e,_),v(c,Ci,_),e(Ci,vp),e(vp,JJ),h(N3,JJ,null),e(Ci,xmo),e(Ci,KJ),e(KJ,kmo),v(c,S5e,_),v(c,zo,_),h(j3,zo,null),e(zo,Rmo),e(zo,Mi),e(Mi,Pmo),e(Mi,YJ),e(YJ,Smo),e(Mi,$mo),e(Mi,ZJ),e(ZJ,Imo),e(Mi,Dmo),e(zo,Nmo),e(zo,O3),e(O3,jmo),e(O3,eK),e(eK,Omo),e(O3,Gmo),e(zo,qmo),e(zo,$r),h(G3,$r,null),e($r,zmo),e($r,oK),e(oK,Xmo),e($r,Qmo),e($r,yi),e(yi,Vmo),e(yi,rK),e(rK,Wmo),e(yi,Hmo),e(yi,tK),e(tK,Umo),e(yi,Jmo),e($r,Kmo),e($r,aK),e(aK,Ymo),e($r,Zmo),h(q3,$r,null),e(zo,eho),e(zo,je),h(z3,je,null),e(je,oho),e(je,nK),e(nK,rho),e(je,tho),e(je,xa),e(xa,aho),e(xa,sK),e(sK,nho),e(xa,sho),e(xa,lK),e(lK,lho),e(xa,iho),e(xa,iK),e(iK,dho),e(xa,cho),e(je,fho),e(je,fe),e(fe,bp),e(bp,dK),e(dK,mho),e(bp,hho),e(bp,Sx),e(Sx,gho),e(bp,uho),e(fe,pho),e(fe,Tp),e(Tp,cK),e(cK,_ho),e(Tp,vho),e(Tp,$x),e($x,bho),e(Tp,Tho),e(fe,Fho),e(fe,Fp),e(Fp,fK),e(fK,Eho),e(Fp,Cho),e(Fp,Ix),e(Ix,Mho),e(Fp,yho),e(fe,who),e(fe,Ep),e(Ep,mK),e(mK,Aho),e(Ep,Lho),e(Ep,Dx),e(Dx,Bho),e(Ep,xho),e(fe,kho),e(fe,Cp),e(Cp,hK),e(hK,Rho),e(Cp,Pho),e(Cp,Nx),e(Nx,Sho),e(Cp,$ho),e(fe,Iho),e(fe,Mp),e(Mp,gK),e(gK,Dho),e(Mp,Nho),e(Mp,jx),e(jx,jho),e(Mp,Oho),e(fe,Gho),e(fe,yp),e(yp,uK),e(uK,qho),e(yp,zho),e(yp,Ox),e(Ox,Xho),e(yp,Qho),e(fe,Vho),e(fe,wp),e(wp,pK),e(pK,Who),e(wp,Hho),e(wp,Gx),e(Gx,Uho),e(wp,Jho),e(fe,Kho),e(fe,Ap),e(Ap,_K),e(_K,Yho),e(Ap,Zho),e(Ap,qx),e(qx,ego),e(Ap,ogo),e(fe,rgo),e(fe,Lp),e(Lp,vK),e(vK,tgo),e(Lp,ago),e(Lp,zx),e(zx,ngo),e(Lp,sgo),e(fe,lgo),e(fe,Bp),e(Bp,bK),e(bK,igo),e(Bp,dgo),e(Bp,Xx),e(Xx,cgo),e(Bp,fgo),e(fe,mgo),e(fe,xp),e(xp,TK),e(TK,hgo),e(xp,ggo),e(xp,Qx),e(Qx,ugo),e(xp,pgo),e(fe,_go),e(fe,kp),e(kp,FK),e(FK,vgo),e(kp,bgo),e(kp,Vx),e(Vx,Tgo),e(kp,Fgo),e(fe,Ego),e(fe,Rp),e(Rp,EK),e(EK,Cgo),e(Rp,Mgo),e(Rp,Wx),e(Wx,ygo),e(Rp,wgo),e(fe,Ago),e(fe,Pp),e(Pp,CK),e(CK,Lgo),e(Pp,Bgo),e(Pp,Hx),e(Hx,xgo),e(Pp,kgo),e(je,Rgo),e(je,Sp),e(Sp,Pgo),e(Sp,MK),e(MK,Sgo),e(Sp,$go),e(Sp,yK),e(yK,Igo),e(je,Dgo),e(je,wK),e(wK,Ngo),e(je,jgo),h(X3,je,null),v(c,$5e,_),v(c,wi,_),e(wi,$p),e($p,AK),h(Q3,AK,null),e(wi,Ogo),e(wi,LK),e(LK,Ggo),v(c,I5e,_),v(c,Xo,_),h(V3,Xo,null),e(Xo,qgo),e(Xo,Ai),e(Ai,zgo),e(Ai,BK),e(BK,Xgo),e(Ai,Qgo),e(Ai,xK),e(xK,Vgo),e(Ai,Wgo),e(Xo,Hgo),e(Xo,W3),e(W3,Ugo),e(W3,kK),e(kK,Jgo),e(W3,Kgo),e(Xo,Ygo),e(Xo,Ir),h(H3,Ir,null),e(Ir,Zgo),e(Ir,RK),e(RK,euo),e(Ir,ouo),e(Ir,Li),e(Li,ruo),e(Li,PK),e(PK,tuo),e(Li,auo),e(Li,SK),e(SK,nuo),e(Li,suo),e(Ir,luo),e(Ir,$K),e($K,iuo),e(Ir,duo),h(U3,Ir,null),e(Xo,cuo),e(Xo,Oe),h(J3,Oe,null),e(Oe,fuo),e(Oe,IK),e(IK,muo),e(Oe,huo),e(Oe,ka),e(ka,guo),e(ka,DK),e(DK,uuo),e(ka,puo),e(ka,NK),e(NK,_uo),e(ka,vuo),e(ka,jK),e(jK,buo),e(ka,Tuo),e(Oe,Fuo),e(Oe,A),e(A,Ip),e(Ip,OK),e(OK,Euo),e(Ip,Cuo),e(Ip,Ux),e(Ux,Muo),e(Ip,yuo),e(A,wuo),e(A,Dp),e(Dp,GK),e(GK,Auo),e(Dp,Luo),e(Dp,Jx),e(Jx,Buo),e(Dp,xuo),e(A,kuo),e(A,Np),e(Np,qK),e(qK,Ruo),e(Np,Puo),e(Np,Kx),e(Kx,Suo),e(Np,$uo),e(A,Iuo),e(A,jp),e(jp,zK),e(zK,Duo),e(jp,Nuo),e(jp,Yx),e(Yx,juo),e(jp,Ouo),e(A,Guo),e(A,Op),e(Op,XK),e(XK,quo),e(Op,zuo),e(Op,Zx),e(Zx,Xuo),e(Op,Quo),e(A,Vuo),e(A,Gp),e(Gp,QK),e(QK,Wuo),e(Gp,Huo),e(Gp,ek),e(ek,Uuo),e(Gp,Juo),e(A,Kuo),e(A,qp),e(qp,VK),e(VK,Yuo),e(qp,Zuo),e(qp,ok),e(ok,epo),e(qp,opo),e(A,rpo),e(A,zp),e(zp,WK),e(WK,tpo),e(zp,apo),e(zp,rk),e(rk,npo),e(zp,spo),e(A,lpo),e(A,Xp),e(Xp,HK),e(HK,ipo),e(Xp,dpo),e(Xp,tk),e(tk,cpo),e(Xp,fpo),e(A,mpo),e(A,Qp),e(Qp,UK),e(UK,hpo),e(Qp,gpo),e(Qp,ak),e(ak,upo),e(Qp,ppo),e(A,_po),e(A,Vp),e(Vp,JK),e(JK,vpo),e(Vp,bpo),e(Vp,nk),e(nk,Tpo),e(Vp,Fpo),e(A,Epo),e(A,Wp),e(Wp,KK),e(KK,Cpo),e(Wp,Mpo),e(Wp,sk),e(sk,ypo),e(Wp,wpo),e(A,Apo),e(A,Hp),e(Hp,YK),e(YK,Lpo),e(Hp,Bpo),e(Hp,lk),e(lk,xpo),e(Hp,kpo),e(A,Rpo),e(A,Up),e(Up,ZK),e(ZK,Ppo),e(Up,Spo),e(Up,ik),e(ik,$po),e(Up,Ipo),e(A,Dpo),e(A,Jp),e(Jp,eY),e(eY,Npo),e(Jp,jpo),e(Jp,dk),e(dk,Opo),e(Jp,Gpo),e(A,qpo),e(A,Kp),e(Kp,oY),e(oY,zpo),e(Kp,Xpo),e(Kp,ck),e(ck,Qpo),e(Kp,Vpo),e(A,Wpo),e(A,Yp),e(Yp,rY),e(rY,Hpo),e(Yp,Upo),e(Yp,fk),e(fk,Jpo),e(Yp,Kpo),e(A,Ypo),e(A,Zp),e(Zp,tY),e(tY,Zpo),e(Zp,e_o),e(Zp,mk),e(mk,o_o),e(Zp,r_o),e(A,t_o),e(A,e_),e(e_,aY),e(aY,a_o),e(e_,n_o),e(e_,hk),e(hk,s_o),e(e_,l_o),e(A,i_o),e(A,o_),e(o_,nY),e(nY,d_o),e(o_,c_o),e(o_,gk),e(gk,f_o),e(o_,m_o),e(A,h_o),e(A,r_),e(r_,sY),e(sY,g_o),e(r_,u_o),e(r_,uk),e(uk,p_o),e(r_,__o),e(A,v_o),e(A,t_),e(t_,lY),e(lY,b_o),e(t_,T_o),e(t_,pk),e(pk,F_o),e(t_,E_o),e(A,C_o),e(A,a_),e(a_,iY),e(iY,M_o),e(a_,y_o),e(a_,_k),e(_k,w_o),e(a_,A_o),e(A,L_o),e(A,n_),e(n_,dY),e(dY,B_o),e(n_,x_o),e(n_,vk),e(vk,k_o),e(n_,R_o),e(A,P_o),e(A,s_),e(s_,cY),e(cY,S_o),e(s_,$_o),e(s_,bk),e(bk,I_o),e(s_,D_o),e(A,N_o),e(A,l_),e(l_,fY),e(fY,j_o),e(l_,O_o),e(l_,Tk),e(Tk,G_o),e(l_,q_o),e(A,z_o),e(A,i_),e(i_,mY),e(mY,X_o),e(i_,Q_o),e(i_,Fk),e(Fk,V_o),e(i_,W_o),e(A,H_o),e(A,d_),e(d_,hY),e(hY,U_o),e(d_,J_o),e(d_,Ek),e(Ek,K_o),e(d_,Y_o),e(A,Z_o),e(A,c_),e(c_,gY),e(gY,e1o),e(c_,o1o),e(c_,Ck),e(Ck,r1o),e(c_,t1o),e(A,a1o),e(A,f_),e(f_,uY),e(uY,n1o),e(f_,s1o),e(f_,Mk),e(Mk,l1o),e(f_,i1o),e(A,d1o),e(A,m_),e(m_,pY),e(pY,c1o),e(m_,f1o),e(m_,yk),e(yk,m1o),e(m_,h1o),e(A,g1o),e(A,h_),e(h_,_Y),e(_Y,u1o),e(h_,p1o),e(h_,wk),e(wk,_1o),e(h_,v1o),e(A,b1o),e(A,g_),e(g_,vY),e(vY,T1o),e(g_,F1o),e(g_,Ak),e(Ak,E1o),e(g_,C1o),e(A,M1o),e(A,u_),e(u_,bY),e(bY,y1o),e(u_,w1o),e(u_,Lk),e(Lk,A1o),e(u_,L1o),e(A,B1o),e(A,p_),e(p_,TY),e(TY,x1o),e(p_,k1o),e(p_,Bk),e(Bk,R1o),e(p_,P1o),e(A,S1o),e(A,__),e(__,FY),e(FY,$1o),e(__,I1o),e(__,xk),e(xk,D1o),e(__,N1o),e(A,j1o),e(A,v_),e(v_,EY),e(EY,O1o),e(v_,G1o),e(v_,kk),e(kk,q1o),e(v_,z1o),e(A,X1o),e(A,b_),e(b_,CY),e(CY,Q1o),e(b_,V1o),e(b_,Rk),e(Rk,W1o),e(b_,H1o),e(A,U1o),e(A,T_),e(T_,MY),e(MY,J1o),e(T_,K1o),e(T_,Pk),e(Pk,Y1o),e(T_,Z1o),e(A,e4o),e(A,F_),e(F_,yY),e(yY,o4o),e(F_,r4o),e(F_,Sk),e(Sk,t4o),e(F_,a4o),e(A,n4o),e(A,E_),e(E_,wY),e(wY,s4o),e(E_,l4o),e(E_,$k),e($k,i4o),e(E_,d4o),e(Oe,c4o),e(Oe,C_),e(C_,f4o),e(C_,AY),e(AY,m4o),e(C_,h4o),e(C_,LY),e(LY,g4o),e(Oe,u4o),e(Oe,BY),e(BY,p4o),e(Oe,_4o),h(K3,Oe,null),v(c,D5e,_),v(c,Bi,_),e(Bi,M_),e(M_,xY),h(Y3,xY,null),e(Bi,v4o),e(Bi,kY),e(kY,b4o),v(c,N5e,_),v(c,Qo,_),h(Z3,Qo,null),e(Qo,T4o),e(Qo,xi),e(xi,F4o),e(xi,RY),e(RY,E4o),e(xi,C4o),e(xi,PY),e(PY,M4o),e(xi,y4o),e(Qo,w4o),e(Qo,eM),e(eM,A4o),e(eM,SY),e(SY,L4o),e(eM,B4o),e(Qo,x4o),e(Qo,Dr),h(oM,Dr,null),e(Dr,k4o),e(Dr,$Y),e($Y,R4o),e(Dr,P4o),e(Dr,ki),e(ki,S4o),e(ki,IY),e(IY,$4o),e(ki,I4o),e(ki,DY),e(DY,D4o),e(ki,N4o),e(Dr,j4o),e(Dr,NY),e(NY,O4o),e(Dr,G4o),h(rM,Dr,null),e(Qo,q4o),e(Qo,Ge),h(tM,Ge,null),e(Ge,z4o),e(Ge,jY),e(jY,X4o),e(Ge,Q4o),e(Ge,Ra),e(Ra,V4o),e(Ra,OY),e(OY,W4o),e(Ra,H4o),e(Ra,GY),e(GY,U4o),e(Ra,J4o),e(Ra,qY),e(qY,K4o),e(Ra,Y4o),e(Ge,Z4o),e(Ge,H),e(H,y_),e(y_,zY),e(zY,evo),e(y_,ovo),e(y_,Ik),e(Ik,rvo),e(y_,tvo),e(H,avo),e(H,w_),e(w_,XY),e(XY,nvo),e(w_,svo),e(w_,Dk),e(Dk,lvo),e(w_,ivo),e(H,dvo),e(H,A_),e(A_,QY),e(QY,cvo),e(A_,fvo),e(A_,Nk),e(Nk,mvo),e(A_,hvo),e(H,gvo),e(H,L_),e(L_,VY),e(VY,uvo),e(L_,pvo),e(L_,jk),e(jk,_vo),e(L_,vvo),e(H,bvo),e(H,B_),e(B_,WY),e(WY,Tvo),e(B_,Fvo),e(B_,Ok),e(Ok,Evo),e(B_,Cvo),e(H,Mvo),e(H,x_),e(x_,HY),e(HY,yvo),e(x_,wvo),e(x_,Gk),e(Gk,Avo),e(x_,Lvo),e(H,Bvo),e(H,k_),e(k_,UY),e(UY,xvo),e(k_,kvo),e(k_,qk),e(qk,Rvo),e(k_,Pvo),e(H,Svo),e(H,R_),e(R_,JY),e(JY,$vo),e(R_,Ivo),e(R_,zk),e(zk,Dvo),e(R_,Nvo),e(H,jvo),e(H,P_),e(P_,KY),e(KY,Ovo),e(P_,Gvo),e(P_,Xk),e(Xk,qvo),e(P_,zvo),e(H,Xvo),e(H,S_),e(S_,YY),e(YY,Qvo),e(S_,Vvo),e(S_,Qk),e(Qk,Wvo),e(S_,Hvo),e(H,Uvo),e(H,$_),e($_,ZY),e(ZY,Jvo),e($_,Kvo),e($_,Vk),e(Vk,Yvo),e($_,Zvo),e(H,ebo),e(H,I_),e(I_,eZ),e(eZ,obo),e(I_,rbo),e(I_,Wk),e(Wk,tbo),e(I_,abo),e(H,nbo),e(H,D_),e(D_,oZ),e(oZ,sbo),e(D_,lbo),e(D_,Hk),e(Hk,ibo),e(D_,dbo),e(H,cbo),e(H,N_),e(N_,rZ),e(rZ,fbo),e(N_,mbo),e(N_,Uk),e(Uk,hbo),e(N_,gbo),e(H,ubo),e(H,j_),e(j_,tZ),e(tZ,pbo),e(j_,_bo),e(j_,Jk),e(Jk,vbo),e(j_,bbo),e(H,Tbo),e(H,O_),e(O_,aZ),e(aZ,Fbo),e(O_,Ebo),e(O_,Kk),e(Kk,Cbo),e(O_,Mbo),e(H,ybo),e(H,G_),e(G_,nZ),e(nZ,wbo),e(G_,Abo),e(G_,Yk),e(Yk,Lbo),e(G_,Bbo),e(H,xbo),e(H,q_),e(q_,sZ),e(sZ,kbo),e(q_,Rbo),e(q_,Zk),e(Zk,Pbo),e(q_,Sbo),e(H,$bo),e(H,z_),e(z_,lZ),e(lZ,Ibo),e(z_,Dbo),e(z_,eR),e(eR,Nbo),e(z_,jbo),e(H,Obo),e(H,X_),e(X_,iZ),e(iZ,Gbo),e(X_,qbo),e(X_,oR),e(oR,zbo),e(X_,Xbo),e(H,Qbo),e(H,Q_),e(Q_,dZ),e(dZ,Vbo),e(Q_,Wbo),e(Q_,rR),e(rR,Hbo),e(Q_,Ubo),e(H,Jbo),e(H,V_),e(V_,cZ),e(cZ,Kbo),e(V_,Ybo),e(V_,tR),e(tR,Zbo),e(V_,e2o),e(H,o2o),e(H,W_),e(W_,fZ),e(fZ,r2o),e(W_,t2o),e(W_,aR),e(aR,a2o),e(W_,n2o),e(H,s2o),e(H,H_),e(H_,mZ),e(mZ,l2o),e(H_,i2o),e(H_,nR),e(nR,d2o),e(H_,c2o),e(Ge,f2o),e(Ge,U_),e(U_,m2o),e(U_,hZ),e(hZ,h2o),e(U_,g2o),e(U_,gZ),e(gZ,u2o),e(Ge,p2o),e(Ge,uZ),e(uZ,_2o),e(Ge,v2o),h(aM,Ge,null),v(c,j5e,_),v(c,Ri,_),e(Ri,J_),e(J_,pZ),h(nM,pZ,null),e(Ri,b2o),e(Ri,_Z),e(_Z,T2o),v(c,O5e,_),v(c,Vo,_),h(sM,Vo,null),e(Vo,F2o),e(Vo,Pi),e(Pi,E2o),e(Pi,vZ),e(vZ,C2o),e(Pi,M2o),e(Pi,bZ),e(bZ,y2o),e(Pi,w2o),e(Vo,A2o),e(Vo,lM),e(lM,L2o),e(lM,TZ),e(TZ,B2o),e(lM,x2o),e(Vo,k2o),e(Vo,Nr),h(iM,Nr,null),e(Nr,R2o),e(Nr,FZ),e(FZ,P2o),e(Nr,S2o),e(Nr,Si),e(Si,$2o),e(Si,EZ),e(EZ,I2o),e(Si,D2o),e(Si,CZ),e(CZ,N2o),e(Si,j2o),e(Nr,O2o),e(Nr,MZ),e(MZ,G2o),e(Nr,q2o),h(dM,Nr,null),e(Vo,z2o),e(Vo,qe),h(cM,qe,null),e(qe,X2o),e(qe,yZ),e(yZ,Q2o),e(qe,V2o),e(qe,Pa),e(Pa,W2o),e(Pa,wZ),e(wZ,H2o),e(Pa,U2o),e(Pa,AZ),e(AZ,J2o),e(Pa,K2o),e(Pa,LZ),e(LZ,Y2o),e(Pa,Z2o),e(qe,eTo),e(qe,jt),e(jt,K_),e(K_,BZ),e(BZ,oTo),e(K_,rTo),e(K_,sR),e(sR,tTo),e(K_,aTo),e(jt,nTo),e(jt,Y_),e(Y_,xZ),e(xZ,sTo),e(Y_,lTo),e(Y_,lR),e(lR,iTo),e(Y_,dTo),e(jt,cTo),e(jt,Z_),e(Z_,kZ),e(kZ,fTo),e(Z_,mTo),e(Z_,iR),e(iR,hTo),e(Z_,gTo),e(jt,uTo),e(jt,e1),e(e1,RZ),e(RZ,pTo),e(e1,_To),e(e1,dR),e(dR,vTo),e(e1,bTo),e(jt,TTo),e(jt,o1),e(o1,PZ),e(PZ,FTo),e(o1,ETo),e(o1,cR),e(cR,CTo),e(o1,MTo),e(qe,yTo),e(qe,r1),e(r1,wTo),e(r1,SZ),e(SZ,ATo),e(r1,LTo),e(r1,$Z),e($Z,BTo),e(qe,xTo),e(qe,IZ),e(IZ,kTo),e(qe,RTo),h(fM,qe,null),v(c,G5e,_),v(c,$i,_),e($i,t1),e(t1,DZ),h(mM,DZ,null),e($i,PTo),e($i,NZ),e(NZ,STo),v(c,q5e,_),v(c,Wo,_),h(hM,Wo,null),e(Wo,$To),e(Wo,Ii),e(Ii,ITo),e(Ii,jZ),e(jZ,DTo),e(Ii,NTo),e(Ii,OZ),e(OZ,jTo),e(Ii,OTo),e(Wo,GTo),e(Wo,gM),e(gM,qTo),e(gM,GZ),e(GZ,zTo),e(gM,XTo),e(Wo,QTo),e(Wo,jr),h(uM,jr,null),e(jr,VTo),e(jr,qZ),e(qZ,WTo),e(jr,HTo),e(jr,Di),e(Di,UTo),e(Di,zZ),e(zZ,JTo),e(Di,KTo),e(Di,XZ),e(XZ,YTo),e(Di,ZTo),e(jr,eFo),e(jr,QZ),e(QZ,oFo),e(jr,rFo),h(pM,jr,null),e(Wo,tFo),e(Wo,ze),h(_M,ze,null),e(ze,aFo),e(ze,VZ),e(VZ,nFo),e(ze,sFo),e(ze,Sa),e(Sa,lFo),e(Sa,WZ),e(WZ,iFo),e(Sa,dFo),e(Sa,HZ),e(HZ,cFo),e(Sa,fFo),e(Sa,UZ),e(UZ,mFo),e(Sa,hFo),e(ze,gFo),e(ze,X),e(X,a1),e(a1,JZ),e(JZ,uFo),e(a1,pFo),e(a1,fR),e(fR,_Fo),e(a1,vFo),e(X,bFo),e(X,n1),e(n1,KZ),e(KZ,TFo),e(n1,FFo),e(n1,mR),e(mR,EFo),e(n1,CFo),e(X,MFo),e(X,s1),e(s1,YZ),e(YZ,yFo),e(s1,wFo),e(s1,hR),e(hR,AFo),e(s1,LFo),e(X,BFo),e(X,l1),e(l1,ZZ),e(ZZ,xFo),e(l1,kFo),e(l1,gR),e(gR,RFo),e(l1,PFo),e(X,SFo),e(X,i1),e(i1,eee),e(eee,$Fo),e(i1,IFo),e(i1,uR),e(uR,DFo),e(i1,NFo),e(X,jFo),e(X,d1),e(d1,oee),e(oee,OFo),e(d1,GFo),e(d1,pR),e(pR,qFo),e(d1,zFo),e(X,XFo),e(X,c1),e(c1,ree),e(ree,QFo),e(c1,VFo),e(c1,_R),e(_R,WFo),e(c1,HFo),e(X,UFo),e(X,f1),e(f1,tee),e(tee,JFo),e(f1,KFo),e(f1,vR),e(vR,YFo),e(f1,ZFo),e(X,eEo),e(X,m1),e(m1,aee),e(aee,oEo),e(m1,rEo),e(m1,bR),e(bR,tEo),e(m1,aEo),e(X,nEo),e(X,h1),e(h1,nee),e(nee,sEo),e(h1,lEo),e(h1,TR),e(TR,iEo),e(h1,dEo),e(X,cEo),e(X,g1),e(g1,see),e(see,fEo),e(g1,mEo),e(g1,FR),e(FR,hEo),e(g1,gEo),e(X,uEo),e(X,u1),e(u1,lee),e(lee,pEo),e(u1,_Eo),e(u1,ER),e(ER,vEo),e(u1,bEo),e(X,TEo),e(X,p1),e(p1,iee),e(iee,FEo),e(p1,EEo),e(p1,CR),e(CR,CEo),e(p1,MEo),e(X,yEo),e(X,_1),e(_1,dee),e(dee,wEo),e(_1,AEo),e(_1,MR),e(MR,LEo),e(_1,BEo),e(X,xEo),e(X,v1),e(v1,cee),e(cee,kEo),e(v1,REo),e(v1,yR),e(yR,PEo),e(v1,SEo),e(X,$Eo),e(X,b1),e(b1,fee),e(fee,IEo),e(b1,DEo),e(b1,wR),e(wR,NEo),e(b1,jEo),e(X,OEo),e(X,T1),e(T1,mee),e(mee,GEo),e(T1,qEo),e(T1,AR),e(AR,zEo),e(T1,XEo),e(X,QEo),e(X,F1),e(F1,hee),e(hee,VEo),e(F1,WEo),e(F1,LR),e(LR,HEo),e(F1,UEo),e(X,JEo),e(X,E1),e(E1,gee),e(gee,KEo),e(E1,YEo),e(E1,BR),e(BR,ZEo),e(E1,eCo),e(X,oCo),e(X,C1),e(C1,uee),e(uee,rCo),e(C1,tCo),e(C1,xR),e(xR,aCo),e(C1,nCo),e(X,sCo),e(X,M1),e(M1,pee),e(pee,lCo),e(M1,iCo),e(M1,kR),e(kR,dCo),e(M1,cCo),e(X,fCo),e(X,y1),e(y1,_ee),e(_ee,mCo),e(y1,hCo),e(y1,RR),e(RR,gCo),e(y1,uCo),e(X,pCo),e(X,w1),e(w1,vee),e(vee,_Co),e(w1,vCo),e(w1,PR),e(PR,bCo),e(w1,TCo),e(X,FCo),e(X,A1),e(A1,bee),e(bee,ECo),e(A1,CCo),e(A1,SR),e(SR,MCo),e(A1,yCo),e(X,wCo),e(X,L1),e(L1,Tee),e(Tee,ACo),e(L1,LCo),e(L1,$R),e($R,BCo),e(L1,xCo),e(X,kCo),e(X,B1),e(B1,Fee),e(Fee,RCo),e(B1,PCo),e(B1,IR),e(IR,SCo),e(B1,$Co),e(X,ICo),e(X,x1),e(x1,Eee),e(Eee,DCo),e(x1,NCo),e(x1,DR),e(DR,jCo),e(x1,OCo),e(X,GCo),e(X,k1),e(k1,Cee),e(Cee,qCo),e(k1,zCo),e(k1,NR),e(NR,XCo),e(k1,QCo),e(X,VCo),e(X,R1),e(R1,Mee),e(Mee,WCo),e(R1,HCo),e(R1,jR),e(jR,UCo),e(R1,JCo),e(ze,KCo),e(ze,P1),e(P1,YCo),e(P1,yee),e(yee,ZCo),e(P1,e3o),e(P1,wee),e(wee,o3o),e(ze,r3o),e(ze,Aee),e(Aee,t3o),e(ze,a3o),h(vM,ze,null),v(c,z5e,_),v(c,Ni,_),e(Ni,S1),e(S1,Lee),h(bM,Lee,null),e(Ni,n3o),e(Ni,Bee),e(Bee,s3o),v(c,X5e,_),v(c,Ho,_),h(TM,Ho,null),e(Ho,l3o),e(Ho,ji),e(ji,i3o),e(ji,xee),e(xee,d3o),e(ji,c3o),e(ji,kee),e(kee,f3o),e(ji,m3o),e(Ho,h3o),e(Ho,FM),e(FM,g3o),e(FM,Ree),e(Ree,u3o),e(FM,p3o),e(Ho,_3o),e(Ho,Or),h(EM,Or,null),e(Or,v3o),e(Or,Pee),e(Pee,b3o),e(Or,T3o),e(Or,Oi),e(Oi,F3o),e(Oi,See),e(See,E3o),e(Oi,C3o),e(Oi,$ee),e($ee,M3o),e(Oi,y3o),e(Or,w3o),e(Or,Iee),e(Iee,A3o),e(Or,L3o),h(CM,Or,null),e(Ho,B3o),e(Ho,Xe),h(MM,Xe,null),e(Xe,x3o),e(Xe,Dee),e(Dee,k3o),e(Xe,R3o),e(Xe,$a),e($a,P3o),e($a,Nee),e(Nee,S3o),e($a,$3o),e($a,jee),e(jee,I3o),e($a,D3o),e($a,Oee),e(Oee,N3o),e($a,j3o),e(Xe,O3o),e(Xe,S),e(S,$1),e($1,Gee),e(Gee,G3o),e($1,q3o),e($1,OR),e(OR,z3o),e($1,X3o),e(S,Q3o),e(S,I1),e(I1,qee),e(qee,V3o),e(I1,W3o),e(I1,GR),e(GR,H3o),e(I1,U3o),e(S,J3o),e(S,D1),e(D1,zee),e(zee,K3o),e(D1,Y3o),e(D1,qR),e(qR,Z3o),e(D1,eMo),e(S,oMo),e(S,N1),e(N1,Xee),e(Xee,rMo),e(N1,tMo),e(N1,zR),e(zR,aMo),e(N1,nMo),e(S,sMo),e(S,j1),e(j1,Qee),e(Qee,lMo),e(j1,iMo),e(j1,XR),e(XR,dMo),e(j1,cMo),e(S,fMo),e(S,O1),e(O1,Vee),e(Vee,mMo),e(O1,hMo),e(O1,QR),e(QR,gMo),e(O1,uMo),e(S,pMo),e(S,G1),e(G1,Wee),e(Wee,_Mo),e(G1,vMo),e(G1,VR),e(VR,bMo),e(G1,TMo),e(S,FMo),e(S,q1),e(q1,Hee),e(Hee,EMo),e(q1,CMo),e(q1,WR),e(WR,MMo),e(q1,yMo),e(S,wMo),e(S,z1),e(z1,Uee),e(Uee,AMo),e(z1,LMo),e(z1,HR),e(HR,BMo),e(z1,xMo),e(S,kMo),e(S,X1),e(X1,Jee),e(Jee,RMo),e(X1,PMo),e(X1,UR),e(UR,SMo),e(X1,$Mo),e(S,IMo),e(S,Q1),e(Q1,Kee),e(Kee,DMo),e(Q1,NMo),e(Q1,JR),e(JR,jMo),e(Q1,OMo),e(S,GMo),e(S,V1),e(V1,Yee),e(Yee,qMo),e(V1,zMo),e(V1,KR),e(KR,XMo),e(V1,QMo),e(S,VMo),e(S,W1),e(W1,Zee),e(Zee,WMo),e(W1,HMo),e(W1,YR),e(YR,UMo),e(W1,JMo),e(S,KMo),e(S,H1),e(H1,eoe),e(eoe,YMo),e(H1,ZMo),e(H1,ZR),e(ZR,e5o),e(H1,o5o),e(S,r5o),e(S,U1),e(U1,ooe),e(ooe,t5o),e(U1,a5o),e(U1,eP),e(eP,n5o),e(U1,s5o),e(S,l5o),e(S,J1),e(J1,roe),e(roe,i5o),e(J1,d5o),e(J1,oP),e(oP,c5o),e(J1,f5o),e(S,m5o),e(S,K1),e(K1,toe),e(toe,h5o),e(K1,g5o),e(K1,rP),e(rP,u5o),e(K1,p5o),e(S,_5o),e(S,Y1),e(Y1,aoe),e(aoe,v5o),e(Y1,b5o),e(Y1,tP),e(tP,T5o),e(Y1,F5o),e(S,E5o),e(S,Z1),e(Z1,noe),e(noe,C5o),e(Z1,M5o),e(Z1,aP),e(aP,y5o),e(Z1,w5o),e(S,A5o),e(S,e4),e(e4,soe),e(soe,L5o),e(e4,B5o),e(e4,nP),e(nP,x5o),e(e4,k5o),e(S,R5o),e(S,o4),e(o4,loe),e(loe,P5o),e(o4,S5o),e(o4,sP),e(sP,$5o),e(o4,I5o),e(S,D5o),e(S,r4),e(r4,ioe),e(ioe,N5o),e(r4,j5o),e(r4,lP),e(lP,O5o),e(r4,G5o),e(S,q5o),e(S,t4),e(t4,doe),e(doe,z5o),e(t4,X5o),e(t4,iP),e(iP,Q5o),e(t4,V5o),e(S,W5o),e(S,a4),e(a4,coe),e(coe,H5o),e(a4,U5o),e(a4,dP),e(dP,J5o),e(a4,K5o),e(S,Y5o),e(S,n4),e(n4,foe),e(foe,Z5o),e(n4,eyo),e(n4,cP),e(cP,oyo),e(n4,ryo),e(S,tyo),e(S,s4),e(s4,moe),e(moe,ayo),e(s4,nyo),e(s4,fP),e(fP,syo),e(s4,lyo),e(S,iyo),e(S,l4),e(l4,hoe),e(hoe,dyo),e(l4,cyo),e(l4,mP),e(mP,fyo),e(l4,myo),e(S,hyo),e(S,i4),e(i4,goe),e(goe,gyo),e(i4,uyo),e(i4,hP),e(hP,pyo),e(i4,_yo),e(S,vyo),e(S,d4),e(d4,uoe),e(uoe,byo),e(d4,Tyo),e(d4,gP),e(gP,Fyo),e(d4,Eyo),e(S,Cyo),e(S,c4),e(c4,poe),e(poe,Myo),e(c4,yyo),e(c4,uP),e(uP,wyo),e(c4,Ayo),e(S,Lyo),e(S,f4),e(f4,_oe),e(_oe,Byo),e(f4,xyo),e(f4,pP),e(pP,kyo),e(f4,Ryo),e(S,Pyo),e(S,m4),e(m4,voe),e(voe,Syo),e(m4,$yo),e(m4,_P),e(_P,Iyo),e(m4,Dyo),e(S,Nyo),e(S,h4),e(h4,boe),e(boe,jyo),e(h4,Oyo),e(h4,vP),e(vP,Gyo),e(h4,qyo),e(S,zyo),e(S,g4),e(g4,Toe),e(Toe,Xyo),e(g4,Qyo),e(g4,bP),e(bP,Vyo),e(g4,Wyo),e(S,Hyo),e(S,u4),e(u4,Foe),e(Foe,Uyo),e(u4,Jyo),e(u4,TP),e(TP,Kyo),e(u4,Yyo),e(Xe,Zyo),e(Xe,p4),e(p4,ewo),e(p4,Eoe),e(Eoe,owo),e(p4,rwo),e(p4,Coe),e(Coe,two),e(Xe,awo),e(Xe,Moe),e(Moe,nwo),e(Xe,swo),h(yM,Xe,null),v(c,Q5e,_),v(c,Gi,_),e(Gi,_4),e(_4,yoe),h(wM,yoe,null),e(Gi,lwo),e(Gi,woe),e(woe,iwo),v(c,V5e,_),v(c,Uo,_),h(AM,Uo,null),e(Uo,dwo),e(Uo,qi),e(qi,cwo),e(qi,Aoe),e(Aoe,fwo),e(qi,mwo),e(qi,Loe),e(Loe,hwo),e(qi,gwo),e(Uo,uwo),e(Uo,LM),e(LM,pwo),e(LM,Boe),e(Boe,_wo),e(LM,vwo),e(Uo,bwo),e(Uo,Gr),h(BM,Gr,null),e(Gr,Two),e(Gr,xoe),e(xoe,Fwo),e(Gr,Ewo),e(Gr,zi),e(zi,Cwo),e(zi,koe),e(koe,Mwo),e(zi,ywo),e(zi,Roe),e(Roe,wwo),e(zi,Awo),e(Gr,Lwo),e(Gr,Poe),e(Poe,Bwo),e(Gr,xwo),h(xM,Gr,null),e(Uo,kwo),e(Uo,Qe),h(kM,Qe,null),e(Qe,Rwo),e(Qe,Soe),e(Soe,Pwo),e(Qe,Swo),e(Qe,Ia),e(Ia,$wo),e(Ia,$oe),e($oe,Iwo),e(Ia,Dwo),e(Ia,Ioe),e(Ioe,Nwo),e(Ia,jwo),e(Ia,Doe),e(Doe,Owo),e(Ia,Gwo),e(Qe,qwo),e(Qe,Noe),e(Noe,v4),e(v4,joe),e(joe,zwo),e(v4,Xwo),e(v4,FP),e(FP,Qwo),e(v4,Vwo),e(Qe,Wwo),e(Qe,b4),e(b4,Hwo),e(b4,Ooe),e(Ooe,Uwo),e(b4,Jwo),e(b4,Goe),e(Goe,Kwo),e(Qe,Ywo),e(Qe,qoe),e(qoe,Zwo),e(Qe,e7o),h(RM,Qe,null),v(c,W5e,_),v(c,Xi,_),e(Xi,T4),e(T4,zoe),h(PM,zoe,null),e(Xi,o7o),e(Xi,Xoe),e(Xoe,r7o),v(c,H5e,_),v(c,Jo,_),h(SM,Jo,null),e(Jo,t7o),e(Jo,Qi),e(Qi,a7o),e(Qi,Qoe),e(Qoe,n7o),e(Qi,s7o),e(Qi,Voe),e(Voe,l7o),e(Qi,i7o),e(Jo,d7o),e(Jo,$M),e($M,c7o),e($M,Woe),e(Woe,f7o),e($M,m7o),e(Jo,h7o),e(Jo,qr),h(IM,qr,null),e(qr,g7o),e(qr,Hoe),e(Hoe,u7o),e(qr,p7o),e(qr,Vi),e(Vi,_7o),e(Vi,Uoe),e(Uoe,v7o),e(Vi,b7o),e(Vi,Joe),e(Joe,T7o),e(Vi,F7o),e(qr,E7o),e(qr,Koe),e(Koe,C7o),e(qr,M7o),h(DM,qr,null),e(Jo,y7o),e(Jo,Ve),h(NM,Ve,null),e(Ve,w7o),e(Ve,Yoe),e(Yoe,A7o),e(Ve,L7o),e(Ve,Da),e(Da,B7o),e(Da,Zoe),e(Zoe,x7o),e(Da,k7o),e(Da,ere),e(ere,R7o),e(Da,P7o),e(Da,ore),e(ore,S7o),e(Da,$7o),e(Ve,I7o),e(Ve,Ko),e(Ko,F4),e(F4,rre),e(rre,D7o),e(F4,N7o),e(F4,EP),e(EP,j7o),e(F4,O7o),e(Ko,G7o),e(Ko,ds),e(ds,tre),e(tre,q7o),e(ds,z7o),e(ds,CP),e(CP,X7o),e(ds,Q7o),e(ds,MP),e(MP,V7o),e(ds,W7o),e(Ko,H7o),e(Ko,E4),e(E4,are),e(are,U7o),e(E4,J7o),e(E4,yP),e(yP,K7o),e(E4,Y7o),e(Ko,Z7o),e(Ko,Gt),e(Gt,nre),e(nre,e0o),e(Gt,o0o),e(Gt,wP),e(wP,r0o),e(Gt,t0o),e(Gt,AP),e(AP,a0o),e(Gt,n0o),e(Gt,LP),e(LP,s0o),e(Gt,l0o),e(Ko,i0o),e(Ko,C4),e(C4,sre),e(sre,d0o),e(C4,c0o),e(C4,BP),e(BP,f0o),e(C4,m0o),e(Ko,h0o),e(Ko,M4),e(M4,lre),e(lre,g0o),e(M4,u0o),e(M4,xP),e(xP,p0o),e(M4,_0o),e(Ve,v0o),e(Ve,y4),e(y4,b0o),e(y4,ire),e(ire,T0o),e(y4,F0o),e(y4,dre),e(dre,E0o),e(Ve,C0o),e(Ve,cre),e(cre,M0o),e(Ve,y0o),h(jM,Ve,null),v(c,U5e,_),v(c,Wi,_),e(Wi,w4),e(w4,fre),h(OM,fre,null),e(Wi,w0o),e(Wi,mre),e(mre,A0o),v(c,J5e,_),v(c,Yo,_),h(GM,Yo,null),e(Yo,L0o),e(Yo,Hi),e(Hi,B0o),e(Hi,hre),e(hre,x0o),e(Hi,k0o),e(Hi,gre),e(gre,R0o),e(Hi,P0o),e(Yo,S0o),e(Yo,qM),e(qM,$0o),e(qM,ure),e(ure,I0o),e(qM,D0o),e(Yo,N0o),e(Yo,zr),h(zM,zr,null),e(zr,j0o),e(zr,pre),e(pre,O0o),e(zr,G0o),e(zr,Ui),e(Ui,q0o),e(Ui,_re),e(_re,z0o),e(Ui,X0o),e(Ui,vre),e(vre,Q0o),e(Ui,V0o),e(zr,W0o),e(zr,bre),e(bre,H0o),e(zr,U0o),h(XM,zr,null),e(Yo,J0o),e(Yo,We),h(QM,We,null),e(We,K0o),e(We,Tre),e(Tre,Y0o),e(We,Z0o),e(We,Na),e(Na,eAo),e(Na,Fre),e(Fre,oAo),e(Na,rAo),e(Na,Ere),e(Ere,tAo),e(Na,aAo),e(Na,Cre),e(Cre,nAo),e(Na,sAo),e(We,lAo),e(We,Mre),e(Mre,A4),e(A4,yre),e(yre,iAo),e(A4,dAo),e(A4,kP),e(kP,cAo),e(A4,fAo),e(We,mAo),e(We,L4),e(L4,hAo),e(L4,wre),e(wre,gAo),e(L4,uAo),e(L4,Are),e(Are,pAo),e(We,_Ao),e(We,Lre),e(Lre,vAo),e(We,bAo),h(VM,We,null),v(c,K5e,_),v(c,Ji,_),e(Ji,B4),e(B4,Bre),h(WM,Bre,null),e(Ji,TAo),e(Ji,xre),e(xre,FAo),v(c,Y5e,_),v(c,Zo,_),h(HM,Zo,null),e(Zo,EAo),e(Zo,Ki),e(Ki,CAo),e(Ki,kre),e(kre,MAo),e(Ki,yAo),e(Ki,Rre),e(Rre,wAo),e(Ki,AAo),e(Zo,LAo),e(Zo,UM),e(UM,BAo),e(UM,Pre),e(Pre,xAo),e(UM,kAo),e(Zo,RAo),e(Zo,Xr),h(JM,Xr,null),e(Xr,PAo),e(Xr,Sre),e(Sre,SAo),e(Xr,$Ao),e(Xr,Yi),e(Yi,IAo),e(Yi,$re),e($re,DAo),e(Yi,NAo),e(Yi,Ire),e(Ire,jAo),e(Yi,OAo),e(Xr,GAo),e(Xr,Dre),e(Dre,qAo),e(Xr,zAo),h(KM,Xr,null),e(Zo,XAo),e(Zo,He),h(YM,He,null),e(He,QAo),e(He,Nre),e(Nre,VAo),e(He,WAo),e(He,ja),e(ja,HAo),e(ja,jre),e(jre,UAo),e(ja,JAo),e(ja,Ore),e(Ore,KAo),e(ja,YAo),e(ja,Gre),e(Gre,ZAo),e(ja,e6o),e(He,o6o),e(He,er),e(er,x4),e(x4,qre),e(qre,r6o),e(x4,t6o),e(x4,RP),e(RP,a6o),e(x4,n6o),e(er,s6o),e(er,k4),e(k4,zre),e(zre,l6o),e(k4,i6o),e(k4,PP),e(PP,d6o),e(k4,c6o),e(er,f6o),e(er,R4),e(R4,Xre),e(Xre,m6o),e(R4,h6o),e(R4,SP),e(SP,g6o),e(R4,u6o),e(er,p6o),e(er,P4),e(P4,Qre),e(Qre,_6o),e(P4,v6o),e(P4,$P),e($P,b6o),e(P4,T6o),e(er,F6o),e(er,S4),e(S4,Vre),e(Vre,E6o),e(S4,C6o),e(S4,IP),e(IP,M6o),e(S4,y6o),e(er,w6o),e(er,$4),e($4,Wre),e(Wre,A6o),e($4,L6o),e($4,DP),e(DP,B6o),e($4,x6o),e(He,k6o),e(He,I4),e(I4,R6o),e(I4,Hre),e(Hre,P6o),e(I4,S6o),e(I4,Ure),e(Ure,$6o),e(He,I6o),e(He,Jre),e(Jre,D6o),e(He,N6o),h(ZM,He,null),v(c,Z5e,_),v(c,Zi,_),e(Zi,D4),e(D4,Kre),h(e5,Kre,null),e(Zi,j6o),e(Zi,Yre),e(Yre,O6o),v(c,eye,_),v(c,or,_),h(o5,or,null),e(or,G6o),e(or,ed),e(ed,q6o),e(ed,Zre),e(Zre,z6o),e(ed,X6o),e(ed,ete),e(ete,Q6o),e(ed,V6o),e(or,W6o),e(or,r5),e(r5,H6o),e(r5,ote),e(ote,U6o),e(r5,J6o),e(or,K6o),e(or,Qr),h(t5,Qr,null),e(Qr,Y6o),e(Qr,rte),e(rte,Z6o),e(Qr,eLo),e(Qr,od),e(od,oLo),e(od,tte),e(tte,rLo),e(od,tLo),e(od,ate),e(ate,aLo),e(od,nLo),e(Qr,sLo),e(Qr,nte),e(nte,lLo),e(Qr,iLo),h(a5,Qr,null),e(or,dLo),e(or,Ue),h(n5,Ue,null),e(Ue,cLo),e(Ue,ste),e(ste,fLo),e(Ue,mLo),e(Ue,Oa),e(Oa,hLo),e(Oa,lte),e(lte,gLo),e(Oa,uLo),e(Oa,ite),e(ite,pLo),e(Oa,_Lo),e(Oa,dte),e(dte,vLo),e(Oa,bLo),e(Ue,TLo),e(Ue,rr),e(rr,N4),e(N4,cte),e(cte,FLo),e(N4,ELo),e(N4,NP),e(NP,CLo),e(N4,MLo),e(rr,yLo),e(rr,j4),e(j4,fte),e(fte,wLo),e(j4,ALo),e(j4,jP),e(jP,LLo),e(j4,BLo),e(rr,xLo),e(rr,O4),e(O4,mte),e(mte,kLo),e(O4,RLo),e(O4,OP),e(OP,PLo),e(O4,SLo),e(rr,$Lo),e(rr,G4),e(G4,hte),e(hte,ILo),e(G4,DLo),e(G4,GP),e(GP,NLo),e(G4,jLo),e(rr,OLo),e(rr,q4),e(q4,gte),e(gte,GLo),e(q4,qLo),e(q4,qP),e(qP,zLo),e(q4,XLo),e(rr,QLo),e(rr,z4),e(z4,ute),e(ute,VLo),e(z4,WLo),e(z4,zP),e(zP,HLo),e(z4,ULo),e(Ue,JLo),e(Ue,X4),e(X4,KLo),e(X4,pte),e(pte,YLo),e(X4,ZLo),e(X4,_te),e(_te,e8o),e(Ue,o8o),e(Ue,vte),e(vte,r8o),e(Ue,t8o),h(s5,Ue,null),v(c,oye,_),v(c,rd,_),e(rd,Q4),e(Q4,bte),h(l5,bte,null),e(rd,a8o),e(rd,Tte),e(Tte,n8o),v(c,rye,_),v(c,tr,_),h(i5,tr,null),e(tr,s8o),e(tr,td),e(td,l8o),e(td,Fte),e(Fte,i8o),e(td,d8o),e(td,Ete),e(Ete,c8o),e(td,f8o),e(tr,m8o),e(tr,d5),e(d5,h8o),e(d5,Cte),e(Cte,g8o),e(d5,u8o),e(tr,p8o),e(tr,Vr),h(c5,Vr,null),e(Vr,_8o),e(Vr,Mte),e(Mte,v8o),e(Vr,b8o),e(Vr,ad),e(ad,T8o),e(ad,yte),e(yte,F8o),e(ad,E8o),e(ad,wte),e(wte,C8o),e(ad,M8o),e(Vr,y8o),e(Vr,Ate),e(Ate,w8o),e(Vr,A8o),h(f5,Vr,null),e(tr,L8o),e(tr,Je),h(m5,Je,null),e(Je,B8o),e(Je,Lte),e(Lte,x8o),e(Je,k8o),e(Je,Ga),e(Ga,R8o),e(Ga,Bte),e(Bte,P8o),e(Ga,S8o),e(Ga,xte),e(xte,$8o),e(Ga,I8o),e(Ga,kte),e(kte,D8o),e(Ga,N8o),e(Je,j8o),e(Je,h5),e(h5,V4),e(V4,Rte),e(Rte,O8o),e(V4,G8o),e(V4,XP),e(XP,q8o),e(V4,z8o),e(h5,X8o),e(h5,W4),e(W4,Pte),e(Pte,Q8o),e(W4,V8o),e(W4,QP),e(QP,W8o),e(W4,H8o),e(Je,U8o),e(Je,H4),e(H4,J8o),e(H4,Ste),e(Ste,K8o),e(H4,Y8o),e(H4,$te),e($te,Z8o),e(Je,eBo),e(Je,Ite),e(Ite,oBo),e(Je,rBo),h(g5,Je,null),v(c,tye,_),v(c,nd,_),e(nd,U4),e(U4,Dte),h(u5,Dte,null),e(nd,tBo),e(nd,Nte),e(Nte,aBo),v(c,aye,_),v(c,ar,_),h(p5,ar,null),e(ar,nBo),e(ar,sd),e(sd,sBo),e(sd,jte),e(jte,lBo),e(sd,iBo),e(sd,Ote),e(Ote,dBo),e(sd,cBo),e(ar,fBo),e(ar,_5),e(_5,mBo),e(_5,Gte),e(Gte,hBo),e(_5,gBo),e(ar,uBo),e(ar,Wr),h(v5,Wr,null),e(Wr,pBo),e(Wr,qte),e(qte,_Bo),e(Wr,vBo),e(Wr,ld),e(ld,bBo),e(ld,zte),e(zte,TBo),e(ld,FBo),e(ld,Xte),e(Xte,EBo),e(ld,CBo),e(Wr,MBo),e(Wr,Qte),e(Qte,yBo),e(Wr,wBo),h(b5,Wr,null),e(ar,ABo),e(ar,Ke),h(T5,Ke,null),e(Ke,LBo),e(Ke,Vte),e(Vte,BBo),e(Ke,xBo),e(Ke,qa),e(qa,kBo),e(qa,Wte),e(Wte,RBo),e(qa,PBo),e(qa,Hte),e(Hte,SBo),e(qa,$Bo),e(qa,Ute),e(Ute,IBo),e(qa,DBo),e(Ke,NBo),e(Ke,Jte),e(Jte,J4),e(J4,Kte),e(Kte,jBo),e(J4,OBo),e(J4,VP),e(VP,GBo),e(J4,qBo),e(Ke,zBo),e(Ke,K4),e(K4,XBo),e(K4,Yte),e(Yte,QBo),e(K4,VBo),e(K4,Zte),e(Zte,WBo),e(Ke,HBo),e(Ke,eae),e(eae,UBo),e(Ke,JBo),h(F5,Ke,null),v(c,nye,_),v(c,id,_),e(id,Y4),e(Y4,oae),h(E5,oae,null),e(id,KBo),e(id,rae),e(rae,YBo),v(c,sye,_),v(c,nr,_),h(C5,nr,null),e(nr,ZBo),e(nr,dd),e(dd,e9o),e(dd,tae),e(tae,o9o),e(dd,r9o),e(dd,aae),e(aae,t9o),e(dd,a9o),e(nr,n9o),e(nr,M5),e(M5,s9o),e(M5,nae),e(nae,l9o),e(M5,i9o),e(nr,d9o),e(nr,Hr),h(y5,Hr,null),e(Hr,c9o),e(Hr,sae),e(sae,f9o),e(Hr,m9o),e(Hr,cd),e(cd,h9o),e(cd,lae),e(lae,g9o),e(cd,u9o),e(cd,iae),e(iae,p9o),e(cd,_9o),e(Hr,v9o),e(Hr,dae),e(dae,b9o),e(Hr,T9o),h(w5,Hr,null),e(nr,F9o),e(nr,Ye),h(A5,Ye,null),e(Ye,E9o),e(Ye,cae),e(cae,C9o),e(Ye,M9o),e(Ye,za),e(za,y9o),e(za,fae),e(fae,w9o),e(za,A9o),e(za,mae),e(mae,L9o),e(za,B9o),e(za,hae),e(hae,x9o),e(za,k9o),e(Ye,R9o),e(Ye,gae),e(gae,Z4),e(Z4,uae),e(uae,P9o),e(Z4,S9o),e(Z4,WP),e(WP,$9o),e(Z4,I9o),e(Ye,D9o),e(Ye,ev),e(ev,N9o),e(ev,pae),e(pae,j9o),e(ev,O9o),e(ev,_ae),e(_ae,G9o),e(Ye,q9o),e(Ye,vae),e(vae,z9o),e(Ye,X9o),h(L5,Ye,null),v(c,lye,_),v(c,fd,_),e(fd,ov),e(ov,bae),h(B5,bae,null),e(fd,Q9o),e(fd,Tae),e(Tae,V9o),v(c,iye,_),v(c,sr,_),h(x5,sr,null),e(sr,W9o),e(sr,md),e(md,H9o),e(md,Fae),e(Fae,U9o),e(md,J9o),e(md,Eae),e(Eae,K9o),e(md,Y9o),e(sr,Z9o),e(sr,k5),e(k5,exo),e(k5,Cae),e(Cae,oxo),e(k5,rxo),e(sr,txo),e(sr,Ur),h(R5,Ur,null),e(Ur,axo),e(Ur,Mae),e(Mae,nxo),e(Ur,sxo),e(Ur,hd),e(hd,lxo),e(hd,yae),e(yae,ixo),e(hd,dxo),e(hd,wae),e(wae,cxo),e(hd,fxo),e(Ur,mxo),e(Ur,Aae),e(Aae,hxo),e(Ur,gxo),h(P5,Ur,null),e(sr,uxo),e(sr,lo),h(S5,lo,null),e(lo,pxo),e(lo,Lae),e(Lae,_xo),e(lo,vxo),e(lo,Xa),e(Xa,bxo),e(Xa,Bae),e(Bae,Txo),e(Xa,Fxo),e(Xa,xae),e(xae,Exo),e(Xa,Cxo),e(Xa,kae),e(kae,Mxo),e(Xa,yxo),e(lo,wxo),e(lo,B),e(B,rv),e(rv,Rae),e(Rae,Axo),e(rv,Lxo),e(rv,HP),e(HP,Bxo),e(rv,xxo),e(B,kxo),e(B,tv),e(tv,Pae),e(Pae,Rxo),e(tv,Pxo),e(tv,UP),e(UP,Sxo),e(tv,$xo),e(B,Ixo),e(B,av),e(av,Sae),e(Sae,Dxo),e(av,Nxo),e(av,JP),e(JP,jxo),e(av,Oxo),e(B,Gxo),e(B,nv),e(nv,$ae),e($ae,qxo),e(nv,zxo),e(nv,KP),e(KP,Xxo),e(nv,Qxo),e(B,Vxo),e(B,sv),e(sv,Iae),e(Iae,Wxo),e(sv,Hxo),e(sv,YP),e(YP,Uxo),e(sv,Jxo),e(B,Kxo),e(B,lv),e(lv,Dae),e(Dae,Yxo),e(lv,Zxo),e(lv,ZP),e(ZP,eko),e(lv,oko),e(B,rko),e(B,iv),e(iv,Nae),e(Nae,tko),e(iv,ako),e(iv,eS),e(eS,nko),e(iv,sko),e(B,lko),e(B,dv),e(dv,jae),e(jae,iko),e(dv,dko),e(dv,oS),e(oS,cko),e(dv,fko),e(B,mko),e(B,cv),e(cv,Oae),e(Oae,hko),e(cv,gko),e(cv,rS),e(rS,uko),e(cv,pko),e(B,_ko),e(B,fv),e(fv,Gae),e(Gae,vko),e(fv,bko),e(fv,tS),e(tS,Tko),e(fv,Fko),e(B,Eko),e(B,mv),e(mv,qae),e(qae,Cko),e(mv,Mko),e(mv,aS),e(aS,yko),e(mv,wko),e(B,Ako),e(B,hv),e(hv,zae),e(zae,Lko),e(hv,Bko),e(hv,nS),e(nS,xko),e(hv,kko),e(B,Rko),e(B,gv),e(gv,Xae),e(Xae,Pko),e(gv,Sko),e(gv,sS),e(sS,$ko),e(gv,Iko),e(B,Dko),e(B,uv),e(uv,Qae),e(Qae,Nko),e(uv,jko),e(uv,lS),e(lS,Oko),e(uv,Gko),e(B,qko),e(B,cs),e(cs,Vae),e(Vae,zko),e(cs,Xko),e(cs,iS),e(iS,Qko),e(cs,Vko),e(cs,dS),e(dS,Wko),e(cs,Hko),e(B,Uko),e(B,pv),e(pv,Wae),e(Wae,Jko),e(pv,Kko),e(pv,cS),e(cS,Yko),e(pv,Zko),e(B,eRo),e(B,_v),e(_v,Hae),e(Hae,oRo),e(_v,rRo),e(_v,fS),e(fS,tRo),e(_v,aRo),e(B,nRo),e(B,vv),e(vv,Uae),e(Uae,sRo),e(vv,lRo),e(vv,mS),e(mS,iRo),e(vv,dRo),e(B,cRo),e(B,bv),e(bv,Jae),e(Jae,fRo),e(bv,mRo),e(bv,hS),e(hS,hRo),e(bv,gRo),e(B,uRo),e(B,Tv),e(Tv,Kae),e(Kae,pRo),e(Tv,_Ro),e(Tv,gS),e(gS,vRo),e(Tv,bRo),e(B,TRo),e(B,Fv),e(Fv,Yae),e(Yae,FRo),e(Fv,ERo),e(Fv,uS),e(uS,CRo),e(Fv,MRo),e(B,yRo),e(B,Ev),e(Ev,Zae),e(Zae,wRo),e(Ev,ARo),e(Ev,pS),e(pS,LRo),e(Ev,BRo),e(B,xRo),e(B,Cv),e(Cv,ene),e(ene,kRo),e(Cv,RRo),e(Cv,_S),e(_S,PRo),e(Cv,SRo),e(B,$Ro),e(B,Mv),e(Mv,one),e(one,IRo),e(Mv,DRo),e(Mv,vS),e(vS,NRo),e(Mv,jRo),e(B,ORo),e(B,yv),e(yv,rne),e(rne,GRo),e(yv,qRo),e(yv,bS),e(bS,zRo),e(yv,XRo),e(B,QRo),e(B,wv),e(wv,tne),e(tne,VRo),e(wv,WRo),e(wv,TS),e(TS,HRo),e(wv,URo),e(B,JRo),e(B,Av),e(Av,ane),e(ane,KRo),e(Av,YRo),e(Av,FS),e(FS,ZRo),e(Av,ePo),e(B,oPo),e(B,Lv),e(Lv,nne),e(nne,rPo),e(Lv,tPo),e(Lv,ES),e(ES,aPo),e(Lv,nPo),e(B,sPo),e(B,Bv),e(Bv,sne),e(sne,lPo),e(Bv,iPo),e(Bv,CS),e(CS,dPo),e(Bv,cPo),e(B,fPo),e(B,xv),e(xv,lne),e(lne,mPo),e(xv,hPo),e(xv,MS),e(MS,gPo),e(xv,uPo),e(B,pPo),e(B,kv),e(kv,ine),e(ine,_Po),e(kv,vPo),e(kv,yS),e(yS,bPo),e(kv,TPo),e(B,FPo),e(B,Rv),e(Rv,dne),e(dne,EPo),e(Rv,CPo),e(Rv,wS),e(wS,MPo),e(Rv,yPo),e(B,wPo),e(B,Pv),e(Pv,cne),e(cne,APo),e(Pv,LPo),e(Pv,AS),e(AS,BPo),e(Pv,xPo),e(B,kPo),e(B,Sv),e(Sv,fne),e(fne,RPo),e(Sv,PPo),e(Sv,LS),e(LS,SPo),e(Sv,$Po),e(B,IPo),e(B,$v),e($v,mne),e(mne,DPo),e($v,NPo),e($v,BS),e(BS,jPo),e($v,OPo),e(B,GPo),e(B,Iv),e(Iv,hne),e(hne,qPo),e(Iv,zPo),e(Iv,xS),e(xS,XPo),e(Iv,QPo),e(B,VPo),e(B,Dv),e(Dv,gne),e(gne,WPo),e(Dv,HPo),e(Dv,kS),e(kS,UPo),e(Dv,JPo),e(B,KPo),e(B,Nv),e(Nv,une),e(une,YPo),e(Nv,ZPo),e(Nv,RS),e(RS,eSo),e(Nv,oSo),e(B,rSo),e(B,jv),e(jv,pne),e(pne,tSo),e(jv,aSo),e(jv,PS),e(PS,nSo),e(jv,sSo),e(lo,lSo),e(lo,_ne),e(_ne,iSo),e(lo,dSo),h($5,lo,null),v(c,dye,_),v(c,gd,_),e(gd,Ov),e(Ov,vne),h(I5,vne,null),e(gd,cSo),e(gd,bne),e(bne,fSo),v(c,cye,_),v(c,lr,_),h(D5,lr,null),e(lr,mSo),e(lr,ud),e(ud,hSo),e(ud,Tne),e(Tne,gSo),e(ud,uSo),e(ud,Fne),e(Fne,pSo),e(ud,_So),e(lr,vSo),e(lr,N5),e(N5,bSo),e(N5,Ene),e(Ene,TSo),e(N5,FSo),e(lr,ESo),e(lr,Jr),h(j5,Jr,null),e(Jr,CSo),e(Jr,Cne),e(Cne,MSo),e(Jr,ySo),e(Jr,pd),e(pd,wSo),e(pd,Mne),e(Mne,ASo),e(pd,LSo),e(pd,yne),e(yne,BSo),e(pd,xSo),e(Jr,kSo),e(Jr,wne),e(wne,RSo),e(Jr,PSo),h(O5,Jr,null),e(lr,SSo),e(lr,io),h(G5,io,null),e(io,$So),e(io,Ane),e(Ane,ISo),e(io,DSo),e(io,Qa),e(Qa,NSo),e(Qa,Lne),e(Lne,jSo),e(Qa,OSo),e(Qa,Bne),e(Bne,GSo),e(Qa,qSo),e(Qa,xne),e(xne,zSo),e(Qa,XSo),e(io,QSo),e(io,K),e(K,Gv),e(Gv,kne),e(kne,VSo),e(Gv,WSo),e(Gv,SS),e(SS,HSo),e(Gv,USo),e(K,JSo),e(K,qv),e(qv,Rne),e(Rne,KSo),e(qv,YSo),e(qv,$S),e($S,ZSo),e(qv,e$o),e(K,o$o),e(K,zv),e(zv,Pne),e(Pne,r$o),e(zv,t$o),e(zv,IS),e(IS,a$o),e(zv,n$o),e(K,s$o),e(K,Xv),e(Xv,Sne),e(Sne,l$o),e(Xv,i$o),e(Xv,DS),e(DS,d$o),e(Xv,c$o),e(K,f$o),e(K,Qv),e(Qv,$ne),e($ne,m$o),e(Qv,h$o),e(Qv,NS),e(NS,g$o),e(Qv,u$o),e(K,p$o),e(K,Vv),e(Vv,Ine),e(Ine,_$o),e(Vv,v$o),e(Vv,jS),e(jS,b$o),e(Vv,T$o),e(K,F$o),e(K,Wv),e(Wv,Dne),e(Dne,E$o),e(Wv,C$o),e(Wv,OS),e(OS,M$o),e(Wv,y$o),e(K,w$o),e(K,Hv),e(Hv,Nne),e(Nne,A$o),e(Hv,L$o),e(Hv,GS),e(GS,B$o),e(Hv,x$o),e(K,k$o),e(K,Uv),e(Uv,jne),e(jne,R$o),e(Uv,P$o),e(Uv,qS),e(qS,S$o),e(Uv,$$o),e(K,I$o),e(K,Jv),e(Jv,One),e(One,D$o),e(Jv,N$o),e(Jv,zS),e(zS,j$o),e(Jv,O$o),e(K,G$o),e(K,Kv),e(Kv,Gne),e(Gne,q$o),e(Kv,z$o),e(Kv,XS),e(XS,X$o),e(Kv,Q$o),e(K,V$o),e(K,Yv),e(Yv,qne),e(qne,W$o),e(Yv,H$o),e(Yv,QS),e(QS,U$o),e(Yv,J$o),e(K,K$o),e(K,Zv),e(Zv,zne),e(zne,Y$o),e(Zv,Z$o),e(Zv,VS),e(VS,eIo),e(Zv,oIo),e(K,rIo),e(K,eb),e(eb,Xne),e(Xne,tIo),e(eb,aIo),e(eb,WS),e(WS,nIo),e(eb,sIo),e(K,lIo),e(K,ob),e(ob,Qne),e(Qne,iIo),e(ob,dIo),e(ob,HS),e(HS,cIo),e(ob,fIo),e(K,mIo),e(K,rb),e(rb,Vne),e(Vne,hIo),e(rb,gIo),e(rb,US),e(US,uIo),e(rb,pIo),e(K,_Io),e(K,tb),e(tb,Wne),e(Wne,vIo),e(tb,bIo),e(tb,JS),e(JS,TIo),e(tb,FIo),e(K,EIo),e(K,ab),e(ab,Hne),e(Hne,CIo),e(ab,MIo),e(ab,KS),e(KS,yIo),e(ab,wIo),e(K,AIo),e(K,nb),e(nb,Une),e(Une,LIo),e(nb,BIo),e(nb,YS),e(YS,xIo),e(nb,kIo),e(K,RIo),e(K,sb),e(sb,Jne),e(Jne,PIo),e(sb,SIo),e(sb,ZS),e(ZS,$Io),e(sb,IIo),e(K,DIo),e(K,lb),e(lb,Kne),e(Kne,NIo),e(lb,jIo),e(lb,e$),e(e$,OIo),e(lb,GIo),e(K,qIo),e(K,ib),e(ib,Yne),e(Yne,zIo),e(ib,XIo),e(ib,o$),e(o$,QIo),e(ib,VIo),e(io,WIo),e(io,Zne),e(Zne,HIo),e(io,UIo),h(q5,io,null),v(c,fye,_),v(c,_d,_),e(_d,db),e(db,ese),h(z5,ese,null),e(_d,JIo),e(_d,ose),e(ose,KIo),v(c,mye,_),v(c,ir,_),h(X5,ir,null),e(ir,YIo),e(ir,vd),e(vd,ZIo),e(vd,rse),e(rse,eDo),e(vd,oDo),e(vd,tse),e(tse,rDo),e(vd,tDo),e(ir,aDo),e(ir,Q5),e(Q5,nDo),e(Q5,ase),e(ase,sDo),e(Q5,lDo),e(ir,iDo),e(ir,Kr),h(V5,Kr,null),e(Kr,dDo),e(Kr,nse),e(nse,cDo),e(Kr,fDo),e(Kr,bd),e(bd,mDo),e(bd,sse),e(sse,hDo),e(bd,gDo),e(bd,lse),e(lse,uDo),e(bd,pDo),e(Kr,_Do),e(Kr,ise),e(ise,vDo),e(Kr,bDo),h(W5,Kr,null),e(ir,TDo),e(ir,co),h(H5,co,null),e(co,FDo),e(co,dse),e(dse,EDo),e(co,CDo),e(co,Va),e(Va,MDo),e(Va,cse),e(cse,yDo),e(Va,wDo),e(Va,fse),e(fse,ADo),e(Va,LDo),e(Va,mse),e(mse,BDo),e(Va,xDo),e(co,kDo),e(co,ve),e(ve,cb),e(cb,hse),e(hse,RDo),e(cb,PDo),e(cb,r$),e(r$,SDo),e(cb,$Do),e(ve,IDo),e(ve,fb),e(fb,gse),e(gse,DDo),e(fb,NDo),e(fb,t$),e(t$,jDo),e(fb,ODo),e(ve,GDo),e(ve,mb),e(mb,use),e(use,qDo),e(mb,zDo),e(mb,a$),e(a$,XDo),e(mb,QDo),e(ve,VDo),e(ve,hb),e(hb,pse),e(pse,WDo),e(hb,HDo),e(hb,n$),e(n$,UDo),e(hb,JDo),e(ve,KDo),e(ve,gb),e(gb,_se),e(_se,YDo),e(gb,ZDo),e(gb,s$),e(s$,eNo),e(gb,oNo),e(ve,rNo),e(ve,ub),e(ub,vse),e(vse,tNo),e(ub,aNo),e(ub,l$),e(l$,nNo),e(ub,sNo),e(ve,lNo),e(ve,pb),e(pb,bse),e(bse,iNo),e(pb,dNo),e(pb,i$),e(i$,cNo),e(pb,fNo),e(ve,mNo),e(ve,_b),e(_b,Tse),e(Tse,hNo),e(_b,gNo),e(_b,d$),e(d$,uNo),e(_b,pNo),e(ve,_No),e(ve,vb),e(vb,Fse),e(Fse,vNo),e(vb,bNo),e(vb,c$),e(c$,TNo),e(vb,FNo),e(ve,ENo),e(ve,bb),e(bb,Ese),e(Ese,CNo),e(bb,MNo),e(bb,f$),e(f$,yNo),e(bb,wNo),e(co,ANo),e(co,Cse),e(Cse,LNo),e(co,BNo),h(U5,co,null),v(c,hye,_),v(c,Td,_),e(Td,Tb),e(Tb,Mse),h(J5,Mse,null),e(Td,xNo),e(Td,yse),e(yse,kNo),v(c,gye,_),v(c,dr,_),h(K5,dr,null),e(dr,RNo),e(dr,Fd),e(Fd,PNo),e(Fd,wse),e(wse,SNo),e(Fd,$No),e(Fd,Ase),e(Ase,INo),e(Fd,DNo),e(dr,NNo),e(dr,Y5),e(Y5,jNo),e(Y5,Lse),e(Lse,ONo),e(Y5,GNo),e(dr,qNo),e(dr,Yr),h(Z5,Yr,null),e(Yr,zNo),e(Yr,Bse),e(Bse,XNo),e(Yr,QNo),e(Yr,Ed),e(Ed,VNo),e(Ed,xse),e(xse,WNo),e(Ed,HNo),e(Ed,kse),e(kse,UNo),e(Ed,JNo),e(Yr,KNo),e(Yr,Rse),e(Rse,YNo),e(Yr,ZNo),h(ey,Yr,null),e(dr,ejo),e(dr,fo),h(oy,fo,null),e(fo,ojo),e(fo,Pse),e(Pse,rjo),e(fo,tjo),e(fo,Wa),e(Wa,ajo),e(Wa,Sse),e(Sse,njo),e(Wa,sjo),e(Wa,$se),e($se,ljo),e(Wa,ijo),e(Wa,Ise),e(Ise,djo),e(Wa,cjo),e(fo,fjo),e(fo,Dse),e(Dse,Fb),e(Fb,Nse),e(Nse,mjo),e(Fb,hjo),e(Fb,m$),e(m$,gjo),e(Fb,ujo),e(fo,pjo),e(fo,jse),e(jse,_jo),e(fo,vjo),h(ry,fo,null),v(c,uye,_),v(c,Cd,_),e(Cd,Eb),e(Eb,Ose),h(ty,Ose,null),e(Cd,bjo),e(Cd,Gse),e(Gse,Tjo),v(c,pye,_),v(c,cr,_),h(ay,cr,null),e(cr,Fjo),e(cr,Md),e(Md,Ejo),e(Md,qse),e(qse,Cjo),e(Md,Mjo),e(Md,zse),e(zse,yjo),e(Md,wjo),e(cr,Ajo),e(cr,ny),e(ny,Ljo),e(ny,Xse),e(Xse,Bjo),e(ny,xjo),e(cr,kjo),e(cr,Zr),h(sy,Zr,null),e(Zr,Rjo),e(Zr,Qse),e(Qse,Pjo),e(Zr,Sjo),e(Zr,yd),e(yd,$jo),e(yd,Vse),e(Vse,Ijo),e(yd,Djo),e(yd,Wse),e(Wse,Njo),e(yd,jjo),e(Zr,Ojo),e(Zr,Hse),e(Hse,Gjo),e(Zr,qjo),h(ly,Zr,null),e(cr,zjo),e(cr,mo),h(iy,mo,null),e(mo,Xjo),e(mo,Use),e(Use,Qjo),e(mo,Vjo),e(mo,Ha),e(Ha,Wjo),e(Ha,Jse),e(Jse,Hjo),e(Ha,Ujo),e(Ha,Kse),e(Kse,Jjo),e(Ha,Kjo),e(Ha,Yse),e(Yse,Yjo),e(Ha,Zjo),e(mo,eOo),e(mo,te),e(te,Cb),e(Cb,Zse),e(Zse,oOo),e(Cb,rOo),e(Cb,h$),e(h$,tOo),e(Cb,aOo),e(te,nOo),e(te,Mb),e(Mb,ele),e(ele,sOo),e(Mb,lOo),e(Mb,g$),e(g$,iOo),e(Mb,dOo),e(te,cOo),e(te,yb),e(yb,ole),e(ole,fOo),e(yb,mOo),e(yb,u$),e(u$,hOo),e(yb,gOo),e(te,uOo),e(te,wb),e(wb,rle),e(rle,pOo),e(wb,_Oo),e(wb,p$),e(p$,vOo),e(wb,bOo),e(te,TOo),e(te,Ab),e(Ab,tle),e(tle,FOo),e(Ab,EOo),e(Ab,_$),e(_$,COo),e(Ab,MOo),e(te,yOo),e(te,Lb),e(Lb,ale),e(ale,wOo),e(Lb,AOo),e(Lb,v$),e(v$,LOo),e(Lb,BOo),e(te,xOo),e(te,Bb),e(Bb,nle),e(nle,kOo),e(Bb,ROo),e(Bb,b$),e(b$,POo),e(Bb,SOo),e(te,$Oo),e(te,xb),e(xb,sle),e(sle,IOo),e(xb,DOo),e(xb,T$),e(T$,NOo),e(xb,jOo),e(te,OOo),e(te,kb),e(kb,lle),e(lle,GOo),e(kb,qOo),e(kb,F$),e(F$,zOo),e(kb,XOo),e(te,QOo),e(te,Rb),e(Rb,ile),e(ile,VOo),e(Rb,WOo),e(Rb,E$),e(E$,HOo),e(Rb,UOo),e(te,JOo),e(te,Pb),e(Pb,dle),e(dle,KOo),e(Pb,YOo),e(Pb,C$),e(C$,ZOo),e(Pb,eGo),e(te,oGo),e(te,Sb),e(Sb,cle),e(cle,rGo),e(Sb,tGo),e(Sb,M$),e(M$,aGo),e(Sb,nGo),e(te,sGo),e(te,$b),e($b,fle),e(fle,lGo),e($b,iGo),e($b,y$),e(y$,dGo),e($b,cGo),e(te,fGo),e(te,Ib),e(Ib,mle),e(mle,mGo),e(Ib,hGo),e(Ib,w$),e(w$,gGo),e(Ib,uGo),e(te,pGo),e(te,Db),e(Db,hle),e(hle,_Go),e(Db,vGo),e(Db,A$),e(A$,bGo),e(Db,TGo),e(te,FGo),e(te,Nb),e(Nb,gle),e(gle,EGo),e(Nb,CGo),e(Nb,L$),e(L$,MGo),e(Nb,yGo),e(te,wGo),e(te,jb),e(jb,ule),e(ule,AGo),e(jb,LGo),e(jb,B$),e(B$,BGo),e(jb,xGo),e(te,kGo),e(te,Ob),e(Ob,ple),e(ple,RGo),e(Ob,PGo),e(Ob,x$),e(x$,SGo),e(Ob,$Go),e(te,IGo),e(te,Gb),e(Gb,_le),e(_le,DGo),e(Gb,NGo),e(Gb,k$),e(k$,jGo),e(Gb,OGo),e(te,GGo),e(te,qb),e(qb,vle),e(vle,qGo),e(qb,zGo),e(qb,R$),e(R$,XGo),e(qb,QGo),e(mo,VGo),e(mo,ble),e(ble,WGo),e(mo,HGo),h(dy,mo,null),v(c,_ye,_),v(c,wd,_),e(wd,zb),e(zb,Tle),h(cy,Tle,null),e(wd,UGo),e(wd,Fle),e(Fle,JGo),v(c,vye,_),v(c,fr,_),h(fy,fr,null),e(fr,KGo),e(fr,Ad),e(Ad,YGo),e(Ad,Ele),e(Ele,ZGo),e(Ad,eqo),e(Ad,Cle),e(Cle,oqo),e(Ad,rqo),e(fr,tqo),e(fr,my),e(my,aqo),e(my,Mle),e(Mle,nqo),e(my,sqo),e(fr,lqo),e(fr,et),h(hy,et,null),e(et,iqo),e(et,yle),e(yle,dqo),e(et,cqo),e(et,Ld),e(Ld,fqo),e(Ld,wle),e(wle,mqo),e(Ld,hqo),e(Ld,Ale),e(Ale,gqo),e(Ld,uqo),e(et,pqo),e(et,Lle),e(Lle,_qo),e(et,vqo),h(gy,et,null),e(fr,bqo),e(fr,ho),h(uy,ho,null),e(ho,Tqo),e(ho,Ble),e(Ble,Fqo),e(ho,Eqo),e(ho,Ua),e(Ua,Cqo),e(Ua,xle),e(xle,Mqo),e(Ua,yqo),e(Ua,kle),e(kle,wqo),e(Ua,Aqo),e(Ua,Rle),e(Rle,Lqo),e(Ua,Bqo),e(ho,xqo),e(ho,be),e(be,Xb),e(Xb,Ple),e(Ple,kqo),e(Xb,Rqo),e(Xb,P$),e(P$,Pqo),e(Xb,Sqo),e(be,$qo),e(be,Qb),e(Qb,Sle),e(Sle,Iqo),e(Qb,Dqo),e(Qb,S$),e(S$,Nqo),e(Qb,jqo),e(be,Oqo),e(be,Vb),e(Vb,$le),e($le,Gqo),e(Vb,qqo),e(Vb,$$),e($$,zqo),e(Vb,Xqo),e(be,Qqo),e(be,Wb),e(Wb,Ile),e(Ile,Vqo),e(Wb,Wqo),e(Wb,I$),e(I$,Hqo),e(Wb,Uqo),e(be,Jqo),e(be,Hb),e(Hb,Dle),e(Dle,Kqo),e(Hb,Yqo),e(Hb,D$),e(D$,Zqo),e(Hb,ezo),e(be,ozo),e(be,Ub),e(Ub,Nle),e(Nle,rzo),e(Ub,tzo),e(Ub,N$),e(N$,azo),e(Ub,nzo),e(be,szo),e(be,Jb),e(Jb,jle),e(jle,lzo),e(Jb,izo),e(Jb,j$),e(j$,dzo),e(Jb,czo),e(be,fzo),e(be,Kb),e(Kb,Ole),e(Ole,mzo),e(Kb,hzo),e(Kb,O$),e(O$,gzo),e(Kb,uzo),e(be,pzo),e(be,Yb),e(Yb,Gle),e(Gle,_zo),e(Yb,vzo),e(Yb,G$),e(G$,bzo),e(Yb,Tzo),e(be,Fzo),e(be,Zb),e(Zb,qle),e(qle,Ezo),e(Zb,Czo),e(Zb,q$),e(q$,Mzo),e(Zb,yzo),e(ho,wzo),e(ho,zle),e(zle,Azo),e(ho,Lzo),h(py,ho,null),v(c,bye,_),v(c,Bd,_),e(Bd,e2),e(e2,Xle),h(_y,Xle,null),e(Bd,Bzo),e(Bd,Qle),e(Qle,xzo),v(c,Tye,_),v(c,mr,_),h(vy,mr,null),e(mr,kzo),e(mr,xd),e(xd,Rzo),e(xd,Vle),e(Vle,Pzo),e(xd,Szo),e(xd,Wle),e(Wle,$zo),e(xd,Izo),e(mr,Dzo),e(mr,by),e(by,Nzo),e(by,Hle),e(Hle,jzo),e(by,Ozo),e(mr,Gzo),e(mr,ot),h(Ty,ot,null),e(ot,qzo),e(ot,Ule),e(Ule,zzo),e(ot,Xzo),e(ot,kd),e(kd,Qzo),e(kd,Jle),e(Jle,Vzo),e(kd,Wzo),e(kd,Kle),e(Kle,Hzo),e(kd,Uzo),e(ot,Jzo),e(ot,Yle),e(Yle,Kzo),e(ot,Yzo),h(Fy,ot,null),e(mr,Zzo),e(mr,go),h(Ey,go,null),e(go,eXo),e(go,Zle),e(Zle,oXo),e(go,rXo),e(go,Ja),e(Ja,tXo),e(Ja,eie),e(eie,aXo),e(Ja,nXo),e(Ja,oie),e(oie,sXo),e(Ja,lXo),e(Ja,rie),e(rie,iXo),e(Ja,dXo),e(go,cXo),e(go,W),e(W,o2),e(o2,tie),e(tie,fXo),e(o2,mXo),e(o2,z$),e(z$,hXo),e(o2,gXo),e(W,uXo),e(W,r2),e(r2,aie),e(aie,pXo),e(r2,_Xo),e(r2,X$),e(X$,vXo),e(r2,bXo),e(W,TXo),e(W,t2),e(t2,nie),e(nie,FXo),e(t2,EXo),e(t2,Q$),e(Q$,CXo),e(t2,MXo),e(W,yXo),e(W,a2),e(a2,sie),e(sie,wXo),e(a2,AXo),e(a2,V$),e(V$,LXo),e(a2,BXo),e(W,xXo),e(W,n2),e(n2,lie),e(lie,kXo),e(n2,RXo),e(n2,W$),e(W$,PXo),e(n2,SXo),e(W,$Xo),e(W,s2),e(s2,iie),e(iie,IXo),e(s2,DXo),e(s2,H$),e(H$,NXo),e(s2,jXo),e(W,OXo),e(W,l2),e(l2,die),e(die,GXo),e(l2,qXo),e(l2,U$),e(U$,zXo),e(l2,XXo),e(W,QXo),e(W,i2),e(i2,cie),e(cie,VXo),e(i2,WXo),e(i2,J$),e(J$,HXo),e(i2,UXo),e(W,JXo),e(W,d2),e(d2,fie),e(fie,KXo),e(d2,YXo),e(d2,K$),e(K$,ZXo),e(d2,eQo),e(W,oQo),e(W,c2),e(c2,mie),e(mie,rQo),e(c2,tQo),e(c2,Y$),e(Y$,aQo),e(c2,nQo),e(W,sQo),e(W,f2),e(f2,hie),e(hie,lQo),e(f2,iQo),e(f2,Z$),e(Z$,dQo),e(f2,cQo),e(W,fQo),e(W,m2),e(m2,gie),e(gie,mQo),e(m2,hQo),e(m2,eI),e(eI,gQo),e(m2,uQo),e(W,pQo),e(W,h2),e(h2,uie),e(uie,_Qo),e(h2,vQo),e(h2,oI),e(oI,bQo),e(h2,TQo),e(W,FQo),e(W,g2),e(g2,pie),e(pie,EQo),e(g2,CQo),e(g2,rI),e(rI,MQo),e(g2,yQo),e(W,wQo),e(W,u2),e(u2,_ie),e(_ie,AQo),e(u2,LQo),e(u2,tI),e(tI,BQo),e(u2,xQo),e(W,kQo),e(W,p2),e(p2,vie),e(vie,RQo),e(p2,PQo),e(p2,aI),e(aI,SQo),e(p2,$Qo),e(W,IQo),e(W,_2),e(_2,bie),e(bie,DQo),e(_2,NQo),e(_2,nI),e(nI,jQo),e(_2,OQo),e(W,GQo),e(W,v2),e(v2,Tie),e(Tie,qQo),e(v2,zQo),e(v2,sI),e(sI,XQo),e(v2,QQo),e(W,VQo),e(W,b2),e(b2,Fie),e(Fie,WQo),e(b2,HQo),e(b2,lI),e(lI,UQo),e(b2,JQo),e(W,KQo),e(W,T2),e(T2,Eie),e(Eie,YQo),e(T2,ZQo),e(T2,iI),e(iI,eVo),e(T2,oVo),e(W,rVo),e(W,F2),e(F2,Cie),e(Cie,tVo),e(F2,aVo),e(F2,dI),e(dI,nVo),e(F2,sVo),e(W,lVo),e(W,E2),e(E2,Mie),e(Mie,iVo),e(E2,dVo),e(E2,cI),e(cI,cVo),e(E2,fVo),e(W,mVo),e(W,C2),e(C2,yie),e(yie,hVo),e(C2,gVo),e(C2,fI),e(fI,uVo),e(C2,pVo),e(W,_Vo),e(W,M2),e(M2,wie),e(wie,vVo),e(M2,bVo),e(M2,mI),e(mI,TVo),e(M2,FVo),e(W,EVo),e(W,y2),e(y2,Aie),e(Aie,CVo),e(y2,MVo),e(y2,hI),e(hI,yVo),e(y2,wVo),e(go,AVo),e(go,Lie),e(Lie,LVo),e(go,BVo),h(Cy,go,null),v(c,Fye,_),v(c,Rd,_),e(Rd,w2),e(w2,Bie),h(My,Bie,null),e(Rd,xVo),e(Rd,xie),e(xie,kVo),v(c,Eye,_),v(c,hr,_),h(yy,hr,null),e(hr,RVo),e(hr,Pd),e(Pd,PVo),e(Pd,kie),e(kie,SVo),e(Pd,$Vo),e(Pd,Rie),e(Rie,IVo),e(Pd,DVo),e(hr,NVo),e(hr,wy),e(wy,jVo),e(wy,Pie),e(Pie,OVo),e(wy,GVo),e(hr,qVo),e(hr,rt),h(Ay,rt,null),e(rt,zVo),e(rt,Sie),e(Sie,XVo),e(rt,QVo),e(rt,Sd),e(Sd,VVo),e(Sd,$ie),e($ie,WVo),e(Sd,HVo),e(Sd,Iie),e(Iie,UVo),e(Sd,JVo),e(rt,KVo),e(rt,Die),e(Die,YVo),e(rt,ZVo),h(Ly,rt,null),e(hr,eWo),e(hr,uo),h(By,uo,null),e(uo,oWo),e(uo,Nie),e(Nie,rWo),e(uo,tWo),e(uo,Ka),e(Ka,aWo),e(Ka,jie),e(jie,nWo),e(Ka,sWo),e(Ka,Oie),e(Oie,lWo),e(Ka,iWo),e(Ka,Gie),e(Gie,dWo),e(Ka,cWo),e(uo,fWo),e(uo,de),e(de,A2),e(A2,qie),e(qie,mWo),e(A2,hWo),e(A2,gI),e(gI,gWo),e(A2,uWo),e(de,pWo),e(de,L2),e(L2,zie),e(zie,_Wo),e(L2,vWo),e(L2,uI),e(uI,bWo),e(L2,TWo),e(de,FWo),e(de,B2),e(B2,Xie),e(Xie,EWo),e(B2,CWo),e(B2,pI),e(pI,MWo),e(B2,yWo),e(de,wWo),e(de,x2),e(x2,Qie),e(Qie,AWo),e(x2,LWo),e(x2,_I),e(_I,BWo),e(x2,xWo),e(de,kWo),e(de,k2),e(k2,Vie),e(Vie,RWo),e(k2,PWo),e(k2,vI),e(vI,SWo),e(k2,$Wo),e(de,IWo),e(de,R2),e(R2,Wie),e(Wie,DWo),e(R2,NWo),e(R2,bI),e(bI,jWo),e(R2,OWo),e(de,GWo),e(de,P2),e(P2,Hie),e(Hie,qWo),e(P2,zWo),e(P2,TI),e(TI,XWo),e(P2,QWo),e(de,VWo),e(de,S2),e(S2,Uie),e(Uie,WWo),e(S2,HWo),e(S2,FI),e(FI,UWo),e(S2,JWo),e(de,KWo),e(de,$2),e($2,Jie),e(Jie,YWo),e($2,ZWo),e($2,EI),e(EI,eHo),e($2,oHo),e(de,rHo),e(de,I2),e(I2,Kie),e(Kie,tHo),e(I2,aHo),e(I2,CI),e(CI,nHo),e(I2,sHo),e(de,lHo),e(de,D2),e(D2,Yie),e(Yie,iHo),e(D2,dHo),e(D2,MI),e(MI,cHo),e(D2,fHo),e(de,mHo),e(de,N2),e(N2,Zie),e(Zie,hHo),e(N2,gHo),e(N2,yI),e(yI,uHo),e(N2,pHo),e(de,_Ho),e(de,j2),e(j2,ede),e(ede,vHo),e(j2,bHo),e(j2,wI),e(wI,THo),e(j2,FHo),e(de,EHo),e(de,O2),e(O2,ode),e(ode,CHo),e(O2,MHo),e(O2,AI),e(AI,yHo),e(O2,wHo),e(de,AHo),e(de,G2),e(G2,rde),e(rde,LHo),e(G2,BHo),e(G2,LI),e(LI,xHo),e(G2,kHo),e(de,RHo),e(de,q2),e(q2,tde),e(tde,PHo),e(q2,SHo),e(q2,BI),e(BI,$Ho),e(q2,IHo),e(de,DHo),e(de,z2),e(z2,ade),e(ade,NHo),e(z2,jHo),e(z2,xI),e(xI,OHo),e(z2,GHo),e(uo,qHo),e(uo,nde),e(nde,zHo),e(uo,XHo),h(xy,uo,null),v(c,Cye,_),v(c,$d,_),e($d,X2),e(X2,sde),h(ky,sde,null),e($d,QHo),e($d,lde),e(lde,VHo),v(c,Mye,_),v(c,gr,_),h(Ry,gr,null),e(gr,WHo),e(gr,Id),e(Id,HHo),e(Id,ide),e(ide,UHo),e(Id,JHo),e(Id,dde),e(dde,KHo),e(Id,YHo),e(gr,ZHo),e(gr,Py),e(Py,eUo),e(Py,cde),e(cde,oUo),e(Py,rUo),e(gr,tUo),e(gr,tt),h(Sy,tt,null),e(tt,aUo),e(tt,fde),e(fde,nUo),e(tt,sUo),e(tt,Dd),e(Dd,lUo),e(Dd,mde),e(mde,iUo),e(Dd,dUo),e(Dd,hde),e(hde,cUo),e(Dd,fUo),e(tt,mUo),e(tt,gde),e(gde,hUo),e(tt,gUo),h($y,tt,null),e(gr,uUo),e(gr,po),h(Iy,po,null),e(po,pUo),e(po,ude),e(ude,_Uo),e(po,vUo),e(po,Ya),e(Ya,bUo),e(Ya,pde),e(pde,TUo),e(Ya,FUo),e(Ya,_de),e(_de,EUo),e(Ya,CUo),e(Ya,vde),e(vde,MUo),e(Ya,yUo),e(po,wUo),e(po,bde),e(bde,Q2),e(Q2,Tde),e(Tde,AUo),e(Q2,LUo),e(Q2,kI),e(kI,BUo),e(Q2,xUo),e(po,kUo),e(po,Fde),e(Fde,RUo),e(po,PUo),h(Dy,po,null),v(c,yye,_),v(c,Nd,_),e(Nd,V2),e(V2,Ede),h(Ny,Ede,null),e(Nd,SUo),e(Nd,Cde),e(Cde,$Uo),v(c,wye,_),v(c,ur,_),h(jy,ur,null),e(ur,IUo),e(ur,jd),e(jd,DUo),e(jd,Mde),e(Mde,NUo),e(jd,jUo),e(jd,yde),e(yde,OUo),e(jd,GUo),e(ur,qUo),e(ur,Oy),e(Oy,zUo),e(Oy,wde),e(wde,XUo),e(Oy,QUo),e(ur,VUo),e(ur,at),h(Gy,at,null),e(at,WUo),e(at,Ade),e(Ade,HUo),e(at,UUo),e(at,Od),e(Od,JUo),e(Od,Lde),e(Lde,KUo),e(Od,YUo),e(Od,Bde),e(Bde,ZUo),e(Od,eJo),e(at,oJo),e(at,xde),e(xde,rJo),e(at,tJo),h(qy,at,null),e(ur,aJo),e(ur,_o),h(zy,_o,null),e(_o,nJo),e(_o,kde),e(kde,sJo),e(_o,lJo),e(_o,Za),e(Za,iJo),e(Za,Rde),e(Rde,dJo),e(Za,cJo),e(Za,Pde),e(Pde,fJo),e(Za,mJo),e(Za,Sde),e(Sde,hJo),e(Za,gJo),e(_o,uJo),e(_o,ae),e(ae,W2),e(W2,$de),e($de,pJo),e(W2,_Jo),e(W2,RI),e(RI,vJo),e(W2,bJo),e(ae,TJo),e(ae,H2),e(H2,Ide),e(Ide,FJo),e(H2,EJo),e(H2,PI),e(PI,CJo),e(H2,MJo),e(ae,yJo),e(ae,U2),e(U2,Dde),e(Dde,wJo),e(U2,AJo),e(U2,SI),e(SI,LJo),e(U2,BJo),e(ae,xJo),e(ae,J2),e(J2,Nde),e(Nde,kJo),e(J2,RJo),e(J2,$I),e($I,PJo),e(J2,SJo),e(ae,$Jo),e(ae,K2),e(K2,jde),e(jde,IJo),e(K2,DJo),e(K2,II),e(II,NJo),e(K2,jJo),e(ae,OJo),e(ae,Y2),e(Y2,Ode),e(Ode,GJo),e(Y2,qJo),e(Y2,DI),e(DI,zJo),e(Y2,XJo),e(ae,QJo),e(ae,Z2),e(Z2,Gde),e(Gde,VJo),e(Z2,WJo),e(Z2,NI),e(NI,HJo),e(Z2,UJo),e(ae,JJo),e(ae,eT),e(eT,qde),e(qde,KJo),e(eT,YJo),e(eT,jI),e(jI,ZJo),e(eT,eKo),e(ae,oKo),e(ae,oT),e(oT,zde),e(zde,rKo),e(oT,tKo),e(oT,OI),e(OI,aKo),e(oT,nKo),e(ae,sKo),e(ae,rT),e(rT,Xde),e(Xde,lKo),e(rT,iKo),e(rT,GI),e(GI,dKo),e(rT,cKo),e(ae,fKo),e(ae,tT),e(tT,Qde),e(Qde,mKo),e(tT,hKo),e(tT,qI),e(qI,gKo),e(tT,uKo),e(ae,pKo),e(ae,aT),e(aT,Vde),e(Vde,_Ko),e(aT,vKo),e(aT,zI),e(zI,bKo),e(aT,TKo),e(ae,FKo),e(ae,nT),e(nT,Wde),e(Wde,EKo),e(nT,CKo),e(nT,XI),e(XI,MKo),e(nT,yKo),e(ae,wKo),e(ae,sT),e(sT,Hde),e(Hde,AKo),e(sT,LKo),e(sT,QI),e(QI,BKo),e(sT,xKo),e(ae,kKo),e(ae,lT),e(lT,Ude),e(Ude,RKo),e(lT,PKo),e(lT,VI),e(VI,SKo),e(lT,$Ko),e(ae,IKo),e(ae,iT),e(iT,Jde),e(Jde,DKo),e(iT,NKo),e(iT,WI),e(WI,jKo),e(iT,OKo),e(ae,GKo),e(ae,dT),e(dT,Kde),e(Kde,qKo),e(dT,zKo),e(dT,HI),e(HI,XKo),e(dT,QKo),e(ae,VKo),e(ae,cT),e(cT,Yde),e(Yde,WKo),e(cT,HKo),e(cT,UI),e(UI,UKo),e(cT,JKo),e(ae,KKo),e(ae,fT),e(fT,Zde),e(Zde,YKo),e(fT,ZKo),e(fT,JI),e(JI,eYo),e(fT,oYo),e(ae,rYo),e(ae,mT),e(mT,ece),e(ece,tYo),e(mT,aYo),e(mT,KI),e(KI,nYo),e(mT,sYo),e(_o,lYo),e(_o,oce),e(oce,iYo),e(_o,dYo),h(Xy,_o,null),v(c,Aye,_),v(c,Gd,_),e(Gd,hT),e(hT,rce),h(Qy,rce,null),e(Gd,cYo),e(Gd,tce),e(tce,fYo),v(c,Lye,_),v(c,pr,_),h(Vy,pr,null),e(pr,mYo),e(pr,qd),e(qd,hYo),e(qd,ace),e(ace,gYo),e(qd,uYo),e(qd,nce),e(nce,pYo),e(qd,_Yo),e(pr,vYo),e(pr,Wy),e(Wy,bYo),e(Wy,sce),e(sce,TYo),e(Wy,FYo),e(pr,EYo),e(pr,nt),h(Hy,nt,null),e(nt,CYo),e(nt,lce),e(lce,MYo),e(nt,yYo),e(nt,zd),e(zd,wYo),e(zd,ice),e(ice,AYo),e(zd,LYo),e(zd,dce),e(dce,BYo),e(zd,xYo),e(nt,kYo),e(nt,cce),e(cce,RYo),e(nt,PYo),h(Uy,nt,null),e(pr,SYo),e(pr,vo),h(Jy,vo,null),e(vo,$Yo),e(vo,fce),e(fce,IYo),e(vo,DYo),e(vo,en),e(en,NYo),e(en,mce),e(mce,jYo),e(en,OYo),e(en,hce),e(hce,GYo),e(en,qYo),e(en,gce),e(gce,zYo),e(en,XYo),e(vo,QYo),e(vo,ne),e(ne,gT),e(gT,uce),e(uce,VYo),e(gT,WYo),e(gT,YI),e(YI,HYo),e(gT,UYo),e(ne,JYo),e(ne,uT),e(uT,pce),e(pce,KYo),e(uT,YYo),e(uT,ZI),e(ZI,ZYo),e(uT,eZo),e(ne,oZo),e(ne,pT),e(pT,_ce),e(_ce,rZo),e(pT,tZo),e(pT,eD),e(eD,aZo),e(pT,nZo),e(ne,sZo),e(ne,_T),e(_T,vce),e(vce,lZo),e(_T,iZo),e(_T,oD),e(oD,dZo),e(_T,cZo),e(ne,fZo),e(ne,vT),e(vT,bce),e(bce,mZo),e(vT,hZo),e(vT,rD),e(rD,gZo),e(vT,uZo),e(ne,pZo),e(ne,bT),e(bT,Tce),e(Tce,_Zo),e(bT,vZo),e(bT,tD),e(tD,bZo),e(bT,TZo),e(ne,FZo),e(ne,TT),e(TT,Fce),e(Fce,EZo),e(TT,CZo),e(TT,aD),e(aD,MZo),e(TT,yZo),e(ne,wZo),e(ne,FT),e(FT,Ece),e(Ece,AZo),e(FT,LZo),e(FT,nD),e(nD,BZo),e(FT,xZo),e(ne,kZo),e(ne,ET),e(ET,Cce),e(Cce,RZo),e(ET,PZo),e(ET,sD),e(sD,SZo),e(ET,$Zo),e(ne,IZo),e(ne,CT),e(CT,Mce),e(Mce,DZo),e(CT,NZo),e(CT,lD),e(lD,jZo),e(CT,OZo),e(ne,GZo),e(ne,MT),e(MT,yce),e(yce,qZo),e(MT,zZo),e(MT,iD),e(iD,XZo),e(MT,QZo),e(ne,VZo),e(ne,yT),e(yT,wce),e(wce,WZo),e(yT,HZo),e(yT,dD),e(dD,UZo),e(yT,JZo),e(ne,KZo),e(ne,wT),e(wT,Ace),e(Ace,YZo),e(wT,ZZo),e(wT,cD),e(cD,eer),e(wT,oer),e(ne,rer),e(ne,AT),e(AT,Lce),e(Lce,ter),e(AT,aer),e(AT,fD),e(fD,ner),e(AT,ser),e(ne,ler),e(ne,LT),e(LT,Bce),e(Bce,ier),e(LT,der),e(LT,mD),e(mD,cer),e(LT,fer),e(ne,mer),e(ne,BT),e(BT,xce),e(xce,her),e(BT,ger),e(BT,hD),e(hD,uer),e(BT,per),e(ne,_er),e(ne,xT),e(xT,kce),e(kce,ver),e(xT,ber),e(xT,gD),e(gD,Ter),e(xT,Fer),e(ne,Eer),e(ne,kT),e(kT,Rce),e(Rce,Cer),e(kT,Mer),e(kT,uD),e(uD,yer),e(kT,wer),e(ne,Aer),e(ne,RT),e(RT,Pce),e(Pce,Ler),e(RT,Ber),e(RT,pD),e(pD,xer),e(RT,ker),e(vo,Rer),e(vo,Sce),e(Sce,Per),e(vo,Ser),h(Ky,vo,null),v(c,Bye,_),v(c,Xd,_),e(Xd,PT),e(PT,$ce),h(Yy,$ce,null),e(Xd,$er),e(Xd,Ice),e(Ice,Ier),v(c,xye,_),v(c,_r,_),h(Zy,_r,null),e(_r,Der),e(_r,Qd),e(Qd,Ner),e(Qd,Dce),e(Dce,jer),e(Qd,Oer),e(Qd,Nce),e(Nce,Ger),e(Qd,qer),e(_r,zer),e(_r,ew),e(ew,Xer),e(ew,jce),e(jce,Qer),e(ew,Ver),e(_r,Wer),e(_r,st),h(ow,st,null),e(st,Her),e(st,Oce),e(Oce,Uer),e(st,Jer),e(st,Vd),e(Vd,Ker),e(Vd,Gce),e(Gce,Yer),e(Vd,Zer),e(Vd,qce),e(qce,eor),e(Vd,oor),e(st,ror),e(st,zce),e(zce,tor),e(st,aor),h(rw,st,null),e(_r,nor),e(_r,bo),h(tw,bo,null),e(bo,sor),e(bo,Xce),e(Xce,lor),e(bo,ior),e(bo,on),e(on,dor),e(on,Qce),e(Qce,cor),e(on,mor),e(on,Vce),e(Vce,hor),e(on,gor),e(on,Wce),e(Wce,uor),e(on,por),e(bo,_or),e(bo,Y),e(Y,ST),e(ST,Hce),e(Hce,vor),e(ST,bor),e(ST,_D),e(_D,Tor),e(ST,For),e(Y,Eor),e(Y,$T),e($T,Uce),e(Uce,Cor),e($T,Mor),e($T,vD),e(vD,yor),e($T,wor),e(Y,Aor),e(Y,IT),e(IT,Jce),e(Jce,Lor),e(IT,Bor),e(IT,bD),e(bD,xor),e(IT,kor),e(Y,Ror),e(Y,DT),e(DT,Kce),e(Kce,Por),e(DT,Sor),e(DT,TD),e(TD,$or),e(DT,Ior),e(Y,Dor),e(Y,NT),e(NT,Yce),e(Yce,Nor),e(NT,jor),e(NT,FD),e(FD,Oor),e(NT,Gor),e(Y,qor),e(Y,jT),e(jT,Zce),e(Zce,zor),e(jT,Xor),e(jT,ED),e(ED,Qor),e(jT,Vor),e(Y,Wor),e(Y,OT),e(OT,efe),e(efe,Hor),e(OT,Uor),e(OT,CD),e(CD,Jor),e(OT,Kor),e(Y,Yor),e(Y,GT),e(GT,ofe),e(ofe,Zor),e(GT,err),e(GT,MD),e(MD,orr),e(GT,rrr),e(Y,trr),e(Y,qT),e(qT,rfe),e(rfe,arr),e(qT,nrr),e(qT,yD),e(yD,srr),e(qT,lrr),e(Y,irr),e(Y,zT),e(zT,tfe),e(tfe,drr),e(zT,crr),e(zT,wD),e(wD,frr),e(zT,mrr),e(Y,hrr),e(Y,XT),e(XT,afe),e(afe,grr),e(XT,urr),e(XT,AD),e(AD,prr),e(XT,_rr),e(Y,vrr),e(Y,QT),e(QT,nfe),e(nfe,brr),e(QT,Trr),e(QT,LD),e(LD,Frr),e(QT,Err),e(Y,Crr),e(Y,VT),e(VT,sfe),e(sfe,Mrr),e(VT,yrr),e(VT,BD),e(BD,wrr),e(VT,Arr),e(Y,Lrr),e(Y,WT),e(WT,lfe),e(lfe,Brr),e(WT,xrr),e(WT,xD),e(xD,krr),e(WT,Rrr),e(Y,Prr),e(Y,HT),e(HT,ife),e(ife,Srr),e(HT,$rr),e(HT,kD),e(kD,Irr),e(HT,Drr),e(Y,Nrr),e(Y,UT),e(UT,dfe),e(dfe,jrr),e(UT,Orr),e(UT,RD),e(RD,Grr),e(UT,qrr),e(Y,zrr),e(Y,JT),e(JT,cfe),e(cfe,Xrr),e(JT,Qrr),e(JT,PD),e(PD,Vrr),e(JT,Wrr),e(Y,Hrr),e(Y,KT),e(KT,ffe),e(ffe,Urr),e(KT,Jrr),e(KT,SD),e(SD,Krr),e(KT,Yrr),e(Y,Zrr),e(Y,YT),e(YT,mfe),e(mfe,etr),e(YT,otr),e(YT,$D),e($D,rtr),e(YT,ttr),e(Y,atr),e(Y,ZT),e(ZT,hfe),e(hfe,ntr),e(ZT,str),e(ZT,ID),e(ID,ltr),e(ZT,itr),e(Y,dtr),e(Y,eF),e(eF,gfe),e(gfe,ctr),e(eF,ftr),e(eF,DD),e(DD,mtr),e(eF,htr),e(Y,gtr),e(Y,oF),e(oF,ufe),e(ufe,utr),e(oF,ptr),e(oF,ND),e(ND,_tr),e(oF,vtr),e(bo,btr),e(bo,pfe),e(pfe,Ttr),e(bo,Ftr),h(aw,bo,null),v(c,kye,_),v(c,Wd,_),e(Wd,rF),e(rF,_fe),h(nw,_fe,null),e(Wd,Etr),e(Wd,vfe),e(vfe,Ctr),v(c,Rye,_),v(c,vr,_),h(sw,vr,null),e(vr,Mtr),e(vr,Hd),e(Hd,ytr),e(Hd,bfe),e(bfe,wtr),e(Hd,Atr),e(Hd,Tfe),e(Tfe,Ltr),e(Hd,Btr),e(vr,xtr),e(vr,lw),e(lw,ktr),e(lw,Ffe),e(Ffe,Rtr),e(lw,Ptr),e(vr,Str),e(vr,lt),h(iw,lt,null),e(lt,$tr),e(lt,Efe),e(Efe,Itr),e(lt,Dtr),e(lt,Ud),e(Ud,Ntr),e(Ud,Cfe),e(Cfe,jtr),e(Ud,Otr),e(Ud,Mfe),e(Mfe,Gtr),e(Ud,qtr),e(lt,ztr),e(lt,yfe),e(yfe,Xtr),e(lt,Qtr),h(dw,lt,null),e(vr,Vtr),e(vr,To),h(cw,To,null),e(To,Wtr),e(To,wfe),e(wfe,Htr),e(To,Utr),e(To,rn),e(rn,Jtr),e(rn,Afe),e(Afe,Ktr),e(rn,Ytr),e(rn,Lfe),e(Lfe,Ztr),e(rn,ear),e(rn,Bfe),e(Bfe,oar),e(rn,rar),e(To,tar),e(To,Jd),e(Jd,tF),e(tF,xfe),e(xfe,aar),e(tF,nar),e(tF,jD),e(jD,sar),e(tF,lar),e(Jd,iar),e(Jd,aF),e(aF,kfe),e(kfe,dar),e(aF,car),e(aF,OD),e(OD,far),e(aF,mar),e(Jd,har),e(Jd,nF),e(nF,Rfe),e(Rfe,gar),e(nF,uar),e(nF,GD),e(GD,par),e(nF,_ar),e(To,bar),e(To,Pfe),e(Pfe,Tar),e(To,Far),h(fw,To,null),v(c,Pye,_),v(c,Kd,_),e(Kd,sF),e(sF,Sfe),h(mw,Sfe,null),e(Kd,Ear),e(Kd,$fe),e($fe,Car),v(c,Sye,_),v(c,br,_),h(hw,br,null),e(br,Mar),e(br,Yd),e(Yd,yar),e(Yd,Ife),e(Ife,war),e(Yd,Aar),e(Yd,Dfe),e(Dfe,Lar),e(Yd,Bar),e(br,xar),e(br,gw),e(gw,kar),e(gw,Nfe),e(Nfe,Rar),e(gw,Par),e(br,Sar),e(br,it),h(uw,it,null),e(it,$ar),e(it,jfe),e(jfe,Iar),e(it,Dar),e(it,Zd),e(Zd,Nar),e(Zd,Ofe),e(Ofe,jar),e(Zd,Oar),e(Zd,Gfe),e(Gfe,Gar),e(Zd,qar),e(it,zar),e(it,qfe),e(qfe,Xar),e(it,Qar),h(pw,it,null),e(br,Var),e(br,Fo),h(_w,Fo,null),e(Fo,War),e(Fo,zfe),e(zfe,Har),e(Fo,Uar),e(Fo,tn),e(tn,Jar),e(tn,Xfe),e(Xfe,Kar),e(tn,Yar),e(tn,Qfe),e(Qfe,Zar),e(tn,enr),e(tn,Vfe),e(Vfe,onr),e(tn,rnr),e(Fo,tnr),e(Fo,Te),e(Te,lF),e(lF,Wfe),e(Wfe,anr),e(lF,nnr),e(lF,qD),e(qD,snr),e(lF,lnr),e(Te,inr),e(Te,iF),e(iF,Hfe),e(Hfe,dnr),e(iF,cnr),e(iF,zD),e(zD,fnr),e(iF,mnr),e(Te,hnr),e(Te,dF),e(dF,Ufe),e(Ufe,gnr),e(dF,unr),e(dF,XD),e(XD,pnr),e(dF,_nr),e(Te,vnr),e(Te,cF),e(cF,Jfe),e(Jfe,bnr),e(cF,Tnr),e(cF,QD),e(QD,Fnr),e(cF,Enr),e(Te,Cnr),e(Te,fF),e(fF,Kfe),e(Kfe,Mnr),e(fF,ynr),e(fF,VD),e(VD,wnr),e(fF,Anr),e(Te,Lnr),e(Te,mF),e(mF,Yfe),e(Yfe,Bnr),e(mF,xnr),e(mF,WD),e(WD,knr),e(mF,Rnr),e(Te,Pnr),e(Te,hF),e(hF,Zfe),e(Zfe,Snr),e(hF,$nr),e(hF,HD),e(HD,Inr),e(hF,Dnr),e(Te,Nnr),e(Te,gF),e(gF,eme),e(eme,jnr),e(gF,Onr),e(gF,UD),e(UD,Gnr),e(gF,qnr),e(Te,znr),e(Te,uF),e(uF,ome),e(ome,Xnr),e(uF,Qnr),e(uF,JD),e(JD,Vnr),e(uF,Wnr),e(Te,Hnr),e(Te,pF),e(pF,rme),e(rme,Unr),e(pF,Jnr),e(pF,KD),e(KD,Knr),e(pF,Ynr),e(Fo,Znr),e(Fo,tme),e(tme,esr),e(Fo,osr),h(vw,Fo,null),v(c,$ye,_),v(c,ec,_),e(ec,_F),e(_F,ame),h(bw,ame,null),e(ec,rsr),e(ec,nme),e(nme,tsr),v(c,Iye,_),v(c,Tr,_),h(Tw,Tr,null),e(Tr,asr),e(Tr,oc),e(oc,nsr),e(oc,sme),e(sme,ssr),e(oc,lsr),e(oc,lme),e(lme,isr),e(oc,dsr),e(Tr,csr),e(Tr,Fw),e(Fw,fsr),e(Fw,ime),e(ime,msr),e(Fw,hsr),e(Tr,gsr),e(Tr,dt),h(Ew,dt,null),e(dt,usr),e(dt,dme),e(dme,psr),e(dt,_sr),e(dt,rc),e(rc,vsr),e(rc,cme),e(cme,bsr),e(rc,Tsr),e(rc,fme),e(fme,Fsr),e(rc,Esr),e(dt,Csr),e(dt,mme),e(mme,Msr),e(dt,ysr),h(Cw,dt,null),e(Tr,wsr),e(Tr,Eo),h(Mw,Eo,null),e(Eo,Asr),e(Eo,hme),e(hme,Lsr),e(Eo,Bsr),e(Eo,an),e(an,xsr),e(an,gme),e(gme,ksr),e(an,Rsr),e(an,ume),e(ume,Psr),e(an,Ssr),e(an,pme),e(pme,$sr),e(an,Isr),e(Eo,Dsr),e(Eo,ke),e(ke,vF),e(vF,_me),e(_me,Nsr),e(vF,jsr),e(vF,YD),e(YD,Osr),e(vF,Gsr),e(ke,qsr),e(ke,bF),e(bF,vme),e(vme,zsr),e(bF,Xsr),e(bF,ZD),e(ZD,Qsr),e(bF,Vsr),e(ke,Wsr),e(ke,TF),e(TF,bme),e(bme,Hsr),e(TF,Usr),e(TF,eN),e(eN,Jsr),e(TF,Ksr),e(ke,Ysr),e(ke,FF),e(FF,Tme),e(Tme,Zsr),e(FF,elr),e(FF,oN),e(oN,olr),e(FF,rlr),e(ke,tlr),e(ke,EF),e(EF,Fme),e(Fme,alr),e(EF,nlr),e(EF,rN),e(rN,slr),e(EF,llr),e(ke,ilr),e(ke,CF),e(CF,Eme),e(Eme,dlr),e(CF,clr),e(CF,tN),e(tN,flr),e(CF,mlr),e(ke,hlr),e(ke,MF),e(MF,Cme),e(Cme,glr),e(MF,ulr),e(MF,aN),e(aN,plr),e(MF,_lr),e(ke,vlr),e(ke,yF),e(yF,Mme),e(Mme,blr),e(yF,Tlr),e(yF,nN),e(nN,Flr),e(yF,Elr),e(Eo,Clr),e(Eo,yme),e(yme,Mlr),e(Eo,ylr),h(yw,Eo,null),v(c,Dye,_),v(c,tc,_),e(tc,wF),e(wF,wme),h(ww,wme,null),e(tc,wlr),e(tc,Ame),e(Ame,Alr),v(c,Nye,_),v(c,Fr,_),h(Aw,Fr,null),e(Fr,Llr),e(Fr,ac),e(ac,Blr),e(ac,Lme),e(Lme,xlr),e(ac,klr),e(ac,Bme),e(Bme,Rlr),e(ac,Plr),e(Fr,Slr),e(Fr,Lw),e(Lw,$lr),e(Lw,xme),e(xme,Ilr),e(Lw,Dlr),e(Fr,Nlr),e(Fr,ct),h(Bw,ct,null),e(ct,jlr),e(ct,kme),e(kme,Olr),e(ct,Glr),e(ct,nc),e(nc,qlr),e(nc,Rme),e(Rme,zlr),e(nc,Xlr),e(nc,Pme),e(Pme,Qlr),e(nc,Vlr),e(ct,Wlr),e(ct,Sme),e(Sme,Hlr),e(ct,Ulr),h(xw,ct,null),e(Fr,Jlr),e(Fr,Co),h(kw,Co,null),e(Co,Klr),e(Co,$me),e($me,Ylr),e(Co,Zlr),e(Co,nn),e(nn,eir),e(nn,Ime),e(Ime,oir),e(nn,rir),e(nn,Dme),e(Dme,tir),e(nn,air),e(nn,Nme),e(Nme,nir),e(nn,sir),e(Co,lir),e(Co,Me),e(Me,AF),e(AF,jme),e(jme,iir),e(AF,dir),e(AF,sN),e(sN,cir),e(AF,fir),e(Me,mir),e(Me,LF),e(LF,Ome),e(Ome,hir),e(LF,gir),e(LF,lN),e(lN,uir),e(LF,pir),e(Me,_ir),e(Me,BF),e(BF,Gme),e(Gme,vir),e(BF,bir),e(BF,iN),e(iN,Tir),e(BF,Fir),e(Me,Eir),e(Me,xF),e(xF,qme),e(qme,Cir),e(xF,Mir),e(xF,dN),e(dN,yir),e(xF,wir),e(Me,Air),e(Me,kF),e(kF,zme),e(zme,Lir),e(kF,Bir),e(kF,cN),e(cN,xir),e(kF,kir),e(Me,Rir),e(Me,RF),e(RF,Xme),e(Xme,Pir),e(RF,Sir),e(RF,fN),e(fN,$ir),e(RF,Iir),e(Me,Dir),e(Me,PF),e(PF,Qme),e(Qme,Nir),e(PF,jir),e(PF,mN),e(mN,Oir),e(PF,Gir),e(Me,qir),e(Me,SF),e(SF,Vme),e(Vme,zir),e(SF,Xir),e(SF,hN),e(hN,Qir),e(SF,Vir),e(Me,Wir),e(Me,$F),e($F,Wme),e(Wme,Hir),e($F,Uir),e($F,gN),e(gN,Jir),e($F,Kir),e(Co,Yir),e(Co,Hme),e(Hme,Zir),e(Co,edr),h(Rw,Co,null),v(c,jye,_),v(c,sc,_),e(sc,IF),e(IF,Ume),h(Pw,Ume,null),e(sc,odr),e(sc,Jme),e(Jme,rdr),v(c,Oye,_),v(c,Er,_),h(Sw,Er,null),e(Er,tdr),e(Er,lc),e(lc,adr),e(lc,Kme),e(Kme,ndr),e(lc,sdr),e(lc,Yme),e(Yme,ldr),e(lc,idr),e(Er,ddr),e(Er,$w),e($w,cdr),e($w,Zme),e(Zme,fdr),e($w,mdr),e(Er,hdr),e(Er,ft),h(Iw,ft,null),e(ft,gdr),e(ft,ehe),e(ehe,udr),e(ft,pdr),e(ft,ic),e(ic,_dr),e(ic,ohe),e(ohe,vdr),e(ic,bdr),e(ic,rhe),e(rhe,Tdr),e(ic,Fdr),e(ft,Edr),e(ft,the),e(the,Cdr),e(ft,Mdr),h(Dw,ft,null),e(Er,ydr),e(Er,Mo),h(Nw,Mo,null),e(Mo,wdr),e(Mo,ahe),e(ahe,Adr),e(Mo,Ldr),e(Mo,sn),e(sn,Bdr),e(sn,nhe),e(nhe,xdr),e(sn,kdr),e(sn,she),e(she,Rdr),e(sn,Pdr),e(sn,lhe),e(lhe,Sdr),e(sn,$dr),e(Mo,Idr),e(Mo,Re),e(Re,DF),e(DF,ihe),e(ihe,Ddr),e(DF,Ndr),e(DF,uN),e(uN,jdr),e(DF,Odr),e(Re,Gdr),e(Re,NF),e(NF,dhe),e(dhe,qdr),e(NF,zdr),e(NF,pN),e(pN,Xdr),e(NF,Qdr),e(Re,Vdr),e(Re,jF),e(jF,che),e(che,Wdr),e(jF,Hdr),e(jF,_N),e(_N,Udr),e(jF,Jdr),e(Re,Kdr),e(Re,OF),e(OF,fhe),e(fhe,Ydr),e(OF,Zdr),e(OF,vN),e(vN,ecr),e(OF,ocr),e(Re,rcr),e(Re,GF),e(GF,mhe),e(mhe,tcr),e(GF,acr),e(GF,bN),e(bN,ncr),e(GF,scr),e(Re,lcr),e(Re,qF),e(qF,hhe),e(hhe,icr),e(qF,dcr),e(qF,TN),e(TN,ccr),e(qF,fcr),e(Re,mcr),e(Re,zF),e(zF,ghe),e(ghe,hcr),e(zF,gcr),e(zF,FN),e(FN,ucr),e(zF,pcr),e(Re,_cr),e(Re,XF),e(XF,uhe),e(uhe,vcr),e(XF,bcr),e(XF,EN),e(EN,Tcr),e(XF,Fcr),e(Mo,Ecr),e(Mo,phe),e(phe,Ccr),e(Mo,Mcr),h(jw,Mo,null),v(c,Gye,_),v(c,dc,_),e(dc,QF),e(QF,_he),h(Ow,_he,null),e(dc,ycr),e(dc,vhe),e(vhe,wcr),v(c,qye,_),v(c,Cr,_),h(Gw,Cr,null),e(Cr,Acr),e(Cr,cc),e(cc,Lcr),e(cc,bhe),e(bhe,Bcr),e(cc,xcr),e(cc,The),e(The,kcr),e(cc,Rcr),e(Cr,Pcr),e(Cr,qw),e(qw,Scr),e(qw,Fhe),e(Fhe,$cr),e(qw,Icr),e(Cr,Dcr),e(Cr,mt),h(zw,mt,null),e(mt,Ncr),e(mt,Ehe),e(Ehe,jcr),e(mt,Ocr),e(mt,fc),e(fc,Gcr),e(fc,Che),e(Che,qcr),e(fc,zcr),e(fc,Mhe),e(Mhe,Xcr),e(fc,Qcr),e(mt,Vcr),e(mt,yhe),e(yhe,Wcr),e(mt,Hcr),h(Xw,mt,null),e(Cr,Ucr),e(Cr,yo),h(Qw,yo,null),e(yo,Jcr),e(yo,whe),e(whe,Kcr),e(yo,Ycr),e(yo,ln),e(ln,Zcr),e(ln,Ahe),e(Ahe,efr),e(ln,ofr),e(ln,Lhe),e(Lhe,rfr),e(ln,tfr),e(ln,Bhe),e(Bhe,afr),e(ln,nfr),e(yo,sfr),e(yo,Pe),e(Pe,VF),e(VF,xhe),e(xhe,lfr),e(VF,ifr),e(VF,CN),e(CN,dfr),e(VF,cfr),e(Pe,ffr),e(Pe,WF),e(WF,khe),e(khe,mfr),e(WF,hfr),e(WF,MN),e(MN,gfr),e(WF,ufr),e(Pe,pfr),e(Pe,HF),e(HF,Rhe),e(Rhe,_fr),e(HF,vfr),e(HF,yN),e(yN,bfr),e(HF,Tfr),e(Pe,Ffr),e(Pe,UF),e(UF,Phe),e(Phe,Efr),e(UF,Cfr),e(UF,wN),e(wN,Mfr),e(UF,yfr),e(Pe,wfr),e(Pe,JF),e(JF,She),e(She,Afr),e(JF,Lfr),e(JF,AN),e(AN,Bfr),e(JF,xfr),e(Pe,kfr),e(Pe,KF),e(KF,$he),e($he,Rfr),e(KF,Pfr),e(KF,LN),e(LN,Sfr),e(KF,$fr),e(Pe,Ifr),e(Pe,YF),e(YF,Ihe),e(Ihe,Dfr),e(YF,Nfr),e(YF,BN),e(BN,jfr),e(YF,Ofr),e(Pe,Gfr),e(Pe,ZF),e(ZF,Dhe),e(Dhe,qfr),e(ZF,zfr),e(ZF,xN),e(xN,Xfr),e(ZF,Qfr),e(yo,Vfr),e(yo,Nhe),e(Nhe,Wfr),e(yo,Hfr),h(Vw,yo,null),v(c,zye,_),v(c,mc,_),e(mc,eE),e(eE,jhe),h(Ww,jhe,null),e(mc,Ufr),e(mc,Ohe),e(Ohe,Jfr),v(c,Xye,_),v(c,Mr,_),h(Hw,Mr,null),e(Mr,Kfr),e(Mr,hc),e(hc,Yfr),e(hc,Ghe),e(Ghe,Zfr),e(hc,emr),e(hc,qhe),e(qhe,omr),e(hc,rmr),e(Mr,tmr),e(Mr,Uw),e(Uw,amr),e(Uw,zhe),e(zhe,nmr),e(Uw,smr),e(Mr,lmr),e(Mr,ht),h(Jw,ht,null),e(ht,imr),e(ht,Xhe),e(Xhe,dmr),e(ht,cmr),e(ht,gc),e(gc,fmr),e(gc,Qhe),e(Qhe,mmr),e(gc,hmr),e(gc,Vhe),e(Vhe,gmr),e(gc,umr),e(ht,pmr),e(ht,Whe),e(Whe,_mr),e(ht,vmr),h(Kw,ht,null),e(Mr,bmr),e(Mr,wo),h(Yw,wo,null),e(wo,Tmr),e(wo,Hhe),e(Hhe,Fmr),e(wo,Emr),e(wo,dn),e(dn,Cmr),e(dn,Uhe),e(Uhe,Mmr),e(dn,ymr),e(dn,Jhe),e(Jhe,wmr),e(dn,Amr),e(dn,Khe),e(Khe,Lmr),e(dn,Bmr),e(wo,xmr),e(wo,yr),e(yr,oE),e(oE,Yhe),e(Yhe,kmr),e(oE,Rmr),e(oE,kN),e(kN,Pmr),e(oE,Smr),e(yr,$mr),e(yr,rE),e(rE,Zhe),e(Zhe,Imr),e(rE,Dmr),e(rE,RN),e(RN,Nmr),e(rE,jmr),e(yr,Omr),e(yr,tE),e(tE,ege),e(ege,Gmr),e(tE,qmr),e(tE,PN),e(PN,zmr),e(tE,Xmr),e(yr,Qmr),e(yr,aE),e(aE,oge),e(oge,Vmr),e(aE,Wmr),e(aE,SN),e(SN,Hmr),e(aE,Umr),e(yr,Jmr),e(yr,nE),e(nE,rge),e(rge,Kmr),e(nE,Ymr),e(nE,$N),e($N,Zmr),e(nE,ehr),e(yr,ohr),e(yr,sE),e(sE,tge),e(tge,rhr),e(sE,thr),e(sE,IN),e(IN,ahr),e(sE,nhr),e(wo,shr),e(wo,age),e(age,lhr),e(wo,ihr),h(Zw,wo,null),v(c,Qye,_),v(c,uc,_),e(uc,lE),e(lE,nge),h(e7,nge,null),e(uc,dhr),e(uc,sge),e(sge,chr),v(c,Vye,_),v(c,wr,_),h(o7,wr,null),e(wr,fhr),e(wr,pc),e(pc,mhr),e(pc,lge),e(lge,hhr),e(pc,ghr),e(pc,ige),e(ige,uhr),e(pc,phr),e(wr,_hr),e(wr,r7),e(r7,vhr),e(r7,dge),e(dge,bhr),e(r7,Thr),e(wr,Fhr),e(wr,gt),h(t7,gt,null),e(gt,Ehr),e(gt,cge),e(cge,Chr),e(gt,Mhr),e(gt,_c),e(_c,yhr),e(_c,fge),e(fge,whr),e(_c,Ahr),e(_c,mge),e(mge,Lhr),e(_c,Bhr),e(gt,xhr),e(gt,hge),e(hge,khr),e(gt,Rhr),h(a7,gt,null),e(wr,Phr),e(wr,Ao),h(n7,Ao,null),e(Ao,Shr),e(Ao,gge),e(gge,$hr),e(Ao,Ihr),e(Ao,cn),e(cn,Dhr),e(cn,uge),e(uge,Nhr),e(cn,jhr),e(cn,pge),e(pge,Ohr),e(cn,Ghr),e(cn,_ge),e(_ge,qhr),e(cn,zhr),e(Ao,Xhr),e(Ao,Ar),e(Ar,iE),e(iE,vge),e(vge,Qhr),e(iE,Vhr),e(iE,DN),e(DN,Whr),e(iE,Hhr),e(Ar,Uhr),e(Ar,dE),e(dE,bge),e(bge,Jhr),e(dE,Khr),e(dE,NN),e(NN,Yhr),e(dE,Zhr),e(Ar,egr),e(Ar,cE),e(cE,Tge),e(Tge,ogr),e(cE,rgr),e(cE,jN),e(jN,tgr),e(cE,agr),e(Ar,ngr),e(Ar,fE),e(fE,Fge),e(Fge,sgr),e(fE,lgr),e(fE,ON),e(ON,igr),e(fE,dgr),e(Ar,cgr),e(Ar,mE),e(mE,Ege),e(Ege,fgr),e(mE,mgr),e(mE,GN),e(GN,hgr),e(mE,ggr),e(Ar,ugr),e(Ar,hE),e(hE,Cge),e(Cge,pgr),e(hE,_gr),e(hE,qN),e(qN,vgr),e(hE,bgr),e(Ao,Tgr),e(Ao,Mge),e(Mge,Fgr),e(Ao,Egr),h(s7,Ao,null),v(c,Wye,_),v(c,vc,_),e(vc,gE),e(gE,yge),h(l7,yge,null),e(vc,Cgr),e(vc,wge),e(wge,Mgr),v(c,Hye,_),v(c,Lr,_),h(i7,Lr,null),e(Lr,ygr),e(Lr,bc),e(bc,wgr),e(bc,Age),e(Age,Agr),e(bc,Lgr),e(bc,Lge),e(Lge,Bgr),e(bc,xgr),e(Lr,kgr),e(Lr,d7),e(d7,Rgr),e(d7,Bge),e(Bge,Pgr),e(d7,Sgr),e(Lr,$gr),e(Lr,ut),h(c7,ut,null),e(ut,Igr),e(ut,xge),e(xge,Dgr),e(ut,Ngr),e(ut,Tc),e(Tc,jgr),e(Tc,kge),e(kge,Ogr),e(Tc,Ggr),e(Tc,Rge),e(Rge,qgr),e(Tc,zgr),e(ut,Xgr),e(ut,Pge),e(Pge,Qgr),e(ut,Vgr),h(f7,ut,null),e(Lr,Wgr),e(Lr,Lo),h(m7,Lo,null),e(Lo,Hgr),e(Lo,Sge),e(Sge,Ugr),e(Lo,Jgr),e(Lo,fn),e(fn,Kgr),e(fn,$ge),e($ge,Ygr),e(fn,Zgr),e(fn,Ige),e(Ige,eur),e(fn,our),e(fn,Dge),e(Dge,rur),e(fn,tur),e(Lo,aur),e(Lo,Nge),e(Nge,uE),e(uE,jge),e(jge,nur),e(uE,sur),e(uE,zN),e(zN,lur),e(uE,iur),e(Lo,dur),e(Lo,Oge),e(Oge,cur),e(Lo,fur),h(h7,Lo,null),v(c,Uye,_),v(c,Fc,_),e(Fc,pE),e(pE,Gge),h(g7,Gge,null),e(Fc,mur),e(Fc,qge),e(qge,hur),v(c,Jye,_),v(c,Br,_),h(u7,Br,null),e(Br,gur),e(Br,Ec),e(Ec,uur),e(Ec,zge),e(zge,pur),e(Ec,_ur),e(Ec,Xge),e(Xge,vur),e(Ec,bur),e(Br,Tur),e(Br,p7),e(p7,Fur),e(p7,Qge),e(Qge,Eur),e(p7,Cur),e(Br,Mur),e(Br,pt),h(_7,pt,null),e(pt,yur),e(pt,Vge),e(Vge,wur),e(pt,Aur),e(pt,Cc),e(Cc,Lur),e(Cc,Wge),e(Wge,Bur),e(Cc,xur),e(Cc,Hge),e(Hge,kur),e(Cc,Rur),e(pt,Pur),e(pt,Uge),e(Uge,Sur),e(pt,$ur),h(v7,pt,null),e(Br,Iur),e(Br,Bo),h(b7,Bo,null),e(Bo,Dur),e(Bo,Jge),e(Jge,Nur),e(Bo,jur),e(Bo,mn),e(mn,Our),e(mn,Kge),e(Kge,Gur),e(mn,qur),e(mn,Yge),e(Yge,zur),e(mn,Xur),e(mn,Zge),e(Zge,Qur),e(mn,Vur),e(Bo,Wur),e(Bo,T7),e(T7,_E),e(_E,eue),e(eue,Hur),e(_E,Uur),e(_E,XN),e(XN,Jur),e(_E,Kur),e(T7,Yur),e(T7,vE),e(vE,oue),e(oue,Zur),e(vE,epr),e(vE,QN),e(QN,opr),e(vE,rpr),e(Bo,tpr),e(Bo,rue),e(rue,apr),e(Bo,npr),h(F7,Bo,null),v(c,Kye,_),v(c,Mc,_),e(Mc,bE),e(bE,tue),h(E7,tue,null),e(Mc,spr),e(Mc,aue),e(aue,lpr),v(c,Yye,_),v(c,xr,_),h(C7,xr,null),e(xr,ipr),e(xr,yc),e(yc,dpr),e(yc,nue),e(nue,cpr),e(yc,fpr),e(yc,sue),e(sue,mpr),e(yc,hpr),e(xr,gpr),e(xr,M7),e(M7,upr),e(M7,lue),e(lue,ppr),e(M7,_pr),e(xr,vpr),e(xr,_t),h(y7,_t,null),e(_t,bpr),e(_t,iue),e(iue,Tpr),e(_t,Fpr),e(_t,wc),e(wc,Epr),e(wc,due),e(due,Cpr),e(wc,Mpr),e(wc,cue),e(cue,ypr),e(wc,wpr),e(_t,Apr),e(_t,fue),e(fue,Lpr),e(_t,Bpr),h(w7,_t,null),e(xr,xpr),e(xr,xo),h(A7,xo,null),e(xo,kpr),e(xo,mue),e(mue,Rpr),e(xo,Ppr),e(xo,hn),e(hn,Spr),e(hn,hue),e(hue,$pr),e(hn,Ipr),e(hn,gue),e(gue,Dpr),e(hn,Npr),e(hn,uue),e(uue,jpr),e(hn,Opr),e(xo,Gpr),e(xo,pue),e(pue,TE),e(TE,_ue),e(_ue,qpr),e(TE,zpr),e(TE,VN),e(VN,Xpr),e(TE,Qpr),e(xo,Vpr),e(xo,vue),e(vue,Wpr),e(xo,Hpr),h(L7,xo,null),Zye=!0},p(c,[_]){const B7={};_&2&&(B7.$$scope={dirty:_,ctx:c}),Pc.$set(B7);const bue={};_&2&&(bue.$$scope={dirty:_,ctx:c}),Wm.$set(bue);const Tue={};_&2&&(Tue.$$scope={dirty:_,ctx:c}),rh.$set(Tue)},i(c){Zye||(g(ge.$$.fragment,c),g(_a.$$.fragment,c),g(EC.$$.fragment,c),g(CC.$$.fragment,c),g(Pc.$$.fragment,c),g(MC.$$.fragment,c),g(yC.$$.fragment,c),g(LC.$$.fragment,c),g(BC.$$.fragment,c),g(xC.$$.fragment,c),g(kC.$$.fragment,c),g(RC.$$.fragment,c),g($C.$$.fragment,c),g(GC.$$.fragment,c),g(qC.$$.fragment,c),g(zC.$$.fragment,c),g(XC.$$.fragment,c),g(WC.$$.fragment,c),g(Wm.$$.fragment,c),g(YC.$$.fragment,c),g(ZC.$$.fragment,c),g(e3.$$.fragment,c),g(t3.$$.fragment,c),g(rh.$$.fragment,c),g(d3.$$.fragment,c),g(c3.$$.fragment,c),g(f3.$$.fragment,c),g(h3.$$.fragment,c),g(g3.$$.fragment,c),g(u3.$$.fragment,c),g(p3.$$.fragment,c),g(_3.$$.fragment,c),g(v3.$$.fragment,c),g(T3.$$.fragment,c),g(F3.$$.fragment,c),g(E3.$$.fragment,c),g(C3.$$.fragment,c),g(M3.$$.fragment,c),g(y3.$$.fragment,c),g(A3.$$.fragment,c),g(L3.$$.fragment,c),g(B3.$$.fragment,c),g(x3.$$.fragment,c),g(k3.$$.fragment,c),g(R3.$$.fragment,c),g(S3.$$.fragment,c),g($3.$$.fragment,c),g(I3.$$.fragment,c),g(D3.$$.fragment,c),g(N3.$$.fragment,c),g(j3.$$.fragment,c),g(G3.$$.fragment,c),g(q3.$$.fragment,c),g(z3.$$.fragment,c),g(X3.$$.fragment,c),g(Q3.$$.fragment,c),g(V3.$$.fragment,c),g(H3.$$.fragment,c),g(U3.$$.fragment,c),g(J3.$$.fragment,c),g(K3.$$.fragment,c),g(Y3.$$.fragment,c),g(Z3.$$.fragment,c),g(oM.$$.fragment,c),g(rM.$$.fragment,c),g(tM.$$.fragment,c),g(aM.$$.fragment,c),g(nM.$$.fragment,c),g(sM.$$.fragment,c),g(iM.$$.fragment,c),g(dM.$$.fragment,c),g(cM.$$.fragment,c),g(fM.$$.fragment,c),g(mM.$$.fragment,c),g(hM.$$.fragment,c),g(uM.$$.fragment,c),g(pM.$$.fragment,c),g(_M.$$.fragment,c),g(vM.$$.fragment,c),g(bM.$$.fragment,c),g(TM.$$.fragment,c),g(EM.$$.fragment,c),g(CM.$$.fragment,c),g(MM.$$.fragment,c),g(yM.$$.fragment,c),g(wM.$$.fragment,c),g(AM.$$.fragment,c),g(BM.$$.fragment,c),g(xM.$$.fragment,c),g(kM.$$.fragment,c),g(RM.$$.fragment,c),g(PM.$$.fragment,c),g(SM.$$.fragment,c),g(IM.$$.fragment,c),g(DM.$$.fragment,c),g(NM.$$.fragment,c),g(jM.$$.fragment,c),g(OM.$$.fragment,c),g(GM.$$.fragment,c),g(zM.$$.fragment,c),g(XM.$$.fragment,c),g(QM.$$.fragment,c),g(VM.$$.fragment,c),g(WM.$$.fragment,c),g(HM.$$.fragment,c),g(JM.$$.fragment,c),g(KM.$$.fragment,c),g(YM.$$.fragment,c),g(ZM.$$.fragment,c),g(e5.$$.fragment,c),g(o5.$$.fragment,c),g(t5.$$.fragment,c),g(a5.$$.fragment,c),g(n5.$$.fragment,c),g(s5.$$.fragment,c),g(l5.$$.fragment,c),g(i5.$$.fragment,c),g(c5.$$.fragment,c),g(f5.$$.fragment,c),g(m5.$$.fragment,c),g(g5.$$.fragment,c),g(u5.$$.fragment,c),g(p5.$$.fragment,c),g(v5.$$.fragment,c),g(b5.$$.fragment,c),g(T5.$$.fragment,c),g(F5.$$.fragment,c),g(E5.$$.fragment,c),g(C5.$$.fragment,c),g(y5.$$.fragment,c),g(w5.$$.fragment,c),g(A5.$$.fragment,c),g(L5.$$.fragment,c),g(B5.$$.fragment,c),g(x5.$$.fragment,c),g(R5.$$.fragment,c),g(P5.$$.fragment,c),g(S5.$$.fragment,c),g($5.$$.fragment,c),g(I5.$$.fragment,c),g(D5.$$.fragment,c),g(j5.$$.fragment,c),g(O5.$$.fragment,c),g(G5.$$.fragment,c),g(q5.$$.fragment,c),g(z5.$$.fragment,c),g(X5.$$.fragment,c),g(V5.$$.fragment,c),g(W5.$$.fragment,c),g(H5.$$.fragment,c),g(U5.$$.fragment,c),g(J5.$$.fragment,c),g(K5.$$.fragment,c),g(Z5.$$.fragment,c),g(ey.$$.fragment,c),g(oy.$$.fragment,c),g(ry.$$.fragment,c),g(ty.$$.fragment,c),g(ay.$$.fragment,c),g(sy.$$.fragment,c),g(ly.$$.fragment,c),g(iy.$$.fragment,c),g(dy.$$.fragment,c),g(cy.$$.fragment,c),g(fy.$$.fragment,c),g(hy.$$.fragment,c),g(gy.$$.fragment,c),g(uy.$$.fragment,c),g(py.$$.fragment,c),g(_y.$$.fragment,c),g(vy.$$.fragment,c),g(Ty.$$.fragment,c),g(Fy.$$.fragment,c),g(Ey.$$.fragment,c),g(Cy.$$.fragment,c),g(My.$$.fragment,c),g(yy.$$.fragment,c),g(Ay.$$.fragment,c),g(Ly.$$.fragment,c),g(By.$$.fragment,c),g(xy.$$.fragment,c),g(ky.$$.fragment,c),g(Ry.$$.fragment,c),g(Sy.$$.fragment,c),g($y.$$.fragment,c),g(Iy.$$.fragment,c),g(Dy.$$.fragment,c),g(Ny.$$.fragment,c),g(jy.$$.fragment,c),g(Gy.$$.fragment,c),g(qy.$$.fragment,c),g(zy.$$.fragment,c),g(Xy.$$.fragment,c),g(Qy.$$.fragment,c),g(Vy.$$.fragment,c),g(Hy.$$.fragment,c),g(Uy.$$.fragment,c),g(Jy.$$.fragment,c),g(Ky.$$.fragment,c),g(Yy.$$.fragment,c),g(Zy.$$.fragment,c),g(ow.$$.fragment,c),g(rw.$$.fragment,c),g(tw.$$.fragment,c),g(aw.$$.fragment,c),g(nw.$$.fragment,c),g(sw.$$.fragment,c),g(iw.$$.fragment,c),g(dw.$$.fragment,c),g(cw.$$.fragment,c),g(fw.$$.fragment,c),g(mw.$$.fragment,c),g(hw.$$.fragment,c),g(uw.$$.fragment,c),g(pw.$$.fragment,c),g(_w.$$.fragment,c),g(vw.$$.fragment,c),g(bw.$$.fragment,c),g(Tw.$$.fragment,c),g(Ew.$$.fragment,c),g(Cw.$$.fragment,c),g(Mw.$$.fragment,c),g(yw.$$.fragment,c),g(ww.$$.fragment,c),g(Aw.$$.fragment,c),g(Bw.$$.fragment,c),g(xw.$$.fragment,c),g(kw.$$.fragment,c),g(Rw.$$.fragment,c),g(Pw.$$.fragment,c),g(Sw.$$.fragment,c),g(Iw.$$.fragment,c),g(Dw.$$.fragment,c),g(Nw.$$.fragment,c),g(jw.$$.fragment,c),g(Ow.$$.fragment,c),g(Gw.$$.fragment,c),g(zw.$$.fragment,c),g(Xw.$$.fragment,c),g(Qw.$$.fragment,c),g(Vw.$$.fragment,c),g(Ww.$$.fragment,c),g(Hw.$$.fragment,c),g(Jw.$$.fragment,c),g(Kw.$$.fragment,c),g(Yw.$$.fragment,c),g(Zw.$$.fragment,c),g(e7.$$.fragment,c),g(o7.$$.fragment,c),g(t7.$$.fragment,c),g(a7.$$.fragment,c),g(n7.$$.fragment,c),g(s7.$$.fragment,c),g(l7.$$.fragment,c),g(i7.$$.fragment,c),g(c7.$$.fragment,c),g(f7.$$.fragment,c),g(m7.$$.fragment,c),g(h7.$$.fragment,c),g(g7.$$.fragment,c),g(u7.$$.fragment,c),g(_7.$$.fragment,c),g(v7.$$.fragment,c),g(b7.$$.fragment,c),g(F7.$$.fragment,c),g(E7.$$.fragment,c),g(C7.$$.fragment,c),g(y7.$$.fragment,c),g(w7.$$.fragment,c),g(A7.$$.fragment,c),g(L7.$$.fragment,c),Zye=!0)},o(c){u(ge.$$.fragment,c),u(_a.$$.fragment,c),u(EC.$$.fragment,c),u(CC.$$.fragment,c),u(Pc.$$.fragment,c),u(MC.$$.fragment,c),u(yC.$$.fragment,c),u(LC.$$.fragment,c),u(BC.$$.fragment,c),u(xC.$$.fragment,c),u(kC.$$.fragment,c),u(RC.$$.fragment,c),u($C.$$.fragment,c),u(GC.$$.fragment,c),u(qC.$$.fragment,c),u(zC.$$.fragment,c),u(XC.$$.fragment,c),u(WC.$$.fragment,c),u(Wm.$$.fragment,c),u(YC.$$.fragment,c),u(ZC.$$.fragment,c),u(e3.$$.fragment,c),u(t3.$$.fragment,c),u(rh.$$.fragment,c),u(d3.$$.fragment,c),u(c3.$$.fragment,c),u(f3.$$.fragment,c),u(h3.$$.fragment,c),u(g3.$$.fragment,c),u(u3.$$.fragment,c),u(p3.$$.fragment,c),u(_3.$$.fragment,c),u(v3.$$.fragment,c),u(T3.$$.fragment,c),u(F3.$$.fragment,c),u(E3.$$.fragment,c),u(C3.$$.fragment,c),u(M3.$$.fragment,c),u(y3.$$.fragment,c),u(A3.$$.fragment,c),u(L3.$$.fragment,c),u(B3.$$.fragment,c),u(x3.$$.fragment,c),u(k3.$$.fragment,c),u(R3.$$.fragment,c),u(S3.$$.fragment,c),u($3.$$.fragment,c),u(I3.$$.fragment,c),u(D3.$$.fragment,c),u(N3.$$.fragment,c),u(j3.$$.fragment,c),u(G3.$$.fragment,c),u(q3.$$.fragment,c),u(z3.$$.fragment,c),u(X3.$$.fragment,c),u(Q3.$$.fragment,c),u(V3.$$.fragment,c),u(H3.$$.fragment,c),u(U3.$$.fragment,c),u(J3.$$.fragment,c),u(K3.$$.fragment,c),u(Y3.$$.fragment,c),u(Z3.$$.fragment,c),u(oM.$$.fragment,c),u(rM.$$.fragment,c),u(tM.$$.fragment,c),u(aM.$$.fragment,c),u(nM.$$.fragment,c),u(sM.$$.fragment,c),u(iM.$$.fragment,c),u(dM.$$.fragment,c),u(cM.$$.fragment,c),u(fM.$$.fragment,c),u(mM.$$.fragment,c),u(hM.$$.fragment,c),u(uM.$$.fragment,c),u(pM.$$.fragment,c),u(_M.$$.fragment,c),u(vM.$$.fragment,c),u(bM.$$.fragment,c),u(TM.$$.fragment,c),u(EM.$$.fragment,c),u(CM.$$.fragment,c),u(MM.$$.fragment,c),u(yM.$$.fragment,c),u(wM.$$.fragment,c),u(AM.$$.fragment,c),u(BM.$$.fragment,c),u(xM.$$.fragment,c),u(kM.$$.fragment,c),u(RM.$$.fragment,c),u(PM.$$.fragment,c),u(SM.$$.fragment,c),u(IM.$$.fragment,c),u(DM.$$.fragment,c),u(NM.$$.fragment,c),u(jM.$$.fragment,c),u(OM.$$.fragment,c),u(GM.$$.fragment,c),u(zM.$$.fragment,c),u(XM.$$.fragment,c),u(QM.$$.fragment,c),u(VM.$$.fragment,c),u(WM.$$.fragment,c),u(HM.$$.fragment,c),u(JM.$$.fragment,c),u(KM.$$.fragment,c),u(YM.$$.fragment,c),u(ZM.$$.fragment,c),u(e5.$$.fragment,c),u(o5.$$.fragment,c),u(t5.$$.fragment,c),u(a5.$$.fragment,c),u(n5.$$.fragment,c),u(s5.$$.fragment,c),u(l5.$$.fragment,c),u(i5.$$.fragment,c),u(c5.$$.fragment,c),u(f5.$$.fragment,c),u(m5.$$.fragment,c),u(g5.$$.fragment,c),u(u5.$$.fragment,c),u(p5.$$.fragment,c),u(v5.$$.fragment,c),u(b5.$$.fragment,c),u(T5.$$.fragment,c),u(F5.$$.fragment,c),u(E5.$$.fragment,c),u(C5.$$.fragment,c),u(y5.$$.fragment,c),u(w5.$$.fragment,c),u(A5.$$.fragment,c),u(L5.$$.fragment,c),u(B5.$$.fragment,c),u(x5.$$.fragment,c),u(R5.$$.fragment,c),u(P5.$$.fragment,c),u(S5.$$.fragment,c),u($5.$$.fragment,c),u(I5.$$.fragment,c),u(D5.$$.fragment,c),u(j5.$$.fragment,c),u(O5.$$.fragment,c),u(G5.$$.fragment,c),u(q5.$$.fragment,c),u(z5.$$.fragment,c),u(X5.$$.fragment,c),u(V5.$$.fragment,c),u(W5.$$.fragment,c),u(H5.$$.fragment,c),u(U5.$$.fragment,c),u(J5.$$.fragment,c),u(K5.$$.fragment,c),u(Z5.$$.fragment,c),u(ey.$$.fragment,c),u(oy.$$.fragment,c),u(ry.$$.fragment,c),u(ty.$$.fragment,c),u(ay.$$.fragment,c),u(sy.$$.fragment,c),u(ly.$$.fragment,c),u(iy.$$.fragment,c),u(dy.$$.fragment,c),u(cy.$$.fragment,c),u(fy.$$.fragment,c),u(hy.$$.fragment,c),u(gy.$$.fragment,c),u(uy.$$.fragment,c),u(py.$$.fragment,c),u(_y.$$.fragment,c),u(vy.$$.fragment,c),u(Ty.$$.fragment,c),u(Fy.$$.fragment,c),u(Ey.$$.fragment,c),u(Cy.$$.fragment,c),u(My.$$.fragment,c),u(yy.$$.fragment,c),u(Ay.$$.fragment,c),u(Ly.$$.fragment,c),u(By.$$.fragment,c),u(xy.$$.fragment,c),u(ky.$$.fragment,c),u(Ry.$$.fragment,c),u(Sy.$$.fragment,c),u($y.$$.fragment,c),u(Iy.$$.fragment,c),u(Dy.$$.fragment,c),u(Ny.$$.fragment,c),u(jy.$$.fragment,c),u(Gy.$$.fragment,c),u(qy.$$.fragment,c),u(zy.$$.fragment,c),u(Xy.$$.fragment,c),u(Qy.$$.fragment,c),u(Vy.$$.fragment,c),u(Hy.$$.fragment,c),u(Uy.$$.fragment,c),u(Jy.$$.fragment,c),u(Ky.$$.fragment,c),u(Yy.$$.fragment,c),u(Zy.$$.fragment,c),u(ow.$$.fragment,c),u(rw.$$.fragment,c),u(tw.$$.fragment,c),u(aw.$$.fragment,c),u(nw.$$.fragment,c),u(sw.$$.fragment,c),u(iw.$$.fragment,c),u(dw.$$.fragment,c),u(cw.$$.fragment,c),u(fw.$$.fragment,c),u(mw.$$.fragment,c),u(hw.$$.fragment,c),u(uw.$$.fragment,c),u(pw.$$.fragment,c),u(_w.$$.fragment,c),u(vw.$$.fragment,c),u(bw.$$.fragment,c),u(Tw.$$.fragment,c),u(Ew.$$.fragment,c),u(Cw.$$.fragment,c),u(Mw.$$.fragment,c),u(yw.$$.fragment,c),u(ww.$$.fragment,c),u(Aw.$$.fragment,c),u(Bw.$$.fragment,c),u(xw.$$.fragment,c),u(kw.$$.fragment,c),u(Rw.$$.fragment,c),u(Pw.$$.fragment,c),u(Sw.$$.fragment,c),u(Iw.$$.fragment,c),u(Dw.$$.fragment,c),u(Nw.$$.fragment,c),u(jw.$$.fragment,c),u(Ow.$$.fragment,c),u(Gw.$$.fragment,c),u(zw.$$.fragment,c),u(Xw.$$.fragment,c),u(Qw.$$.fragment,c),u(Vw.$$.fragment,c),u(Ww.$$.fragment,c),u(Hw.$$.fragment,c),u(Jw.$$.fragment,c),u(Kw.$$.fragment,c),u(Yw.$$.fragment,c),u(Zw.$$.fragment,c),u(e7.$$.fragment,c),u(o7.$$.fragment,c),u(t7.$$.fragment,c),u(a7.$$.fragment,c),u(n7.$$.fragment,c),u(s7.$$.fragment,c),u(l7.$$.fragment,c),u(i7.$$.fragment,c),u(c7.$$.fragment,c),u(f7.$$.fragment,c),u(m7.$$.fragment,c),u(h7.$$.fragment,c),u(g7.$$.fragment,c),u(u7.$$.fragment,c),u(_7.$$.fragment,c),u(v7.$$.fragment,c),u(b7.$$.fragment,c),u(F7.$$.fragment,c),u(E7.$$.fragment,c),u(C7.$$.fragment,c),u(y7.$$.fragment,c),u(w7.$$.fragment,c),u(A7.$$.fragment,c),u(L7.$$.fragment,c),Zye=!1},d(c){t(re),c&&t(Se),c&&t(me),p(ge),c&&t(Lc),c&&t(Ot),c&&t(xe),c&&t(ao),c&&t(xc),p(_a,c),c&&t(no),c&&t(pe),c&&t(Io),c&&t(va),c&&t(m5e),c&&t(Jl),p(EC),c&&t(h5e),c&&t(vn),c&&t(g5e),p(CC,c),c&&t(u5e),c&&t(A0),c&&t(p5e),p(Pc,c),c&&t(_5e),c&&t(Kl),p(MC),c&&t(v5e),c&&t(Do),p(yC),p(LC),p(BC),p(xC),c&&t(b5e),c&&t(Zl),p(kC),c&&t(T5e),c&&t(No),p(RC),p($C),p(GC),p(qC),c&&t(F5e),c&&t(ti),p(zC),c&&t(E5e),c&&t(Dt),p(XC),p(WC),p(Wm),p(YC),c&&t(C5e),c&&t(li),p(ZC),c&&t(M5e),c&&t(Nt),p(e3),p(t3),p(rh),p(d3),c&&t(y5e),c&&t(fi),p(c3),c&&t(w5e),c&&t(jo),p(f3),p(h3),p(g3),p(u3),p(p3),c&&t(A5e),c&&t(gi),p(_3),c&&t(L5e),c&&t(Oo),p(v3),p(T3),p(F3),p(E3),p(C3),c&&t(B5e),c&&t(_i),p(M3),c&&t(x5e),c&&t(Go),p(y3),p(A3),p(L3),p(B3),p(x3),c&&t(k5e),c&&t(Ti),p(k3),c&&t(R5e),c&&t(qo),p(R3),p(S3),p($3),p(I3),p(D3),c&&t(P5e),c&&t(Ci),p(N3),c&&t(S5e),c&&t(zo),p(j3),p(G3),p(q3),p(z3),p(X3),c&&t($5e),c&&t(wi),p(Q3),c&&t(I5e),c&&t(Xo),p(V3),p(H3),p(U3),p(J3),p(K3),c&&t(D5e),c&&t(Bi),p(Y3),c&&t(N5e),c&&t(Qo),p(Z3),p(oM),p(rM),p(tM),p(aM),c&&t(j5e),c&&t(Ri),p(nM),c&&t(O5e),c&&t(Vo),p(sM),p(iM),p(dM),p(cM),p(fM),c&&t(G5e),c&&t($i),p(mM),c&&t(q5e),c&&t(Wo),p(hM),p(uM),p(pM),p(_M),p(vM),c&&t(z5e),c&&t(Ni),p(bM),c&&t(X5e),c&&t(Ho),p(TM),p(EM),p(CM),p(MM),p(yM),c&&t(Q5e),c&&t(Gi),p(wM),c&&t(V5e),c&&t(Uo),p(AM),p(BM),p(xM),p(kM),p(RM),c&&t(W5e),c&&t(Xi),p(PM),c&&t(H5e),c&&t(Jo),p(SM),p(IM),p(DM),p(NM),p(jM),c&&t(U5e),c&&t(Wi),p(OM),c&&t(J5e),c&&t(Yo),p(GM),p(zM),p(XM),p(QM),p(VM),c&&t(K5e),c&&t(Ji),p(WM),c&&t(Y5e),c&&t(Zo),p(HM),p(JM),p(KM),p(YM),p(ZM),c&&t(Z5e),c&&t(Zi),p(e5),c&&t(eye),c&&t(or),p(o5),p(t5),p(a5),p(n5),p(s5),c&&t(oye),c&&t(rd),p(l5),c&&t(rye),c&&t(tr),p(i5),p(c5),p(f5),p(m5),p(g5),c&&t(tye),c&&t(nd),p(u5),c&&t(aye),c&&t(ar),p(p5),p(v5),p(b5),p(T5),p(F5),c&&t(nye),c&&t(id),p(E5),c&&t(sye),c&&t(nr),p(C5),p(y5),p(w5),p(A5),p(L5),c&&t(lye),c&&t(fd),p(B5),c&&t(iye),c&&t(sr),p(x5),p(R5),p(P5),p(S5),p($5),c&&t(dye),c&&t(gd),p(I5),c&&t(cye),c&&t(lr),p(D5),p(j5),p(O5),p(G5),p(q5),c&&t(fye),c&&t(_d),p(z5),c&&t(mye),c&&t(ir),p(X5),p(V5),p(W5),p(H5),p(U5),c&&t(hye),c&&t(Td),p(J5),c&&t(gye),c&&t(dr),p(K5),p(Z5),p(ey),p(oy),p(ry),c&&t(uye),c&&t(Cd),p(ty),c&&t(pye),c&&t(cr),p(ay),p(sy),p(ly),p(iy),p(dy),c&&t(_ye),c&&t(wd),p(cy),c&&t(vye),c&&t(fr),p(fy),p(hy),p(gy),p(uy),p(py),c&&t(bye),c&&t(Bd),p(_y),c&&t(Tye),c&&t(mr),p(vy),p(Ty),p(Fy),p(Ey),p(Cy),c&&t(Fye),c&&t(Rd),p(My),c&&t(Eye),c&&t(hr),p(yy),p(Ay),p(Ly),p(By),p(xy),c&&t(Cye),c&&t($d),p(ky),c&&t(Mye),c&&t(gr),p(Ry),p(Sy),p($y),p(Iy),p(Dy),c&&t(yye),c&&t(Nd),p(Ny),c&&t(wye),c&&t(ur),p(jy),p(Gy),p(qy),p(zy),p(Xy),c&&t(Aye),c&&t(Gd),p(Qy),c&&t(Lye),c&&t(pr),p(Vy),p(Hy),p(Uy),p(Jy),p(Ky),c&&t(Bye),c&&t(Xd),p(Yy),c&&t(xye),c&&t(_r),p(Zy),p(ow),p(rw),p(tw),p(aw),c&&t(kye),c&&t(Wd),p(nw),c&&t(Rye),c&&t(vr),p(sw),p(iw),p(dw),p(cw),p(fw),c&&t(Pye),c&&t(Kd),p(mw),c&&t(Sye),c&&t(br),p(hw),p(uw),p(pw),p(_w),p(vw),c&&t($ye),c&&t(ec),p(bw),c&&t(Iye),c&&t(Tr),p(Tw),p(Ew),p(Cw),p(Mw),p(yw),c&&t(Dye),c&&t(tc),p(ww),c&&t(Nye),c&&t(Fr),p(Aw),p(Bw),p(xw),p(kw),p(Rw),c&&t(jye),c&&t(sc),p(Pw),c&&t(Oye),c&&t(Er),p(Sw),p(Iw),p(Dw),p(Nw),p(jw),c&&t(Gye),c&&t(dc),p(Ow),c&&t(qye),c&&t(Cr),p(Gw),p(zw),p(Xw),p(Qw),p(Vw),c&&t(zye),c&&t(mc),p(Ww),c&&t(Xye),c&&t(Mr),p(Hw),p(Jw),p(Kw),p(Yw),p(Zw),c&&t(Qye),c&&t(uc),p(e7),c&&t(Vye),c&&t(wr),p(o7),p(t7),p(a7),p(n7),p(s7),c&&t(Wye),c&&t(vc),p(l7),c&&t(Hye),c&&t(Lr),p(i7),p(c7),p(f7),p(m7),p(h7),c&&t(Uye),c&&t(Fc),p(g7),c&&t(Jye),c&&t(Br),p(u7),p(_7),p(v7),p(b7),p(F7),c&&t(Kye),c&&t(Mc),p(E7),c&&t(Yye),c&&t(xr),p(C7),p(y7),p(w7),p(A7),p(L7)}}}const dYr={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function cYr(ql,re,Se){let{fw:me}=re;return ql.$$set=ue=>{"fw"in ue&&Se(0,me=ue.fw)},[me]}class _Yr extends oYr{constructor(re){super();rYr(this,re,cYr,iYr,tYr,{fw:0})}}export{_Yr as default,dYr as metadata};
