import{S as Oa,i as Va,s as Wa,e as r,k as p,w as m,t as n,M as Ya,c as o,d as s,m as h,a,x as f,h as i,b as l,F as t,g as c,y as u,L as Ja,q as g,o as _,B as v}from"../../chunks/vendor-ab4e3193.js";import{D as X}from"../../chunks/Docstring-b69c0bd4.js";import{C as Rt}from"../../chunks/CodeBlock-516df0c5.js";import{I as B}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function Ua(Ss){let j,Ye,b,E,ht,le,Is,dt,Fs,Ht,z,ct,Gs,Bs,pe,Rs,Hs,Ot,T,R,mt,he,Os,ft,Vs,Vt,H,Ws,de,Ys,Js,Wt,Je,Us,Yt,Ue,Qs,Jt,Qe,ut,Zs,Ut,O,Ks,ce,er,tr,Qt,q,V,gt,me,sr,_t,rr,Zt,C,fe,or,ue,ar,Ze,nr,ir,Kt,D,W,vt,ge,lr,kt,pr,es,k,_e,hr,x,dr,Ke,cr,mr,et,fr,ur,ve,gr,_r,vr,ke,kr,tt,Pr,$r,Nr,Pe,wr,$e,Lr,Pt,Mr,br,Er,xr,y,Ne,Xr,$t,jr,yr,we,st,zr,Nt,Tr,qr,rt,Cr,wt,Dr,Ar,Y,Le,Sr,Lt,Ir,Fr,J,Me,Gr,Mt,Br,Rr,U,be,Hr,Ee,Or,bt,Vr,Wr,ts,A,Q,Et,xe,Yr,xt,Jr,ss,$,Xe,Ur,je,Qr,ot,Zr,Kr,eo,Xt,to,so,ye,rs,S,Z,jt,ze,ro,yt,oo,os,N,Te,ao,qe,no,at,io,lo,po,zt,ho,co,Ce,as,I,K,Tt,De,mo,qt,fo,ns,w,Ae,uo,Se,go,nt,_o,vo,ko,Ct,Po,$o,Ie,is,F,ee,Dt,Fe,No,At,wo,ls,L,Ge,Lo,Be,Mo,it,bo,Eo,xo,St,Xo,jo,Re,ps,G,te,It,He,yo,Ft,zo,hs,M,Oe,To,Ve,qo,lt,Co,Do,Ao,Gt,So,Io,We,ds;return le=new B({}),he=new B({}),me=new B({}),fe=new X({props:{name:"class transformers.XLMProphetNetConfig",anchor:"transformers.XLMProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py#L29"}}),ge=new B({}),_e=new X({props:{name:"class transformers.XLMProphetNetTokenizer",anchor:"transformers.XLMProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '[SEP]'"},{name:"eos_token",val:" = '[SEP]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"unk_token",val:" = '[UNK]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L57",parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.XLMProphetNetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"}]}}),Ne=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L303",parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Le=new X({props:{name:"convert_tokens_to_string",anchor:"transformers.XLMProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L285"}}),Me=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L234",parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),be=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L206",parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),xe=new B({}),Xe=new X({props:{name:"class transformers.XLMProphetNetModel",anchor:"transformers.XLMProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L82"}}),ye=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetModel

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetModel.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')

input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetModel.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),ze=new B({}),Te=new X({props:{name:"class transformers.XLMProphetNetEncoder",anchor:"transformers.XLMProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L38"}}),Ce=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetEncoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone')
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetEncoder.from_pretrained(<span class="hljs-string">&#x27;patrickvonplaten/xprophetnet-large-uncased-standalone&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),De=new B({}),Ae=new X({props:{name:"class transformers.XLMProphetNetDecoder",anchor:"transformers.XLMProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L60"}}),Ie=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetDecoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone', add_cross_attention=False)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetDecoder.from_pretrained(<span class="hljs-string">&#x27;patrickvonplaten/xprophetnet-large-uncased-standalone&#x27;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Fe=new B({}),Ge=new X({props:{name:"class transformers.XLMProphetNetForConditionalGeneration",anchor:"transformers.XLMProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L105"}}),Re=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model =  XLMProphetNetForConditionalGeneration.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')

input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model =  XLMProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),He=new B({}),Oe=new X({props:{name:"class transformers.XLMProphetNetForCausalLM",anchor:"transformers.XLMProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L128"}}),We=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetForCausalLM.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
import torch

tokenizer_enc = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')
tokenizer_dec = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = EncoderDecoderModel.from_encoder_decoder_pretrained("xlm-roberta-large", 'microsoft/xprophetnet-large-wiki100-cased')

ARTICLE = (
"the us state department said wednesday it had received no "
"formal word from bolivia that it was expelling the us ambassador there "
"but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec("us rejects charges against its ambassador in bolivia", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = XLMRobertaTokenizer.from_pretrained(<span class="hljs-string">&#x27;xlm-roberta-large&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&quot;xlm-roberta-large&quot;</span>, <span class="hljs-string">&#x27;microsoft/xprophetnet-large-wiki100-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span><span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span><span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span><span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(<span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){j=r("meta"),Ye=p(),b=r("h1"),E=r("a"),ht=r("span"),m(le.$$.fragment),Is=p(),dt=r("span"),Fs=n("XLM-ProphetNet"),Ht=p(),z=r("p"),ct=r("strong"),Gs=n("DISCLAIMER:"),Bs=n(" If you see something strange, file a "),pe=r("a"),Rs=n("Github Issue"),Hs=n(` and assign
@patrickvonplaten`),Ot=p(),T=r("h2"),R=r("a"),mt=r("span"),m(he.$$.fragment),Os=p(),ft=r("span"),Vs=n("Overview"),Vt=p(),H=r("p"),Ws=n("The XLM-ProphetNet model was proposed in "),de=r("a"),Ys=n("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Js=n(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Wt=p(),Je=r("p"),Us=n(`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),Yt=p(),Ue=r("p"),Qs=n("The abstract from the paper is the following:"),Jt=p(),Qe=r("p"),ut=r("em"),Zs=n(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Ut=p(),O=r("p"),Ks=n("The Authors\u2019 code can be found "),ce=r("a"),er=n("here"),tr=n("."),Qt=p(),q=r("h2"),V=r("a"),gt=r("span"),m(me.$$.fragment),sr=p(),_t=r("span"),rr=n("XLMProphetNetConfig"),Zt=p(),C=r("div"),m(fe.$$.fragment),or=p(),ue=r("p"),ar=n("This class overrides "),Ze=r("a"),nr=n("ProphetNetConfig"),ir=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Kt=p(),D=r("h2"),W=r("a"),vt=r("span"),m(ge.$$.fragment),lr=p(),kt=r("span"),pr=n("XLMProphetNetTokenizer"),es=p(),k=r("div"),m(_e.$$.fragment),hr=p(),x=r("p"),dr=n("Adapted from "),Ke=r("a"),cr=n("RobertaTokenizer"),mr=n(" and "),et=r("a"),fr=n("XLNetTokenizer"),ur=n(`. Based on
`),ve=r("a"),gr=n("SentencePiece"),_r=n("."),vr=p(),ke=r("p"),kr=n("This tokenizer inherits from "),tt=r("a"),Pr=n("PreTrainedTokenizer"),$r=n(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Nr=p(),Pe=r("p"),wr=n(`Attributes:
sp`),$e=r("em"),Lr=n("model ("),Pt=r("code"),Mr=n("SentencePieceProcessor"),br=n(`):
The _SentencePiece`),Er=n(" processor that is used for every conversion (string, tokens and IDs)."),xr=p(),y=r("div"),m(Ne.$$.fragment),Xr=p(),$t=r("p"),jr=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),yr=p(),we=r("ul"),st=r("li"),zr=n("single sequence: "),Nt=r("code"),Tr=n("X [SEP]"),qr=p(),rt=r("li"),Cr=n("pair of sequences: "),wt=r("code"),Dr=n("A [SEP] B [SEP]"),Ar=p(),Y=r("div"),m(Le.$$.fragment),Sr=p(),Lt=r("p"),Ir=n("Converts a sequence of tokens (strings for sub-words) in a single string."),Fr=p(),J=r("div"),m(Me.$$.fragment),Gr=p(),Mt=r("p"),Br=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),Rr=p(),U=r("div"),m(be.$$.fragment),Hr=p(),Ee=r("p"),Or=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bt=r("code"),Vr=n("prepare_for_model"),Wr=n(" method."),ts=p(),A=r("h2"),Q=r("a"),Et=r("span"),m(xe.$$.fragment),Yr=p(),xt=r("span"),Jr=n("XLMProphetNetModel"),ss=p(),$=r("div"),m(Xe.$$.fragment),Ur=p(),je=r("p"),Qr=n("This class overrides "),ot=r("a"),Zr=n("ProphetNetModel"),Kr=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),eo=p(),Xt=r("p"),to=n("Example:"),so=p(),m(ye.$$.fragment),rs=p(),S=r("h2"),Z=r("a"),jt=r("span"),m(ze.$$.fragment),ro=p(),yt=r("span"),oo=n("XLMProphetNetEncoder"),os=p(),N=r("div"),m(Te.$$.fragment),ao=p(),qe=r("p"),no=n("This class overrides "),at=r("a"),io=n("ProphetNetEncoder"),lo=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),po=p(),zt=r("p"),ho=n("Example:"),co=p(),m(Ce.$$.fragment),as=p(),I=r("h2"),K=r("a"),Tt=r("span"),m(De.$$.fragment),mo=p(),qt=r("span"),fo=n("XLMProphetNetDecoder"),ns=p(),w=r("div"),m(Ae.$$.fragment),uo=p(),Se=r("p"),go=n("This class overrides "),nt=r("a"),_o=n("ProphetNetDecoder"),vo=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ko=p(),Ct=r("p"),Po=n("Example:"),$o=p(),m(Ie.$$.fragment),is=p(),F=r("h2"),ee=r("a"),Dt=r("span"),m(Fe.$$.fragment),No=p(),At=r("span"),wo=n("XLMProphetNetForConditionalGeneration"),ls=p(),L=r("div"),m(Ge.$$.fragment),Lo=p(),Be=r("p"),Mo=n("This class overrides "),it=r("a"),bo=n("ProphetNetForConditionalGeneration"),Eo=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),xo=p(),St=r("p"),Xo=n("Example:"),jo=p(),m(Re.$$.fragment),ps=p(),G=r("h2"),te=r("a"),It=r("span"),m(He.$$.fragment),yo=p(),Ft=r("span"),zo=n("XLMProphetNetForCausalLM"),hs=p(),M=r("div"),m(Oe.$$.fragment),To=p(),Ve=r("p"),qo=n("This class overrides "),lt=r("a"),Co=n("ProphetNetForCausalLM"),Do=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ao=p(),Gt=r("p"),So=n("Example:"),Io=p(),m(We.$$.fragment),this.h()},l(e){const d=Ya('[data-svelte="svelte-1phssyn"]',document.head);j=o(d,"META",{name:!0,content:!0}),d.forEach(s),Ye=h(e),b=o(e,"H1",{class:!0});var cs=a(b);E=o(cs,"A",{id:!0,class:!0,href:!0});var Bo=a(E);ht=o(Bo,"SPAN",{});var Ro=a(ht);f(le.$$.fragment,Ro),Ro.forEach(s),Bo.forEach(s),Is=h(cs),dt=o(cs,"SPAN",{});var Ho=a(dt);Fs=i(Ho,"XLM-ProphetNet"),Ho.forEach(s),cs.forEach(s),Ht=h(e),z=o(e,"P",{});var Bt=a(z);ct=o(Bt,"STRONG",{});var Oo=a(ct);Gs=i(Oo,"DISCLAIMER:"),Oo.forEach(s),Bs=i(Bt," If you see something strange, file a "),pe=o(Bt,"A",{href:!0,rel:!0});var Vo=a(pe);Rs=i(Vo,"Github Issue"),Vo.forEach(s),Hs=i(Bt,` and assign
@patrickvonplaten`),Bt.forEach(s),Ot=h(e),T=o(e,"H2",{class:!0});var ms=a(T);R=o(ms,"A",{id:!0,class:!0,href:!0});var Wo=a(R);mt=o(Wo,"SPAN",{});var Yo=a(mt);f(he.$$.fragment,Yo),Yo.forEach(s),Wo.forEach(s),Os=h(ms),ft=o(ms,"SPAN",{});var Jo=a(ft);Vs=i(Jo,"Overview"),Jo.forEach(s),ms.forEach(s),Vt=h(e),H=o(e,"P",{});var fs=a(H);Ws=i(fs,"The XLM-ProphetNet model was proposed in "),de=o(fs,"A",{href:!0,rel:!0});var Uo=a(de);Ys=i(Uo,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Uo.forEach(s),Js=i(fs,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),fs.forEach(s),Wt=h(e),Je=o(e,"P",{});var Qo=a(Je);Us=i(Qo,`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),Qo.forEach(s),Yt=h(e),Ue=o(e,"P",{});var Zo=a(Ue);Qs=i(Zo,"The abstract from the paper is the following:"),Zo.forEach(s),Jt=h(e),Qe=o(e,"P",{});var Ko=a(Qe);ut=o(Ko,"EM",{});var ea=a(ut);Zs=i(ea,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),ea.forEach(s),Ko.forEach(s),Ut=h(e),O=o(e,"P",{});var us=a(O);Ks=i(us,"The Authors\u2019 code can be found "),ce=o(us,"A",{href:!0,rel:!0});var ta=a(ce);er=i(ta,"here"),ta.forEach(s),tr=i(us,"."),us.forEach(s),Qt=h(e),q=o(e,"H2",{class:!0});var gs=a(q);V=o(gs,"A",{id:!0,class:!0,href:!0});var sa=a(V);gt=o(sa,"SPAN",{});var ra=a(gt);f(me.$$.fragment,ra),ra.forEach(s),sa.forEach(s),sr=h(gs),_t=o(gs,"SPAN",{});var oa=a(_t);rr=i(oa,"XLMProphetNetConfig"),oa.forEach(s),gs.forEach(s),Zt=h(e),C=o(e,"DIV",{class:!0});var _s=a(C);f(fe.$$.fragment,_s),or=h(_s),ue=o(_s,"P",{});var vs=a(ue);ar=i(vs,"This class overrides "),Ze=o(vs,"A",{href:!0});var aa=a(Ze);nr=i(aa,"ProphetNetConfig"),aa.forEach(s),ir=i(vs,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),vs.forEach(s),_s.forEach(s),Kt=h(e),D=o(e,"H2",{class:!0});var ks=a(D);W=o(ks,"A",{id:!0,class:!0,href:!0});var na=a(W);vt=o(na,"SPAN",{});var ia=a(vt);f(ge.$$.fragment,ia),ia.forEach(s),na.forEach(s),lr=h(ks),kt=o(ks,"SPAN",{});var la=a(kt);pr=i(la,"XLMProphetNetTokenizer"),la.forEach(s),ks.forEach(s),es=h(e),k=o(e,"DIV",{class:!0});var P=a(k);f(_e.$$.fragment,P),hr=h(P),x=o(P,"P",{});var se=a(x);dr=i(se,"Adapted from "),Ke=o(se,"A",{href:!0});var pa=a(Ke);cr=i(pa,"RobertaTokenizer"),pa.forEach(s),mr=i(se," and "),et=o(se,"A",{href:!0});var ha=a(et);fr=i(ha,"XLNetTokenizer"),ha.forEach(s),ur=i(se,`. Based on
`),ve=o(se,"A",{href:!0,rel:!0});var da=a(ve);gr=i(da,"SentencePiece"),da.forEach(s),_r=i(se,"."),se.forEach(s),vr=h(P),ke=o(P,"P",{});var Ps=a(ke);kr=i(Ps,"This tokenizer inherits from "),tt=o(Ps,"A",{href:!0});var ca=a(tt);Pr=i(ca,"PreTrainedTokenizer"),ca.forEach(s),$r=i(Ps,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Ps.forEach(s),Nr=h(P),Pe=o(P,"P",{});var $s=a(Pe);wr=i($s,`Attributes:
sp`),$e=o($s,"EM",{});var Ns=a($e);Lr=i(Ns,"model ("),Pt=o(Ns,"CODE",{});var ma=a(Pt);Mr=i(ma,"SentencePieceProcessor"),ma.forEach(s),br=i(Ns,`):
The _SentencePiece`),Ns.forEach(s),Er=i($s," processor that is used for every conversion (string, tokens and IDs)."),$s.forEach(s),xr=h(P),y=o(P,"DIV",{class:!0});var pt=a(y);f(Ne.$$.fragment,pt),Xr=h(pt),$t=o(pt,"P",{});var fa=a($t);jr=i(fa,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),fa.forEach(s),yr=h(pt),we=o(pt,"UL",{});var ws=a(we);st=o(ws,"LI",{});var Fo=a(st);zr=i(Fo,"single sequence: "),Nt=o(Fo,"CODE",{});var ua=a(Nt);Tr=i(ua,"X [SEP]"),ua.forEach(s),Fo.forEach(s),qr=h(ws),rt=o(ws,"LI",{});var Go=a(rt);Cr=i(Go,"pair of sequences: "),wt=o(Go,"CODE",{});var ga=a(wt);Dr=i(ga,"A [SEP] B [SEP]"),ga.forEach(s),Go.forEach(s),ws.forEach(s),pt.forEach(s),Ar=h(P),Y=o(P,"DIV",{class:!0});var Ls=a(Y);f(Le.$$.fragment,Ls),Sr=h(Ls),Lt=o(Ls,"P",{});var _a=a(Lt);Ir=i(_a,"Converts a sequence of tokens (strings for sub-words) in a single string."),_a.forEach(s),Ls.forEach(s),Fr=h(P),J=o(P,"DIV",{class:!0});var Ms=a(J);f(Me.$$.fragment,Ms),Gr=h(Ms),Mt=o(Ms,"P",{});var va=a(Mt);Br=i(va,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),va.forEach(s),Ms.forEach(s),Rr=h(P),U=o(P,"DIV",{class:!0});var bs=a(U);f(be.$$.fragment,bs),Hr=h(bs),Ee=o(bs,"P",{});var Es=a(Ee);Or=i(Es,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bt=o(Es,"CODE",{});var ka=a(bt);Vr=i(ka,"prepare_for_model"),ka.forEach(s),Wr=i(Es," method."),Es.forEach(s),bs.forEach(s),P.forEach(s),ts=h(e),A=o(e,"H2",{class:!0});var xs=a(A);Q=o(xs,"A",{id:!0,class:!0,href:!0});var Pa=a(Q);Et=o(Pa,"SPAN",{});var $a=a(Et);f(xe.$$.fragment,$a),$a.forEach(s),Pa.forEach(s),Yr=h(xs),xt=o(xs,"SPAN",{});var Na=a(xt);Jr=i(Na,"XLMProphetNetModel"),Na.forEach(s),xs.forEach(s),ss=h(e),$=o(e,"DIV",{class:!0});var re=a($);f(Xe.$$.fragment,re),Ur=h(re),je=o(re,"P",{});var Xs=a(je);Qr=i(Xs,"This class overrides "),ot=o(Xs,"A",{href:!0});var wa=a(ot);Zr=i(wa,"ProphetNetModel"),wa.forEach(s),Kr=i(Xs,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Xs.forEach(s),eo=h(re),Xt=o(re,"P",{});var La=a(Xt);to=i(La,"Example:"),La.forEach(s),so=h(re),f(ye.$$.fragment,re),re.forEach(s),rs=h(e),S=o(e,"H2",{class:!0});var js=a(S);Z=o(js,"A",{id:!0,class:!0,href:!0});var Ma=a(Z);jt=o(Ma,"SPAN",{});var ba=a(jt);f(ze.$$.fragment,ba),ba.forEach(s),Ma.forEach(s),ro=h(js),yt=o(js,"SPAN",{});var Ea=a(yt);oo=i(Ea,"XLMProphetNetEncoder"),Ea.forEach(s),js.forEach(s),os=h(e),N=o(e,"DIV",{class:!0});var oe=a(N);f(Te.$$.fragment,oe),ao=h(oe),qe=o(oe,"P",{});var ys=a(qe);no=i(ys,"This class overrides "),at=o(ys,"A",{href:!0});var xa=a(at);io=i(xa,"ProphetNetEncoder"),xa.forEach(s),lo=i(ys,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ys.forEach(s),po=h(oe),zt=o(oe,"P",{});var Xa=a(zt);ho=i(Xa,"Example:"),Xa.forEach(s),co=h(oe),f(Ce.$$.fragment,oe),oe.forEach(s),as=h(e),I=o(e,"H2",{class:!0});var zs=a(I);K=o(zs,"A",{id:!0,class:!0,href:!0});var ja=a(K);Tt=o(ja,"SPAN",{});var ya=a(Tt);f(De.$$.fragment,ya),ya.forEach(s),ja.forEach(s),mo=h(zs),qt=o(zs,"SPAN",{});var za=a(qt);fo=i(za,"XLMProphetNetDecoder"),za.forEach(s),zs.forEach(s),ns=h(e),w=o(e,"DIV",{class:!0});var ae=a(w);f(Ae.$$.fragment,ae),uo=h(ae),Se=o(ae,"P",{});var Ts=a(Se);go=i(Ts,"This class overrides "),nt=o(Ts,"A",{href:!0});var Ta=a(nt);_o=i(Ta,"ProphetNetDecoder"),Ta.forEach(s),vo=i(Ts,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ts.forEach(s),ko=h(ae),Ct=o(ae,"P",{});var qa=a(Ct);Po=i(qa,"Example:"),qa.forEach(s),$o=h(ae),f(Ie.$$.fragment,ae),ae.forEach(s),is=h(e),F=o(e,"H2",{class:!0});var qs=a(F);ee=o(qs,"A",{id:!0,class:!0,href:!0});var Ca=a(ee);Dt=o(Ca,"SPAN",{});var Da=a(Dt);f(Fe.$$.fragment,Da),Da.forEach(s),Ca.forEach(s),No=h(qs),At=o(qs,"SPAN",{});var Aa=a(At);wo=i(Aa,"XLMProphetNetForConditionalGeneration"),Aa.forEach(s),qs.forEach(s),ls=h(e),L=o(e,"DIV",{class:!0});var ne=a(L);f(Ge.$$.fragment,ne),Lo=h(ne),Be=o(ne,"P",{});var Cs=a(Be);Mo=i(Cs,"This class overrides "),it=o(Cs,"A",{href:!0});var Sa=a(it);bo=i(Sa,"ProphetNetForConditionalGeneration"),Sa.forEach(s),Eo=i(Cs,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Cs.forEach(s),xo=h(ne),St=o(ne,"P",{});var Ia=a(St);Xo=i(Ia,"Example:"),Ia.forEach(s),jo=h(ne),f(Re.$$.fragment,ne),ne.forEach(s),ps=h(e),G=o(e,"H2",{class:!0});var Ds=a(G);te=o(Ds,"A",{id:!0,class:!0,href:!0});var Fa=a(te);It=o(Fa,"SPAN",{});var Ga=a(It);f(He.$$.fragment,Ga),Ga.forEach(s),Fa.forEach(s),yo=h(Ds),Ft=o(Ds,"SPAN",{});var Ba=a(Ft);zo=i(Ba,"XLMProphetNetForCausalLM"),Ba.forEach(s),Ds.forEach(s),hs=h(e),M=o(e,"DIV",{class:!0});var ie=a(M);f(Oe.$$.fragment,ie),To=h(ie),Ve=o(ie,"P",{});var As=a(Ve);qo=i(As,"This class overrides "),lt=o(As,"A",{href:!0});var Ra=a(lt);Co=i(Ra,"ProphetNetForCausalLM"),Ra.forEach(s),Do=i(As,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),As.forEach(s),Ao=h(ie),Gt=o(ie,"P",{});var Ha=a(Gt);So=i(Ha,"Example:"),Ha.forEach(s),Io=h(ie),f(We.$$.fragment,ie),ie.forEach(s),this.h()},h(){l(j,"name","hf:doc:metadata"),l(j,"content",JSON.stringify(Qa)),l(E,"id","xlmprophetnet"),l(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(E,"href","#xlmprophetnet"),l(b,"class","relative group"),l(pe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),l(pe,"rel","nofollow"),l(R,"id","overview"),l(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(R,"href","#overview"),l(T,"class","relative group"),l(de,"href","https://arxiv.org/abs/2001.04063"),l(de,"rel","nofollow"),l(ce,"href","https://github.com/microsoft/ProphetNet"),l(ce,"rel","nofollow"),l(V,"id","transformers.XLMProphetNetConfig"),l(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(V,"href","#transformers.XLMProphetNetConfig"),l(q,"class","relative group"),l(Ze,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig"),l(C,"class","docstring"),l(W,"id","transformers.XLMProphetNetTokenizer"),l(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(W,"href","#transformers.XLMProphetNetTokenizer"),l(D,"class","relative group"),l(Ke,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizer"),l(et,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),l(ve,"href","https://github.com/google/sentencepiece"),l(ve,"rel","nofollow"),l(tt,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(y,"class","docstring"),l(Y,"class","docstring"),l(J,"class","docstring"),l(U,"class","docstring"),l(k,"class","docstring"),l(Q,"id","transformers.XLMProphetNetModel"),l(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Q,"href","#transformers.XLMProphetNetModel"),l(A,"class","relative group"),l(ot,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel"),l($,"class","docstring"),l(Z,"id","transformers.XLMProphetNetEncoder"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#transformers.XLMProphetNetEncoder"),l(S,"class","relative group"),l(at,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),l(N,"class","docstring"),l(K,"id","transformers.XLMProphetNetDecoder"),l(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(K,"href","#transformers.XLMProphetNetDecoder"),l(I,"class","relative group"),l(nt,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),l(w,"class","docstring"),l(ee,"id","transformers.XLMProphetNetForConditionalGeneration"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#transformers.XLMProphetNetForConditionalGeneration"),l(F,"class","relative group"),l(it,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),l(L,"class","docstring"),l(te,"id","transformers.XLMProphetNetForCausalLM"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#transformers.XLMProphetNetForCausalLM"),l(G,"class","relative group"),l(lt,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),l(M,"class","docstring")},m(e,d){t(document.head,j),c(e,Ye,d),c(e,b,d),t(b,E),t(E,ht),u(le,ht,null),t(b,Is),t(b,dt),t(dt,Fs),c(e,Ht,d),c(e,z,d),t(z,ct),t(ct,Gs),t(z,Bs),t(z,pe),t(pe,Rs),t(z,Hs),c(e,Ot,d),c(e,T,d),t(T,R),t(R,mt),u(he,mt,null),t(T,Os),t(T,ft),t(ft,Vs),c(e,Vt,d),c(e,H,d),t(H,Ws),t(H,de),t(de,Ys),t(H,Js),c(e,Wt,d),c(e,Je,d),t(Je,Us),c(e,Yt,d),c(e,Ue,d),t(Ue,Qs),c(e,Jt,d),c(e,Qe,d),t(Qe,ut),t(ut,Zs),c(e,Ut,d),c(e,O,d),t(O,Ks),t(O,ce),t(ce,er),t(O,tr),c(e,Qt,d),c(e,q,d),t(q,V),t(V,gt),u(me,gt,null),t(q,sr),t(q,_t),t(_t,rr),c(e,Zt,d),c(e,C,d),u(fe,C,null),t(C,or),t(C,ue),t(ue,ar),t(ue,Ze),t(Ze,nr),t(ue,ir),c(e,Kt,d),c(e,D,d),t(D,W),t(W,vt),u(ge,vt,null),t(D,lr),t(D,kt),t(kt,pr),c(e,es,d),c(e,k,d),u(_e,k,null),t(k,hr),t(k,x),t(x,dr),t(x,Ke),t(Ke,cr),t(x,mr),t(x,et),t(et,fr),t(x,ur),t(x,ve),t(ve,gr),t(x,_r),t(k,vr),t(k,ke),t(ke,kr),t(ke,tt),t(tt,Pr),t(ke,$r),t(k,Nr),t(k,Pe),t(Pe,wr),t(Pe,$e),t($e,Lr),t($e,Pt),t(Pt,Mr),t($e,br),t(Pe,Er),t(k,xr),t(k,y),u(Ne,y,null),t(y,Xr),t(y,$t),t($t,jr),t(y,yr),t(y,we),t(we,st),t(st,zr),t(st,Nt),t(Nt,Tr),t(we,qr),t(we,rt),t(rt,Cr),t(rt,wt),t(wt,Dr),t(k,Ar),t(k,Y),u(Le,Y,null),t(Y,Sr),t(Y,Lt),t(Lt,Ir),t(k,Fr),t(k,J),u(Me,J,null),t(J,Gr),t(J,Mt),t(Mt,Br),t(k,Rr),t(k,U),u(be,U,null),t(U,Hr),t(U,Ee),t(Ee,Or),t(Ee,bt),t(bt,Vr),t(Ee,Wr),c(e,ts,d),c(e,A,d),t(A,Q),t(Q,Et),u(xe,Et,null),t(A,Yr),t(A,xt),t(xt,Jr),c(e,ss,d),c(e,$,d),u(Xe,$,null),t($,Ur),t($,je),t(je,Qr),t(je,ot),t(ot,Zr),t(je,Kr),t($,eo),t($,Xt),t(Xt,to),t($,so),u(ye,$,null),c(e,rs,d),c(e,S,d),t(S,Z),t(Z,jt),u(ze,jt,null),t(S,ro),t(S,yt),t(yt,oo),c(e,os,d),c(e,N,d),u(Te,N,null),t(N,ao),t(N,qe),t(qe,no),t(qe,at),t(at,io),t(qe,lo),t(N,po),t(N,zt),t(zt,ho),t(N,co),u(Ce,N,null),c(e,as,d),c(e,I,d),t(I,K),t(K,Tt),u(De,Tt,null),t(I,mo),t(I,qt),t(qt,fo),c(e,ns,d),c(e,w,d),u(Ae,w,null),t(w,uo),t(w,Se),t(Se,go),t(Se,nt),t(nt,_o),t(Se,vo),t(w,ko),t(w,Ct),t(Ct,Po),t(w,$o),u(Ie,w,null),c(e,is,d),c(e,F,d),t(F,ee),t(ee,Dt),u(Fe,Dt,null),t(F,No),t(F,At),t(At,wo),c(e,ls,d),c(e,L,d),u(Ge,L,null),t(L,Lo),t(L,Be),t(Be,Mo),t(Be,it),t(it,bo),t(Be,Eo),t(L,xo),t(L,St),t(St,Xo),t(L,jo),u(Re,L,null),c(e,ps,d),c(e,G,d),t(G,te),t(te,It),u(He,It,null),t(G,yo),t(G,Ft),t(Ft,zo),c(e,hs,d),c(e,M,d),u(Oe,M,null),t(M,To),t(M,Ve),t(Ve,qo),t(Ve,lt),t(lt,Co),t(Ve,Do),t(M,Ao),t(M,Gt),t(Gt,So),t(M,Io),u(We,M,null),ds=!0},p:Ja,i(e){ds||(g(le.$$.fragment,e),g(he.$$.fragment,e),g(me.$$.fragment,e),g(fe.$$.fragment,e),g(ge.$$.fragment,e),g(_e.$$.fragment,e),g(Ne.$$.fragment,e),g(Le.$$.fragment,e),g(Me.$$.fragment,e),g(be.$$.fragment,e),g(xe.$$.fragment,e),g(Xe.$$.fragment,e),g(ye.$$.fragment,e),g(ze.$$.fragment,e),g(Te.$$.fragment,e),g(Ce.$$.fragment,e),g(De.$$.fragment,e),g(Ae.$$.fragment,e),g(Ie.$$.fragment,e),g(Fe.$$.fragment,e),g(Ge.$$.fragment,e),g(Re.$$.fragment,e),g(He.$$.fragment,e),g(Oe.$$.fragment,e),g(We.$$.fragment,e),ds=!0)},o(e){_(le.$$.fragment,e),_(he.$$.fragment,e),_(me.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(_e.$$.fragment,e),_(Ne.$$.fragment,e),_(Le.$$.fragment,e),_(Me.$$.fragment,e),_(be.$$.fragment,e),_(xe.$$.fragment,e),_(Xe.$$.fragment,e),_(ye.$$.fragment,e),_(ze.$$.fragment,e),_(Te.$$.fragment,e),_(Ce.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ie.$$.fragment,e),_(Fe.$$.fragment,e),_(Ge.$$.fragment,e),_(Re.$$.fragment,e),_(He.$$.fragment,e),_(Oe.$$.fragment,e),_(We.$$.fragment,e),ds=!1},d(e){s(j),e&&s(Ye),e&&s(b),v(le),e&&s(Ht),e&&s(z),e&&s(Ot),e&&s(T),v(he),e&&s(Vt),e&&s(H),e&&s(Wt),e&&s(Je),e&&s(Yt),e&&s(Ue),e&&s(Jt),e&&s(Qe),e&&s(Ut),e&&s(O),e&&s(Qt),e&&s(q),v(me),e&&s(Zt),e&&s(C),v(fe),e&&s(Kt),e&&s(D),v(ge),e&&s(es),e&&s(k),v(_e),v(Ne),v(Le),v(Me),v(be),e&&s(ts),e&&s(A),v(xe),e&&s(ss),e&&s($),v(Xe),v(ye),e&&s(rs),e&&s(S),v(ze),e&&s(os),e&&s(N),v(Te),v(Ce),e&&s(as),e&&s(I),v(De),e&&s(ns),e&&s(w),v(Ae),v(Ie),e&&s(is),e&&s(F),v(Fe),e&&s(ls),e&&s(L),v(Ge),v(Re),e&&s(ps),e&&s(G),v(He),e&&s(hs),e&&s(M),v(Oe),v(We)}}}const Qa={local:"xlmprophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.XLMProphetNetConfig",title:"XLMProphetNetConfig"},{local:"transformers.XLMProphetNetTokenizer",title:"XLMProphetNetTokenizer"},{local:"transformers.XLMProphetNetModel",title:"XLMProphetNetModel"},{local:"transformers.XLMProphetNetEncoder",title:"XLMProphetNetEncoder"},{local:"transformers.XLMProphetNetDecoder",title:"XLMProphetNetDecoder"},{local:"transformers.XLMProphetNetForConditionalGeneration",title:"XLMProphetNetForConditionalGeneration"},{local:"transformers.XLMProphetNetForCausalLM",title:"XLMProphetNetForCausalLM"}],title:"XLM-ProphetNet"};function Za(Ss,j,Ye){let{fw:b}=j;return Ss.$$set=E=>{"fw"in E&&Ye(0,b=E.fw)},[b]}class on extends Oa{constructor(j){super();Va(this,j,Za,Ua,Wa,{fw:0})}}export{on as default,Qa as metadata};
