import{S as Rs,i as Ms,s as Gs,e as o,k as p,w as k,t as a,M as Us,c as n,d as s,m as c,a as r,x as g,h as i,b as d,F as t,g as f,y,L as Vs,q as v,o as b,B as w}from"../../chunks/vendor-ab4e3193.js";import{D as Le}from"../../chunks/Docstring-b69c0bd4.js";import{C as Os}from"../../chunks/CodeBlock-516df0c5.js";import{I as at}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function Ws(it){let T,te,u,_,he,F,lt,ue,pt,Ae,z,B,_e,N,ct,ke,dt,je,L,ft,O,mt,ht,Pe,se,ut,Ce,oe,ge,_t,De,$,kt,R,gt,yt,M,vt,bt,Ie,A,wt,ne,Tt,$t,Se,re,qt,Fe,x,j,ye,G,zt,ve,xt,Ne,ae,Et,Oe,U,Re,ie,Bt,Me,V,Ge,E,P,be,W,Lt,we,At,Ue,m,H,jt,Te,Pt,Ct,X,Dt,le,It,St,Ft,q,K,Nt,$e,Ot,Rt,J,pe,Mt,qe,Gt,Ut,ce,Vt,ze,Wt,Ht,C,Q,Xt,xe,Kt,Jt,D,Y,Qt,Ee,Yt,Zt,I,Z,es,ee,ts,Be,ss,os,Ve,S,ns,de,rs,as,We;return F=new at({}),N=new at({}),G=new at({}),U=new Os({props:{code:`from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')

input_ids = torch.tensor([list("Life is like a box of chocolates.".encode("utf-8"))]) + 3  # add 3 for special tokens
labels = torch.tensor([list("La vie est comme une bo\xEEte de chocolat.".encode("utf-8"))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass,`,highlighted:`from transformers <span class="hljs-built_in">import</span> T5ForConditionalGeneration
<span class="hljs-built_in">import</span> torch

<span class="hljs-attr">model</span> = T5ForConditionalGeneration.from_pretrained(&#x27;google/byt5-small&#x27;)

<span class="hljs-attr">input_ids</span> = torch.tensor([list(<span class="hljs-string">&quot;Life is like a box of chocolates.&quot;</span>.encode(<span class="hljs-string">&quot;utf-8&quot;</span>))]) + <span class="hljs-number">3</span>  <span class="hljs-comment"># add 3 for special tokens</span>
<span class="hljs-attr">labels</span> = torch.tensor([list(<span class="hljs-string">&quot;La vie est comme une bo\xEEte de chocolat.&quot;</span>.encode(<span class="hljs-string">&quot;utf-8&quot;</span>))]) + <span class="hljs-number">3</span>  <span class="hljs-comment"># add 3 for special tokens</span>

<span class="hljs-attr">loss</span> = model(input_ids, <span class="hljs-attr">labels=labels).loss</span> <span class="hljs-comment"># forward pass</span>`}}),V=new Os({props:{code:`from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')

model_inputs = tokenizer(["Life is like a box of chocolates.", "Today is Monday."], padding="longest", return_tensors="pt")
labels = tokenizer(["La vie est comme une bo\xEEte de chocolat.", "Aujourd'hui c'est lundi."], padding="longest", return_tensors="pt").input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass,`,highlighted:`<span class="hljs-keyword">from</span> transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&#x27;google/byt5-small&#x27;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;google/byt5-small&#x27;</span>)

model_inputs = tokenizer([<span class="hljs-string">&quot;Life is like a box of chocolates.&quot;</span>, <span class="hljs-string">&quot;Today is Monday.&quot;</span>], <span class="hljs-attribute">padding</span>=<span class="hljs-string">&quot;longest&quot;</span>, <span class="hljs-attribute">return_tensors</span>=<span class="hljs-string">&quot;pt&quot;</span>)
labels = tokenizer([<span class="hljs-string">&quot;La vie est comme une bo\xEEte de chocolat.&quot;</span>, <span class="hljs-string">&quot;Aujourd&#x27;hui c&#x27;est lundi.&quot;</span>], <span class="hljs-attribute">padding</span>=<span class="hljs-string">&quot;longest&quot;</span>, <span class="hljs-attribute">return_tensors</span>=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

loss = model(**model_inputs, <span class="hljs-attribute">labels</span>=labels).loss # forward pass`}}),W=new at({}),H=new Le({props:{name:"class transformers.ByT5Tokenizer",anchor:"transformers.ByT5Tokenizer",parameters:[{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 125"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/byt5/tokenization_byt5.py#L28",parametersDescription:[{anchor:"transformers.ByT5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"}]}}),K=new Le({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/byt5/tokenization_byt5.py#L174",parametersDescription:[{anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Q=new Le({props:{name:"convert_tokens_to_string",anchor:"transformers.ByT5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/byt5/tokenization_byt5.py#L225"}}),Y=new Le({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/byt5/tokenization_byt5.py#L152",parametersDescription:[{anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Z=new Le({props:{name:"get_special_tokens_mask",anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/byt5/tokenization_byt5.py#L114",parametersDescription:[{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){T=o("meta"),te=p(),u=o("h1"),_=o("a"),he=o("span"),k(F.$$.fragment),lt=p(),ue=o("span"),pt=a("ByT5"),Ae=p(),z=o("h2"),B=o("a"),_e=o("span"),k(N.$$.fragment),ct=p(),ke=o("span"),dt=a("Overview"),je=p(),L=o("p"),ft=a("The ByT5 model was presented in "),O=o("a"),mt=a("ByT5: Towards a token-free future with pre-trained byte-to-byte models"),ht=a(` by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, Colin Raffel.`),Pe=p(),se=o("p"),ut=a("The abstract from the paper is the following:"),Ce=p(),oe=o("p"),ge=o("em"),_t=a(`Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.
Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from
the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they
can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by
removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token
sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of
operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with
minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,
training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level
counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on
tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of
pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our
experiments.`),De=p(),$=o("p"),kt=a("This model was contributed by "),R=o("a"),gt=a("patrickvonplaten"),yt=a(`. The original code can be
found `),M=o("a"),vt=a("here"),bt=a("."),Ie=p(),A=o("p"),wt=a("ByT5\u2019s architecture is based on the T5v1.1 model, so one can refer to "),ne=o("a"),Tt=a("T5v1.1\u2019s documentation page"),$t=a(`. They
only differ in how inputs should be prepared for the model, see the code examples below.`),Se=p(),re=o("p"),qt=a(`Since ByT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Fe=p(),x=o("h3"),j=o("a"),ye=o("span"),k(G.$$.fragment),zt=p(),ve=o("span"),xt=a("Example"),Ne=p(),ae=o("p"),Et=a("ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:"),Oe=p(),k(U.$$.fragment),Re=p(),ie=o("p"),Bt=a("For batched inference and training it is however recommended to make use of the tokenizer:"),Me=p(),k(V.$$.fragment),Ge=p(),E=o("h2"),P=o("a"),be=o("span"),k(W.$$.fragment),Lt=p(),we=o("span"),At=a("ByT5Tokenizer"),Ue=p(),m=o("div"),k(H.$$.fragment),jt=p(),Te=o("p"),Pt=a("Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding."),Ct=p(),X=o("p"),Dt=a("This tokenizer inherits from "),le=o("a"),It=a("PreTrainedTokenizer"),St=a(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Ft=p(),q=o("div"),k(K.$$.fragment),Nt=p(),$e=o("p"),Ot=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Rt=p(),J=o("ul"),pe=o("li"),Mt=a("single sequence: "),qe=o("code"),Gt=a("X </s>"),Ut=p(),ce=o("li"),Vt=a("pair of sequences: "),ze=o("code"),Wt=a("A </s> B </s>"),Ht=p(),C=o("div"),k(Q.$$.fragment),Xt=p(),xe=o("p"),Kt=a("Converts a sequence of tokens (string) in a single string."),Jt=p(),D=o("div"),k(Y.$$.fragment),Qt=p(),Ee=o("p"),Yt=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. ByT5 does not
make use of token type ids, therefore a list of zeros is returned.`),Zt=p(),I=o("div"),k(Z.$$.fragment),es=p(),ee=o("p"),ts=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Be=o("code"),ss=a("prepare_for_model"),os=a(" method."),Ve=p(),S=o("p"),ns=a("See "),de=o("a"),rs=a("ByT5Tokenizer"),as=a(" for all details."),this.h()},l(e){const l=Us('[data-svelte="svelte-1phssyn"]',document.head);T=n(l,"META",{name:!0,content:!0}),l.forEach(s),te=c(e),u=n(e,"H1",{class:!0});var He=r(u);_=n(He,"A",{id:!0,class:!0,href:!0});var ps=r(_);he=n(ps,"SPAN",{});var cs=r(he);g(F.$$.fragment,cs),cs.forEach(s),ps.forEach(s),lt=c(He),ue=n(He,"SPAN",{});var ds=r(ue);pt=i(ds,"ByT5"),ds.forEach(s),He.forEach(s),Ae=c(e),z=n(e,"H2",{class:!0});var Xe=r(z);B=n(Xe,"A",{id:!0,class:!0,href:!0});var fs=r(B);_e=n(fs,"SPAN",{});var ms=r(_e);g(N.$$.fragment,ms),ms.forEach(s),fs.forEach(s),ct=c(Xe),ke=n(Xe,"SPAN",{});var hs=r(ke);dt=i(hs,"Overview"),hs.forEach(s),Xe.forEach(s),je=c(e),L=n(e,"P",{});var Ke=r(L);ft=i(Ke,"The ByT5 model was presented in "),O=n(Ke,"A",{href:!0,rel:!0});var us=r(O);mt=i(us,"ByT5: Towards a token-free future with pre-trained byte-to-byte models"),us.forEach(s),ht=i(Ke,` by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, Colin Raffel.`),Ke.forEach(s),Pe=c(e),se=n(e,"P",{});var _s=r(se);ut=i(_s,"The abstract from the paper is the following:"),_s.forEach(s),Ce=c(e),oe=n(e,"P",{});var ks=r(oe);ge=n(ks,"EM",{});var gs=r(ge);_t=i(gs,`Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.
Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from
the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they
can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by
removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token
sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of
operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with
minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,
training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level
counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on
tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of
pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our
experiments.`),gs.forEach(s),ks.forEach(s),De=c(e),$=n(e,"P",{});var fe=r($);kt=i(fe,"This model was contributed by "),R=n(fe,"A",{href:!0,rel:!0});var ys=r(R);gt=i(ys,"patrickvonplaten"),ys.forEach(s),yt=i(fe,`. The original code can be
found `),M=n(fe,"A",{href:!0,rel:!0});var vs=r(M);vt=i(vs,"here"),vs.forEach(s),bt=i(fe,"."),fe.forEach(s),Ie=c(e),A=n(e,"P",{});var Je=r(A);wt=i(Je,"ByT5\u2019s architecture is based on the T5v1.1 model, so one can refer to "),ne=n(Je,"A",{href:!0});var bs=r(ne);Tt=i(bs,"T5v1.1\u2019s documentation page"),bs.forEach(s),$t=i(Je,`. They
only differ in how inputs should be prepared for the model, see the code examples below.`),Je.forEach(s),Se=c(e),re=n(e,"P",{});var ws=r(re);qt=i(ws,`Since ByT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),ws.forEach(s),Fe=c(e),x=n(e,"H3",{class:!0});var Qe=r(x);j=n(Qe,"A",{id:!0,class:!0,href:!0});var Ts=r(j);ye=n(Ts,"SPAN",{});var $s=r(ye);g(G.$$.fragment,$s),$s.forEach(s),Ts.forEach(s),zt=c(Qe),ve=n(Qe,"SPAN",{});var qs=r(ve);xt=i(qs,"Example"),qs.forEach(s),Qe.forEach(s),Ne=c(e),ae=n(e,"P",{});var zs=r(ae);Et=i(zs,"ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:"),zs.forEach(s),Oe=c(e),g(U.$$.fragment,e),Re=c(e),ie=n(e,"P",{});var xs=r(ie);Bt=i(xs,"For batched inference and training it is however recommended to make use of the tokenizer:"),xs.forEach(s),Me=c(e),g(V.$$.fragment,e),Ge=c(e),E=n(e,"H2",{class:!0});var Ye=r(E);P=n(Ye,"A",{id:!0,class:!0,href:!0});var Es=r(P);be=n(Es,"SPAN",{});var Bs=r(be);g(W.$$.fragment,Bs),Bs.forEach(s),Es.forEach(s),Lt=c(Ye),we=n(Ye,"SPAN",{});var Ls=r(we);At=i(Ls,"ByT5Tokenizer"),Ls.forEach(s),Ye.forEach(s),Ue=c(e),m=n(e,"DIV",{class:!0});var h=r(m);g(H.$$.fragment,h),jt=c(h),Te=n(h,"P",{});var As=r(Te);Pt=i(As,"Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding."),As.forEach(s),Ct=c(h),X=n(h,"P",{});var Ze=r(X);Dt=i(Ze,"This tokenizer inherits from "),le=n(Ze,"A",{href:!0});var js=r(le);It=i(js,"PreTrainedTokenizer"),js.forEach(s),St=i(Ze,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Ze.forEach(s),Ft=c(h),q=n(h,"DIV",{class:!0});var me=r(q);g(K.$$.fragment,me),Nt=c(me),$e=n(me,"P",{});var Ps=r($e);Ot=i(Ps,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Ps.forEach(s),Rt=c(me),J=n(me,"UL",{});var et=r(J);pe=n(et,"LI",{});var is=r(pe);Mt=i(is,"single sequence: "),qe=n(is,"CODE",{});var Cs=r(qe);Gt=i(Cs,"X </s>"),Cs.forEach(s),is.forEach(s),Ut=c(et),ce=n(et,"LI",{});var ls=r(ce);Vt=i(ls,"pair of sequences: "),ze=n(ls,"CODE",{});var Ds=r(ze);Wt=i(Ds,"A </s> B </s>"),Ds.forEach(s),ls.forEach(s),et.forEach(s),me.forEach(s),Ht=c(h),C=n(h,"DIV",{class:!0});var tt=r(C);g(Q.$$.fragment,tt),Xt=c(tt),xe=n(tt,"P",{});var Is=r(xe);Kt=i(Is,"Converts a sequence of tokens (string) in a single string."),Is.forEach(s),tt.forEach(s),Jt=c(h),D=n(h,"DIV",{class:!0});var st=r(D);g(Y.$$.fragment,st),Qt=c(st),Ee=n(st,"P",{});var Ss=r(Ee);Yt=i(Ss,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. ByT5 does not
make use of token type ids, therefore a list of zeros is returned.`),Ss.forEach(s),st.forEach(s),Zt=c(h),I=n(h,"DIV",{class:!0});var ot=r(I);g(Z.$$.fragment,ot),es=c(ot),ee=n(ot,"P",{});var nt=r(ee);ts=i(nt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Be=n(nt,"CODE",{});var Fs=r(Be);ss=i(Fs,"prepare_for_model"),Fs.forEach(s),os=i(nt," method."),nt.forEach(s),ot.forEach(s),h.forEach(s),Ve=c(e),S=n(e,"P",{});var rt=r(S);ns=i(rt,"See "),de=n(rt,"A",{href:!0});var Ns=r(de);rs=i(Ns,"ByT5Tokenizer"),Ns.forEach(s),as=i(rt," for all details."),rt.forEach(s),this.h()},h(){d(T,"name","hf:doc:metadata"),d(T,"content",JSON.stringify(Hs)),d(_,"id","byt5"),d(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_,"href","#byt5"),d(u,"class","relative group"),d(B,"id","overview"),d(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B,"href","#overview"),d(z,"class","relative group"),d(O,"href","https://arxiv.org/abs/2105.13626"),d(O,"rel","nofollow"),d(R,"href","https://huggingface.co/patrickvonplaten"),d(R,"rel","nofollow"),d(M,"href","https://github.com/google-research/byt5"),d(M,"rel","nofollow"),d(ne,"href","/docs/transformers/v4.14.1/en/t5v1.1"),d(j,"id","example"),d(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(j,"href","#example"),d(x,"class","relative group"),d(P,"id","transformers.ByT5Tokenizer"),d(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P,"href","#transformers.ByT5Tokenizer"),d(E,"class","relative group"),d(le,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(q,"class","docstring"),d(C,"class","docstring"),d(D,"class","docstring"),d(I,"class","docstring"),d(m,"class","docstring"),d(de,"href","/docs/transformers/v4.14.1/en/model_doc/byt5#transformers.ByT5Tokenizer")},m(e,l){t(document.head,T),f(e,te,l),f(e,u,l),t(u,_),t(_,he),y(F,he,null),t(u,lt),t(u,ue),t(ue,pt),f(e,Ae,l),f(e,z,l),t(z,B),t(B,_e),y(N,_e,null),t(z,ct),t(z,ke),t(ke,dt),f(e,je,l),f(e,L,l),t(L,ft),t(L,O),t(O,mt),t(L,ht),f(e,Pe,l),f(e,se,l),t(se,ut),f(e,Ce,l),f(e,oe,l),t(oe,ge),t(ge,_t),f(e,De,l),f(e,$,l),t($,kt),t($,R),t(R,gt),t($,yt),t($,M),t(M,vt),t($,bt),f(e,Ie,l),f(e,A,l),t(A,wt),t(A,ne),t(ne,Tt),t(A,$t),f(e,Se,l),f(e,re,l),t(re,qt),f(e,Fe,l),f(e,x,l),t(x,j),t(j,ye),y(G,ye,null),t(x,zt),t(x,ve),t(ve,xt),f(e,Ne,l),f(e,ae,l),t(ae,Et),f(e,Oe,l),y(U,e,l),f(e,Re,l),f(e,ie,l),t(ie,Bt),f(e,Me,l),y(V,e,l),f(e,Ge,l),f(e,E,l),t(E,P),t(P,be),y(W,be,null),t(E,Lt),t(E,we),t(we,At),f(e,Ue,l),f(e,m,l),y(H,m,null),t(m,jt),t(m,Te),t(Te,Pt),t(m,Ct),t(m,X),t(X,Dt),t(X,le),t(le,It),t(X,St),t(m,Ft),t(m,q),y(K,q,null),t(q,Nt),t(q,$e),t($e,Ot),t(q,Rt),t(q,J),t(J,pe),t(pe,Mt),t(pe,qe),t(qe,Gt),t(J,Ut),t(J,ce),t(ce,Vt),t(ce,ze),t(ze,Wt),t(m,Ht),t(m,C),y(Q,C,null),t(C,Xt),t(C,xe),t(xe,Kt),t(m,Jt),t(m,D),y(Y,D,null),t(D,Qt),t(D,Ee),t(Ee,Yt),t(m,Zt),t(m,I),y(Z,I,null),t(I,es),t(I,ee),t(ee,ts),t(ee,Be),t(Be,ss),t(ee,os),f(e,Ve,l),f(e,S,l),t(S,ns),t(S,de),t(de,rs),t(S,as),We=!0},p:Vs,i(e){We||(v(F.$$.fragment,e),v(N.$$.fragment,e),v(G.$$.fragment,e),v(U.$$.fragment,e),v(V.$$.fragment,e),v(W.$$.fragment,e),v(H.$$.fragment,e),v(K.$$.fragment,e),v(Q.$$.fragment,e),v(Y.$$.fragment,e),v(Z.$$.fragment,e),We=!0)},o(e){b(F.$$.fragment,e),b(N.$$.fragment,e),b(G.$$.fragment,e),b(U.$$.fragment,e),b(V.$$.fragment,e),b(W.$$.fragment,e),b(H.$$.fragment,e),b(K.$$.fragment,e),b(Q.$$.fragment,e),b(Y.$$.fragment,e),b(Z.$$.fragment,e),We=!1},d(e){s(T),e&&s(te),e&&s(u),w(F),e&&s(Ae),e&&s(z),w(N),e&&s(je),e&&s(L),e&&s(Pe),e&&s(se),e&&s(Ce),e&&s(oe),e&&s(De),e&&s($),e&&s(Ie),e&&s(A),e&&s(Se),e&&s(re),e&&s(Fe),e&&s(x),w(G),e&&s(Ne),e&&s(ae),e&&s(Oe),w(U,e),e&&s(Re),e&&s(ie),e&&s(Me),w(V,e),e&&s(Ge),e&&s(E),w(W),e&&s(Ue),e&&s(m),w(H),w(K),w(Q),w(Y),w(Z),e&&s(Ve),e&&s(S)}}}const Hs={local:"byt5",sections:[{local:"overview",sections:[{local:"example",title:"Example"}],title:"Overview"},{local:"transformers.ByT5Tokenizer",title:"ByT5Tokenizer"}],title:"ByT5"};function Xs(it,T,te){let{fw:u}=T;return it.$$set=_=>{"fw"in _&&te(0,u=_.fw)},[u]}class eo extends Rs{constructor(T){super();Ms(this,T,Xs,Ws,Gs,{fw:0})}}export{eo as default,Hs as metadata};
