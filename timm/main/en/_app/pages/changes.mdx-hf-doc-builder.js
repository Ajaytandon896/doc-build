import{S as r4,i as a4,s as i4,e as o,k as n,w as N,t as a,M as s4,c as l,d as t,m as d,a as r,x as T,h as i,b as c,G as e,g as f,y as $,L as n4,q as S,o as P,B as V,v as d4}from"../chunks/vendor-hf-doc-builder.js";import{I as M}from"../chunks/IconCopyLink-hf-doc-builder.js";function c4(n_){let te,_s,oe,Ae,fr,ot,On,pr,Dn,Es,le,Ce,mr,lt,Nn,vr,Tn,gs,I,ur,$n,Sn,xo,Pn,k,Ao,_r,Vn,Mn,Rn,Co,Er,Un,zn,Hn,Oo,gr,Bn,Fn,Xn,Do,wr,Gn,jn,Jn,No,br,qn,Wn,Yn,To,yr,Kn,Qn,Zn,$o,Lr,ed,td,od,Oe,kr,ld,rd,Ir,ad,id,sd,xr,nd,dd,Ar,cd,hd,Cr,fd,ws,re,De,Or,rt,pd,Dr,md,bs,So,vd,ys,E,Po,ud,ae,at,_d,it,Ed,gd,wd,st,bd,nt,yd,Ld,kd,dt,Id,ct,xd,Ad,Cd,Vo,Od,H,ht,Dd,Nr,Nd,Td,$d,Ne,Tr,Sd,Pd,$r,Vd,Md,Rd,ft,Ud,Sr,zd,Hd,Bd,pt,Fd,Pr,Xd,Gd,jd,Mo,Jd,g,Ro,Vr,qd,Wd,Yd,Uo,Mr,Kd,Qd,Zd,zo,Rr,ec,tc,oc,Ho,Ur,lc,rc,ac,Bo,zr,ic,sc,nc,Fo,Hr,dc,cc,hc,Xo,Br,fc,pc,mc,Go,Fr,vc,uc,_c,jo,Xr,Ec,gc,wc,Jo,Gr,bc,yc,Lc,qo,jr,kc,Ic,xc,Wo,Jr,Ac,Cc,Oc,j,qr,Dc,Nc,Wr,Tc,$c,Yr,Sc,Pc,Vc,Kr,Mc,Rc,Qr,Uc,zc,ie,Hc,Zr,Bc,Fc,mt,Xc,Gc,jc,J,Jc,ea,qc,Wc,ta,Yc,Kc,vt,oa,Qc,Zc,Te,eh,la,th,oh,ra,lh,rh,aa,ah,ih,ia,sh,nh,sa,dh,ch,na,hh,Ls,se,$e,da,ut,fh,ca,ph,ks,x,_t,mh,Et,vh,uh,_h,gt,Eh,ha,gh,wh,bh,Yo,yh,B,Ko,fa,Lh,kh,Ih,Qo,pa,xh,Ah,Ch,Zo,ma,Oh,Dh,Nh,el,va,Th,$h,Sh,ua,Ph,Vh,_a,Mh,Rh,ne,Uh,wt,zh,Hh,bt,Bh,Fh,Is,de,Se,Ea,yt,Xh,ga,Gh,xs,q,W,jh,wa,Jh,qh,ba,Wh,Yh,ce,tl,ya,Kh,Qh,Zh,ol,La,e2,t2,o2,ll,ka,l2,r2,a2,Lt,i2,Ia,s2,n2,d2,rl,xa,c2,h2,As,he,Pe,Aa,kt,f2,Ca,p2,Cs,Ve,R,Oa,m2,v2,It,u2,_2,Da,E2,g2,xt,w2,b2,y2,Me,L2,At,k2,I2,Ct,al,Na,x2,A2,C2,il,Ta,O2,D2,Os,fe,Re,$a,Ot,N2,Sa,T2,Ds,Ue,Y,$2,Pa,S2,P2,Va,V2,M2,Dt,R2,U2,sl,Ma,z2,H2,Ns,pe,ze,Ra,Nt,B2,Ua,F2,Ts,m,F,X2,za,G2,j2,Ha,J2,q2,Tt,Ba,W2,Y2,K2,$t,Q2,St,Z2,p,nl,Fa,ef,tf,of,dl,Xa,lf,rf,af,cl,Ga,sf,nf,df,hl,ja,cf,hf,ff,fl,Ja,pf,mf,vf,pl,qa,uf,_f,Ef,ml,Wa,gf,wf,bf,vl,Ya,yf,Lf,kf,ul,Ka,If,xf,Af,_l,Qa,Cf,Of,Df,El,Za,Nf,Tf,$f,gl,ei,Sf,Pf,Vf,wl,ti,Mf,Rf,Uf,bl,oi,zf,Hf,Bf,yl,li,Ff,Xf,Gf,Ll,ri,jf,Jf,qf,kl,ai,Wf,Yf,Kf,ii,Qf,Zf,Pt,e1,Vt,t1,o1,l1,Il,r1,Mt,a1,i1,xl,s1,Rt,n1,d1,Al,c1,Ut,h1,f1,Cl,p1,zt,m1,v1,si,u1,_1,ni,E1,g1,di,w1,b1,ci,y1,L1,hi,k1,I1,me,fi,x1,A1,pi,C1,O1,mi,D1,N1,He,T1,vi,$1,S1,ui,P1,$s,ve,Be,_i,Ht,V1,Ei,M1,Ss,Fe,ue,Bt,R1,U1,gi,z1,H1,Ft,B1,F1,Xe,X1,wi,G1,j1,Xt,Gt,J1,bi,q1,W1,Y1,Ge,yi,K1,Q1,Li,Z1,ep,Ps,_e,je,ki,jt,tp,Ii,op,Vs,A,xi,lp,rp,Jt,ap,qt,ip,sp,np,Ol,dp,D,Dl,Ai,cp,hp,fp,Nl,Ci,pp,mp,vp,Tl,Oi,up,_p,Ep,$l,Di,gp,wp,bp,Sl,Ni,yp,Lp,kp,Pl,Ip,Wt,xp,Ap,Ti,Cp,Ms,Ee,Je,$i,Yt,Op,Si,Dp,Rs,K,Vl,Np,_,Ml,Pi,Tp,$p,Sp,Rl,Vi,Pp,Vp,Mp,qe,Mi,Rp,Up,Kt,zp,Hp,Bp,Ul,Ri,Fp,Xp,Gp,zl,Ui,jp,Jp,qp,Hl,zi,Wp,Yp,Kp,Bl,Hi,Qp,Zp,em,Fl,Bi,tm,om,lm,Xl,Fi,rm,am,im,Gl,Xi,sm,nm,dm,jl,Gi,cm,hm,fm,Jl,ji,pm,mm,vm,ql,Ji,um,_m,Em,ge,qi,gm,wm,Wi,bm,ym,Qt,Lm,km,Zt,Im,eo,xm,Yi,Ki,Am,Us,we,We,Qi,to,Cm,Zi,Om,zs,L,be,Dm,oo,Nm,Tm,lo,$m,Sm,Pm,es,Vm,Mm,ro,Rm,ao,Um,zm,Hm,Wl,Bm,ye,io,Fm,so,Xm,Gm,jm,no,Jm,co,qm,Wm,Ym,ho,Km,fo,Qm,Zm,ev,Le,tv,po,ov,lv,mo,rv,av,iv,X,sv,vo,nv,dv,uo,cv,hv,_o,fv,pv,mv,Yl,vv,Eo,uv,Hs,ke,Ye,ts,go,_v,os,Ev,Bs,Q,Kl,gv,G,Ie,wv,ls,bv,yv,wo,Lv,kv,Iv,rs,xv,Av,bo,Cv,as,Ov,Dv,Nv,is,Tv,$v,ss,Sv,Pv,ns,Vv,Fs;return ot=new M({}),lt=new M({}),rt=new M({}),ut=new M({}),yt=new M({}),kt=new M({}),Ot=new M({}),Nt=new M({}),Ht=new M({}),jt=new M({}),Yt=new M({}),to=new M({}),go=new M({}),{c(){te=o("meta"),_s=n(),oe=o("h1"),Ae=o("a"),fr=o("span"),N(ot.$$.fragment),On=n(),pr=o("span"),Dn=a("Recent Changes"),Es=n(),le=o("h3"),Ce=o("a"),mr=o("span"),N(lt.$$.fragment),Nn=n(),vr=o("span"),Tn=a("July 27, 2022"),gs=n(),I=o("ul"),ur=o("li"),$n=a("All runtime benchmark and validation result csv files are up-to-date!"),Sn=n(),xo=o("li"),Pn=a("A few more weights & model defs added:"),k=o("ul"),Ao=o("li"),_r=o("code"),Vn=a("darknetaa53"),Mn=a(" -  79.8 @ 256, 80.5 @ 288"),Rn=n(),Co=o("li"),Er=o("code"),Un=a("convnext_nano"),zn=a(" - 80.8 @ 224, 81.5 @ 288"),Hn=n(),Oo=o("li"),gr=o("code"),Bn=a("cs3sedarknet_l"),Fn=a(" - 81.2 @ 256, 81.8 @ 288"),Xn=n(),Do=o("li"),wr=o("code"),Gn=a("cs3darknet_x"),jn=a(" - 81.8 @ 256, 82.2 @ 288"),Jn=n(),No=o("li"),br=o("code"),qn=a("cs3sedarknet_x"),Wn=a(" - 82.2 @ 256, 82.7 @ 288"),Yn=n(),To=o("li"),yr=o("code"),Kn=a("cs3edgenet_x"),Qn=a(" - 82.2 @ 256, 82.7 @ 288"),Zn=n(),$o=o("li"),Lr=o("code"),ed=a("cs3se_edgenet_x"),td=a(" - 82.8 @ 256, 83.5 @ 320"),od=n(),Oe=o("li"),kr=o("code"),ld=a("cs3*"),rd=a(" weights above all trained on TPU w/ "),Ir=o("code"),ad=a("bits_and_tpu"),id=a(" branch. Thanks to TRC program!"),sd=n(),xr=o("li"),nd=a("Add output_stride=8 and 16 support to ConvNeXt (dilation)"),dd=n(),Ar=o("li"),cd=a("deit3 models not being able to resize pos_emb fixed"),hd=n(),Cr=o("li"),fd=a("Version 0.6.7 PyPi release (/w above bug fixes and new weighs since 0.6.5)"),ws=n(),re=o("h3"),De=o("a"),Or=o("span"),N(rt.$$.fragment),pd=n(),Dr=o("span"),md=a("July 8, 2022"),bs=n(),So=o("p"),vd=a("More models, more fixes"),ys=n(),E=o("ul"),Po=o("li"),ud=a("Official research models (w/ weights) added:"),ae=o("ul"),at=o("li"),_d=a("EdgeNeXt from ("),it=o("a"),Ed=a("https://github.com/mmaaz60/EdgeNeXt"),gd=a(")"),wd=n(),st=o("li"),bd=a("MobileViT-V2 from ("),nt=o("a"),yd=a("https://github.com/apple/ml-cvnets"),Ld=a(")"),kd=n(),dt=o("li"),Id=a("DeiT III (Revenge of the ViT) from ("),ct=o("a"),xd=a("https://github.com/facebookresearch/deit"),Ad=a(")"),Cd=n(),Vo=o("li"),Od=a("My own models:"),H=o("ul"),ht=o("li"),Dd=a("Small "),Nr=o("code"),Nd=a("ResNet"),Td=a(" defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14)"),$d=n(),Ne=o("li"),Tr=o("code"),Sd=a("CspNet"),Pd=a(" refactored with dataclass config, simplified CrossStage3 ("),$r=o("code"),Vd=a("cs3"),Md=a(") option. These are closer to YOLO-v5+ backbone defs."),Rd=n(),ft=o("li"),Ud=a("More relative position vit fiddling. Two "),Sr=o("code"),zd=a("srelpos"),Hd=a(" (shared relative position) models trained, and a medium w/ class token."),Bd=n(),pt=o("li"),Fd=a("Add an alternate downsample mode to EdgeNeXt and train a "),Pr=o("code"),Xd=a("small"),Gd=a(" model. Better than original small, but not their new USI trained weights."),jd=n(),Mo=o("li"),Jd=a("My own model weight results (all ImageNet-1k training)"),g=o("ul"),Ro=o("li"),Vr=o("code"),qd=a("resnet10t"),Wd=a(" - 66.5 @ 176, 68.3 @ 224"),Yd=n(),Uo=o("li"),Mr=o("code"),Kd=a("resnet14t"),Qd=a(" - 71.3 @ 176, 72.3 @ 224"),Zd=n(),zo=o("li"),Rr=o("code"),ec=a("resnetaa50"),tc=a(" - 80.6 @ 224 , 81.6 @ 288"),oc=n(),Ho=o("li"),Ur=o("code"),lc=a("darknet53"),rc=a(" -  80.0 @ 256, 80.5 @ 288"),ac=n(),Bo=o("li"),zr=o("code"),ic=a("cs3darknet_m"),sc=a(" - 77.0 @ 256, 77.6 @ 288"),nc=n(),Fo=o("li"),Hr=o("code"),dc=a("cs3darknet_focus_m"),cc=a(" - 76.7 @ 256, 77.3 @ 288"),hc=n(),Xo=o("li"),Br=o("code"),fc=a("cs3darknet_l"),pc=a(" - 80.4 @ 256, 80.9 @ 288"),mc=n(),Go=o("li"),Fr=o("code"),vc=a("cs3darknet_focus_l"),uc=a(" - 80.3 @ 256, 80.9 @ 288"),_c=n(),jo=o("li"),Xr=o("code"),Ec=a("vit_srelpos_small_patch16_224"),gc=a(" - 81.1 @ 224, 82.1 @ 320"),wc=n(),Jo=o("li"),Gr=o("code"),bc=a("vit_srelpos_medium_patch16_224"),yc=a(" - 82.3 @ 224, 83.1 @ 320"),Lc=n(),qo=o("li"),jr=o("code"),kc=a("vit_relpos_small_patch16_cls_224"),Ic=a(" - 82.6 @ 224, 83.6 @ 320"),xc=n(),Wo=o("li"),Jr=o("code"),Ac=a("edgnext_small_rw"),Cc=a(" - 79.6 @ 224, 80.4 @ 320"),Oc=n(),j=o("li"),qr=o("code"),Dc=a("cs3"),Nc=a(", "),Wr=o("code"),Tc=a("darknet"),$c=a(", and "),Yr=o("code"),Sc=a("vit_*relpos"),Pc=a(" weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs."),Vc=n(),Kr=o("li"),Mc=a("Hugging Face Hub support fixes verified, demo notebook TBA"),Rc=n(),Qr=o("li"),Uc=a("Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation."),zc=n(),ie=o("li"),Hc=a("Add support to change image extensions scanned by "),Zr=o("code"),Bc=a("timm"),Fc=a(" datasets/parsers. See ("),mt=o("a"),Xc=a("https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103"),Gc=a(")"),jc=n(),J=o("li"),Jc=a("Default ConvNeXt LayerNorm impl to use "),ea=o("code"),qc=a("F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)"),Wc=a(" via "),ta=o("code"),Yc=a("LayerNorm2d"),Kc=a(" in all cases. "),vt=o("ul"),oa=o("li"),Qc=a("a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges."),Zc=n(),Te=o("li"),eh=a("previous impl exists as "),la=o("code"),th=a("LayerNormExp2d"),oh=a(" in "),ra=o("code"),lh=a("models/layers/norm.py"),rh=n(),aa=o("li"),ah=a("Numerous bug fixes"),ih=n(),ia=o("li"),sh=a("Currently testing for imminent PyPi 0.6.x release"),nh=n(),sa=o("li"),dh=a("LeViT pretraining of larger models still a WIP, they don\u2019t train well / easily without distillation. Time to add distill support (finally)?"),ch=n(),na=o("li"),hh=a("ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) \u2026"),Ls=n(),se=o("h3"),$e=o("a"),da=o("span"),N(ut.$$.fragment),fh=n(),ca=o("span"),ph=a("May 13, 2022"),ks=n(),x=o("ul"),_t=o("li"),mh=a("Official Swin-V2 models and weights added from ("),Et=o("a"),vh=a("https://github.com/microsoft/Swin-Transformer"),uh=a("). Cleaned up to support torchscript."),_h=n(),gt=o("li"),Eh=a("Some refactoring for existing "),ha=o("code"),gh=a("timm"),wh=a(" Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects."),bh=n(),Yo=o("li"),yh=a("More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)"),B=o("ul"),Ko=o("li"),fa=o("code"),Lh=a("vit_relpos_small_patch16_224"),kh=a(" - 81.5 @ 224, 82.5 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),Ih=n(),Qo=o("li"),pa=o("code"),xh=a("vit_relpos_medium_patch16_rpn_224"),Ah=a(" - 82.3 @ 224, 83.1 @ 320 \u2014 rel pos + res-post-norm, no class token, avg pool"),Ch=n(),Zo=o("li"),ma=o("code"),Oh=a("vit_relpos_medium_patch16_224"),Dh=a(" - 82.5 @ 224, 83.3 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),Nh=n(),el=o("li"),va=o("code"),Th=a("vit_relpos_base_patch16_gapcls_224"),$h=a(" - 82.8 @ 224, 83.9 @ 320 \u2014 rel pos, layer scale, class token, avg pool (by mistake)"),Sh=n(),ua=o("li"),Ph=a("Bring 512 dim, 8-head \u2018medium\u2019 ViT model variant back to life (after using in a pre DeiT \u2018small\u2019 model for first ViT impl back in 2020)"),Vh=n(),_a=o("li"),Mh=a("Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials"),Rh=n(),ne=o("li"),Uh=a("Sequencer2D impl ("),wt=o("a"),zh=a("https://arxiv.org/abs/2205.01972"),Hh=a("), added via PR from author ("),bt=o("a"),Bh=a("https://github.com/okojoalg"),Fh=a(")"),Is=n(),de=o("h3"),Se=o("a"),Ea=o("span"),N(yt.$$.fragment),Xh=n(),ga=o("span"),Gh=a("May 2, 2022"),xs=n(),q=o("ul"),W=o("li"),jh=a("Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) ("),wa=o("code"),Jh=a("vision_transformer_relpos.py"),qh=a(") and Residual Post-Norm branches (from Swin-V2) ("),ba=o("code"),Wh=a("vision_transformer*.py"),Yh=a(")"),ce=o("ul"),tl=o("li"),ya=o("code"),Kh=a("vit_relpos_base_patch32_plus_rpn_256"),Qh=a(" - 79.5 @ 256, 80.6 @ 320 \u2014 rel pos + extended width + res-post-norm, no class token, avg pool"),Zh=n(),ol=o("li"),La=o("code"),e2=a("vit_relpos_base_patch16_224"),t2=a(" - 82.5 @ 224, 83.6 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),o2=n(),ll=o("li"),ka=o("code"),l2=a("vit_base_patch16_rpn_224"),r2=a(" - 82.3 @ 224 \u2014 rel pos + res-post-norm, no class token, avg pool"),a2=n(),Lt=o("li"),i2=a("Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie "),Ia=o("code"),s2=a("How to Train Your ViT"),n2=a(")"),d2=n(),rl=o("li"),xa=o("code"),c2=a("vit_*"),h2=a(" models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae)."),As=n(),he=o("h3"),Pe=o("a"),Aa=o("span"),N(kt.$$.fragment),f2=n(),Ca=o("span"),p2=a("April 22, 2022"),Cs=n(),Ve=o("ul"),R=o("li"),Oa=o("code"),m2=a("timm"),v2=a(" models are now officially supported in "),It=o("a"),u2=a("fast.ai"),_2=a("! Just in time for the new Practical Deep Learning course. "),Da=o("code"),E2=a("timmdocs"),g2=a(" documentation link updated to "),xt=o("a"),w2=a("timm.fast.ai"),b2=a("."),y2=n(),Me=o("li"),L2=a("Two more model weights added in the TPU trained "),At=o("a"),k2=a("series"),I2=a(". Some In22k pretrain still in progress."),Ct=o("ul"),al=o("li"),Na=o("code"),x2=a("seresnext101d_32x8d"),A2=a(" - 83.69 @ 224, 84.35 @ 288"),C2=n(),il=o("li"),Ta=o("code"),O2=a("seresnextaa101d_32x8d"),D2=a(" (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288"),Os=n(),fe=o("h3"),Re=o("a"),$a=o("span"),N(Ot.$$.fragment),N2=n(),Sa=o("span"),T2=a("March 23, 2022"),Ds=n(),Ue=o("ul"),Y=o("li"),$2=a("Add "),Pa=o("code"),S2=a("ParallelBlock"),P2=a(" and "),Va=o("code"),V2=a("LayerScale"),M2=a(" option to base vit models to support model configs in "),Dt=o("a"),R2=a("Three things everyone should know about ViT"),U2=n(),sl=o("li"),Ma=o("code"),z2=a("convnext_tiny_hnf"),H2=a(" (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs."),Ns=n(),pe=o("h3"),ze=o("a"),Ra=o("span"),N(Nt.$$.fragment),B2=n(),Ua=o("span"),F2=a("March 21, 2022"),Ts=n(),m=o("ul"),F=o("li"),X2=a("Merge "),za=o("code"),G2=a("norm_norm_norm"),j2=a(". "),Ha=o("strong"),J2=a("IMPORTANT"),q2=a(" this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch "),Tt=o("a"),Ba=o("code"),W2=a("0.5.x"),Y2=a(" or a previous 0.5.x release can be used if stability is required."),K2=n(),$t=o("li"),Q2=a("Significant weights update (all TPU trained) as described in this "),St=o("a"),Z2=a("release"),p=o("ul"),nl=o("li"),Fa=o("code"),ef=a("regnety_040"),tf=a(" - 82.3 @ 224, 82.96 @ 288"),of=n(),dl=o("li"),Xa=o("code"),lf=a("regnety_064"),rf=a(" - 83.0 @ 224, 83.65 @ 288"),af=n(),cl=o("li"),Ga=o("code"),sf=a("regnety_080"),nf=a(" - 83.17 @ 224, 83.86 @ 288"),df=n(),hl=o("li"),ja=o("code"),cf=a("regnetv_040"),hf=a(" - 82.44 @ 224, 83.18 @ 288   (timm pre-act)"),ff=n(),fl=o("li"),Ja=o("code"),pf=a("regnetv_064"),mf=a(" - 83.1 @ 224, 83.71 @ 288   (timm pre-act)"),vf=n(),pl=o("li"),qa=o("code"),uf=a("regnetz_040"),_f=a(" - 83.67 @ 256, 84.25 @ 320"),Ef=n(),ml=o("li"),Wa=o("code"),gf=a("regnetz_040h"),wf=a(" - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)"),bf=n(),vl=o("li"),Ya=o("code"),yf=a("resnetv2_50d_gn"),Lf=a(" - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)"),kf=n(),ul=o("li"),Ka=o("code"),If=a("resnetv2_50d_evos"),xf=a(" 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)"),Af=n(),_l=o("li"),Qa=o("code"),Cf=a("regnetz_c16_evos"),Of=a("  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)"),Df=n(),El=o("li"),Za=o("code"),Nf=a("regnetz_d8_evos"),Tf=a("  - 83.42 @ 256, 84.04 @ 320 (EvoNormS)"),$f=n(),gl=o("li"),ei=o("code"),Sf=a("xception41p"),Pf=a(" - 82 @ 299   (timm pre-act)"),Vf=n(),wl=o("li"),ti=o("code"),Mf=a("xception65"),Rf=a(" -  83.17 @ 299"),Uf=n(),bl=o("li"),oi=o("code"),zf=a("xception65p"),Hf=a(" -  83.14 @ 299   (timm pre-act)"),Bf=n(),yl=o("li"),li=o("code"),Ff=a("resnext101_64x4d"),Xf=a(" - 82.46 @ 224, 83.16 @ 288"),Gf=n(),Ll=o("li"),ri=o("code"),jf=a("seresnext101_32x8d"),Jf=a(" - 83.57 @ 224, 84.270 @ 288"),qf=n(),kl=o("li"),ai=o("code"),Wf=a("resnetrs200"),Yf=a(" - 83.85 @ 256, 84.44 @ 320"),Kf=n(),ii=o("li"),Qf=a("HuggingFace hub support fixed w/ initial groundwork for allowing alternative \u2018config sources\u2019 for pretrained model definitions and weights (generic local file / remote url support soon)"),Zf=n(),Pt=o("li"),e1=a("SwinTransformer-V2 implementation added. Submitted by "),Vt=o("a"),t1=a("Christoph Reich"),o1=a(". Training experiments and model changes by myself are ongoing so expect compat breaks."),l1=n(),Il=o("li"),r1=a("Swin-S3 (AutoFormerV2) models / weights added from "),Mt=o("a"),a1=a("https://github.com/microsoft/Cream/tree/main/AutoFormerV2"),i1=n(),xl=o("li"),s1=a("MobileViT models w/ weights adapted from "),Rt=o("a"),n1=a("https://github.com/apple/ml-cvnets"),d1=n(),Al=o("li"),c1=a("PoolFormer models w/ weights adapted from "),Ut=o("a"),h1=a("https://github.com/sail-sg/poolformer"),f1=n(),Cl=o("li"),p1=a("VOLO models w/ weights adapted from "),zt=o("a"),m1=a("https://github.com/sail-sg/volo"),v1=n(),si=o("li"),u1=a("Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc"),_1=n(),ni=o("li"),E1=a("Enhance support for alternate norm + act (\u2018NormAct\u2019) layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception"),g1=n(),di=o("li"),w1=a("Grouped conv support added to EfficientNet family"),b1=n(),ci=o("li"),y1=a("Add \u2018group matching\u2019 API to all models to allow grouping model parameters for application of \u2018layer-wise\u2019 LR decay, lr scale added to LR scheduler"),L1=n(),hi=o("li"),k1=a("Gradient checkpointing support added to many models"),I1=n(),me=o("li"),fi=o("code"),x1=a("forward_head(x, pre_logits=False)"),A1=a(" fn added to all models to allow separate calls of "),pi=o("code"),C1=a("forward_features"),O1=a(" + "),mi=o("code"),D1=a("forward_head"),N1=n(),He=o("li"),T1=a("All vision transformer and vision MLP models update to return non-pooled / non-token selected features from "),vi=o("code"),$1=a("foward_features"),S1=a(", for consistency with CNN models, token selection or pooling now applied in "),ui=o("code"),P1=a("forward_head"),$s=n(),ve=o("h3"),Be=o("a"),_i=o("span"),N(Ht.$$.fragment),V1=n(),Ei=o("span"),M1=a("Feb 2, 2022"),Ss=n(),Fe=o("ul"),ue=o("li"),Bt=o("a"),R1=a("Chris Hughes"),U1=a(" posted an exhaustive run through of "),gi=o("code"),z1=a("timm"),H1=a(" on his blog yesterday. Well worth a read. "),Ft=o("a"),B1=a("Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide"),F1=n(),Xe=o("li"),X1=a("I\u2019m currently prepping to merge the "),wi=o("code"),G1=a("norm_norm_norm"),j1=a(" branch back to master (ver 0.6.x) in next week or so."),Xt=o("ul"),Gt=o("li"),J1=a("The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware "),bi=o("code"),q1=a("pip install git+https://github.com/rwightman/pytorch-image-models"),W1=a(" installs!"),Y1=n(),Ge=o("li"),yi=o("code"),K1=a("0.5.x"),Q1=a(" releases and a "),Li=o("code"),Z1=a("0.5.x"),ep=a(" branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable."),Ps=n(),_e=o("h3"),je=o("a"),ki=o("span"),N(jt.$$.fragment),tp=n(),Ii=o("span"),op=a("Jan 14, 2022"),Vs=n(),A=o("ul"),xi=o("li"),lp=a("Version 0.5.4 w/ release to be pushed to pypi. It\u2019s been a while since last pypi update and riskier changes will be merged to main branch soon\u2026"),rp=n(),Jt=o("li"),ap=a("Add ConvNeXT models /w weights from official impl ("),qt=o("a"),ip=a("https://github.com/facebookresearch/ConvNeXt"),sp=a("), a few perf tweaks, compatible with timm features"),np=n(),Ol=o("li"),dp=a("Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way\u2026"),D=o("ul"),Dl=o("li"),Ai=o("code"),cp=a("mnasnet_small"),hp=a(" - 65.6 top-1"),fp=n(),Nl=o("li"),Ci=o("code"),pp=a("mobilenetv2_050"),mp=a(" - 65.9"),vp=n(),Tl=o("li"),Oi=o("code"),up=a("lcnet_100/075/050"),_p=a(" - 72.1 / 68.8 / 63.1"),Ep=n(),$l=o("li"),Di=o("code"),gp=a("semnasnet_075"),wp=a(" - 73"),bp=n(),Sl=o("li"),Ni=o("code"),yp=a("fbnetv3_b/d/g"),Lp=a(" - 79.1 / 79.7 / 82.0"),kp=n(),Pl=o("li"),Ip=a("TinyNet models added by "),Wt=o("a"),xp=a("rsomani95"),Ap=n(),Ti=o("li"),Cp=a("LCNet added via MobileNetV3 architecture"),Ms=n(),Ee=o("h3"),Je=o("a"),$i=o("span"),N(Yt.$$.fragment),Op=n(),Si=o("span"),Dp=a("Nov 22, 2021"),Rs=n(),K=o("ul"),Vl=o("li"),Np=a("A number of updated weights anew new model defs"),_=o("ul"),Ml=o("li"),Pi=o("code"),Tp=a("eca_halonext26ts"),$p=a(" - 79.5 @ 256"),Sp=n(),Rl=o("li"),Vi=o("code"),Pp=a("resnet50_gn"),Vp=a(" (new) - 80.1 @ 224, 81.3 @ 288"),Mp=n(),qe=o("li"),Mi=o("code"),Rp=a("resnet50"),Up=a(" - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don\u2019t scale as well to higher res, "),Kt=o("a"),zp=a("weights"),Hp=a(")"),Bp=n(),Ul=o("li"),Ri=o("code"),Fp=a("resnext50_32x4d"),Xp=a(" - 81.1 @ 224, 82.0 @ 288"),Gp=n(),zl=o("li"),Ui=o("code"),jp=a("sebotnet33ts_256"),Jp=a(" (new) - 81.2 @ 224"),qp=n(),Hl=o("li"),zi=o("code"),Wp=a("lamhalobotnet50ts_256"),Yp=a(" - 81.5 @ 256"),Kp=n(),Bl=o("li"),Hi=o("code"),Qp=a("halonet50ts"),Zp=a(" - 81.7 @ 256"),em=n(),Fl=o("li"),Bi=o("code"),tm=a("halo2botnet50ts_256"),om=a(" - 82.0 @ 256"),lm=n(),Xl=o("li"),Fi=o("code"),rm=a("resnet101"),am=a(" - 82.0 @ 224, 82.8 @ 288"),im=n(),Gl=o("li"),Xi=o("code"),sm=a("resnetv2_101"),nm=a(" (new) - 82.1 @ 224, 83.0 @ 288"),dm=n(),jl=o("li"),Gi=o("code"),cm=a("resnet152"),hm=a(" - 82.8 @ 224, 83.5 @ 288"),fm=n(),Jl=o("li"),ji=o("code"),pm=a("regnetz_d8"),mm=a(" (new) - 83.5 @ 256, 84.0 @ 320"),vm=n(),ql=o("li"),Ji=o("code"),um=a("regnetz_e8"),_m=a(" (new) - 84.5 @ 256, 85.0 @ 320"),Em=n(),ge=o("li"),qi=o("code"),gm=a("vit_base_patch8_224"),wm=a(" (85.8 top-1) & "),Wi=o("code"),bm=a("in21k"),ym=a(" variant weights added thanks "),Qt=o("a"),Lm=a("Martins Bruveris"),km=n(),Zt=o("li"),Im=a("Groundwork in for FX feature extraction thanks to "),eo=o("a"),xm=a("Alexander Soare"),Yi=o("ul"),Ki=o("li"),Am=a("models updated for tracing compatibility (almost full support with some distlled transformer exceptions)"),Us=n(),we=o("h3"),We=o("a"),Qi=o("span"),N(to.$$.fragment),Cm=n(),Zi=o("span"),Om=a("Oct 19, 2021"),zs=n(),L=o("ul"),be=o("li"),Dm=a("ResNet strikes back ("),oo=o("a"),Nm=a("https://arxiv.org/abs/2110.00476"),Tm=a(") weights added, plus any extra training components used. Model weights and some more details here ("),lo=o("a"),$m=a("https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights"),Sm=a(")"),Pm=n(),es=o("li"),Vm=a("BCE loss and Repeated Augmentation support for RSB paper"),Mm=n(),ro=o("li"),Rm=a("4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here ("),ao=o("a"),Um=a("https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),zm=a(")"),Hm=n(),Wl=o("li"),Bm=a("Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl):"),ye=o("ul"),io=o("li"),Fm=a("Halo ("),so=o("a"),Xm=a("https://arxiv.org/abs/2103.12731"),Gm=a(")"),jm=n(),no=o("li"),Jm=a("Bottleneck Transformer ("),co=o("a"),qm=a("https://arxiv.org/abs/2101.11605"),Wm=a(")"),Ym=n(),ho=o("li"),Km=a("LambdaNetworks ("),fo=o("a"),Qm=a("https://arxiv.org/abs/2102.08602"),Zm=a(")"),ev=n(),Le=o("li"),tv=a("A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper ("),po=o("a"),ov=a("https://arxiv.org/abs/2103.06877"),lv=a(") in any way other than block architecture, details of official models are not available. See more here ("),mo=o("a"),rv=a("https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),av=a(")"),iv=n(),X=o("li"),sv=a("ConvMixer ("),vo=o("a"),nv=a("https://openreview.net/forum?id=TVHS5Y4dNvM"),dv=a("), CrossVit ("),uo=o("a"),cv=a("https://arxiv.org/abs/2103.14899"),hv=a("), and BeiT ("),_o=o("a"),fv=a("https://arxiv.org/abs/2106.08254"),pv=a(") architectures + weights added"),mv=n(),Yl=o("li"),vv=a("freeze/unfreeze helpers by "),Eo=o("a"),uv=a("Alexander Soare"),Hs=n(),ke=o("h3"),Ye=o("a"),ts=o("span"),N(go.$$.fragment),_v=n(),os=o("span"),Ev=a("Aug 18, 2021"),Bs=n(),Q=o("ul"),Kl=o("li"),gv=a("Optimizer bonanza!"),G=o("ul"),Ie=o("li"),wv=a("Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ "),ls=o("code"),bv=a("timm bits"),yv=n(),wo=o("a"),Lv=a("branch"),kv=a(")"),Iv=n(),rs=o("li"),xv=a("Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)"),Av=n(),bo=o("li"),Cv=a("Some cleanup on all optimizers and factory. No more "),as=o("code"),Ov=a(".data"),Dv=a(", a bit more consistency, unit tests for all!"),Nv=n(),is=o("li"),Tv=a("SGDP and AdamP still won\u2019t work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself)."),$v=n(),ss=o("li"),Sv=a("EfficientNet-V2 XL TF ported weights added, but they don\u2019t validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights."),Pv=n(),ns=o("li"),Vv=a("Added PyTorch trained EfficientNet-V2 \u2018Tiny\u2019 w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested."),this.h()},l(s){const h=s4('[data-svelte="svelte-1phssyn"]',document.head);te=l(h,"META",{name:!0,content:!0}),h.forEach(t),_s=d(s),oe=l(s,"H1",{class:!0});var Xs=r(oe);Ae=l(Xs,"A",{id:!0,class:!0,href:!0});var d_=r(Ae);fr=l(d_,"SPAN",{});var c_=r(fr);T(ot.$$.fragment,c_),c_.forEach(t),d_.forEach(t),On=d(Xs),pr=l(Xs,"SPAN",{});var h_=r(pr);Dn=i(h_,"Recent Changes"),h_.forEach(t),Xs.forEach(t),Es=d(s),le=l(s,"H3",{class:!0});var Gs=r(le);Ce=l(Gs,"A",{id:!0,class:!0,href:!0});var f_=r(Ce);mr=l(f_,"SPAN",{});var p_=r(mr);T(lt.$$.fragment,p_),p_.forEach(t),f_.forEach(t),Nn=d(Gs),vr=l(Gs,"SPAN",{});var m_=r(vr);Tn=i(m_,"July 27, 2022"),m_.forEach(t),Gs.forEach(t),gs=d(s),I=l(s,"UL",{});var U=r(I);ur=l(U,"LI",{});var v_=r(ur);$n=i(v_,"All runtime benchmark and validation result csv files are up-to-date!"),v_.forEach(t),Sn=d(U),xo=l(U,"LI",{});var Mv=r(xo);Pn=i(Mv,"A few more weights & model defs added:"),k=l(Mv,"UL",{});var C=r(k);Ao=l(C,"LI",{});var Rv=r(Ao);_r=l(Rv,"CODE",{});var u_=r(_r);Vn=i(u_,"darknetaa53"),u_.forEach(t),Mn=i(Rv," -  79.8 @ 256, 80.5 @ 288"),Rv.forEach(t),Rn=d(C),Co=l(C,"LI",{});var Uv=r(Co);Er=l(Uv,"CODE",{});var __=r(Er);Un=i(__,"convnext_nano"),__.forEach(t),zn=i(Uv," - 80.8 @ 224, 81.5 @ 288"),Uv.forEach(t),Hn=d(C),Oo=l(C,"LI",{});var zv=r(Oo);gr=l(zv,"CODE",{});var E_=r(gr);Bn=i(E_,"cs3sedarknet_l"),E_.forEach(t),Fn=i(zv," - 81.2 @ 256, 81.8 @ 288"),zv.forEach(t),Xn=d(C),Do=l(C,"LI",{});var Hv=r(Do);wr=l(Hv,"CODE",{});var g_=r(wr);Gn=i(g_,"cs3darknet_x"),g_.forEach(t),jn=i(Hv," - 81.8 @ 256, 82.2 @ 288"),Hv.forEach(t),Jn=d(C),No=l(C,"LI",{});var Bv=r(No);br=l(Bv,"CODE",{});var w_=r(br);qn=i(w_,"cs3sedarknet_x"),w_.forEach(t),Wn=i(Bv," - 82.2 @ 256, 82.7 @ 288"),Bv.forEach(t),Yn=d(C),To=l(C,"LI",{});var Fv=r(To);yr=l(Fv,"CODE",{});var b_=r(yr);Kn=i(b_,"cs3edgenet_x"),b_.forEach(t),Qn=i(Fv," - 82.2 @ 256, 82.7 @ 288"),Fv.forEach(t),Zn=d(C),$o=l(C,"LI",{});var Xv=r($o);Lr=l(Xv,"CODE",{});var y_=r(Lr);ed=i(y_,"cs3se_edgenet_x"),y_.forEach(t),td=i(Xv," - 82.8 @ 256, 83.5 @ 320"),Xv.forEach(t),C.forEach(t),Mv.forEach(t),od=d(U),Oe=l(U,"LI",{});var ds=r(Oe);kr=l(ds,"CODE",{});var L_=r(kr);ld=i(L_,"cs3*"),L_.forEach(t),rd=i(ds," weights above all trained on TPU w/ "),Ir=l(ds,"CODE",{});var k_=r(Ir);ad=i(k_,"bits_and_tpu"),k_.forEach(t),id=i(ds," branch. Thanks to TRC program!"),ds.forEach(t),sd=d(U),xr=l(U,"LI",{});var I_=r(xr);nd=i(I_,"Add output_stride=8 and 16 support to ConvNeXt (dilation)"),I_.forEach(t),dd=d(U),Ar=l(U,"LI",{});var x_=r(Ar);cd=i(x_,"deit3 models not being able to resize pos_emb fixed"),x_.forEach(t),hd=d(U),Cr=l(U,"LI",{});var A_=r(Cr);fd=i(A_,"Version 0.6.7 PyPi release (/w above bug fixes and new weighs since 0.6.5)"),A_.forEach(t),U.forEach(t),ws=d(s),re=l(s,"H3",{class:!0});var js=r(re);De=l(js,"A",{id:!0,class:!0,href:!0});var C_=r(De);Or=l(C_,"SPAN",{});var O_=r(Or);T(rt.$$.fragment,O_),O_.forEach(t),C_.forEach(t),pd=d(js),Dr=l(js,"SPAN",{});var D_=r(Dr);md=i(D_,"July 8, 2022"),D_.forEach(t),js.forEach(t),bs=d(s),So=l(s,"P",{});var N_=r(So);vd=i(N_,"More models, more fixes"),N_.forEach(t),ys=d(s),E=l(s,"UL",{});var b=r(E);Po=l(b,"LI",{});var Gv=r(Po);ud=i(Gv,"Official research models (w/ weights) added:"),ae=l(Gv,"UL",{});var Ql=r(ae);at=l(Ql,"LI",{});var Js=r(at);_d=i(Js,"EdgeNeXt from ("),it=l(Js,"A",{href:!0,rel:!0});var T_=r(it);Ed=i(T_,"https://github.com/mmaaz60/EdgeNeXt"),T_.forEach(t),gd=i(Js,")"),Js.forEach(t),wd=d(Ql),st=l(Ql,"LI",{});var qs=r(st);bd=i(qs,"MobileViT-V2 from ("),nt=l(qs,"A",{href:!0,rel:!0});var $_=r(nt);yd=i($_,"https://github.com/apple/ml-cvnets"),$_.forEach(t),Ld=i(qs,")"),qs.forEach(t),kd=d(Ql),dt=l(Ql,"LI",{});var Ws=r(dt);Id=i(Ws,"DeiT III (Revenge of the ViT) from ("),ct=l(Ws,"A",{href:!0,rel:!0});var S_=r(ct);xd=i(S_,"https://github.com/facebookresearch/deit"),S_.forEach(t),Ad=i(Ws,")"),Ws.forEach(t),Ql.forEach(t),Gv.forEach(t),Cd=d(b),Vo=l(b,"LI",{});var jv=r(Vo);Od=i(jv,"My own models:"),H=l(jv,"UL",{});var Ke=r(H);ht=l(Ke,"LI",{});var Ys=r(ht);Dd=i(Ys,"Small "),Nr=l(Ys,"CODE",{});var P_=r(Nr);Nd=i(P_,"ResNet"),P_.forEach(t),Td=i(Ys," defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14)"),Ys.forEach(t),$d=d(Ke),Ne=l(Ke,"LI",{});var cs=r(Ne);Tr=l(cs,"CODE",{});var V_=r(Tr);Sd=i(V_,"CspNet"),V_.forEach(t),Pd=i(cs," refactored with dataclass config, simplified CrossStage3 ("),$r=l(cs,"CODE",{});var M_=r($r);Vd=i(M_,"cs3"),M_.forEach(t),Md=i(cs,") option. These are closer to YOLO-v5+ backbone defs."),cs.forEach(t),Rd=d(Ke),ft=l(Ke,"LI",{});var Ks=r(ft);Ud=i(Ks,"More relative position vit fiddling. Two "),Sr=l(Ks,"CODE",{});var R_=r(Sr);zd=i(R_,"srelpos"),R_.forEach(t),Hd=i(Ks," (shared relative position) models trained, and a medium w/ class token."),Ks.forEach(t),Bd=d(Ke),pt=l(Ke,"LI",{});var Qs=r(pt);Fd=i(Qs,"Add an alternate downsample mode to EdgeNeXt and train a "),Pr=l(Qs,"CODE",{});var U_=r(Pr);Xd=i(U_,"small"),U_.forEach(t),Gd=i(Qs," model. Better than original small, but not their new USI trained weights."),Qs.forEach(t),Ke.forEach(t),jv.forEach(t),jd=d(b),Mo=l(b,"LI",{});var Jv=r(Mo);Jd=i(Jv,"My own model weight results (all ImageNet-1k training)"),g=l(Jv,"UL",{});var y=r(g);Ro=l(y,"LI",{});var qv=r(Ro);Vr=l(qv,"CODE",{});var z_=r(Vr);qd=i(z_,"resnet10t"),z_.forEach(t),Wd=i(qv," - 66.5 @ 176, 68.3 @ 224"),qv.forEach(t),Yd=d(y),Uo=l(y,"LI",{});var Wv=r(Uo);Mr=l(Wv,"CODE",{});var H_=r(Mr);Kd=i(H_,"resnet14t"),H_.forEach(t),Qd=i(Wv," - 71.3 @ 176, 72.3 @ 224"),Wv.forEach(t),Zd=d(y),zo=l(y,"LI",{});var Yv=r(zo);Rr=l(Yv,"CODE",{});var B_=r(Rr);ec=i(B_,"resnetaa50"),B_.forEach(t),tc=i(Yv," - 80.6 @ 224 , 81.6 @ 288"),Yv.forEach(t),oc=d(y),Ho=l(y,"LI",{});var Kv=r(Ho);Ur=l(Kv,"CODE",{});var F_=r(Ur);lc=i(F_,"darknet53"),F_.forEach(t),rc=i(Kv," -  80.0 @ 256, 80.5 @ 288"),Kv.forEach(t),ac=d(y),Bo=l(y,"LI",{});var Qv=r(Bo);zr=l(Qv,"CODE",{});var X_=r(zr);ic=i(X_,"cs3darknet_m"),X_.forEach(t),sc=i(Qv," - 77.0 @ 256, 77.6 @ 288"),Qv.forEach(t),nc=d(y),Fo=l(y,"LI",{});var Zv=r(Fo);Hr=l(Zv,"CODE",{});var G_=r(Hr);dc=i(G_,"cs3darknet_focus_m"),G_.forEach(t),cc=i(Zv," - 76.7 @ 256, 77.3 @ 288"),Zv.forEach(t),hc=d(y),Xo=l(y,"LI",{});var eu=r(Xo);Br=l(eu,"CODE",{});var j_=r(Br);fc=i(j_,"cs3darknet_l"),j_.forEach(t),pc=i(eu," - 80.4 @ 256, 80.9 @ 288"),eu.forEach(t),mc=d(y),Go=l(y,"LI",{});var tu=r(Go);Fr=l(tu,"CODE",{});var J_=r(Fr);vc=i(J_,"cs3darknet_focus_l"),J_.forEach(t),uc=i(tu," - 80.3 @ 256, 80.9 @ 288"),tu.forEach(t),_c=d(y),jo=l(y,"LI",{});var ou=r(jo);Xr=l(ou,"CODE",{});var q_=r(Xr);Ec=i(q_,"vit_srelpos_small_patch16_224"),q_.forEach(t),gc=i(ou," - 81.1 @ 224, 82.1 @ 320"),ou.forEach(t),wc=d(y),Jo=l(y,"LI",{});var lu=r(Jo);Gr=l(lu,"CODE",{});var W_=r(Gr);bc=i(W_,"vit_srelpos_medium_patch16_224"),W_.forEach(t),yc=i(lu," - 82.3 @ 224, 83.1 @ 320"),lu.forEach(t),Lc=d(y),qo=l(y,"LI",{});var ru=r(qo);jr=l(ru,"CODE",{});var Y_=r(jr);kc=i(Y_,"vit_relpos_small_patch16_cls_224"),Y_.forEach(t),Ic=i(ru," - 82.6 @ 224, 83.6 @ 320"),ru.forEach(t),xc=d(y),Wo=l(y,"LI",{});var au=r(Wo);Jr=l(au,"CODE",{});var K_=r(Jr);Ac=i(K_,"edgnext_small_rw"),K_.forEach(t),Cc=i(au," - 79.6 @ 224, 80.4 @ 320"),au.forEach(t),y.forEach(t),Jv.forEach(t),Oc=d(b),j=l(b,"LI",{});var yo=r(j);qr=l(yo,"CODE",{});var Q_=r(qr);Dc=i(Q_,"cs3"),Q_.forEach(t),Nc=i(yo,", "),Wr=l(yo,"CODE",{});var Z_=r(Wr);Tc=i(Z_,"darknet"),Z_.forEach(t),$c=i(yo,", and "),Yr=l(yo,"CODE",{});var e3=r(Yr);Sc=i(e3,"vit_*relpos"),e3.forEach(t),Pc=i(yo," weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs."),yo.forEach(t),Vc=d(b),Kr=l(b,"LI",{});var t3=r(Kr);Mc=i(t3,"Hugging Face Hub support fixes verified, demo notebook TBA"),t3.forEach(t),Rc=d(b),Qr=l(b,"LI",{});var o3=r(Qr);Uc=i(o3,"Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation."),o3.forEach(t),zc=d(b),ie=l(b,"LI",{});var Zl=r(ie);Hc=i(Zl,"Add support to change image extensions scanned by "),Zr=l(Zl,"CODE",{});var l3=r(Zr);Bc=i(l3,"timm"),l3.forEach(t),Fc=i(Zl," datasets/parsers. See ("),mt=l(Zl,"A",{href:!0,rel:!0});var r3=r(mt);Xc=i(r3,"https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103"),r3.forEach(t),Gc=i(Zl,")"),Zl.forEach(t),jc=d(b),J=l(b,"LI",{});var Lo=r(J);Jc=i(Lo,"Default ConvNeXt LayerNorm impl to use "),ea=l(Lo,"CODE",{});var a3=r(ea);qc=i(a3,"F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)"),a3.forEach(t),Wc=i(Lo," via "),ta=l(Lo,"CODE",{});var i3=r(ta);Yc=i(i3,"LayerNorm2d"),i3.forEach(t),Kc=i(Lo," in all cases. "),vt=l(Lo,"UL",{});var Zs=r(vt);oa=l(Zs,"LI",{});var s3=r(oa);Qc=i(s3,"a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges."),s3.forEach(t),Zc=d(Zs),Te=l(Zs,"LI",{});var hs=r(Te);eh=i(hs,"previous impl exists as "),la=l(hs,"CODE",{});var n3=r(la);th=i(n3,"LayerNormExp2d"),n3.forEach(t),oh=i(hs," in "),ra=l(hs,"CODE",{});var d3=r(ra);lh=i(d3,"models/layers/norm.py"),d3.forEach(t),hs.forEach(t),Zs.forEach(t),Lo.forEach(t),rh=d(b),aa=l(b,"LI",{});var c3=r(aa);ah=i(c3,"Numerous bug fixes"),c3.forEach(t),ih=d(b),ia=l(b,"LI",{});var h3=r(ia);sh=i(h3,"Currently testing for imminent PyPi 0.6.x release"),h3.forEach(t),nh=d(b),sa=l(b,"LI",{});var f3=r(sa);dh=i(f3,"LeViT pretraining of larger models still a WIP, they don\u2019t train well / easily without distillation. Time to add distill support (finally)?"),f3.forEach(t),ch=d(b),na=l(b,"LI",{});var p3=r(na);hh=i(p3,"ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) \u2026"),p3.forEach(t),b.forEach(t),Ls=d(s),se=l(s,"H3",{class:!0});var en=r(se);$e=l(en,"A",{id:!0,class:!0,href:!0});var m3=r($e);da=l(m3,"SPAN",{});var v3=r(da);T(ut.$$.fragment,v3),v3.forEach(t),m3.forEach(t),fh=d(en),ca=l(en,"SPAN",{});var u3=r(ca);ph=i(u3,"May 13, 2022"),u3.forEach(t),en.forEach(t),ks=d(s),x=l(s,"UL",{});var z=r(x);_t=l(z,"LI",{});var tn=r(_t);mh=i(tn,"Official Swin-V2 models and weights added from ("),Et=l(tn,"A",{href:!0,rel:!0});var _3=r(Et);vh=i(_3,"https://github.com/microsoft/Swin-Transformer"),_3.forEach(t),uh=i(tn,"). Cleaned up to support torchscript."),tn.forEach(t),_h=d(z),gt=l(z,"LI",{});var on=r(gt);Eh=i(on,"Some refactoring for existing "),ha=l(on,"CODE",{});var E3=r(ha);gh=i(E3,"timm"),E3.forEach(t),wh=i(on," Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects."),on.forEach(t),bh=d(z),Yo=l(z,"LI",{});var iu=r(Yo);yh=i(iu,"More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)"),B=l(iu,"UL",{});var Qe=r(B);Ko=l(Qe,"LI",{});var su=r(Ko);fa=l(su,"CODE",{});var g3=r(fa);Lh=i(g3,"vit_relpos_small_patch16_224"),g3.forEach(t),kh=i(su," - 81.5 @ 224, 82.5 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),su.forEach(t),Ih=d(Qe),Qo=l(Qe,"LI",{});var nu=r(Qo);pa=l(nu,"CODE",{});var w3=r(pa);xh=i(w3,"vit_relpos_medium_patch16_rpn_224"),w3.forEach(t),Ah=i(nu," - 82.3 @ 224, 83.1 @ 320 \u2014 rel pos + res-post-norm, no class token, avg pool"),nu.forEach(t),Ch=d(Qe),Zo=l(Qe,"LI",{});var du=r(Zo);ma=l(du,"CODE",{});var b3=r(ma);Oh=i(b3,"vit_relpos_medium_patch16_224"),b3.forEach(t),Dh=i(du," - 82.5 @ 224, 83.3 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),du.forEach(t),Nh=d(Qe),el=l(Qe,"LI",{});var cu=r(el);va=l(cu,"CODE",{});var y3=r(va);Th=i(y3,"vit_relpos_base_patch16_gapcls_224"),y3.forEach(t),$h=i(cu," - 82.8 @ 224, 83.9 @ 320 \u2014 rel pos, layer scale, class token, avg pool (by mistake)"),cu.forEach(t),Qe.forEach(t),iu.forEach(t),Sh=d(z),ua=l(z,"LI",{});var L3=r(ua);Ph=i(L3,"Bring 512 dim, 8-head \u2018medium\u2019 ViT model variant back to life (after using in a pre DeiT \u2018small\u2019 model for first ViT impl back in 2020)"),L3.forEach(t),Vh=d(z),_a=l(z,"LI",{});var k3=r(_a);Mh=i(k3,"Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials"),k3.forEach(t),Rh=d(z),ne=l(z,"LI",{});var er=r(ne);Uh=i(er,"Sequencer2D impl ("),wt=l(er,"A",{href:!0,rel:!0});var I3=r(wt);zh=i(I3,"https://arxiv.org/abs/2205.01972"),I3.forEach(t),Hh=i(er,"), added via PR from author ("),bt=l(er,"A",{href:!0,rel:!0});var x3=r(bt);Bh=i(x3,"https://github.com/okojoalg"),x3.forEach(t),Fh=i(er,")"),er.forEach(t),z.forEach(t),Is=d(s),de=l(s,"H3",{class:!0});var ln=r(de);Se=l(ln,"A",{id:!0,class:!0,href:!0});var A3=r(Se);Ea=l(A3,"SPAN",{});var C3=r(Ea);T(yt.$$.fragment,C3),C3.forEach(t),A3.forEach(t),Xh=d(ln),ga=l(ln,"SPAN",{});var O3=r(ga);Gh=i(O3,"May 2, 2022"),O3.forEach(t),ln.forEach(t),xs=d(s),q=l(s,"UL",{});var tr=r(q);W=l(tr,"LI",{});var ko=r(W);jh=i(ko,"Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) ("),wa=l(ko,"CODE",{});var D3=r(wa);Jh=i(D3,"vision_transformer_relpos.py"),D3.forEach(t),qh=i(ko,") and Residual Post-Norm branches (from Swin-V2) ("),ba=l(ko,"CODE",{});var N3=r(ba);Wh=i(N3,"vision_transformer*.py"),N3.forEach(t),Yh=i(ko,")"),ce=l(ko,"UL",{});var or=r(ce);tl=l(or,"LI",{});var hu=r(tl);ya=l(hu,"CODE",{});var T3=r(ya);Kh=i(T3,"vit_relpos_base_patch32_plus_rpn_256"),T3.forEach(t),Qh=i(hu," - 79.5 @ 256, 80.6 @ 320 \u2014 rel pos + extended width + res-post-norm, no class token, avg pool"),hu.forEach(t),Zh=d(or),ol=l(or,"LI",{});var fu=r(ol);La=l(fu,"CODE",{});var $3=r(La);e2=i($3,"vit_relpos_base_patch16_224"),$3.forEach(t),t2=i(fu," - 82.5 @ 224, 83.6 @ 320 \u2014 rel pos, layer scale, no class token, avg pool"),fu.forEach(t),o2=d(or),ll=l(or,"LI",{});var pu=r(ll);ka=l(pu,"CODE",{});var S3=r(ka);l2=i(S3,"vit_base_patch16_rpn_224"),S3.forEach(t),r2=i(pu," - 82.3 @ 224 \u2014 rel pos + res-post-norm, no class token, avg pool"),pu.forEach(t),or.forEach(t),ko.forEach(t),a2=d(tr),Lt=l(tr,"LI",{});var rn=r(Lt);i2=i(rn,"Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie "),Ia=l(rn,"CODE",{});var P3=r(Ia);s2=i(P3,"How to Train Your ViT"),P3.forEach(t),n2=i(rn,")"),rn.forEach(t),d2=d(tr),rl=l(tr,"LI",{});var mu=r(rl);xa=l(mu,"CODE",{});var V3=r(xa);c2=i(V3,"vit_*"),V3.forEach(t),h2=i(mu," models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae)."),mu.forEach(t),tr.forEach(t),As=d(s),he=l(s,"H3",{class:!0});var an=r(he);Pe=l(an,"A",{id:!0,class:!0,href:!0});var M3=r(Pe);Aa=l(M3,"SPAN",{});var R3=r(Aa);T(kt.$$.fragment,R3),R3.forEach(t),M3.forEach(t),f2=d(an),Ca=l(an,"SPAN",{});var U3=r(Ca);p2=i(U3,"April 22, 2022"),U3.forEach(t),an.forEach(t),Cs=d(s),Ve=l(s,"UL",{});var sn=r(Ve);R=l(sn,"LI",{});var xe=r(R);Oa=l(xe,"CODE",{});var z3=r(Oa);m2=i(z3,"timm"),z3.forEach(t),v2=i(xe," models are now officially supported in "),It=l(xe,"A",{href:!0,rel:!0});var H3=r(It);u2=i(H3,"fast.ai"),H3.forEach(t),_2=i(xe,"! Just in time for the new Practical Deep Learning course. "),Da=l(xe,"CODE",{});var B3=r(Da);E2=i(B3,"timmdocs"),B3.forEach(t),g2=i(xe," documentation link updated to "),xt=l(xe,"A",{href:!0,rel:!0});var F3=r(xt);w2=i(F3,"timm.fast.ai"),F3.forEach(t),b2=i(xe,"."),xe.forEach(t),y2=d(sn),Me=l(sn,"LI",{});var fs=r(Me);L2=i(fs,"Two more model weights added in the TPU trained "),At=l(fs,"A",{href:!0,rel:!0});var X3=r(At);k2=i(X3,"series"),X3.forEach(t),I2=i(fs,". Some In22k pretrain still in progress."),Ct=l(fs,"UL",{});var nn=r(Ct);al=l(nn,"LI",{});var vu=r(al);Na=l(vu,"CODE",{});var G3=r(Na);x2=i(G3,"seresnext101d_32x8d"),G3.forEach(t),A2=i(vu," - 83.69 @ 224, 84.35 @ 288"),vu.forEach(t),C2=d(nn),il=l(nn,"LI",{});var uu=r(il);Ta=l(uu,"CODE",{});var j3=r(Ta);O2=i(j3,"seresnextaa101d_32x8d"),j3.forEach(t),D2=i(uu," (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288"),uu.forEach(t),nn.forEach(t),fs.forEach(t),sn.forEach(t),Os=d(s),fe=l(s,"H3",{class:!0});var dn=r(fe);Re=l(dn,"A",{id:!0,class:!0,href:!0});var J3=r(Re);$a=l(J3,"SPAN",{});var q3=r($a);T(Ot.$$.fragment,q3),q3.forEach(t),J3.forEach(t),N2=d(dn),Sa=l(dn,"SPAN",{});var W3=r(Sa);T2=i(W3,"March 23, 2022"),W3.forEach(t),dn.forEach(t),Ds=d(s),Ue=l(s,"UL",{});var cn=r(Ue);Y=l(cn,"LI",{});var Io=r(Y);$2=i(Io,"Add "),Pa=l(Io,"CODE",{});var Y3=r(Pa);S2=i(Y3,"ParallelBlock"),Y3.forEach(t),P2=i(Io," and "),Va=l(Io,"CODE",{});var K3=r(Va);V2=i(K3,"LayerScale"),K3.forEach(t),M2=i(Io," option to base vit models to support model configs in "),Dt=l(Io,"A",{href:!0,rel:!0});var Q3=r(Dt);R2=i(Q3,"Three things everyone should know about ViT"),Q3.forEach(t),Io.forEach(t),U2=d(cn),sl=l(cn,"LI",{});var _u=r(sl);Ma=l(_u,"CODE",{});var Z3=r(Ma);z2=i(Z3,"convnext_tiny_hnf"),Z3.forEach(t),H2=i(_u," (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs."),_u.forEach(t),cn.forEach(t),Ns=d(s),pe=l(s,"H3",{class:!0});var hn=r(pe);ze=l(hn,"A",{id:!0,class:!0,href:!0});var e8=r(ze);Ra=l(e8,"SPAN",{});var t8=r(Ra);T(Nt.$$.fragment,t8),t8.forEach(t),e8.forEach(t),B2=d(hn),Ua=l(hn,"SPAN",{});var o8=r(Ua);F2=i(o8,"March 21, 2022"),o8.forEach(t),hn.forEach(t),Ts=d(s),m=l(s,"UL",{});var u=r(m);F=l(u,"LI",{});var Ze=r(F);X2=i(Ze,"Merge "),za=l(Ze,"CODE",{});var l8=r(za);G2=i(l8,"norm_norm_norm"),l8.forEach(t),j2=i(Ze,". "),Ha=l(Ze,"STRONG",{});var r8=r(Ha);J2=i(r8,"IMPORTANT"),r8.forEach(t),q2=i(Ze," this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch "),Tt=l(Ze,"A",{href:!0,rel:!0});var a8=r(Tt);Ba=l(a8,"CODE",{});var i8=r(Ba);W2=i(i8,"0.5.x"),i8.forEach(t),a8.forEach(t),Y2=i(Ze," or a previous 0.5.x release can be used if stability is required."),Ze.forEach(t),K2=d(u),$t=l(u,"LI",{});var fn=r($t);Q2=i(fn,"Significant weights update (all TPU trained) as described in this "),St=l(fn,"A",{href:!0,rel:!0});var s8=r(St);Z2=i(s8,"release"),s8.forEach(t),p=l(fn,"UL",{});var v=r(p);nl=l(v,"LI",{});var Eu=r(nl);Fa=l(Eu,"CODE",{});var n8=r(Fa);ef=i(n8,"regnety_040"),n8.forEach(t),tf=i(Eu," - 82.3 @ 224, 82.96 @ 288"),Eu.forEach(t),of=d(v),dl=l(v,"LI",{});var gu=r(dl);Xa=l(gu,"CODE",{});var d8=r(Xa);lf=i(d8,"regnety_064"),d8.forEach(t),rf=i(gu," - 83.0 @ 224, 83.65 @ 288"),gu.forEach(t),af=d(v),cl=l(v,"LI",{});var wu=r(cl);Ga=l(wu,"CODE",{});var c8=r(Ga);sf=i(c8,"regnety_080"),c8.forEach(t),nf=i(wu," - 83.17 @ 224, 83.86 @ 288"),wu.forEach(t),df=d(v),hl=l(v,"LI",{});var bu=r(hl);ja=l(bu,"CODE",{});var h8=r(ja);cf=i(h8,"regnetv_040"),h8.forEach(t),hf=i(bu," - 82.44 @ 224, 83.18 @ 288   (timm pre-act)"),bu.forEach(t),ff=d(v),fl=l(v,"LI",{});var yu=r(fl);Ja=l(yu,"CODE",{});var f8=r(Ja);pf=i(f8,"regnetv_064"),f8.forEach(t),mf=i(yu," - 83.1 @ 224, 83.71 @ 288   (timm pre-act)"),yu.forEach(t),vf=d(v),pl=l(v,"LI",{});var Lu=r(pl);qa=l(Lu,"CODE",{});var p8=r(qa);uf=i(p8,"regnetz_040"),p8.forEach(t),_f=i(Lu," - 83.67 @ 256, 84.25 @ 320"),Lu.forEach(t),Ef=d(v),ml=l(v,"LI",{});var ku=r(ml);Wa=l(ku,"CODE",{});var m8=r(Wa);gf=i(m8,"regnetz_040h"),m8.forEach(t),wf=i(ku," - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)"),ku.forEach(t),bf=d(v),vl=l(v,"LI",{});var Iu=r(vl);Ya=l(Iu,"CODE",{});var v8=r(Ya);yf=i(v8,"resnetv2_50d_gn"),v8.forEach(t),Lf=i(Iu," - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)"),Iu.forEach(t),kf=d(v),ul=l(v,"LI",{});var xu=r(ul);Ka=l(xu,"CODE",{});var u8=r(Ka);If=i(u8,"resnetv2_50d_evos"),u8.forEach(t),xf=i(xu," 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)"),xu.forEach(t),Af=d(v),_l=l(v,"LI",{});var Au=r(_l);Qa=l(Au,"CODE",{});var _8=r(Qa);Cf=i(_8,"regnetz_c16_evos"),_8.forEach(t),Of=i(Au,"  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)"),Au.forEach(t),Df=d(v),El=l(v,"LI",{});var Cu=r(El);Za=l(Cu,"CODE",{});var E8=r(Za);Nf=i(E8,"regnetz_d8_evos"),E8.forEach(t),Tf=i(Cu,"  - 83.42 @ 256, 84.04 @ 320 (EvoNormS)"),Cu.forEach(t),$f=d(v),gl=l(v,"LI",{});var Ou=r(gl);ei=l(Ou,"CODE",{});var g8=r(ei);Sf=i(g8,"xception41p"),g8.forEach(t),Pf=i(Ou," - 82 @ 299   (timm pre-act)"),Ou.forEach(t),Vf=d(v),wl=l(v,"LI",{});var Du=r(wl);ti=l(Du,"CODE",{});var w8=r(ti);Mf=i(w8,"xception65"),w8.forEach(t),Rf=i(Du," -  83.17 @ 299"),Du.forEach(t),Uf=d(v),bl=l(v,"LI",{});var Nu=r(bl);oi=l(Nu,"CODE",{});var b8=r(oi);zf=i(b8,"xception65p"),b8.forEach(t),Hf=i(Nu," -  83.14 @ 299   (timm pre-act)"),Nu.forEach(t),Bf=d(v),yl=l(v,"LI",{});var Tu=r(yl);li=l(Tu,"CODE",{});var y8=r(li);Ff=i(y8,"resnext101_64x4d"),y8.forEach(t),Xf=i(Tu," - 82.46 @ 224, 83.16 @ 288"),Tu.forEach(t),Gf=d(v),Ll=l(v,"LI",{});var $u=r(Ll);ri=l($u,"CODE",{});var L8=r(ri);jf=i(L8,"seresnext101_32x8d"),L8.forEach(t),Jf=i($u," - 83.57 @ 224, 84.270 @ 288"),$u.forEach(t),qf=d(v),kl=l(v,"LI",{});var Su=r(kl);ai=l(Su,"CODE",{});var k8=r(ai);Wf=i(k8,"resnetrs200"),k8.forEach(t),Yf=i(Su," - 83.85 @ 256, 84.44 @ 320"),Su.forEach(t),v.forEach(t),fn.forEach(t),Kf=d(u),ii=l(u,"LI",{});var I8=r(ii);Qf=i(I8,"HuggingFace hub support fixed w/ initial groundwork for allowing alternative \u2018config sources\u2019 for pretrained model definitions and weights (generic local file / remote url support soon)"),I8.forEach(t),Zf=d(u),Pt=l(u,"LI",{});var pn=r(Pt);e1=i(pn,"SwinTransformer-V2 implementation added. Submitted by "),Vt=l(pn,"A",{href:!0,rel:!0});var x8=r(Vt);t1=i(x8,"Christoph Reich"),x8.forEach(t),o1=i(pn,". Training experiments and model changes by myself are ongoing so expect compat breaks."),pn.forEach(t),l1=d(u),Il=l(u,"LI",{});var Pu=r(Il);r1=i(Pu,"Swin-S3 (AutoFormerV2) models / weights added from "),Mt=l(Pu,"A",{href:!0,rel:!0});var A8=r(Mt);a1=i(A8,"https://github.com/microsoft/Cream/tree/main/AutoFormerV2"),A8.forEach(t),Pu.forEach(t),i1=d(u),xl=l(u,"LI",{});var Vu=r(xl);s1=i(Vu,"MobileViT models w/ weights adapted from "),Rt=l(Vu,"A",{href:!0,rel:!0});var C8=r(Rt);n1=i(C8,"https://github.com/apple/ml-cvnets"),C8.forEach(t),Vu.forEach(t),d1=d(u),Al=l(u,"LI",{});var Mu=r(Al);c1=i(Mu,"PoolFormer models w/ weights adapted from "),Ut=l(Mu,"A",{href:!0,rel:!0});var O8=r(Ut);h1=i(O8,"https://github.com/sail-sg/poolformer"),O8.forEach(t),Mu.forEach(t),f1=d(u),Cl=l(u,"LI",{});var Ru=r(Cl);p1=i(Ru,"VOLO models w/ weights adapted from "),zt=l(Ru,"A",{href:!0,rel:!0});var D8=r(zt);m1=i(D8,"https://github.com/sail-sg/volo"),D8.forEach(t),Ru.forEach(t),v1=d(u),si=l(u,"LI",{});var N8=r(si);u1=i(N8,"Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc"),N8.forEach(t),_1=d(u),ni=l(u,"LI",{});var T8=r(ni);E1=i(T8,"Enhance support for alternate norm + act (\u2018NormAct\u2019) layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception"),T8.forEach(t),g1=d(u),di=l(u,"LI",{});var $8=r(di);w1=i($8,"Grouped conv support added to EfficientNet family"),$8.forEach(t),b1=d(u),ci=l(u,"LI",{});var S8=r(ci);y1=i(S8,"Add \u2018group matching\u2019 API to all models to allow grouping model parameters for application of \u2018layer-wise\u2019 LR decay, lr scale added to LR scheduler"),S8.forEach(t),L1=d(u),hi=l(u,"LI",{});var P8=r(hi);k1=i(P8,"Gradient checkpointing support added to many models"),P8.forEach(t),I1=d(u),me=l(u,"LI",{});var lr=r(me);fi=l(lr,"CODE",{});var V8=r(fi);x1=i(V8,"forward_head(x, pre_logits=False)"),V8.forEach(t),A1=i(lr," fn added to all models to allow separate calls of "),pi=l(lr,"CODE",{});var M8=r(pi);C1=i(M8,"forward_features"),M8.forEach(t),O1=i(lr," + "),mi=l(lr,"CODE",{});var R8=r(mi);D1=i(R8,"forward_head"),R8.forEach(t),lr.forEach(t),N1=d(u),He=l(u,"LI",{});var ps=r(He);T1=i(ps,"All vision transformer and vision MLP models update to return non-pooled / non-token selected features from "),vi=l(ps,"CODE",{});var U8=r(vi);$1=i(U8,"foward_features"),U8.forEach(t),S1=i(ps,", for consistency with CNN models, token selection or pooling now applied in "),ui=l(ps,"CODE",{});var z8=r(ui);P1=i(z8,"forward_head"),z8.forEach(t),ps.forEach(t),u.forEach(t),$s=d(s),ve=l(s,"H3",{class:!0});var mn=r(ve);Be=l(mn,"A",{id:!0,class:!0,href:!0});var H8=r(Be);_i=l(H8,"SPAN",{});var B8=r(_i);T(Ht.$$.fragment,B8),B8.forEach(t),H8.forEach(t),V1=d(mn),Ei=l(mn,"SPAN",{});var F8=r(Ei);M1=i(F8,"Feb 2, 2022"),F8.forEach(t),mn.forEach(t),Ss=d(s),Fe=l(s,"UL",{});var vn=r(Fe);ue=l(vn,"LI",{});var rr=r(ue);Bt=l(rr,"A",{href:!0,rel:!0});var X8=r(Bt);R1=i(X8,"Chris Hughes"),X8.forEach(t),U1=i(rr," posted an exhaustive run through of "),gi=l(rr,"CODE",{});var G8=r(gi);z1=i(G8,"timm"),G8.forEach(t),H1=i(rr," on his blog yesterday. Well worth a read. "),Ft=l(rr,"A",{href:!0,rel:!0});var j8=r(Ft);B1=i(j8,"Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide"),j8.forEach(t),rr.forEach(t),F1=d(vn),Xe=l(vn,"LI",{});var ms=r(Xe);X1=i(ms,"I\u2019m currently prepping to merge the "),wi=l(ms,"CODE",{});var J8=r(wi);G1=i(J8,"norm_norm_norm"),J8.forEach(t),j1=i(ms," branch back to master (ver 0.6.x) in next week or so."),Xt=l(ms,"UL",{});var un=r(Xt);Gt=l(un,"LI",{});var _n=r(Gt);J1=i(_n,"The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware "),bi=l(_n,"CODE",{});var q8=r(bi);q1=i(q8,"pip install git+https://github.com/rwightman/pytorch-image-models"),q8.forEach(t),W1=i(_n," installs!"),_n.forEach(t),Y1=d(un),Ge=l(un,"LI",{});var vs=r(Ge);yi=l(vs,"CODE",{});var W8=r(yi);K1=i(W8,"0.5.x"),W8.forEach(t),Q1=i(vs," releases and a "),Li=l(vs,"CODE",{});var Y8=r(Li);Z1=i(Y8,"0.5.x"),Y8.forEach(t),ep=i(vs," branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable."),vs.forEach(t),un.forEach(t),ms.forEach(t),vn.forEach(t),Ps=d(s),_e=l(s,"H3",{class:!0});var En=r(_e);je=l(En,"A",{id:!0,class:!0,href:!0});var K8=r(je);ki=l(K8,"SPAN",{});var Q8=r(ki);T(jt.$$.fragment,Q8),Q8.forEach(t),K8.forEach(t),tp=d(En),Ii=l(En,"SPAN",{});var Z8=r(Ii);op=i(Z8,"Jan 14, 2022"),Z8.forEach(t),En.forEach(t),Vs=d(s),A=l(s,"UL",{});var Z=r(A);xi=l(Z,"LI",{});var e0=r(xi);lp=i(e0,"Version 0.5.4 w/ release to be pushed to pypi. It\u2019s been a while since last pypi update and riskier changes will be merged to main branch soon\u2026"),e0.forEach(t),rp=d(Z),Jt=l(Z,"LI",{});var gn=r(Jt);ap=i(gn,"Add ConvNeXT models /w weights from official impl ("),qt=l(gn,"A",{href:!0,rel:!0});var t0=r(qt);ip=i(t0,"https://github.com/facebookresearch/ConvNeXt"),t0.forEach(t),sp=i(gn,"), a few perf tweaks, compatible with timm features"),gn.forEach(t),np=d(Z),Ol=l(Z,"LI",{});var Uu=r(Ol);dp=i(Uu,"Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way\u2026"),D=l(Uu,"UL",{});var ee=r(D);Dl=l(ee,"LI",{});var zu=r(Dl);Ai=l(zu,"CODE",{});var o0=r(Ai);cp=i(o0,"mnasnet_small"),o0.forEach(t),hp=i(zu," - 65.6 top-1"),zu.forEach(t),fp=d(ee),Nl=l(ee,"LI",{});var Hu=r(Nl);Ci=l(Hu,"CODE",{});var l0=r(Ci);pp=i(l0,"mobilenetv2_050"),l0.forEach(t),mp=i(Hu," - 65.9"),Hu.forEach(t),vp=d(ee),Tl=l(ee,"LI",{});var Bu=r(Tl);Oi=l(Bu,"CODE",{});var r0=r(Oi);up=i(r0,"lcnet_100/075/050"),r0.forEach(t),_p=i(Bu," - 72.1 / 68.8 / 63.1"),Bu.forEach(t),Ep=d(ee),$l=l(ee,"LI",{});var Fu=r($l);Di=l(Fu,"CODE",{});var a0=r(Di);gp=i(a0,"semnasnet_075"),a0.forEach(t),wp=i(Fu," - 73"),Fu.forEach(t),bp=d(ee),Sl=l(ee,"LI",{});var Xu=r(Sl);Ni=l(Xu,"CODE",{});var i0=r(Ni);yp=i(i0,"fbnetv3_b/d/g"),i0.forEach(t),Lp=i(Xu," - 79.1 / 79.7 / 82.0"),Xu.forEach(t),ee.forEach(t),Uu.forEach(t),kp=d(Z),Pl=l(Z,"LI",{});var Gu=r(Pl);Ip=i(Gu,"TinyNet models added by "),Wt=l(Gu,"A",{href:!0,rel:!0});var s0=r(Wt);xp=i(s0,"rsomani95"),s0.forEach(t),Gu.forEach(t),Ap=d(Z),Ti=l(Z,"LI",{});var n0=r(Ti);Cp=i(n0,"LCNet added via MobileNetV3 architecture"),n0.forEach(t),Z.forEach(t),Ms=d(s),Ee=l(s,"H3",{class:!0});var wn=r(Ee);Je=l(wn,"A",{id:!0,class:!0,href:!0});var d0=r(Je);$i=l(d0,"SPAN",{});var c0=r($i);T(Yt.$$.fragment,c0),c0.forEach(t),d0.forEach(t),Op=d(wn),Si=l(wn,"SPAN",{});var h0=r(Si);Dp=i(h0,"Nov 22, 2021"),h0.forEach(t),wn.forEach(t),Rs=d(s),K=l(s,"UL",{});var ar=r(K);Vl=l(ar,"LI",{});var ju=r(Vl);Np=i(ju,"A number of updated weights anew new model defs"),_=l(ju,"UL",{});var w=r(_);Ml=l(w,"LI",{});var Ju=r(Ml);Pi=l(Ju,"CODE",{});var f0=r(Pi);Tp=i(f0,"eca_halonext26ts"),f0.forEach(t),$p=i(Ju," - 79.5 @ 256"),Ju.forEach(t),Sp=d(w),Rl=l(w,"LI",{});var qu=r(Rl);Vi=l(qu,"CODE",{});var p0=r(Vi);Pp=i(p0,"resnet50_gn"),p0.forEach(t),Vp=i(qu," (new) - 80.1 @ 224, 81.3 @ 288"),qu.forEach(t),Mp=d(w),qe=l(w,"LI",{});var us=r(qe);Mi=l(us,"CODE",{});var m0=r(Mi);Rp=i(m0,"resnet50"),m0.forEach(t),Up=i(us," - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don\u2019t scale as well to higher res, "),Kt=l(us,"A",{href:!0,rel:!0});var v0=r(Kt);zp=i(v0,"weights"),v0.forEach(t),Hp=i(us,")"),us.forEach(t),Bp=d(w),Ul=l(w,"LI",{});var Wu=r(Ul);Ri=l(Wu,"CODE",{});var u0=r(Ri);Fp=i(u0,"resnext50_32x4d"),u0.forEach(t),Xp=i(Wu," - 81.1 @ 224, 82.0 @ 288"),Wu.forEach(t),Gp=d(w),zl=l(w,"LI",{});var Yu=r(zl);Ui=l(Yu,"CODE",{});var _0=r(Ui);jp=i(_0,"sebotnet33ts_256"),_0.forEach(t),Jp=i(Yu," (new) - 81.2 @ 224"),Yu.forEach(t),qp=d(w),Hl=l(w,"LI",{});var Ku=r(Hl);zi=l(Ku,"CODE",{});var E0=r(zi);Wp=i(E0,"lamhalobotnet50ts_256"),E0.forEach(t),Yp=i(Ku," - 81.5 @ 256"),Ku.forEach(t),Kp=d(w),Bl=l(w,"LI",{});var Qu=r(Bl);Hi=l(Qu,"CODE",{});var g0=r(Hi);Qp=i(g0,"halonet50ts"),g0.forEach(t),Zp=i(Qu," - 81.7 @ 256"),Qu.forEach(t),em=d(w),Fl=l(w,"LI",{});var Zu=r(Fl);Bi=l(Zu,"CODE",{});var w0=r(Bi);tm=i(w0,"halo2botnet50ts_256"),w0.forEach(t),om=i(Zu," - 82.0 @ 256"),Zu.forEach(t),lm=d(w),Xl=l(w,"LI",{});var e_=r(Xl);Fi=l(e_,"CODE",{});var b0=r(Fi);rm=i(b0,"resnet101"),b0.forEach(t),am=i(e_," - 82.0 @ 224, 82.8 @ 288"),e_.forEach(t),im=d(w),Gl=l(w,"LI",{});var t_=r(Gl);Xi=l(t_,"CODE",{});var y0=r(Xi);sm=i(y0,"resnetv2_101"),y0.forEach(t),nm=i(t_," (new) - 82.1 @ 224, 83.0 @ 288"),t_.forEach(t),dm=d(w),jl=l(w,"LI",{});var o_=r(jl);Gi=l(o_,"CODE",{});var L0=r(Gi);cm=i(L0,"resnet152"),L0.forEach(t),hm=i(o_," - 82.8 @ 224, 83.5 @ 288"),o_.forEach(t),fm=d(w),Jl=l(w,"LI",{});var l_=r(Jl);ji=l(l_,"CODE",{});var k0=r(ji);pm=i(k0,"regnetz_d8"),k0.forEach(t),mm=i(l_," (new) - 83.5 @ 256, 84.0 @ 320"),l_.forEach(t),vm=d(w),ql=l(w,"LI",{});var r_=r(ql);Ji=l(r_,"CODE",{});var I0=r(Ji);um=i(I0,"regnetz_e8"),I0.forEach(t),_m=i(r_," (new) - 84.5 @ 256, 85.0 @ 320"),r_.forEach(t),w.forEach(t),ju.forEach(t),Em=d(ar),ge=l(ar,"LI",{});var ir=r(ge);qi=l(ir,"CODE",{});var x0=r(qi);gm=i(x0,"vit_base_patch8_224"),x0.forEach(t),wm=i(ir," (85.8 top-1) & "),Wi=l(ir,"CODE",{});var A0=r(Wi);bm=i(A0,"in21k"),A0.forEach(t),ym=i(ir," variant weights added thanks "),Qt=l(ir,"A",{href:!0,rel:!0});var C0=r(Qt);Lm=i(C0,"Martins Bruveris"),C0.forEach(t),ir.forEach(t),km=d(ar),Zt=l(ar,"LI",{});var bn=r(Zt);Im=i(bn,"Groundwork in for FX feature extraction thanks to "),eo=l(bn,"A",{href:!0,rel:!0});var O0=r(eo);xm=i(O0,"Alexander Soare"),O0.forEach(t),Yi=l(bn,"UL",{});var D0=r(Yi);Ki=l(D0,"LI",{});var N0=r(Ki);Am=i(N0,"models updated for tracing compatibility (almost full support with some distlled transformer exceptions)"),N0.forEach(t),D0.forEach(t),bn.forEach(t),ar.forEach(t),Us=d(s),we=l(s,"H3",{class:!0});var yn=r(we);We=l(yn,"A",{id:!0,class:!0,href:!0});var T0=r(We);Qi=l(T0,"SPAN",{});var $0=r(Qi);T(to.$$.fragment,$0),$0.forEach(t),T0.forEach(t),Cm=d(yn),Zi=l(yn,"SPAN",{});var S0=r(Zi);Om=i(S0,"Oct 19, 2021"),S0.forEach(t),yn.forEach(t),zs=d(s),L=l(s,"UL",{});var O=r(L);be=l(O,"LI",{});var sr=r(be);Dm=i(sr,"ResNet strikes back ("),oo=l(sr,"A",{href:!0,rel:!0});var P0=r(oo);Nm=i(P0,"https://arxiv.org/abs/2110.00476"),P0.forEach(t),Tm=i(sr,") weights added, plus any extra training components used. Model weights and some more details here ("),lo=l(sr,"A",{href:!0,rel:!0});var V0=r(lo);$m=i(V0,"https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights"),V0.forEach(t),Sm=i(sr,")"),sr.forEach(t),Pm=d(O),es=l(O,"LI",{});var M0=r(es);Vm=i(M0,"BCE loss and Repeated Augmentation support for RSB paper"),M0.forEach(t),Mm=d(O),ro=l(O,"LI",{});var Ln=r(ro);Rm=i(Ln,"4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here ("),ao=l(Ln,"A",{href:!0,rel:!0});var R0=r(ao);Um=i(R0,"https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),R0.forEach(t),zm=i(Ln,")"),Ln.forEach(t),Hm=d(O),Wl=l(O,"LI",{});var a_=r(Wl);Bm=i(a_,"Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl):"),ye=l(a_,"UL",{});var nr=r(ye);io=l(nr,"LI",{});var kn=r(io);Fm=i(kn,"Halo ("),so=l(kn,"A",{href:!0,rel:!0});var U0=r(so);Xm=i(U0,"https://arxiv.org/abs/2103.12731"),U0.forEach(t),Gm=i(kn,")"),kn.forEach(t),jm=d(nr),no=l(nr,"LI",{});var In=r(no);Jm=i(In,"Bottleneck Transformer ("),co=l(In,"A",{href:!0,rel:!0});var z0=r(co);qm=i(z0,"https://arxiv.org/abs/2101.11605"),z0.forEach(t),Wm=i(In,")"),In.forEach(t),Ym=d(nr),ho=l(nr,"LI",{});var xn=r(ho);Km=i(xn,"LambdaNetworks ("),fo=l(xn,"A",{href:!0,rel:!0});var H0=r(fo);Qm=i(H0,"https://arxiv.org/abs/2102.08602"),H0.forEach(t),Zm=i(xn,")"),xn.forEach(t),nr.forEach(t),a_.forEach(t),ev=d(O),Le=l(O,"LI",{});var dr=r(Le);tv=i(dr,"A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper ("),po=l(dr,"A",{href:!0,rel:!0});var B0=r(po);ov=i(B0,"https://arxiv.org/abs/2103.06877"),B0.forEach(t),lv=i(dr,") in any way other than block architecture, details of official models are not available. See more here ("),mo=l(dr,"A",{href:!0,rel:!0});var F0=r(mo);rv=i(F0,"https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),F0.forEach(t),av=i(dr,")"),dr.forEach(t),iv=d(O),X=l(O,"LI",{});var et=r(X);sv=i(et,"ConvMixer ("),vo=l(et,"A",{href:!0,rel:!0});var X0=r(vo);nv=i(X0,"https://openreview.net/forum?id=TVHS5Y4dNvM"),X0.forEach(t),dv=i(et,"), CrossVit ("),uo=l(et,"A",{href:!0,rel:!0});var G0=r(uo);cv=i(G0,"https://arxiv.org/abs/2103.14899"),G0.forEach(t),hv=i(et,"), and BeiT ("),_o=l(et,"A",{href:!0,rel:!0});var j0=r(_o);fv=i(j0,"https://arxiv.org/abs/2106.08254"),j0.forEach(t),pv=i(et,") architectures + weights added"),et.forEach(t),mv=d(O),Yl=l(O,"LI",{});var i_=r(Yl);vv=i(i_,"freeze/unfreeze helpers by "),Eo=l(i_,"A",{href:!0,rel:!0});var J0=r(Eo);uv=i(J0,"Alexander Soare"),J0.forEach(t),i_.forEach(t),O.forEach(t),Hs=d(s),ke=l(s,"H3",{class:!0});var An=r(ke);Ye=l(An,"A",{id:!0,class:!0,href:!0});var q0=r(Ye);ts=l(q0,"SPAN",{});var W0=r(ts);T(go.$$.fragment,W0),W0.forEach(t),q0.forEach(t),_v=d(An),os=l(An,"SPAN",{});var Y0=r(os);Ev=i(Y0,"Aug 18, 2021"),Y0.forEach(t),An.forEach(t),Bs=d(s),Q=l(s,"UL",{});var cr=r(Q);Kl=l(cr,"LI",{});var s_=r(Kl);gv=i(s_,"Optimizer bonanza!"),G=l(s_,"UL",{});var tt=r(G);Ie=l(tt,"LI",{});var hr=r(Ie);wv=i(hr,"Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ "),ls=l(hr,"CODE",{});var K0=r(ls);bv=i(K0,"timm bits"),K0.forEach(t),yv=d(hr),wo=l(hr,"A",{href:!0,rel:!0});var Q0=r(wo);Lv=i(Q0,"branch"),Q0.forEach(t),kv=i(hr,")"),hr.forEach(t),Iv=d(tt),rs=l(tt,"LI",{});var Z0=r(rs);xv=i(Z0,"Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)"),Z0.forEach(t),Av=d(tt),bo=l(tt,"LI",{});var Cn=r(bo);Cv=i(Cn,"Some cleanup on all optimizers and factory. No more "),as=l(Cn,"CODE",{});var e4=r(as);Ov=i(e4,".data"),e4.forEach(t),Dv=i(Cn,", a bit more consistency, unit tests for all!"),Cn.forEach(t),Nv=d(tt),is=l(tt,"LI",{});var t4=r(is);Tv=i(t4,"SGDP and AdamP still won\u2019t work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself)."),t4.forEach(t),tt.forEach(t),s_.forEach(t),$v=d(cr),ss=l(cr,"LI",{});var o4=r(ss);Sv=i(o4,"EfficientNet-V2 XL TF ported weights added, but they don\u2019t validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights."),o4.forEach(t),Pv=d(cr),ns=l(cr,"LI",{});var l4=r(ns);Vv=i(l4,"Added PyTorch trained EfficientNet-V2 \u2018Tiny\u2019 w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested."),l4.forEach(t),cr.forEach(t),this.h()},h(){c(te,"name","hf:doc:metadata"),c(te,"content",JSON.stringify(h4)),c(Ae,"id","recent-changes"),c(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ae,"href","#recent-changes"),c(oe,"class","relative group"),c(Ce,"id","july-27-2022"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#july-27-2022"),c(le,"class","relative group"),c(De,"id","july-8-2022"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#july-8-2022"),c(re,"class","relative group"),c(it,"href","https://github.com/mmaaz60/EdgeNeXt"),c(it,"rel","nofollow"),c(nt,"href","https://github.com/apple/ml-cvnets"),c(nt,"rel","nofollow"),c(ct,"href","https://github.com/facebookresearch/deit"),c(ct,"rel","nofollow"),c(mt,"href","https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103"),c(mt,"rel","nofollow"),c($e,"id","may-13-2022"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#may-13-2022"),c(se,"class","relative group"),c(Et,"href","https://github.com/microsoft/Swin-Transformer"),c(Et,"rel","nofollow"),c(wt,"href","https://arxiv.org/abs/2205.01972"),c(wt,"rel","nofollow"),c(bt,"href","https://github.com/okojoalg"),c(bt,"rel","nofollow"),c(Se,"id","may-2-2022"),c(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Se,"href","#may-2-2022"),c(de,"class","relative group"),c(Pe,"id","april-22-2022"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#april-22-2022"),c(he,"class","relative group"),c(It,"href","https://www.fast.ai/"),c(It,"rel","nofollow"),c(xt,"href","http://timm.fast.ai/"),c(xt,"rel","nofollow"),c(At,"href","https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights"),c(At,"rel","nofollow"),c(Re,"id","march-23-2022"),c(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Re,"href","#march-23-2022"),c(fe,"class","relative group"),c(Dt,"href","https://arxiv.org/abs/2203.09795"),c(Dt,"rel","nofollow"),c(ze,"id","march-21-2022"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#march-21-2022"),c(pe,"class","relative group"),c(Tt,"href","https://github.com/rwightman/pytorch-image-models/tree/0.5.x"),c(Tt,"rel","nofollow"),c(St,"href","https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights"),c(St,"rel","nofollow"),c(Vt,"href","https://github.com/ChristophReich1996"),c(Vt,"rel","nofollow"),c(Mt,"href","https://github.com/microsoft/Cream/tree/main/AutoFormerV2"),c(Mt,"rel","nofollow"),c(Rt,"href","https://github.com/apple/ml-cvnets"),c(Rt,"rel","nofollow"),c(Ut,"href","https://github.com/sail-sg/poolformer"),c(Ut,"rel","nofollow"),c(zt,"href","https://github.com/sail-sg/volo"),c(zt,"rel","nofollow"),c(Be,"id","feb-2-2022"),c(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Be,"href","#feb-2-2022"),c(ve,"class","relative group"),c(Bt,"href","https://github.com/Chris-hughes10"),c(Bt,"rel","nofollow"),c(Ft,"href","https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055"),c(Ft,"rel","nofollow"),c(je,"id","jan-14-2022"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#jan-14-2022"),c(_e,"class","relative group"),c(qt,"href","https://github.com/facebookresearch/ConvNeXt"),c(qt,"rel","nofollow"),c(Wt,"href","https://github.com/rsomani95"),c(Wt,"rel","nofollow"),c(Je,"id","nov-22-2021"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#nov-22-2021"),c(Ee,"class","relative group"),c(Kt,"href","https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1h2_176-001a1197.pth"),c(Kt,"rel","nofollow"),c(Qt,"href","https://github.com/martinsbruveris"),c(Qt,"rel","nofollow"),c(eo,"href","https://github.com/alexander-soare"),c(eo,"rel","nofollow"),c(We,"id","oct-19-2021"),c(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(We,"href","#oct-19-2021"),c(we,"class","relative group"),c(oo,"href","https://arxiv.org/abs/2110.00476"),c(oo,"rel","nofollow"),c(lo,"href","https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights"),c(lo,"rel","nofollow"),c(ao,"href","https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),c(ao,"rel","nofollow"),c(so,"href","https://arxiv.org/abs/2103.12731"),c(so,"rel","nofollow"),c(co,"href","https://arxiv.org/abs/2101.11605"),c(co,"rel","nofollow"),c(fo,"href","https://arxiv.org/abs/2102.08602"),c(fo,"rel","nofollow"),c(po,"href","https://arxiv.org/abs/2103.06877"),c(po,"rel","nofollow"),c(mo,"href","https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights"),c(mo,"rel","nofollow"),c(vo,"href","https://openreview.net/forum?id=TVHS5Y4dNvM"),c(vo,"rel","nofollow"),c(uo,"href","https://arxiv.org/abs/2103.14899"),c(uo,"rel","nofollow"),c(_o,"href","https://arxiv.org/abs/2106.08254"),c(_o,"rel","nofollow"),c(Eo,"href","https://github.com/alexander-soare"),c(Eo,"rel","nofollow"),c(Ye,"id","aug-18-2021"),c(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ye,"href","#aug-18-2021"),c(ke,"class","relative group"),c(wo,"href","https://github.com/rwightman/pytorch-image-models/tree/bits_and_tpu/timm/bits"),c(wo,"rel","nofollow")},m(s,h){e(document.head,te),f(s,_s,h),f(s,oe,h),e(oe,Ae),e(Ae,fr),$(ot,fr,null),e(oe,On),e(oe,pr),e(pr,Dn),f(s,Es,h),f(s,le,h),e(le,Ce),e(Ce,mr),$(lt,mr,null),e(le,Nn),e(le,vr),e(vr,Tn),f(s,gs,h),f(s,I,h),e(I,ur),e(ur,$n),e(I,Sn),e(I,xo),e(xo,Pn),e(xo,k),e(k,Ao),e(Ao,_r),e(_r,Vn),e(Ao,Mn),e(k,Rn),e(k,Co),e(Co,Er),e(Er,Un),e(Co,zn),e(k,Hn),e(k,Oo),e(Oo,gr),e(gr,Bn),e(Oo,Fn),e(k,Xn),e(k,Do),e(Do,wr),e(wr,Gn),e(Do,jn),e(k,Jn),e(k,No),e(No,br),e(br,qn),e(No,Wn),e(k,Yn),e(k,To),e(To,yr),e(yr,Kn),e(To,Qn),e(k,Zn),e(k,$o),e($o,Lr),e(Lr,ed),e($o,td),e(I,od),e(I,Oe),e(Oe,kr),e(kr,ld),e(Oe,rd),e(Oe,Ir),e(Ir,ad),e(Oe,id),e(I,sd),e(I,xr),e(xr,nd),e(I,dd),e(I,Ar),e(Ar,cd),e(I,hd),e(I,Cr),e(Cr,fd),f(s,ws,h),f(s,re,h),e(re,De),e(De,Or),$(rt,Or,null),e(re,pd),e(re,Dr),e(Dr,md),f(s,bs,h),f(s,So,h),e(So,vd),f(s,ys,h),f(s,E,h),e(E,Po),e(Po,ud),e(Po,ae),e(ae,at),e(at,_d),e(at,it),e(it,Ed),e(at,gd),e(ae,wd),e(ae,st),e(st,bd),e(st,nt),e(nt,yd),e(st,Ld),e(ae,kd),e(ae,dt),e(dt,Id),e(dt,ct),e(ct,xd),e(dt,Ad),e(E,Cd),e(E,Vo),e(Vo,Od),e(Vo,H),e(H,ht),e(ht,Dd),e(ht,Nr),e(Nr,Nd),e(ht,Td),e(H,$d),e(H,Ne),e(Ne,Tr),e(Tr,Sd),e(Ne,Pd),e(Ne,$r),e($r,Vd),e(Ne,Md),e(H,Rd),e(H,ft),e(ft,Ud),e(ft,Sr),e(Sr,zd),e(ft,Hd),e(H,Bd),e(H,pt),e(pt,Fd),e(pt,Pr),e(Pr,Xd),e(pt,Gd),e(E,jd),e(E,Mo),e(Mo,Jd),e(Mo,g),e(g,Ro),e(Ro,Vr),e(Vr,qd),e(Ro,Wd),e(g,Yd),e(g,Uo),e(Uo,Mr),e(Mr,Kd),e(Uo,Qd),e(g,Zd),e(g,zo),e(zo,Rr),e(Rr,ec),e(zo,tc),e(g,oc),e(g,Ho),e(Ho,Ur),e(Ur,lc),e(Ho,rc),e(g,ac),e(g,Bo),e(Bo,zr),e(zr,ic),e(Bo,sc),e(g,nc),e(g,Fo),e(Fo,Hr),e(Hr,dc),e(Fo,cc),e(g,hc),e(g,Xo),e(Xo,Br),e(Br,fc),e(Xo,pc),e(g,mc),e(g,Go),e(Go,Fr),e(Fr,vc),e(Go,uc),e(g,_c),e(g,jo),e(jo,Xr),e(Xr,Ec),e(jo,gc),e(g,wc),e(g,Jo),e(Jo,Gr),e(Gr,bc),e(Jo,yc),e(g,Lc),e(g,qo),e(qo,jr),e(jr,kc),e(qo,Ic),e(g,xc),e(g,Wo),e(Wo,Jr),e(Jr,Ac),e(Wo,Cc),e(E,Oc),e(E,j),e(j,qr),e(qr,Dc),e(j,Nc),e(j,Wr),e(Wr,Tc),e(j,$c),e(j,Yr),e(Yr,Sc),e(j,Pc),e(E,Vc),e(E,Kr),e(Kr,Mc),e(E,Rc),e(E,Qr),e(Qr,Uc),e(E,zc),e(E,ie),e(ie,Hc),e(ie,Zr),e(Zr,Bc),e(ie,Fc),e(ie,mt),e(mt,Xc),e(ie,Gc),e(E,jc),e(E,J),e(J,Jc),e(J,ea),e(ea,qc),e(J,Wc),e(J,ta),e(ta,Yc),e(J,Kc),e(J,vt),e(vt,oa),e(oa,Qc),e(vt,Zc),e(vt,Te),e(Te,eh),e(Te,la),e(la,th),e(Te,oh),e(Te,ra),e(ra,lh),e(E,rh),e(E,aa),e(aa,ah),e(E,ih),e(E,ia),e(ia,sh),e(E,nh),e(E,sa),e(sa,dh),e(E,ch),e(E,na),e(na,hh),f(s,Ls,h),f(s,se,h),e(se,$e),e($e,da),$(ut,da,null),e(se,fh),e(se,ca),e(ca,ph),f(s,ks,h),f(s,x,h),e(x,_t),e(_t,mh),e(_t,Et),e(Et,vh),e(_t,uh),e(x,_h),e(x,gt),e(gt,Eh),e(gt,ha),e(ha,gh),e(gt,wh),e(x,bh),e(x,Yo),e(Yo,yh),e(Yo,B),e(B,Ko),e(Ko,fa),e(fa,Lh),e(Ko,kh),e(B,Ih),e(B,Qo),e(Qo,pa),e(pa,xh),e(Qo,Ah),e(B,Ch),e(B,Zo),e(Zo,ma),e(ma,Oh),e(Zo,Dh),e(B,Nh),e(B,el),e(el,va),e(va,Th),e(el,$h),e(x,Sh),e(x,ua),e(ua,Ph),e(x,Vh),e(x,_a),e(_a,Mh),e(x,Rh),e(x,ne),e(ne,Uh),e(ne,wt),e(wt,zh),e(ne,Hh),e(ne,bt),e(bt,Bh),e(ne,Fh),f(s,Is,h),f(s,de,h),e(de,Se),e(Se,Ea),$(yt,Ea,null),e(de,Xh),e(de,ga),e(ga,Gh),f(s,xs,h),f(s,q,h),e(q,W),e(W,jh),e(W,wa),e(wa,Jh),e(W,qh),e(W,ba),e(ba,Wh),e(W,Yh),e(W,ce),e(ce,tl),e(tl,ya),e(ya,Kh),e(tl,Qh),e(ce,Zh),e(ce,ol),e(ol,La),e(La,e2),e(ol,t2),e(ce,o2),e(ce,ll),e(ll,ka),e(ka,l2),e(ll,r2),e(q,a2),e(q,Lt),e(Lt,i2),e(Lt,Ia),e(Ia,s2),e(Lt,n2),e(q,d2),e(q,rl),e(rl,xa),e(xa,c2),e(rl,h2),f(s,As,h),f(s,he,h),e(he,Pe),e(Pe,Aa),$(kt,Aa,null),e(he,f2),e(he,Ca),e(Ca,p2),f(s,Cs,h),f(s,Ve,h),e(Ve,R),e(R,Oa),e(Oa,m2),e(R,v2),e(R,It),e(It,u2),e(R,_2),e(R,Da),e(Da,E2),e(R,g2),e(R,xt),e(xt,w2),e(R,b2),e(Ve,y2),e(Ve,Me),e(Me,L2),e(Me,At),e(At,k2),e(Me,I2),e(Me,Ct),e(Ct,al),e(al,Na),e(Na,x2),e(al,A2),e(Ct,C2),e(Ct,il),e(il,Ta),e(Ta,O2),e(il,D2),f(s,Os,h),f(s,fe,h),e(fe,Re),e(Re,$a),$(Ot,$a,null),e(fe,N2),e(fe,Sa),e(Sa,T2),f(s,Ds,h),f(s,Ue,h),e(Ue,Y),e(Y,$2),e(Y,Pa),e(Pa,S2),e(Y,P2),e(Y,Va),e(Va,V2),e(Y,M2),e(Y,Dt),e(Dt,R2),e(Ue,U2),e(Ue,sl),e(sl,Ma),e(Ma,z2),e(sl,H2),f(s,Ns,h),f(s,pe,h),e(pe,ze),e(ze,Ra),$(Nt,Ra,null),e(pe,B2),e(pe,Ua),e(Ua,F2),f(s,Ts,h),f(s,m,h),e(m,F),e(F,X2),e(F,za),e(za,G2),e(F,j2),e(F,Ha),e(Ha,J2),e(F,q2),e(F,Tt),e(Tt,Ba),e(Ba,W2),e(F,Y2),e(m,K2),e(m,$t),e($t,Q2),e($t,St),e(St,Z2),e($t,p),e(p,nl),e(nl,Fa),e(Fa,ef),e(nl,tf),e(p,of),e(p,dl),e(dl,Xa),e(Xa,lf),e(dl,rf),e(p,af),e(p,cl),e(cl,Ga),e(Ga,sf),e(cl,nf),e(p,df),e(p,hl),e(hl,ja),e(ja,cf),e(hl,hf),e(p,ff),e(p,fl),e(fl,Ja),e(Ja,pf),e(fl,mf),e(p,vf),e(p,pl),e(pl,qa),e(qa,uf),e(pl,_f),e(p,Ef),e(p,ml),e(ml,Wa),e(Wa,gf),e(ml,wf),e(p,bf),e(p,vl),e(vl,Ya),e(Ya,yf),e(vl,Lf),e(p,kf),e(p,ul),e(ul,Ka),e(Ka,If),e(ul,xf),e(p,Af),e(p,_l),e(_l,Qa),e(Qa,Cf),e(_l,Of),e(p,Df),e(p,El),e(El,Za),e(Za,Nf),e(El,Tf),e(p,$f),e(p,gl),e(gl,ei),e(ei,Sf),e(gl,Pf),e(p,Vf),e(p,wl),e(wl,ti),e(ti,Mf),e(wl,Rf),e(p,Uf),e(p,bl),e(bl,oi),e(oi,zf),e(bl,Hf),e(p,Bf),e(p,yl),e(yl,li),e(li,Ff),e(yl,Xf),e(p,Gf),e(p,Ll),e(Ll,ri),e(ri,jf),e(Ll,Jf),e(p,qf),e(p,kl),e(kl,ai),e(ai,Wf),e(kl,Yf),e(m,Kf),e(m,ii),e(ii,Qf),e(m,Zf),e(m,Pt),e(Pt,e1),e(Pt,Vt),e(Vt,t1),e(Pt,o1),e(m,l1),e(m,Il),e(Il,r1),e(Il,Mt),e(Mt,a1),e(m,i1),e(m,xl),e(xl,s1),e(xl,Rt),e(Rt,n1),e(m,d1),e(m,Al),e(Al,c1),e(Al,Ut),e(Ut,h1),e(m,f1),e(m,Cl),e(Cl,p1),e(Cl,zt),e(zt,m1),e(m,v1),e(m,si),e(si,u1),e(m,_1),e(m,ni),e(ni,E1),e(m,g1),e(m,di),e(di,w1),e(m,b1),e(m,ci),e(ci,y1),e(m,L1),e(m,hi),e(hi,k1),e(m,I1),e(m,me),e(me,fi),e(fi,x1),e(me,A1),e(me,pi),e(pi,C1),e(me,O1),e(me,mi),e(mi,D1),e(m,N1),e(m,He),e(He,T1),e(He,vi),e(vi,$1),e(He,S1),e(He,ui),e(ui,P1),f(s,$s,h),f(s,ve,h),e(ve,Be),e(Be,_i),$(Ht,_i,null),e(ve,V1),e(ve,Ei),e(Ei,M1),f(s,Ss,h),f(s,Fe,h),e(Fe,ue),e(ue,Bt),e(Bt,R1),e(ue,U1),e(ue,gi),e(gi,z1),e(ue,H1),e(ue,Ft),e(Ft,B1),e(Fe,F1),e(Fe,Xe),e(Xe,X1),e(Xe,wi),e(wi,G1),e(Xe,j1),e(Xe,Xt),e(Xt,Gt),e(Gt,J1),e(Gt,bi),e(bi,q1),e(Gt,W1),e(Xt,Y1),e(Xt,Ge),e(Ge,yi),e(yi,K1),e(Ge,Q1),e(Ge,Li),e(Li,Z1),e(Ge,ep),f(s,Ps,h),f(s,_e,h),e(_e,je),e(je,ki),$(jt,ki,null),e(_e,tp),e(_e,Ii),e(Ii,op),f(s,Vs,h),f(s,A,h),e(A,xi),e(xi,lp),e(A,rp),e(A,Jt),e(Jt,ap),e(Jt,qt),e(qt,ip),e(Jt,sp),e(A,np),e(A,Ol),e(Ol,dp),e(Ol,D),e(D,Dl),e(Dl,Ai),e(Ai,cp),e(Dl,hp),e(D,fp),e(D,Nl),e(Nl,Ci),e(Ci,pp),e(Nl,mp),e(D,vp),e(D,Tl),e(Tl,Oi),e(Oi,up),e(Tl,_p),e(D,Ep),e(D,$l),e($l,Di),e(Di,gp),e($l,wp),e(D,bp),e(D,Sl),e(Sl,Ni),e(Ni,yp),e(Sl,Lp),e(A,kp),e(A,Pl),e(Pl,Ip),e(Pl,Wt),e(Wt,xp),e(A,Ap),e(A,Ti),e(Ti,Cp),f(s,Ms,h),f(s,Ee,h),e(Ee,Je),e(Je,$i),$(Yt,$i,null),e(Ee,Op),e(Ee,Si),e(Si,Dp),f(s,Rs,h),f(s,K,h),e(K,Vl),e(Vl,Np),e(Vl,_),e(_,Ml),e(Ml,Pi),e(Pi,Tp),e(Ml,$p),e(_,Sp),e(_,Rl),e(Rl,Vi),e(Vi,Pp),e(Rl,Vp),e(_,Mp),e(_,qe),e(qe,Mi),e(Mi,Rp),e(qe,Up),e(qe,Kt),e(Kt,zp),e(qe,Hp),e(_,Bp),e(_,Ul),e(Ul,Ri),e(Ri,Fp),e(Ul,Xp),e(_,Gp),e(_,zl),e(zl,Ui),e(Ui,jp),e(zl,Jp),e(_,qp),e(_,Hl),e(Hl,zi),e(zi,Wp),e(Hl,Yp),e(_,Kp),e(_,Bl),e(Bl,Hi),e(Hi,Qp),e(Bl,Zp),e(_,em),e(_,Fl),e(Fl,Bi),e(Bi,tm),e(Fl,om),e(_,lm),e(_,Xl),e(Xl,Fi),e(Fi,rm),e(Xl,am),e(_,im),e(_,Gl),e(Gl,Xi),e(Xi,sm),e(Gl,nm),e(_,dm),e(_,jl),e(jl,Gi),e(Gi,cm),e(jl,hm),e(_,fm),e(_,Jl),e(Jl,ji),e(ji,pm),e(Jl,mm),e(_,vm),e(_,ql),e(ql,Ji),e(Ji,um),e(ql,_m),e(K,Em),e(K,ge),e(ge,qi),e(qi,gm),e(ge,wm),e(ge,Wi),e(Wi,bm),e(ge,ym),e(ge,Qt),e(Qt,Lm),e(K,km),e(K,Zt),e(Zt,Im),e(Zt,eo),e(eo,xm),e(Zt,Yi),e(Yi,Ki),e(Ki,Am),f(s,Us,h),f(s,we,h),e(we,We),e(We,Qi),$(to,Qi,null),e(we,Cm),e(we,Zi),e(Zi,Om),f(s,zs,h),f(s,L,h),e(L,be),e(be,Dm),e(be,oo),e(oo,Nm),e(be,Tm),e(be,lo),e(lo,$m),e(be,Sm),e(L,Pm),e(L,es),e(es,Vm),e(L,Mm),e(L,ro),e(ro,Rm),e(ro,ao),e(ao,Um),e(ro,zm),e(L,Hm),e(L,Wl),e(Wl,Bm),e(Wl,ye),e(ye,io),e(io,Fm),e(io,so),e(so,Xm),e(io,Gm),e(ye,jm),e(ye,no),e(no,Jm),e(no,co),e(co,qm),e(no,Wm),e(ye,Ym),e(ye,ho),e(ho,Km),e(ho,fo),e(fo,Qm),e(ho,Zm),e(L,ev),e(L,Le),e(Le,tv),e(Le,po),e(po,ov),e(Le,lv),e(Le,mo),e(mo,rv),e(Le,av),e(L,iv),e(L,X),e(X,sv),e(X,vo),e(vo,nv),e(X,dv),e(X,uo),e(uo,cv),e(X,hv),e(X,_o),e(_o,fv),e(X,pv),e(L,mv),e(L,Yl),e(Yl,vv),e(Yl,Eo),e(Eo,uv),f(s,Hs,h),f(s,ke,h),e(ke,Ye),e(Ye,ts),$(go,ts,null),e(ke,_v),e(ke,os),e(os,Ev),f(s,Bs,h),f(s,Q,h),e(Q,Kl),e(Kl,gv),e(Kl,G),e(G,Ie),e(Ie,wv),e(Ie,ls),e(ls,bv),e(Ie,yv),e(Ie,wo),e(wo,Lv),e(Ie,kv),e(G,Iv),e(G,rs),e(rs,xv),e(G,Av),e(G,bo),e(bo,Cv),e(bo,as),e(as,Ov),e(bo,Dv),e(G,Nv),e(G,is),e(is,Tv),e(Q,$v),e(Q,ss),e(ss,Sv),e(Q,Pv),e(Q,ns),e(ns,Vv),Fs=!0},p:n4,i(s){Fs||(S(ot.$$.fragment,s),S(lt.$$.fragment,s),S(rt.$$.fragment,s),S(ut.$$.fragment,s),S(yt.$$.fragment,s),S(kt.$$.fragment,s),S(Ot.$$.fragment,s),S(Nt.$$.fragment,s),S(Ht.$$.fragment,s),S(jt.$$.fragment,s),S(Yt.$$.fragment,s),S(to.$$.fragment,s),S(go.$$.fragment,s),Fs=!0)},o(s){P(ot.$$.fragment,s),P(lt.$$.fragment,s),P(rt.$$.fragment,s),P(ut.$$.fragment,s),P(yt.$$.fragment,s),P(kt.$$.fragment,s),P(Ot.$$.fragment,s),P(Nt.$$.fragment,s),P(Ht.$$.fragment,s),P(jt.$$.fragment,s),P(Yt.$$.fragment,s),P(to.$$.fragment,s),P(go.$$.fragment,s),Fs=!1},d(s){t(te),s&&t(_s),s&&t(oe),V(ot),s&&t(Es),s&&t(le),V(lt),s&&t(gs),s&&t(I),s&&t(ws),s&&t(re),V(rt),s&&t(bs),s&&t(So),s&&t(ys),s&&t(E),s&&t(Ls),s&&t(se),V(ut),s&&t(ks),s&&t(x),s&&t(Is),s&&t(de),V(yt),s&&t(xs),s&&t(q),s&&t(As),s&&t(he),V(kt),s&&t(Cs),s&&t(Ve),s&&t(Os),s&&t(fe),V(Ot),s&&t(Ds),s&&t(Ue),s&&t(Ns),s&&t(pe),V(Nt),s&&t(Ts),s&&t(m),s&&t($s),s&&t(ve),V(Ht),s&&t(Ss),s&&t(Fe),s&&t(Ps),s&&t(_e),V(jt),s&&t(Vs),s&&t(A),s&&t(Ms),s&&t(Ee),V(Yt),s&&t(Rs),s&&t(K),s&&t(Us),s&&t(we),V(to),s&&t(zs),s&&t(L),s&&t(Hs),s&&t(ke),V(go),s&&t(Bs),s&&t(Q)}}}const h4={local:"recent-changes",sections:[{local:"july-27-2022",title:"July 27, 2022"},{local:"july-8-2022",title:"July 8, 2022"},{local:"may-13-2022",title:"May 13, 2022"},{local:"may-2-2022",title:"May 2, 2022"},{local:"april-22-2022",title:"April 22, 2022"},{local:"march-23-2022",title:"March 23, 2022"},{local:"march-21-2022",title:"March 21, 2022"},{local:"feb-2-2022",title:"Feb 2, 2022"},{local:"jan-14-2022",title:"Jan 14, 2022"},{local:"nov-22-2021",title:"Nov 22, 2021"},{local:"oct-19-2021",title:"Oct 19, 2021"},{local:"aug-18-2021",title:"Aug 18, 2021"}],title:"Recent Changes"};function f4(n_){return d4(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class v4 extends r4{constructor(te){super();a4(this,te,f4,c4,i4,{})}}export{v4 as default,h4 as metadata};
