import{S as la,i as na,s as ra,e as o,k as h,w as f,t as p,M as ia,c as l,d as e,m as c,a as n,x as u,h as m,b as i,G as a,g as r,y as g,L as pa,q as d,o as v,B as w,v as ma}from"../../chunks/vendor-hf-doc-builder.js";import{I as vt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as st}from"../../chunks/CodeBlock-hf-doc-builder.js";function ha(Ce){let _,wt,b,E,ot,O,Kt,lt,Qt,_t,j,nt,Zt,te,Y,ee,ae,bt,P,se,L,oe,le,jt,y,S,rt,D,ne,it,re,yt,J,ie,$t,M,kt,K,pe,xt,z,Et,Q,me,Pt,B,St,Z,he,Nt,G,At,N,ce,pt,fe,ue,Ct,A,ge,tt,de,ve,Tt,$,C,mt,R,we,ht,_e,It,et,be,qt,U,Ht,T,je,F,ye,$e,Ot,k,I,ct,V,ke,ft,xe,Yt,q,Ee,at,Pe,Se,Lt,x,H,ut,W,Ne,gt,Ae,Dt,X,Mt;return O=new vt({}),D=new vt({}),M=new st({props:{code:`import timm
model = timm.create_model('cspdarknet53', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;cspdarknet53&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),z=new st({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),B=new st({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),G=new st({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),R=new vt({}),U=new st({props:{code:"model = timm.create_model('cspdarknet53', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;cspdarknet53&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),V=new vt({}),W=new vt({}),X=new st({props:{code:`@misc{bochkovskiy2020yolov4,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`<span class="language-xml">@misc</span><span class="hljs-template-variable">{bochkovskiy2020yolov4,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}</span><span class="language-xml">, 
      author=</span><span class="hljs-template-variable">{Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao}</span><span class="language-xml">,
      year=</span><span class="hljs-template-variable">{2020}</span><span class="language-xml">,
      eprint=</span><span class="hljs-template-variable">{2004.10934}</span><span class="language-xml">,
      archivePrefix=</span><span class="hljs-template-variable">{arXiv}</span><span class="language-xml">,
      primaryClass=</span><span class="hljs-template-variable">{cs.CV}</span><span class="language-xml">
}</span>`}}),{c(){_=o("meta"),wt=h(),b=o("h1"),E=o("a"),ot=o("span"),f(O.$$.fragment),Kt=h(),lt=o("span"),Qt=p("CSP-DarkNet"),_t=h(),j=o("p"),nt=o("strong"),Zt=p("CSPDarknet53"),te=p(" is a convolutional neural network and backbone for object detection that uses "),Y=o("a"),ee=p("DarkNet-53"),ae=p(". It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network."),bt=h(),P=o("p"),se=p("This CNN is used as the backbone for "),L=o("a"),oe=p("YOLOv4"),le=p("."),jt=h(),y=o("h2"),S=o("a"),rt=o("span"),f(D.$$.fragment),ne=h(),it=o("span"),re=p("How do I use this model on an image?"),yt=h(),J=o("p"),ie=p("To load a pretrained model:"),$t=h(),f(M.$$.fragment),kt=h(),K=o("p"),pe=p("To load and preprocess the image:"),xt=h(),f(z.$$.fragment),Et=h(),Q=o("p"),me=p("To get the model predictions:"),Pt=h(),f(B.$$.fragment),St=h(),Z=o("p"),he=p("To get the top-5 predictions class names:"),Nt=h(),f(G.$$.fragment),At=h(),N=o("p"),ce=p("Replace the model name with the variant you want to use, e.g. "),pt=o("code"),fe=p("cspdarknet53"),ue=p(". You can find the IDs in the model summaries at the top of this page."),Ct=h(),A=o("p"),ge=p("To extract image features with this model, follow the "),tt=o("a"),de=p("timm feature extraction examples"),ve=p(", just change the name of the model you want to use."),Tt=h(),$=o("h2"),C=o("a"),mt=o("span"),f(R.$$.fragment),we=h(),ht=o("span"),_e=p("How do I finetune this model?"),It=h(),et=o("p"),be=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qt=h(),f(U.$$.fragment),Ht=h(),T=o("p"),je=p("To finetune on your own dataset, you have to write a training loop or adapt "),F=o("a"),ye=p(`timm\u2019s training
script`),$e=p(" to use your dataset."),Ot=h(),k=o("h2"),I=o("a"),ct=o("span"),f(V.$$.fragment),ke=h(),ft=o("span"),xe=p("How do I train this model?"),Yt=h(),q=o("p"),Ee=p("You can follow the "),at=o("a"),Pe=p("timm recipe scripts"),Se=p(" for training a new model afresh."),Lt=h(),x=o("h2"),H=o("a"),ut=o("span"),f(W.$$.fragment),Ne=h(),gt=o("span"),Ae=p("Citation"),Dt=h(),f(X.$$.fragment),this.h()},l(t){const s=ia('[data-svelte="svelte-1phssyn"]',document.head);_=l(s,"META",{name:!0,content:!0}),s.forEach(e),wt=c(t),b=l(t,"H1",{class:!0});var zt=n(b);E=l(zt,"A",{id:!0,class:!0,href:!0});var Te=n(E);ot=l(Te,"SPAN",{});var Ie=n(ot);u(O.$$.fragment,Ie),Ie.forEach(e),Te.forEach(e),Kt=c(zt),lt=l(zt,"SPAN",{});var qe=n(lt);Qt=m(qe,"CSP-DarkNet"),qe.forEach(e),zt.forEach(e),_t=c(t),j=l(t,"P",{});var dt=n(j);nt=l(dt,"STRONG",{});var He=n(nt);Zt=m(He,"CSPDarknet53"),He.forEach(e),te=m(dt," is a convolutional neural network and backbone for object detection that uses "),Y=l(dt,"A",{href:!0,rel:!0});var Oe=n(Y);ee=m(Oe,"DarkNet-53"),Oe.forEach(e),ae=m(dt,". It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network."),dt.forEach(e),bt=c(t),P=l(t,"P",{});var Bt=n(P);se=m(Bt,"This CNN is used as the backbone for "),L=l(Bt,"A",{href:!0,rel:!0});var Ye=n(L);oe=m(Ye,"YOLOv4"),Ye.forEach(e),le=m(Bt,"."),Bt.forEach(e),jt=c(t),y=l(t,"H2",{class:!0});var Gt=n(y);S=l(Gt,"A",{id:!0,class:!0,href:!0});var Le=n(S);rt=l(Le,"SPAN",{});var De=n(rt);u(D.$$.fragment,De),De.forEach(e),Le.forEach(e),ne=c(Gt),it=l(Gt,"SPAN",{});var Me=n(it);re=m(Me,"How do I use this model on an image?"),Me.forEach(e),Gt.forEach(e),yt=c(t),J=l(t,"P",{});var ze=n(J);ie=m(ze,"To load a pretrained model:"),ze.forEach(e),$t=c(t),u(M.$$.fragment,t),kt=c(t),K=l(t,"P",{});var Be=n(K);pe=m(Be,"To load and preprocess the image:"),Be.forEach(e),xt=c(t),u(z.$$.fragment,t),Et=c(t),Q=l(t,"P",{});var Ge=n(Q);me=m(Ge,"To get the model predictions:"),Ge.forEach(e),Pt=c(t),u(B.$$.fragment,t),St=c(t),Z=l(t,"P",{});var Re=n(Z);he=m(Re,"To get the top-5 predictions class names:"),Re.forEach(e),Nt=c(t),u(G.$$.fragment,t),At=c(t),N=l(t,"P",{});var Rt=n(N);ce=m(Rt,"Replace the model name with the variant you want to use, e.g. "),pt=l(Rt,"CODE",{});var Ue=n(pt);fe=m(Ue,"cspdarknet53"),Ue.forEach(e),ue=m(Rt,". You can find the IDs in the model summaries at the top of this page."),Rt.forEach(e),Ct=c(t),A=l(t,"P",{});var Ut=n(A);ge=m(Ut,"To extract image features with this model, follow the "),tt=l(Ut,"A",{href:!0});var Fe=n(tt);de=m(Fe,"timm feature extraction examples"),Fe.forEach(e),ve=m(Ut,", just change the name of the model you want to use."),Ut.forEach(e),Tt=c(t),$=l(t,"H2",{class:!0});var Ft=n($);C=l(Ft,"A",{id:!0,class:!0,href:!0});var Ve=n(C);mt=l(Ve,"SPAN",{});var We=n(mt);u(R.$$.fragment,We),We.forEach(e),Ve.forEach(e),we=c(Ft),ht=l(Ft,"SPAN",{});var Xe=n(ht);_e=m(Xe,"How do I finetune this model?"),Xe.forEach(e),Ft.forEach(e),It=c(t),et=l(t,"P",{});var Je=n(et);be=m(Je,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Je.forEach(e),qt=c(t),u(U.$$.fragment,t),Ht=c(t),T=l(t,"P",{});var Vt=n(T);je=m(Vt,"To finetune on your own dataset, you have to write a training loop or adapt "),F=l(Vt,"A",{href:!0,rel:!0});var Ke=n(F);ye=m(Ke,`timm\u2019s training
script`),Ke.forEach(e),$e=m(Vt," to use your dataset."),Vt.forEach(e),Ot=c(t),k=l(t,"H2",{class:!0});var Wt=n(k);I=l(Wt,"A",{id:!0,class:!0,href:!0});var Qe=n(I);ct=l(Qe,"SPAN",{});var Ze=n(ct);u(V.$$.fragment,Ze),Ze.forEach(e),Qe.forEach(e),ke=c(Wt),ft=l(Wt,"SPAN",{});var ta=n(ft);xe=m(ta,"How do I train this model?"),ta.forEach(e),Wt.forEach(e),Yt=c(t),q=l(t,"P",{});var Xt=n(q);Ee=m(Xt,"You can follow the "),at=l(Xt,"A",{href:!0});var ea=n(at);Pe=m(ea,"timm recipe scripts"),ea.forEach(e),Se=m(Xt," for training a new model afresh."),Xt.forEach(e),Lt=c(t),x=l(t,"H2",{class:!0});var Jt=n(x);H=l(Jt,"A",{id:!0,class:!0,href:!0});var aa=n(H);ut=l(aa,"SPAN",{});var sa=n(ut);u(W.$$.fragment,sa),sa.forEach(e),aa.forEach(e),Ne=c(Jt),gt=l(Jt,"SPAN",{});var oa=n(gt);Ae=m(oa,"Citation"),oa.forEach(e),Jt.forEach(e),Dt=c(t),u(X.$$.fragment,t),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(ca)),i(E,"id","cspdarknet"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#cspdarknet"),i(b,"class","relative group"),i(Y,"href","https://paperswithcode.com/method/darknet-53"),i(Y,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/yolov4"),i(L,"rel","nofollow"),i(S,"id","how-do-i-use-this-model-on-an-image"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-use-this-model-on-an-image"),i(y,"class","relative group"),i(tt,"href","../feature_extraction"),i(C,"id","how-do-i-finetune-this-model"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#how-do-i-finetune-this-model"),i($,"class","relative group"),i(F,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(F,"rel","nofollow"),i(I,"id","how-do-i-train-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(at,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(x,"class","relative group")},m(t,s){a(document.head,_),r(t,wt,s),r(t,b,s),a(b,E),a(E,ot),g(O,ot,null),a(b,Kt),a(b,lt),a(lt,Qt),r(t,_t,s),r(t,j,s),a(j,nt),a(nt,Zt),a(j,te),a(j,Y),a(Y,ee),a(j,ae),r(t,bt,s),r(t,P,s),a(P,se),a(P,L),a(L,oe),a(P,le),r(t,jt,s),r(t,y,s),a(y,S),a(S,rt),g(D,rt,null),a(y,ne),a(y,it),a(it,re),r(t,yt,s),r(t,J,s),a(J,ie),r(t,$t,s),g(M,t,s),r(t,kt,s),r(t,K,s),a(K,pe),r(t,xt,s),g(z,t,s),r(t,Et,s),r(t,Q,s),a(Q,me),r(t,Pt,s),g(B,t,s),r(t,St,s),r(t,Z,s),a(Z,he),r(t,Nt,s),g(G,t,s),r(t,At,s),r(t,N,s),a(N,ce),a(N,pt),a(pt,fe),a(N,ue),r(t,Ct,s),r(t,A,s),a(A,ge),a(A,tt),a(tt,de),a(A,ve),r(t,Tt,s),r(t,$,s),a($,C),a(C,mt),g(R,mt,null),a($,we),a($,ht),a(ht,_e),r(t,It,s),r(t,et,s),a(et,be),r(t,qt,s),g(U,t,s),r(t,Ht,s),r(t,T,s),a(T,je),a(T,F),a(F,ye),a(T,$e),r(t,Ot,s),r(t,k,s),a(k,I),a(I,ct),g(V,ct,null),a(k,ke),a(k,ft),a(ft,xe),r(t,Yt,s),r(t,q,s),a(q,Ee),a(q,at),a(at,Pe),a(q,Se),r(t,Lt,s),r(t,x,s),a(x,H),a(H,ut),g(W,ut,null),a(x,Ne),a(x,gt),a(gt,Ae),r(t,Dt,s),g(X,t,s),Mt=!0},p:pa,i(t){Mt||(d(O.$$.fragment,t),d(D.$$.fragment,t),d(M.$$.fragment,t),d(z.$$.fragment,t),d(B.$$.fragment,t),d(G.$$.fragment,t),d(R.$$.fragment,t),d(U.$$.fragment,t),d(V.$$.fragment,t),d(W.$$.fragment,t),d(X.$$.fragment,t),Mt=!0)},o(t){v(O.$$.fragment,t),v(D.$$.fragment,t),v(M.$$.fragment,t),v(z.$$.fragment,t),v(B.$$.fragment,t),v(G.$$.fragment,t),v(R.$$.fragment,t),v(U.$$.fragment,t),v(V.$$.fragment,t),v(W.$$.fragment,t),v(X.$$.fragment,t),Mt=!1},d(t){e(_),t&&e(wt),t&&e(b),w(O),t&&e(_t),t&&e(j),t&&e(bt),t&&e(P),t&&e(jt),t&&e(y),w(D),t&&e(yt),t&&e(J),t&&e($t),w(M,t),t&&e(kt),t&&e(K),t&&e(xt),w(z,t),t&&e(Et),t&&e(Q),t&&e(Pt),w(B,t),t&&e(St),t&&e(Z),t&&e(Nt),w(G,t),t&&e(At),t&&e(N),t&&e(Ct),t&&e(A),t&&e(Tt),t&&e($),w(R),t&&e(It),t&&e(et),t&&e(qt),w(U,t),t&&e(Ht),t&&e(T),t&&e(Ot),t&&e(k),w(V),t&&e(Yt),t&&e(q),t&&e(Lt),t&&e(x),w(W),t&&e(Dt),w(X,t)}}}const ca={local:"cspdarknet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"CSP-DarkNet"};function fa(Ce){return ma(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class va extends la{constructor(_){super();na(this,_,fa,ha,ra,{})}}export{va as default,ca as metadata};
