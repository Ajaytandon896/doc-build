import{S as ns,i as os,s as rs,e as l,k as h,w as u,t as p,M as is,c as n,d as t,m as c,a as o,x as g,h as m,b as i,G as s,g as r,y as d,L as ps,q as v,o as w,B as _,v as ms}from"../../chunks/vendor-hf-doc-builder.js";import{I as ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as le}from"../../chunks/CodeBlock-hf-doc-builder.js";function hs(At){let $,we,j,S,ne,C,Oe,oe,Fe,_e,f,re,Ve,Qe,K,Ze,et,L,tt,st,z,at,lt,$e,b,P,ie,Y,nt,pe,ot,je,V,rt,be,G,ye,Q,it,ke,X,xe,Z,pt,Ee,M,Se,ee,mt,Pe,U,Ne,N,ht,me,ct,ft,Ae,A,ut,te,gt,dt,Ie,y,I,he,B,vt,ce,wt,qe,se,_t,Te,W,He,q,$t,D,jt,bt,Re,k,T,fe,J,yt,ue,kt,Ce,H,xt,ae,Et,St,Ke,x,R,ge,O,Pt,de,Nt,Le,F,ze;return C=new ve({}),Y=new ve({}),G=new le({props:{code:`import timm
model = timm.create_model('skresnet18', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;skresnet18&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),X=new le({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),M=new le({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),U=new le({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),B=new ve({}),W=new le({props:{code:"model = timm.create_model('skresnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;skresnet18&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),J=new ve({}),O=new ve({}),F=new le({props:{code:`@misc{li2019selective,
      title={Selective Kernel Networks}, 
      author={Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang},
      year={2019},
      eprint={1903.06586},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`<span class="language-xml">@misc</span><span class="hljs-template-variable">{li2019selective,
      title={Selective Kernel Networks}</span><span class="language-xml">, 
      author=</span><span class="hljs-template-variable">{Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang}</span><span class="language-xml">,
      year=</span><span class="hljs-template-variable">{2019}</span><span class="language-xml">,
      eprint=</span><span class="hljs-template-variable">{1903.06586}</span><span class="language-xml">,
      archivePrefix=</span><span class="hljs-template-variable">{arXiv}</span><span class="language-xml">,
      primaryClass=</span><span class="hljs-template-variable">{cs.CV}</span><span class="language-xml">
}</span>`}}),{c(){$=l("meta"),we=h(),j=l("h1"),S=l("a"),ne=l("span"),u(C.$$.fragment),Oe=h(),oe=l("span"),Fe=p("SK-ResNet"),_e=h(),f=l("p"),re=l("strong"),Ve=p("SK ResNet"),Qe=p(" is a variant of a "),K=l("a"),Ze=p("ResNet"),et=p(" that employs a "),L=l("a"),tt=p("Selective Kernel"),st=p(" unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNet are replaced by the proposed "),z=l("a"),at=p("SK convolutions"),lt=p(", enabling the network to choose appropriate receptive field sizes in an adaptive manner."),$e=h(),b=l("h2"),P=l("a"),ie=l("span"),u(Y.$$.fragment),nt=h(),pe=l("span"),ot=p("How do I use this model on an image?"),je=h(),V=l("p"),rt=p("To load a pretrained model:"),be=h(),u(G.$$.fragment),ye=h(),Q=l("p"),it=p("To load and preprocess the image:"),ke=h(),u(X.$$.fragment),xe=h(),Z=l("p"),pt=p("To get the model predictions:"),Ee=h(),u(M.$$.fragment),Se=h(),ee=l("p"),mt=p("To get the top-5 predictions class names:"),Pe=h(),u(U.$$.fragment),Ne=h(),N=l("p"),ht=p("Replace the model name with the variant you want to use, e.g. "),me=l("code"),ct=p("skresnet18"),ft=p(". You can find the IDs in the model summaries at the top of this page."),Ae=h(),A=l("p"),ut=p("To extract image features with this model, follow the "),te=l("a"),gt=p("timm feature extraction examples"),dt=p(", just change the name of the model you want to use."),Ie=h(),y=l("h2"),I=l("a"),he=l("span"),u(B.$$.fragment),vt=h(),ce=l("span"),wt=p("How do I finetune this model?"),qe=h(),se=l("p"),_t=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Te=h(),u(W.$$.fragment),He=h(),q=l("p"),$t=p("To finetune on your own dataset, you have to write a training loop or adapt "),D=l("a"),jt=p(`timm\u2019s training
script`),bt=p(" to use your dataset."),Re=h(),k=l("h2"),T=l("a"),fe=l("span"),u(J.$$.fragment),yt=h(),ue=l("span"),kt=p("How do I train this model?"),Ce=h(),H=l("p"),xt=p("You can follow the "),ae=l("a"),Et=p("timm recipe scripts"),St=p(" for training a new model afresh."),Ke=h(),x=l("h2"),R=l("a"),ge=l("span"),u(O.$$.fragment),Pt=h(),de=l("span"),Nt=p("Citation"),Le=h(),u(F.$$.fragment),this.h()},l(e){const a=is('[data-svelte="svelte-1phssyn"]',document.head);$=n(a,"META",{name:!0,content:!0}),a.forEach(t),we=c(e),j=n(e,"H1",{class:!0});var Ye=o(j);S=n(Ye,"A",{id:!0,class:!0,href:!0});var It=o(S);ne=n(It,"SPAN",{});var qt=o(ne);g(C.$$.fragment,qt),qt.forEach(t),It.forEach(t),Oe=c(Ye),oe=n(Ye,"SPAN",{});var Tt=o(oe);Fe=m(Tt,"SK-ResNet"),Tt.forEach(t),Ye.forEach(t),_e=c(e),f=n(e,"P",{});var E=o(f);re=n(E,"STRONG",{});var Ht=o(re);Ve=m(Ht,"SK ResNet"),Ht.forEach(t),Qe=m(E," is a variant of a "),K=n(E,"A",{href:!0,rel:!0});var Rt=o(K);Ze=m(Rt,"ResNet"),Rt.forEach(t),et=m(E," that employs a "),L=n(E,"A",{href:!0,rel:!0});var Ct=o(L);tt=m(Ct,"Selective Kernel"),Ct.forEach(t),st=m(E," unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNet are replaced by the proposed "),z=n(E,"A",{href:!0,rel:!0});var Kt=o(z);at=m(Kt,"SK convolutions"),Kt.forEach(t),lt=m(E,", enabling the network to choose appropriate receptive field sizes in an adaptive manner."),E.forEach(t),$e=c(e),b=n(e,"H2",{class:!0});var Ge=o(b);P=n(Ge,"A",{id:!0,class:!0,href:!0});var Lt=o(P);ie=n(Lt,"SPAN",{});var zt=o(ie);g(Y.$$.fragment,zt),zt.forEach(t),Lt.forEach(t),nt=c(Ge),pe=n(Ge,"SPAN",{});var Yt=o(pe);ot=m(Yt,"How do I use this model on an image?"),Yt.forEach(t),Ge.forEach(t),je=c(e),V=n(e,"P",{});var Gt=o(V);rt=m(Gt,"To load a pretrained model:"),Gt.forEach(t),be=c(e),g(G.$$.fragment,e),ye=c(e),Q=n(e,"P",{});var Xt=o(Q);it=m(Xt,"To load and preprocess the image:"),Xt.forEach(t),ke=c(e),g(X.$$.fragment,e),xe=c(e),Z=n(e,"P",{});var Mt=o(Z);pt=m(Mt,"To get the model predictions:"),Mt.forEach(t),Ee=c(e),g(M.$$.fragment,e),Se=c(e),ee=n(e,"P",{});var Ut=o(ee);mt=m(Ut,"To get the top-5 predictions class names:"),Ut.forEach(t),Pe=c(e),g(U.$$.fragment,e),Ne=c(e),N=n(e,"P",{});var Xe=o(N);ht=m(Xe,"Replace the model name with the variant you want to use, e.g. "),me=n(Xe,"CODE",{});var Bt=o(me);ct=m(Bt,"skresnet18"),Bt.forEach(t),ft=m(Xe,". You can find the IDs in the model summaries at the top of this page."),Xe.forEach(t),Ae=c(e),A=n(e,"P",{});var Me=o(A);ut=m(Me,"To extract image features with this model, follow the "),te=n(Me,"A",{href:!0});var Wt=o(te);gt=m(Wt,"timm feature extraction examples"),Wt.forEach(t),dt=m(Me,", just change the name of the model you want to use."),Me.forEach(t),Ie=c(e),y=n(e,"H2",{class:!0});var Ue=o(y);I=n(Ue,"A",{id:!0,class:!0,href:!0});var Dt=o(I);he=n(Dt,"SPAN",{});var Jt=o(he);g(B.$$.fragment,Jt),Jt.forEach(t),Dt.forEach(t),vt=c(Ue),ce=n(Ue,"SPAN",{});var Ot=o(ce);wt=m(Ot,"How do I finetune this model?"),Ot.forEach(t),Ue.forEach(t),qe=c(e),se=n(e,"P",{});var Ft=o(se);_t=m(Ft,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ft.forEach(t),Te=c(e),g(W.$$.fragment,e),He=c(e),q=n(e,"P",{});var Be=o(q);$t=m(Be,"To finetune on your own dataset, you have to write a training loop or adapt "),D=n(Be,"A",{href:!0,rel:!0});var Vt=o(D);jt=m(Vt,`timm\u2019s training
script`),Vt.forEach(t),bt=m(Be," to use your dataset."),Be.forEach(t),Re=c(e),k=n(e,"H2",{class:!0});var We=o(k);T=n(We,"A",{id:!0,class:!0,href:!0});var Qt=o(T);fe=n(Qt,"SPAN",{});var Zt=o(fe);g(J.$$.fragment,Zt),Zt.forEach(t),Qt.forEach(t),yt=c(We),ue=n(We,"SPAN",{});var es=o(ue);kt=m(es,"How do I train this model?"),es.forEach(t),We.forEach(t),Ce=c(e),H=n(e,"P",{});var De=o(H);xt=m(De,"You can follow the "),ae=n(De,"A",{href:!0});var ts=o(ae);Et=m(ts,"timm recipe scripts"),ts.forEach(t),St=m(De," for training a new model afresh."),De.forEach(t),Ke=c(e),x=n(e,"H2",{class:!0});var Je=o(x);R=n(Je,"A",{id:!0,class:!0,href:!0});var ss=o(R);ge=n(ss,"SPAN",{});var as=o(ge);g(O.$$.fragment,as),as.forEach(t),ss.forEach(t),Pt=c(Je),de=n(Je,"SPAN",{});var ls=o(de);Nt=m(ls,"Citation"),ls.forEach(t),Je.forEach(t),Le=c(e),g(F.$$.fragment,e),this.h()},h(){i($,"name","hf:doc:metadata"),i($,"content",JSON.stringify(cs)),i(S,"id","skresnet"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#skresnet"),i(j,"class","relative group"),i(K,"href","https://www.paperswithcode.com/method/resnet"),i(K,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/selective-kernel"),i(L,"rel","nofollow"),i(z,"href","https://paperswithcode.com/method/selective-kernel-convolution"),i(z,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i(b,"class","relative group"),i(te,"href","../feature_extraction"),i(I,"id","how-do-i-finetune-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(D,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(D,"rel","nofollow"),i(T,"id","how-do-i-train-this-model"),i(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(T,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(ae,"href","../scripts"),i(R,"id","citation"),i(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(R,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,$),r(e,we,a),r(e,j,a),s(j,S),s(S,ne),d(C,ne,null),s(j,Oe),s(j,oe),s(oe,Fe),r(e,_e,a),r(e,f,a),s(f,re),s(re,Ve),s(f,Qe),s(f,K),s(K,Ze),s(f,et),s(f,L),s(L,tt),s(f,st),s(f,z),s(z,at),s(f,lt),r(e,$e,a),r(e,b,a),s(b,P),s(P,ie),d(Y,ie,null),s(b,nt),s(b,pe),s(pe,ot),r(e,je,a),r(e,V,a),s(V,rt),r(e,be,a),d(G,e,a),r(e,ye,a),r(e,Q,a),s(Q,it),r(e,ke,a),d(X,e,a),r(e,xe,a),r(e,Z,a),s(Z,pt),r(e,Ee,a),d(M,e,a),r(e,Se,a),r(e,ee,a),s(ee,mt),r(e,Pe,a),d(U,e,a),r(e,Ne,a),r(e,N,a),s(N,ht),s(N,me),s(me,ct),s(N,ft),r(e,Ae,a),r(e,A,a),s(A,ut),s(A,te),s(te,gt),s(A,dt),r(e,Ie,a),r(e,y,a),s(y,I),s(I,he),d(B,he,null),s(y,vt),s(y,ce),s(ce,wt),r(e,qe,a),r(e,se,a),s(se,_t),r(e,Te,a),d(W,e,a),r(e,He,a),r(e,q,a),s(q,$t),s(q,D),s(D,jt),s(q,bt),r(e,Re,a),r(e,k,a),s(k,T),s(T,fe),d(J,fe,null),s(k,yt),s(k,ue),s(ue,kt),r(e,Ce,a),r(e,H,a),s(H,xt),s(H,ae),s(ae,Et),s(H,St),r(e,Ke,a),r(e,x,a),s(x,R),s(R,ge),d(O,ge,null),s(x,Pt),s(x,de),s(de,Nt),r(e,Le,a),d(F,e,a),ze=!0},p:ps,i(e){ze||(v(C.$$.fragment,e),v(Y.$$.fragment,e),v(G.$$.fragment,e),v(X.$$.fragment,e),v(M.$$.fragment,e),v(U.$$.fragment,e),v(B.$$.fragment,e),v(W.$$.fragment,e),v(J.$$.fragment,e),v(O.$$.fragment,e),v(F.$$.fragment,e),ze=!0)},o(e){w(C.$$.fragment,e),w(Y.$$.fragment,e),w(G.$$.fragment,e),w(X.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(B.$$.fragment,e),w(W.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),ze=!1},d(e){t($),e&&t(we),e&&t(j),_(C),e&&t(_e),e&&t(f),e&&t($e),e&&t(b),_(Y),e&&t(je),e&&t(V),e&&t(be),_(G,e),e&&t(ye),e&&t(Q),e&&t(ke),_(X,e),e&&t(xe),e&&t(Z),e&&t(Ee),_(M,e),e&&t(Se),e&&t(ee),e&&t(Pe),_(U,e),e&&t(Ne),e&&t(N),e&&t(Ae),e&&t(A),e&&t(Ie),e&&t(y),_(B),e&&t(qe),e&&t(se),e&&t(Te),_(W,e),e&&t(He),e&&t(q),e&&t(Re),e&&t(k),_(J),e&&t(Ce),e&&t(H),e&&t(Ke),e&&t(x),_(O),e&&t(Le),_(F,e)}}}const cs={local:"skresnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"SK-ResNet"};function fs(At){return ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vs extends ns{constructor($){super();os(this,$,fs,hs,rs,{})}}export{vs as default,cs as metadata};
