import{S as ts,i as ss,s as as,e as n,k as h,w as u,t as p,M as ns,c as l,d as t,m as c,a as o,x as d,h as m,b as i,G as s,g as r,y as g,L as ls,q as w,o as _,B as v,v as os}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function rs(St){let j,we,y,E,ne,C,Fe,le,Ve,_e,f,oe,We,Ke,z,Qe,Ze,L,et,tt,ve,$,S,re,R,st,ie,at,je,W,nt,ye,X,$e,K,lt,be,G,xe,Q,ot,ke,Y,Ee,Z,rt,Se,M,Pe,P,it,pe,pt,mt,qe,q,ht,ee,ct,ft,Ne,b,N,me,U,ut,he,dt,Ae,te,gt,Ie,B,Te,A,wt,D,_t,vt,He,x,I,ce,J,jt,fe,yt,Ce,T,$t,se,bt,xt,ze,k,H,ue,O,kt,de,Et,Le,F,Re;return C=new ge({}),R=new ge({}),X=new ae({props:{code:`import timm
model = timm.create_model('legacy_seresnext101_32x4d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;legacy_seresnext101_32x4d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),G=new ae({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),Y=new ae({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),M=new ae({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),U=new ge({}),B=new ae({props:{code:"model = timm.create_model('legacy_seresnext101_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;legacy_seresnext101_32x4d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),J=new ge({}),O=new ge({}),F=new ae({props:{code:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-and-Excitation Networks}, 
      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
      year={2019},
      eprint={1709.01507},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-<span class="hljs-keyword">and-Excitation </span>Networks}, 
      author={<span class="hljs-keyword">Jie </span>Hu <span class="hljs-keyword">and </span>Li <span class="hljs-keyword">Shen </span><span class="hljs-keyword">and </span>Samuel Albanie <span class="hljs-keyword">and </span>Gang Sun <span class="hljs-keyword">and </span>Enhua Wu},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1709</span>.<span class="hljs-number">01507</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){j=n("meta"),we=h(),y=n("h1"),E=n("a"),ne=n("span"),u(C.$$.fragment),Fe=h(),le=n("span"),Ve=p("(Legacy) SE-ResNeXt"),_e=h(),f=n("p"),oe=n("strong"),We=p("SE ResNeXt"),Ke=p(" is a variant of a "),z=n("a"),Qe=p("ResNeXt"),Ze=p(" that employs "),L=n("a"),et=p("squeeze-and-excitation blocks"),tt=p(" to enable the network to perform dynamic channel-wise feature recalibration."),ve=h(),$=n("h2"),S=n("a"),re=n("span"),u(R.$$.fragment),st=h(),ie=n("span"),at=p("How do I use this model on an image?"),je=h(),W=n("p"),nt=p("To load a pretrained model:"),ye=h(),u(X.$$.fragment),$e=h(),K=n("p"),lt=p("To load and preprocess the image:"),be=h(),u(G.$$.fragment),xe=h(),Q=n("p"),ot=p("To get the model predictions:"),ke=h(),u(Y.$$.fragment),Ee=h(),Z=n("p"),rt=p("To get the top-5 predictions class names:"),Se=h(),u(M.$$.fragment),Pe=h(),P=n("p"),it=p("Replace the model name with the variant you want to use, e.g. "),pe=n("code"),pt=p("legacy_seresnext101_32x4d"),mt=p(". You can find the IDs in the model summaries at the top of this page."),qe=h(),q=n("p"),ht=p("To extract image features with this model, follow the "),ee=n("a"),ct=p("timm feature extraction examples"),ft=p(", just change the name of the model you want to use."),Ne=h(),b=n("h2"),N=n("a"),me=n("span"),u(U.$$.fragment),ut=h(),he=n("span"),dt=p("How do I finetune this model?"),Ae=h(),te=n("p"),gt=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ie=h(),u(B.$$.fragment),Te=h(),A=n("p"),wt=p("To finetune on your own dataset, you have to write a training loop or adapt "),D=n("a"),_t=p(`timm\u2019s training
script`),vt=p(" to use your dataset."),He=h(),x=n("h2"),I=n("a"),ce=n("span"),u(J.$$.fragment),jt=h(),fe=n("span"),yt=p("How do I train this model?"),Ce=h(),T=n("p"),$t=p("You can follow the "),se=n("a"),bt=p("timm recipe scripts"),xt=p(" for training a new model afresh."),ze=h(),k=n("h2"),H=n("a"),ue=n("span"),u(O.$$.fragment),kt=h(),de=n("span"),Et=p("Citation"),Le=h(),u(F.$$.fragment),this.h()},l(e){const a=ns('[data-svelte="svelte-1phssyn"]',document.head);j=l(a,"META",{name:!0,content:!0}),a.forEach(t),we=c(e),y=l(e,"H1",{class:!0});var Xe=o(y);E=l(Xe,"A",{id:!0,class:!0,href:!0});var Pt=o(E);ne=l(Pt,"SPAN",{});var qt=o(ne);d(C.$$.fragment,qt),qt.forEach(t),Pt.forEach(t),Fe=c(Xe),le=l(Xe,"SPAN",{});var Nt=o(le);Ve=m(Nt,"(Legacy) SE-ResNeXt"),Nt.forEach(t),Xe.forEach(t),_e=c(e),f=l(e,"P",{});var V=o(f);oe=l(V,"STRONG",{});var At=o(oe);We=m(At,"SE ResNeXt"),At.forEach(t),Ke=m(V," is a variant of a "),z=l(V,"A",{href:!0,rel:!0});var It=o(z);Qe=m(It,"ResNeXt"),It.forEach(t),Ze=m(V," that employs "),L=l(V,"A",{href:!0,rel:!0});var Tt=o(L);et=m(Tt,"squeeze-and-excitation blocks"),Tt.forEach(t),tt=m(V," to enable the network to perform dynamic channel-wise feature recalibration."),V.forEach(t),ve=c(e),$=l(e,"H2",{class:!0});var Ge=o($);S=l(Ge,"A",{id:!0,class:!0,href:!0});var Ht=o(S);re=l(Ht,"SPAN",{});var Ct=o(re);d(R.$$.fragment,Ct),Ct.forEach(t),Ht.forEach(t),st=c(Ge),ie=l(Ge,"SPAN",{});var zt=o(ie);at=m(zt,"How do I use this model on an image?"),zt.forEach(t),Ge.forEach(t),je=c(e),W=l(e,"P",{});var Lt=o(W);nt=m(Lt,"To load a pretrained model:"),Lt.forEach(t),ye=c(e),d(X.$$.fragment,e),$e=c(e),K=l(e,"P",{});var Rt=o(K);lt=m(Rt,"To load and preprocess the image:"),Rt.forEach(t),be=c(e),d(G.$$.fragment,e),xe=c(e),Q=l(e,"P",{});var Xt=o(Q);ot=m(Xt,"To get the model predictions:"),Xt.forEach(t),ke=c(e),d(Y.$$.fragment,e),Ee=c(e),Z=l(e,"P",{});var Gt=o(Z);rt=m(Gt,"To get the top-5 predictions class names:"),Gt.forEach(t),Se=c(e),d(M.$$.fragment,e),Pe=c(e),P=l(e,"P",{});var Ye=o(P);it=m(Ye,"Replace the model name with the variant you want to use, e.g. "),pe=l(Ye,"CODE",{});var Yt=o(pe);pt=m(Yt,"legacy_seresnext101_32x4d"),Yt.forEach(t),mt=m(Ye,". You can find the IDs in the model summaries at the top of this page."),Ye.forEach(t),qe=c(e),q=l(e,"P",{});var Me=o(q);ht=m(Me,"To extract image features with this model, follow the "),ee=l(Me,"A",{href:!0});var Mt=o(ee);ct=m(Mt,"timm feature extraction examples"),Mt.forEach(t),ft=m(Me,", just change the name of the model you want to use."),Me.forEach(t),Ne=c(e),b=l(e,"H2",{class:!0});var Ue=o(b);N=l(Ue,"A",{id:!0,class:!0,href:!0});var Ut=o(N);me=l(Ut,"SPAN",{});var Bt=o(me);d(U.$$.fragment,Bt),Bt.forEach(t),Ut.forEach(t),ut=c(Ue),he=l(Ue,"SPAN",{});var Dt=o(he);dt=m(Dt,"How do I finetune this model?"),Dt.forEach(t),Ue.forEach(t),Ae=c(e),te=l(e,"P",{});var Jt=o(te);gt=m(Jt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Jt.forEach(t),Ie=c(e),d(B.$$.fragment,e),Te=c(e),A=l(e,"P",{});var Be=o(A);wt=m(Be,"To finetune on your own dataset, you have to write a training loop or adapt "),D=l(Be,"A",{href:!0,rel:!0});var Ot=o(D);_t=m(Ot,`timm\u2019s training
script`),Ot.forEach(t),vt=m(Be," to use your dataset."),Be.forEach(t),He=c(e),x=l(e,"H2",{class:!0});var De=o(x);I=l(De,"A",{id:!0,class:!0,href:!0});var Ft=o(I);ce=l(Ft,"SPAN",{});var Vt=o(ce);d(J.$$.fragment,Vt),Vt.forEach(t),Ft.forEach(t),jt=c(De),fe=l(De,"SPAN",{});var Wt=o(fe);yt=m(Wt,"How do I train this model?"),Wt.forEach(t),De.forEach(t),Ce=c(e),T=l(e,"P",{});var Je=o(T);$t=m(Je,"You can follow the "),se=l(Je,"A",{href:!0});var Kt=o(se);bt=m(Kt,"timm recipe scripts"),Kt.forEach(t),xt=m(Je," for training a new model afresh."),Je.forEach(t),ze=c(e),k=l(e,"H2",{class:!0});var Oe=o(k);H=l(Oe,"A",{id:!0,class:!0,href:!0});var Qt=o(H);ue=l(Qt,"SPAN",{});var Zt=o(ue);d(O.$$.fragment,Zt),Zt.forEach(t),Qt.forEach(t),kt=c(Oe),de=l(Oe,"SPAN",{});var es=o(de);Et=m(es,"Citation"),es.forEach(t),Oe.forEach(t),Le=c(e),d(F.$$.fragment,e),this.h()},h(){i(j,"name","hf:doc:metadata"),i(j,"content",JSON.stringify(is)),i(E,"id","legacy-seresnext"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#legacy-seresnext"),i(y,"class","relative group"),i(z,"href","https://www.paperswithcode.com/method/resnext"),i(z,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/squeeze-and-excitation-block"),i(L,"rel","nofollow"),i(S,"id","how-do-i-use-this-model-on-an-image"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(ee,"href","../feature_extraction"),i(N,"id","how-do-i-finetune-this-model"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#how-do-i-finetune-this-model"),i(b,"class","relative group"),i(D,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(D,"rel","nofollow"),i(I,"id","how-do-i-train-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-train-this-model"),i(x,"class","relative group"),i(se,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(k,"class","relative group")},m(e,a){s(document.head,j),r(e,we,a),r(e,y,a),s(y,E),s(E,ne),g(C,ne,null),s(y,Fe),s(y,le),s(le,Ve),r(e,_e,a),r(e,f,a),s(f,oe),s(oe,We),s(f,Ke),s(f,z),s(z,Qe),s(f,Ze),s(f,L),s(L,et),s(f,tt),r(e,ve,a),r(e,$,a),s($,S),s(S,re),g(R,re,null),s($,st),s($,ie),s(ie,at),r(e,je,a),r(e,W,a),s(W,nt),r(e,ye,a),g(X,e,a),r(e,$e,a),r(e,K,a),s(K,lt),r(e,be,a),g(G,e,a),r(e,xe,a),r(e,Q,a),s(Q,ot),r(e,ke,a),g(Y,e,a),r(e,Ee,a),r(e,Z,a),s(Z,rt),r(e,Se,a),g(M,e,a),r(e,Pe,a),r(e,P,a),s(P,it),s(P,pe),s(pe,pt),s(P,mt),r(e,qe,a),r(e,q,a),s(q,ht),s(q,ee),s(ee,ct),s(q,ft),r(e,Ne,a),r(e,b,a),s(b,N),s(N,me),g(U,me,null),s(b,ut),s(b,he),s(he,dt),r(e,Ae,a),r(e,te,a),s(te,gt),r(e,Ie,a),g(B,e,a),r(e,Te,a),r(e,A,a),s(A,wt),s(A,D),s(D,_t),s(A,vt),r(e,He,a),r(e,x,a),s(x,I),s(I,ce),g(J,ce,null),s(x,jt),s(x,fe),s(fe,yt),r(e,Ce,a),r(e,T,a),s(T,$t),s(T,se),s(se,bt),s(T,xt),r(e,ze,a),r(e,k,a),s(k,H),s(H,ue),g(O,ue,null),s(k,kt),s(k,de),s(de,Et),r(e,Le,a),g(F,e,a),Re=!0},p:ls,i(e){Re||(w(C.$$.fragment,e),w(R.$$.fragment,e),w(X.$$.fragment,e),w(G.$$.fragment,e),w(Y.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(B.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),Re=!0)},o(e){_(C.$$.fragment,e),_(R.$$.fragment,e),_(X.$$.fragment,e),_(G.$$.fragment,e),_(Y.$$.fragment,e),_(M.$$.fragment,e),_(U.$$.fragment,e),_(B.$$.fragment,e),_(J.$$.fragment,e),_(O.$$.fragment,e),_(F.$$.fragment,e),Re=!1},d(e){t(j),e&&t(we),e&&t(y),v(C),e&&t(_e),e&&t(f),e&&t(ve),e&&t($),v(R),e&&t(je),e&&t(W),e&&t(ye),v(X,e),e&&t($e),e&&t(K),e&&t(be),v(G,e),e&&t(xe),e&&t(Q),e&&t(ke),v(Y,e),e&&t(Ee),e&&t(Z),e&&t(Se),v(M,e),e&&t(Pe),e&&t(P),e&&t(qe),e&&t(q),e&&t(Ne),e&&t(b),v(U),e&&t(Ae),e&&t(te),e&&t(Ie),v(B,e),e&&t(Te),e&&t(A),e&&t(He),e&&t(x),v(J),e&&t(Ce),e&&t(T),e&&t(ze),e&&t(k),v(O),e&&t(Le),v(F,e)}}}const is={local:"legacy-seresnext",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"(Legacy) SE-ResNeXt"};function ps(St){return os(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fs extends ts{constructor(j){super();ss(this,j,ps,rs,as,{})}}export{fs as default,is as metadata};
