import{S as oa,i as ra,s as ia,e as n,k as h,w as u,t as p,M as pa,c as l,d as t,m as c,a as o,x as g,h as m,b as i,G as a,g as r,y as d,L as ma,q as w,o as v,B as _,v as ha}from"../../chunks/vendor-hf-doc-builder.js";import{I as we}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";function ca(St){let j,ve,b,A,le,R,Fe,oe,Ve,_e,f,Xe,re,Je,Ke,z,et,tt,L,at,st,B,nt,lt,$e,y,P,ie,G,ot,pe,rt,je,X,it,be,W,ye,J,pt,Ee,Y,xe,K,mt,ke,D,Ae,ee,ht,Pe,M,Ne,N,ct,me,ft,ut,Ce,C,gt,te,dt,wt,Se,E,S,he,U,vt,ce,_t,qe,ae,$t,Ie,Q,Te,q,jt,Z,bt,yt,He,x,I,fe,O,Et,ue,xt,Re,T,kt,se,At,Pt,ze,k,H,ge,F,Nt,de,Ct,Le,V,Be;return R=new we({}),G=new we({}),W=new ne({props:{code:`import timm
model = timm.create_model('ecaresnet101d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ecaresnet101d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new ne({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),D=new ne({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),M=new ne({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),U=new we({}),Q=new ne({props:{code:"model = timm.create_model('ecaresnet101d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ecaresnet101d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),O=new we({}),F=new we({}),V=new ne({props:{code:`@misc{wang2020ecanet,
      title={ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks}, 
      author={Qilong Wang and Banggu Wu and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu},
      year={2020},
      eprint={1910.03151},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`<span class="language-xml">@misc</span><span class="hljs-template-variable">{wang2020ecanet,
      title={ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks}</span><span class="language-xml">, 
      author=</span><span class="hljs-template-variable">{Qilong Wang and Banggu Wu and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu}</span><span class="language-xml">,
      year=</span><span class="hljs-template-variable">{2020}</span><span class="language-xml">,
      eprint=</span><span class="hljs-template-variable">{1910.03151}</span><span class="language-xml">,
      archivePrefix=</span><span class="hljs-template-variable">{arXiv}</span><span class="language-xml">,
      primaryClass=</span><span class="hljs-template-variable">{cs.CV}</span><span class="language-xml">
}</span>`}}),{c(){j=n("meta"),ve=h(),b=n("h1"),A=n("a"),le=n("span"),u(R.$$.fragment),Fe=h(),oe=n("span"),Ve=p("ECA-ResNet"),_e=h(),f=n("p"),Xe=p("An "),re=n("strong"),Je=p("ECA ResNet"),Ke=p(" is a variant on a "),z=n("a"),et=p("ResNet"),tt=p(" that utilises an "),L=n("a"),at=p("Efficient Channel Attention module"),st=p(". Efficient Channel Attention is an architectural unit based on "),B=n("a"),nt=p("squeeze-and-excitation blocks"),lt=p(" that reduces model complexity without dimensionality reduction."),$e=h(),y=n("h2"),P=n("a"),ie=n("span"),u(G.$$.fragment),ot=h(),pe=n("span"),rt=p("How do I use this model on an image?"),je=h(),X=n("p"),it=p("To load a pretrained model:"),be=h(),u(W.$$.fragment),ye=h(),J=n("p"),pt=p("To load and preprocess the image:"),Ee=h(),u(Y.$$.fragment),xe=h(),K=n("p"),mt=p("To get the model predictions:"),ke=h(),u(D.$$.fragment),Ae=h(),ee=n("p"),ht=p("To get the top-5 predictions class names:"),Pe=h(),u(M.$$.fragment),Ne=h(),N=n("p"),ct=p("Replace the model name with the variant you want to use, e.g. "),me=n("code"),ft=p("ecaresnet101d"),ut=p(". You can find the IDs in the model summaries at the top of this page."),Ce=h(),C=n("p"),gt=p("To extract image features with this model, follow the "),te=n("a"),dt=p("timm feature extraction examples"),wt=p(", just change the name of the model you want to use."),Se=h(),E=n("h2"),S=n("a"),he=n("span"),u(U.$$.fragment),vt=h(),ce=n("span"),_t=p("How do I finetune this model?"),qe=h(),ae=n("p"),$t=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ie=h(),u(Q.$$.fragment),Te=h(),q=n("p"),jt=p("To finetune on your own dataset, you have to write a training loop or adapt "),Z=n("a"),bt=p(`timm\u2019s training
script`),yt=p(" to use your dataset."),He=h(),x=n("h2"),I=n("a"),fe=n("span"),u(O.$$.fragment),Et=h(),ue=n("span"),xt=p("How do I train this model?"),Re=h(),T=n("p"),kt=p("You can follow the "),se=n("a"),At=p("timm recipe scripts"),Pt=p(" for training a new model afresh."),ze=h(),k=n("h2"),H=n("a"),ge=n("span"),u(F.$$.fragment),Nt=h(),de=n("span"),Ct=p("Citation"),Le=h(),u(V.$$.fragment),this.h()},l(e){const s=pa('[data-svelte="svelte-1phssyn"]',document.head);j=l(s,"META",{name:!0,content:!0}),s.forEach(t),ve=c(e),b=l(e,"H1",{class:!0});var Ge=o(b);A=l(Ge,"A",{id:!0,class:!0,href:!0});var qt=o(A);le=l(qt,"SPAN",{});var It=o(le);g(R.$$.fragment,It),It.forEach(t),qt.forEach(t),Fe=c(Ge),oe=l(Ge,"SPAN",{});var Tt=o(oe);Ve=m(Tt,"ECA-ResNet"),Tt.forEach(t),Ge.forEach(t),_e=c(e),f=l(e,"P",{});var $=o(f);Xe=m($,"An "),re=l($,"STRONG",{});var Ht=o(re);Je=m(Ht,"ECA ResNet"),Ht.forEach(t),Ke=m($," is a variant on a "),z=l($,"A",{href:!0,rel:!0});var Rt=o(z);et=m(Rt,"ResNet"),Rt.forEach(t),tt=m($," that utilises an "),L=l($,"A",{href:!0,rel:!0});var zt=o(L);at=m(zt,"Efficient Channel Attention module"),zt.forEach(t),st=m($,". Efficient Channel Attention is an architectural unit based on "),B=l($,"A",{href:!0,rel:!0});var Lt=o(B);nt=m(Lt,"squeeze-and-excitation blocks"),Lt.forEach(t),lt=m($," that reduces model complexity without dimensionality reduction."),$.forEach(t),$e=c(e),y=l(e,"H2",{class:!0});var We=o(y);P=l(We,"A",{id:!0,class:!0,href:!0});var Bt=o(P);ie=l(Bt,"SPAN",{});var Gt=o(ie);g(G.$$.fragment,Gt),Gt.forEach(t),Bt.forEach(t),ot=c(We),pe=l(We,"SPAN",{});var Wt=o(pe);rt=m(Wt,"How do I use this model on an image?"),Wt.forEach(t),We.forEach(t),je=c(e),X=l(e,"P",{});var Yt=o(X);it=m(Yt,"To load a pretrained model:"),Yt.forEach(t),be=c(e),g(W.$$.fragment,e),ye=c(e),J=l(e,"P",{});var Dt=o(J);pt=m(Dt,"To load and preprocess the image:"),Dt.forEach(t),Ee=c(e),g(Y.$$.fragment,e),xe=c(e),K=l(e,"P",{});var Mt=o(K);mt=m(Mt,"To get the model predictions:"),Mt.forEach(t),ke=c(e),g(D.$$.fragment,e),Ae=c(e),ee=l(e,"P",{});var Ut=o(ee);ht=m(Ut,"To get the top-5 predictions class names:"),Ut.forEach(t),Pe=c(e),g(M.$$.fragment,e),Ne=c(e),N=l(e,"P",{});var Ye=o(N);ct=m(Ye,"Replace the model name with the variant you want to use, e.g. "),me=l(Ye,"CODE",{});var Qt=o(me);ft=m(Qt,"ecaresnet101d"),Qt.forEach(t),ut=m(Ye,". You can find the IDs in the model summaries at the top of this page."),Ye.forEach(t),Ce=c(e),C=l(e,"P",{});var De=o(C);gt=m(De,"To extract image features with this model, follow the "),te=l(De,"A",{href:!0});var Zt=o(te);dt=m(Zt,"timm feature extraction examples"),Zt.forEach(t),wt=m(De,", just change the name of the model you want to use."),De.forEach(t),Se=c(e),E=l(e,"H2",{class:!0});var Me=o(E);S=l(Me,"A",{id:!0,class:!0,href:!0});var Ot=o(S);he=l(Ot,"SPAN",{});var Ft=o(he);g(U.$$.fragment,Ft),Ft.forEach(t),Ot.forEach(t),vt=c(Me),ce=l(Me,"SPAN",{});var Vt=o(ce);_t=m(Vt,"How do I finetune this model?"),Vt.forEach(t),Me.forEach(t),qe=c(e),ae=l(e,"P",{});var Xt=o(ae);$t=m(Xt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Xt.forEach(t),Ie=c(e),g(Q.$$.fragment,e),Te=c(e),q=l(e,"P",{});var Ue=o(q);jt=m(Ue,"To finetune on your own dataset, you have to write a training loop or adapt "),Z=l(Ue,"A",{href:!0,rel:!0});var Jt=o(Z);bt=m(Jt,`timm\u2019s training
script`),Jt.forEach(t),yt=m(Ue," to use your dataset."),Ue.forEach(t),He=c(e),x=l(e,"H2",{class:!0});var Qe=o(x);I=l(Qe,"A",{id:!0,class:!0,href:!0});var Kt=o(I);fe=l(Kt,"SPAN",{});var ea=o(fe);g(O.$$.fragment,ea),ea.forEach(t),Kt.forEach(t),Et=c(Qe),ue=l(Qe,"SPAN",{});var ta=o(ue);xt=m(ta,"How do I train this model?"),ta.forEach(t),Qe.forEach(t),Re=c(e),T=l(e,"P",{});var Ze=o(T);kt=m(Ze,"You can follow the "),se=l(Ze,"A",{href:!0});var aa=o(se);At=m(aa,"timm recipe scripts"),aa.forEach(t),Pt=m(Ze," for training a new model afresh."),Ze.forEach(t),ze=c(e),k=l(e,"H2",{class:!0});var Oe=o(k);H=l(Oe,"A",{id:!0,class:!0,href:!0});var sa=o(H);ge=l(sa,"SPAN",{});var na=o(ge);g(F.$$.fragment,na),na.forEach(t),sa.forEach(t),Nt=c(Oe),de=l(Oe,"SPAN",{});var la=o(de);Ct=m(la,"Citation"),la.forEach(t),Oe.forEach(t),Le=c(e),g(V.$$.fragment,e),this.h()},h(){i(j,"name","hf:doc:metadata"),i(j,"content",JSON.stringify(fa)),i(A,"id","ecaresnet"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#ecaresnet"),i(b,"class","relative group"),i(z,"href","https://paperswithcode.com/method/resnet"),i(z,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/efficient-channel-attention"),i(L,"rel","nofollow"),i(B,"href","https://paperswithcode.com/method/squeeze-and-excitation-block"),i(B,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i(y,"class","relative group"),i(te,"href","../feature_extraction"),i(S,"id","how-do-i-finetune-this-model"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-finetune-this-model"),i(E,"class","relative group"),i(Z,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(Z,"rel","nofollow"),i(I,"id","how-do-i-train-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-train-this-model"),i(x,"class","relative group"),i(se,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(k,"class","relative group")},m(e,s){a(document.head,j),r(e,ve,s),r(e,b,s),a(b,A),a(A,le),d(R,le,null),a(b,Fe),a(b,oe),a(oe,Ve),r(e,_e,s),r(e,f,s),a(f,Xe),a(f,re),a(re,Je),a(f,Ke),a(f,z),a(z,et),a(f,tt),a(f,L),a(L,at),a(f,st),a(f,B),a(B,nt),a(f,lt),r(e,$e,s),r(e,y,s),a(y,P),a(P,ie),d(G,ie,null),a(y,ot),a(y,pe),a(pe,rt),r(e,je,s),r(e,X,s),a(X,it),r(e,be,s),d(W,e,s),r(e,ye,s),r(e,J,s),a(J,pt),r(e,Ee,s),d(Y,e,s),r(e,xe,s),r(e,K,s),a(K,mt),r(e,ke,s),d(D,e,s),r(e,Ae,s),r(e,ee,s),a(ee,ht),r(e,Pe,s),d(M,e,s),r(e,Ne,s),r(e,N,s),a(N,ct),a(N,me),a(me,ft),a(N,ut),r(e,Ce,s),r(e,C,s),a(C,gt),a(C,te),a(te,dt),a(C,wt),r(e,Se,s),r(e,E,s),a(E,S),a(S,he),d(U,he,null),a(E,vt),a(E,ce),a(ce,_t),r(e,qe,s),r(e,ae,s),a(ae,$t),r(e,Ie,s),d(Q,e,s),r(e,Te,s),r(e,q,s),a(q,jt),a(q,Z),a(Z,bt),a(q,yt),r(e,He,s),r(e,x,s),a(x,I),a(I,fe),d(O,fe,null),a(x,Et),a(x,ue),a(ue,xt),r(e,Re,s),r(e,T,s),a(T,kt),a(T,se),a(se,At),a(T,Pt),r(e,ze,s),r(e,k,s),a(k,H),a(H,ge),d(F,ge,null),a(k,Nt),a(k,de),a(de,Ct),r(e,Le,s),d(V,e,s),Be=!0},p:ma,i(e){Be||(w(R.$$.fragment,e),w(G.$$.fragment,e),w(W.$$.fragment,e),w(Y.$$.fragment,e),w(D.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(Q.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),w(V.$$.fragment,e),Be=!0)},o(e){v(R.$$.fragment,e),v(G.$$.fragment,e),v(W.$$.fragment,e),v(Y.$$.fragment,e),v(D.$$.fragment,e),v(M.$$.fragment,e),v(U.$$.fragment,e),v(Q.$$.fragment,e),v(O.$$.fragment,e),v(F.$$.fragment,e),v(V.$$.fragment,e),Be=!1},d(e){t(j),e&&t(ve),e&&t(b),_(R),e&&t(_e),e&&t(f),e&&t($e),e&&t(y),_(G),e&&t(je),e&&t(X),e&&t(be),_(W,e),e&&t(ye),e&&t(J),e&&t(Ee),_(Y,e),e&&t(xe),e&&t(K),e&&t(ke),_(D,e),e&&t(Ae),e&&t(ee),e&&t(Pe),_(M,e),e&&t(Ne),e&&t(N),e&&t(Ce),e&&t(C),e&&t(Se),e&&t(E),_(U),e&&t(qe),e&&t(ae),e&&t(Ie),_(Q,e),e&&t(Te),e&&t(q),e&&t(He),e&&t(x),_(O),e&&t(Re),e&&t(T),e&&t(ze),e&&t(k),_(F),e&&t(Le),_(V,e)}}}const fa={local:"ecaresnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"ECA-ResNet"};function ua(St){return ha(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class va extends oa{constructor(j){super();ra(this,j,ua,ca,ia,{})}}export{va as default,fa as metadata};
