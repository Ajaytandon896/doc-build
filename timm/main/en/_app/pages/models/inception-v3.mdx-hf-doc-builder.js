import{S as os,i as ls,s as rs,e as n,k as m,w as u,t as p,M as is,c as o,d as t,m as c,a as l,x as g,h,b as i,G as s,g as r,y as d,L as ps,q as v,o as w,B as b,v as hs}from"../../chunks/vendor-hf-doc-builder.js";import{I as ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";function ms(At){let _,we,j,S,oe,H,Oe,le,Xe,be,f,re,Ze,Ke,R,Qe,et,L,tt,st,V,at,nt,_e,$,I,ie,M,ot,pe,lt,je,Z,rt,$e,B,ye,K,it,ke,G,xe,Q,pt,Ee,W,Se,ee,ht,Ie,Y,Pe,P,mt,he,ct,ft,Ae,A,ut,te,gt,dt,Te,y,T,me,D,vt,ce,wt,qe,se,bt,Ne,U,ze,q,_t,F,jt,$t,Ce,k,N,fe,J,yt,ue,kt,He,z,xt,ae,Et,St,Re,x,C,ge,O,It,de,Pt,Le,X,Ve;return H=new ve({}),M=new ve({}),B=new ne({props:{code:`import timm
model = timm.create_model('inception_v3', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;inception_v3&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),G=new ne({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),W=new ne({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),Y=new ne({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),D=new ve({}),U=new ne({props:{code:"model = timm.create_model('inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;inception_v3&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),J=new ve({}),O=new ve({}),X=new ne({props:{code:`@article{DBLP:journals/corr/SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1512.00567},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00567},
  archivePrefix = {arXiv},
  eprint    = {1512.00567},
  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`@article{DBLP:journals<span class="hljs-regexp">/corr/</span>SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture <span class="hljs-keyword">for</span> Computer Vision},
  journal   = {CoRR},
  volume    = {abs/<span class="hljs-number">1512.00567</span>},
  year      = {<span class="hljs-number">2015</span>},
  url       = {http:<span class="hljs-regexp">//</span>arxiv.org<span class="hljs-regexp">/abs/</span><span class="hljs-number">1512.00567</span>},
  archivePrefix = {arXiv},
  eprint    = {<span class="hljs-number">1512.00567</span>},
  timestamp = {Mon, <span class="hljs-number">13</span> Aug <span class="hljs-number">2018</span> <span class="hljs-number">16</span>:<span class="hljs-number">49</span>:<span class="hljs-number">07</span> +<span class="hljs-number">0200</span>},
  biburl    = {https:<span class="hljs-regexp">//</span>dblp.org<span class="hljs-regexp">/rec/</span>journals<span class="hljs-regexp">/corr/</span>SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https:<span class="hljs-regexp">//</span>dblp.org}
}`}}),{c(){_=n("meta"),we=m(),j=n("h1"),S=n("a"),oe=n("span"),u(H.$$.fragment),Oe=m(),le=n("span"),Xe=p("Inception v3"),be=m(),f=n("p"),re=n("strong"),Ze=p("Inception v3"),Ke=p(" is a convolutional neural network architecture from the Inception family that makes several improvements including using "),R=n("a"),Qe=p("Label Smoothing"),et=p(", Factorized 7 x 7 convolutions, and the use of an "),L=n("a"),tt=p("auxiliary classifer"),st=p(" to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an "),V=n("a"),at=p("Inception Module"),nt=p("."),_e=m(),$=n("h2"),I=n("a"),ie=n("span"),u(M.$$.fragment),ot=m(),pe=n("span"),lt=p("How do I use this model on an image?"),je=m(),Z=n("p"),rt=p("To load a pretrained model:"),$e=m(),u(B.$$.fragment),ye=m(),K=n("p"),it=p("To load and preprocess the image:"),ke=m(),u(G.$$.fragment),xe=m(),Q=n("p"),pt=p("To get the model predictions:"),Ee=m(),u(W.$$.fragment),Se=m(),ee=n("p"),ht=p("To get the top-5 predictions class names:"),Ie=m(),u(Y.$$.fragment),Pe=m(),P=n("p"),mt=p("Replace the model name with the variant you want to use, e.g. "),he=n("code"),ct=p("inception_v3"),ft=p(". You can find the IDs in the model summaries at the top of this page."),Ae=m(),A=n("p"),ut=p("To extract image features with this model, follow the "),te=n("a"),gt=p("timm feature extraction examples"),dt=p(", just change the name of the model you want to use."),Te=m(),y=n("h2"),T=n("a"),me=n("span"),u(D.$$.fragment),vt=m(),ce=n("span"),wt=p("How do I finetune this model?"),qe=m(),se=n("p"),bt=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ne=m(),u(U.$$.fragment),ze=m(),q=n("p"),_t=p("To finetune on your own dataset, you have to write a training loop or adapt "),F=n("a"),jt=p(`timm\u2019s training
script`),$t=p(" to use your dataset."),Ce=m(),k=n("h2"),N=n("a"),fe=n("span"),u(J.$$.fragment),yt=m(),ue=n("span"),kt=p("How do I train this model?"),He=m(),z=n("p"),xt=p("You can follow the "),ae=n("a"),Et=p("timm recipe scripts"),St=p(" for training a new model afresh."),Re=m(),x=n("h2"),C=n("a"),ge=n("span"),u(O.$$.fragment),It=m(),de=n("span"),Pt=p("Citation"),Le=m(),u(X.$$.fragment),this.h()},l(e){const a=is('[data-svelte="svelte-1phssyn"]',document.head);_=o(a,"META",{name:!0,content:!0}),a.forEach(t),we=c(e),j=o(e,"H1",{class:!0});var Me=l(j);S=o(Me,"A",{id:!0,class:!0,href:!0});var Tt=l(S);oe=o(Tt,"SPAN",{});var qt=l(oe);g(H.$$.fragment,qt),qt.forEach(t),Tt.forEach(t),Oe=c(Me),le=o(Me,"SPAN",{});var Nt=l(le);Xe=h(Nt,"Inception v3"),Nt.forEach(t),Me.forEach(t),be=c(e),f=o(e,"P",{});var E=l(f);re=o(E,"STRONG",{});var zt=l(re);Ze=h(zt,"Inception v3"),zt.forEach(t),Ke=h(E," is a convolutional neural network architecture from the Inception family that makes several improvements including using "),R=o(E,"A",{href:!0,rel:!0});var Ct=l(R);Qe=h(Ct,"Label Smoothing"),Ct.forEach(t),et=h(E,", Factorized 7 x 7 convolutions, and the use of an "),L=o(E,"A",{href:!0,rel:!0});var Ht=l(L);tt=h(Ht,"auxiliary classifer"),Ht.forEach(t),st=h(E," to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an "),V=o(E,"A",{href:!0,rel:!0});var Rt=l(V);at=h(Rt,"Inception Module"),Rt.forEach(t),nt=h(E,"."),E.forEach(t),_e=c(e),$=o(e,"H2",{class:!0});var Be=l($);I=o(Be,"A",{id:!0,class:!0,href:!0});var Lt=l(I);ie=o(Lt,"SPAN",{});var Vt=l(ie);g(M.$$.fragment,Vt),Vt.forEach(t),Lt.forEach(t),ot=c(Be),pe=o(Be,"SPAN",{});var Mt=l(pe);lt=h(Mt,"How do I use this model on an image?"),Mt.forEach(t),Be.forEach(t),je=c(e),Z=o(e,"P",{});var Bt=l(Z);rt=h(Bt,"To load a pretrained model:"),Bt.forEach(t),$e=c(e),g(B.$$.fragment,e),ye=c(e),K=o(e,"P",{});var Gt=l(K);it=h(Gt,"To load and preprocess the image:"),Gt.forEach(t),ke=c(e),g(G.$$.fragment,e),xe=c(e),Q=o(e,"P",{});var Wt=l(Q);pt=h(Wt,"To get the model predictions:"),Wt.forEach(t),Ee=c(e),g(W.$$.fragment,e),Se=c(e),ee=o(e,"P",{});var Yt=l(ee);ht=h(Yt,"To get the top-5 predictions class names:"),Yt.forEach(t),Ie=c(e),g(Y.$$.fragment,e),Pe=c(e),P=o(e,"P",{});var Ge=l(P);mt=h(Ge,"Replace the model name with the variant you want to use, e.g. "),he=o(Ge,"CODE",{});var Dt=l(he);ct=h(Dt,"inception_v3"),Dt.forEach(t),ft=h(Ge,". You can find the IDs in the model summaries at the top of this page."),Ge.forEach(t),Ae=c(e),A=o(e,"P",{});var We=l(A);ut=h(We,"To extract image features with this model, follow the "),te=o(We,"A",{href:!0});var Ut=l(te);gt=h(Ut,"timm feature extraction examples"),Ut.forEach(t),dt=h(We,", just change the name of the model you want to use."),We.forEach(t),Te=c(e),y=o(e,"H2",{class:!0});var Ye=l(y);T=o(Ye,"A",{id:!0,class:!0,href:!0});var Ft=l(T);me=o(Ft,"SPAN",{});var Jt=l(me);g(D.$$.fragment,Jt),Jt.forEach(t),Ft.forEach(t),vt=c(Ye),ce=o(Ye,"SPAN",{});var Ot=l(ce);wt=h(Ot,"How do I finetune this model?"),Ot.forEach(t),Ye.forEach(t),qe=c(e),se=o(e,"P",{});var Xt=l(se);bt=h(Xt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Xt.forEach(t),Ne=c(e),g(U.$$.fragment,e),ze=c(e),q=o(e,"P",{});var De=l(q);_t=h(De,"To finetune on your own dataset, you have to write a training loop or adapt "),F=o(De,"A",{href:!0,rel:!0});var Zt=l(F);jt=h(Zt,`timm\u2019s training
script`),Zt.forEach(t),$t=h(De," to use your dataset."),De.forEach(t),Ce=c(e),k=o(e,"H2",{class:!0});var Ue=l(k);N=o(Ue,"A",{id:!0,class:!0,href:!0});var Kt=l(N);fe=o(Kt,"SPAN",{});var Qt=l(fe);g(J.$$.fragment,Qt),Qt.forEach(t),Kt.forEach(t),yt=c(Ue),ue=o(Ue,"SPAN",{});var es=l(ue);kt=h(es,"How do I train this model?"),es.forEach(t),Ue.forEach(t),He=c(e),z=o(e,"P",{});var Fe=l(z);xt=h(Fe,"You can follow the "),ae=o(Fe,"A",{href:!0});var ts=l(ae);Et=h(ts,"timm recipe scripts"),ts.forEach(t),St=h(Fe," for training a new model afresh."),Fe.forEach(t),Re=c(e),x=o(e,"H2",{class:!0});var Je=l(x);C=o(Je,"A",{id:!0,class:!0,href:!0});var ss=l(C);ge=o(ss,"SPAN",{});var as=l(ge);g(O.$$.fragment,as),as.forEach(t),ss.forEach(t),It=c(Je),de=o(Je,"SPAN",{});var ns=l(de);Pt=h(ns,"Citation"),ns.forEach(t),Je.forEach(t),Le=c(e),g(X.$$.fragment,e),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(cs)),i(S,"id","inception-v3"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#inception-v3"),i(j,"class","relative group"),i(R,"href","https://paperswithcode.com/method/label-smoothing"),i(R,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/auxiliary-classifier"),i(L,"rel","nofollow"),i(V,"href","https://paperswithcode.com/method/inception-v3-module"),i(V,"rel","nofollow"),i(I,"id","how-do-i-use-this-model-on-an-image"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(te,"href","../feature_extraction"),i(T,"id","how-do-i-finetune-this-model"),i(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(T,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(F,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(F,"rel","nofollow"),i(N,"id","how-do-i-train-this-model"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(ae,"href","../scripts"),i(C,"id","citation"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,_),r(e,we,a),r(e,j,a),s(j,S),s(S,oe),d(H,oe,null),s(j,Oe),s(j,le),s(le,Xe),r(e,be,a),r(e,f,a),s(f,re),s(re,Ze),s(f,Ke),s(f,R),s(R,Qe),s(f,et),s(f,L),s(L,tt),s(f,st),s(f,V),s(V,at),s(f,nt),r(e,_e,a),r(e,$,a),s($,I),s(I,ie),d(M,ie,null),s($,ot),s($,pe),s(pe,lt),r(e,je,a),r(e,Z,a),s(Z,rt),r(e,$e,a),d(B,e,a),r(e,ye,a),r(e,K,a),s(K,it),r(e,ke,a),d(G,e,a),r(e,xe,a),r(e,Q,a),s(Q,pt),r(e,Ee,a),d(W,e,a),r(e,Se,a),r(e,ee,a),s(ee,ht),r(e,Ie,a),d(Y,e,a),r(e,Pe,a),r(e,P,a),s(P,mt),s(P,he),s(he,ct),s(P,ft),r(e,Ae,a),r(e,A,a),s(A,ut),s(A,te),s(te,gt),s(A,dt),r(e,Te,a),r(e,y,a),s(y,T),s(T,me),d(D,me,null),s(y,vt),s(y,ce),s(ce,wt),r(e,qe,a),r(e,se,a),s(se,bt),r(e,Ne,a),d(U,e,a),r(e,ze,a),r(e,q,a),s(q,_t),s(q,F),s(F,jt),s(q,$t),r(e,Ce,a),r(e,k,a),s(k,N),s(N,fe),d(J,fe,null),s(k,yt),s(k,ue),s(ue,kt),r(e,He,a),r(e,z,a),s(z,xt),s(z,ae),s(ae,Et),s(z,St),r(e,Re,a),r(e,x,a),s(x,C),s(C,ge),d(O,ge,null),s(x,It),s(x,de),s(de,Pt),r(e,Le,a),d(X,e,a),Ve=!0},p:ps,i(e){Ve||(v(H.$$.fragment,e),v(M.$$.fragment,e),v(B.$$.fragment,e),v(G.$$.fragment,e),v(W.$$.fragment,e),v(Y.$$.fragment,e),v(D.$$.fragment,e),v(U.$$.fragment,e),v(J.$$.fragment,e),v(O.$$.fragment,e),v(X.$$.fragment,e),Ve=!0)},o(e){w(H.$$.fragment,e),w(M.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(W.$$.fragment,e),w(Y.$$.fragment,e),w(D.$$.fragment,e),w(U.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(X.$$.fragment,e),Ve=!1},d(e){t(_),e&&t(we),e&&t(j),b(H),e&&t(be),e&&t(f),e&&t(_e),e&&t($),b(M),e&&t(je),e&&t(Z),e&&t($e),b(B,e),e&&t(ye),e&&t(K),e&&t(ke),b(G,e),e&&t(xe),e&&t(Q),e&&t(Ee),b(W,e),e&&t(Se),e&&t(ee),e&&t(Ie),b(Y,e),e&&t(Pe),e&&t(P),e&&t(Ae),e&&t(A),e&&t(Te),e&&t(y),b(D),e&&t(qe),e&&t(se),e&&t(Ne),b(U,e),e&&t(ze),e&&t(q),e&&t(Ce),e&&t(k),b(J),e&&t(He),e&&t(z),e&&t(Re),e&&t(x),b(O),e&&t(Le),b(X,e)}}}const cs={local:"inception-v3",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Inception v3"};function fs(At){return hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vs extends os{constructor(_){super();ls(this,_,fs,ms,rs,{})}}export{vs as default,cs as metadata};
