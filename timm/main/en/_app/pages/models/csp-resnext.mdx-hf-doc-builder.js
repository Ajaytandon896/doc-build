import{S as Kt,i as Qt,s as Zt,e as n,k as p,w as f,t as h,M as es,c as r,d as t,m,a as o,x as u,h as c,b as i,G as s,g as l,y as g,L as ts,q as d,o as w,B as _,v as ss}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as te}from"../../chunks/CodeBlock-hf-doc-builder.js";function as(kt){let v,de,j,E,se,q,Je,ae,Oe,we,$,ne,Fe,Ve,Y,Ke,Qe,_e,y,P,re,R,Ze,oe,et,ve,O,tt,je,L,$e,F,st,ye,X,be,V,at,ke,M,xe,K,nt,Ee,z,Pe,N,rt,le,ot,lt,Ne,S,it,Q,pt,ht,Se,b,C,ie,B,mt,pe,ct,Ce,Z,ft,Ae,G,Te,A,ut,W,gt,dt,Ie,k,T,he,U,wt,me,_t,He,I,vt,ee,jt,$t,qe,x,H,ce,D,yt,fe,bt,Ye,J,Re;return q=new ge({}),R=new ge({}),L=new te({props:{code:`import timm
model = timm.create_model('cspresnext50', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;cspresnext50&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),X=new te({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),M=new te({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),z=new te({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),B=new ge({}),G=new te({props:{code:"model = timm.create_model('cspresnext50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;cspresnext50&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),U=new ge({}),D=new ge({}),J=new te({props:{code:`@misc{wang2019cspnet,
      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, 
      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},
      year={2019},
      eprint={1911.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{wang2019cspnet,
      title={CSPNet: A New <span class="hljs-keyword">Backbone </span>that can Enhance Learning Capability of CNN}, 
      author={Chien-Yao Wang <span class="hljs-keyword">and </span>Hong-Yuan Mark Liao <span class="hljs-keyword">and </span>I-Hau Yeh <span class="hljs-keyword">and </span>Yueh-Hua Wu <span class="hljs-keyword">and </span>Ping-Yang Chen <span class="hljs-keyword">and </span><span class="hljs-keyword">Jun-Wei </span>Hsieh},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1911</span>.<span class="hljs-number">11929</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){v=n("meta"),de=p(),j=n("h1"),E=n("a"),se=n("span"),f(q.$$.fragment),Je=p(),ae=n("span"),Oe=h("CSP-ResNeXt"),we=p(),$=n("p"),ne=n("strong"),Fe=h("CSPResNeXt"),Ve=h(" is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to "),Y=n("a"),Ke=h("ResNeXt"),Qe=h(". The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network."),_e=p(),y=n("h2"),P=n("a"),re=n("span"),f(R.$$.fragment),Ze=p(),oe=n("span"),et=h("How do I use this model on an image?"),ve=p(),O=n("p"),tt=h("To load a pretrained model:"),je=p(),f(L.$$.fragment),$e=p(),F=n("p"),st=h("To load and preprocess the image:"),ye=p(),f(X.$$.fragment),be=p(),V=n("p"),at=h("To get the model predictions:"),ke=p(),f(M.$$.fragment),xe=p(),K=n("p"),nt=h("To get the top-5 predictions class names:"),Ee=p(),f(z.$$.fragment),Pe=p(),N=n("p"),rt=h("Replace the model name with the variant you want to use, e.g. "),le=n("code"),ot=h("cspresnext50"),lt=h(". You can find the IDs in the model summaries at the top of this page."),Ne=p(),S=n("p"),it=h("To extract image features with this model, follow the "),Q=n("a"),pt=h("timm feature extraction examples"),ht=h(", just change the name of the model you want to use."),Se=p(),b=n("h2"),C=n("a"),ie=n("span"),f(B.$$.fragment),mt=p(),pe=n("span"),ct=h("How do I finetune this model?"),Ce=p(),Z=n("p"),ft=h("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ae=p(),f(G.$$.fragment),Te=p(),A=n("p"),ut=h("To finetune on your own dataset, you have to write a training loop or adapt "),W=n("a"),gt=h(`timm\u2019s training
script`),dt=h(" to use your dataset."),Ie=p(),k=n("h2"),T=n("a"),he=n("span"),f(U.$$.fragment),wt=p(),me=n("span"),_t=h("How do I train this model?"),He=p(),I=n("p"),vt=h("You can follow the "),ee=n("a"),jt=h("timm recipe scripts"),$t=h(" for training a new model afresh."),qe=p(),x=n("h2"),H=n("a"),ce=n("span"),f(D.$$.fragment),yt=p(),fe=n("span"),bt=h("Citation"),Ye=p(),f(J.$$.fragment),this.h()},l(e){const a=es('[data-svelte="svelte-1phssyn"]',document.head);v=r(a,"META",{name:!0,content:!0}),a.forEach(t),de=m(e),j=r(e,"H1",{class:!0});var Le=o(j);E=r(Le,"A",{id:!0,class:!0,href:!0});var xt=o(E);se=r(xt,"SPAN",{});var Et=o(se);u(q.$$.fragment,Et),Et.forEach(t),xt.forEach(t),Je=m(Le),ae=r(Le,"SPAN",{});var Pt=o(ae);Oe=c(Pt,"CSP-ResNeXt"),Pt.forEach(t),Le.forEach(t),we=m(e),$=r(e,"P",{});var ue=o($);ne=r(ue,"STRONG",{});var Nt=o(ne);Fe=c(Nt,"CSPResNeXt"),Nt.forEach(t),Ve=c(ue," is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to "),Y=r(ue,"A",{href:!0,rel:!0});var St=o(Y);Ke=c(St,"ResNeXt"),St.forEach(t),Qe=c(ue,". The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network."),ue.forEach(t),_e=m(e),y=r(e,"H2",{class:!0});var Xe=o(y);P=r(Xe,"A",{id:!0,class:!0,href:!0});var Ct=o(P);re=r(Ct,"SPAN",{});var At=o(re);u(R.$$.fragment,At),At.forEach(t),Ct.forEach(t),Ze=m(Xe),oe=r(Xe,"SPAN",{});var Tt=o(oe);et=c(Tt,"How do I use this model on an image?"),Tt.forEach(t),Xe.forEach(t),ve=m(e),O=r(e,"P",{});var It=o(O);tt=c(It,"To load a pretrained model:"),It.forEach(t),je=m(e),u(L.$$.fragment,e),$e=m(e),F=r(e,"P",{});var Ht=o(F);st=c(Ht,"To load and preprocess the image:"),Ht.forEach(t),ye=m(e),u(X.$$.fragment,e),be=m(e),V=r(e,"P",{});var qt=o(V);at=c(qt,"To get the model predictions:"),qt.forEach(t),ke=m(e),u(M.$$.fragment,e),xe=m(e),K=r(e,"P",{});var Yt=o(K);nt=c(Yt,"To get the top-5 predictions class names:"),Yt.forEach(t),Ee=m(e),u(z.$$.fragment,e),Pe=m(e),N=r(e,"P",{});var Me=o(N);rt=c(Me,"Replace the model name with the variant you want to use, e.g. "),le=r(Me,"CODE",{});var Rt=o(le);ot=c(Rt,"cspresnext50"),Rt.forEach(t),lt=c(Me,". You can find the IDs in the model summaries at the top of this page."),Me.forEach(t),Ne=m(e),S=r(e,"P",{});var ze=o(S);it=c(ze,"To extract image features with this model, follow the "),Q=r(ze,"A",{href:!0});var Lt=o(Q);pt=c(Lt,"timm feature extraction examples"),Lt.forEach(t),ht=c(ze,", just change the name of the model you want to use."),ze.forEach(t),Se=m(e),b=r(e,"H2",{class:!0});var Be=o(b);C=r(Be,"A",{id:!0,class:!0,href:!0});var Xt=o(C);ie=r(Xt,"SPAN",{});var Mt=o(ie);u(B.$$.fragment,Mt),Mt.forEach(t),Xt.forEach(t),mt=m(Be),pe=r(Be,"SPAN",{});var zt=o(pe);ct=c(zt,"How do I finetune this model?"),zt.forEach(t),Be.forEach(t),Ce=m(e),Z=r(e,"P",{});var Bt=o(Z);ft=c(Bt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Bt.forEach(t),Ae=m(e),u(G.$$.fragment,e),Te=m(e),A=r(e,"P",{});var Ge=o(A);ut=c(Ge,"To finetune on your own dataset, you have to write a training loop or adapt "),W=r(Ge,"A",{href:!0,rel:!0});var Gt=o(W);gt=c(Gt,`timm\u2019s training
script`),Gt.forEach(t),dt=c(Ge," to use your dataset."),Ge.forEach(t),Ie=m(e),k=r(e,"H2",{class:!0});var We=o(k);T=r(We,"A",{id:!0,class:!0,href:!0});var Wt=o(T);he=r(Wt,"SPAN",{});var Ut=o(he);u(U.$$.fragment,Ut),Ut.forEach(t),Wt.forEach(t),wt=m(We),me=r(We,"SPAN",{});var Dt=o(me);_t=c(Dt,"How do I train this model?"),Dt.forEach(t),We.forEach(t),He=m(e),I=r(e,"P",{});var Ue=o(I);vt=c(Ue,"You can follow the "),ee=r(Ue,"A",{href:!0});var Jt=o(ee);jt=c(Jt,"timm recipe scripts"),Jt.forEach(t),$t=c(Ue," for training a new model afresh."),Ue.forEach(t),qe=m(e),x=r(e,"H2",{class:!0});var De=o(x);H=r(De,"A",{id:!0,class:!0,href:!0});var Ot=o(H);ce=r(Ot,"SPAN",{});var Ft=o(ce);u(D.$$.fragment,Ft),Ft.forEach(t),Ot.forEach(t),yt=m(De),fe=r(De,"SPAN",{});var Vt=o(fe);bt=c(Vt,"Citation"),Vt.forEach(t),De.forEach(t),Ye=m(e),u(J.$$.fragment,e),this.h()},h(){i(v,"name","hf:doc:metadata"),i(v,"content",JSON.stringify(ns)),i(E,"id","cspresnext"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#cspresnext"),i(j,"class","relative group"),i(Y,"href","https://paperswithcode.com/method/resnext"),i(Y,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i(y,"class","relative group"),i(Q,"href","../feature_extraction"),i(C,"id","how-do-i-finetune-this-model"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#how-do-i-finetune-this-model"),i(b,"class","relative group"),i(W,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(W,"rel","nofollow"),i(T,"id","how-do-i-train-this-model"),i(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(T,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(ee,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,v),l(e,de,a),l(e,j,a),s(j,E),s(E,se),g(q,se,null),s(j,Je),s(j,ae),s(ae,Oe),l(e,we,a),l(e,$,a),s($,ne),s(ne,Fe),s($,Ve),s($,Y),s(Y,Ke),s($,Qe),l(e,_e,a),l(e,y,a),s(y,P),s(P,re),g(R,re,null),s(y,Ze),s(y,oe),s(oe,et),l(e,ve,a),l(e,O,a),s(O,tt),l(e,je,a),g(L,e,a),l(e,$e,a),l(e,F,a),s(F,st),l(e,ye,a),g(X,e,a),l(e,be,a),l(e,V,a),s(V,at),l(e,ke,a),g(M,e,a),l(e,xe,a),l(e,K,a),s(K,nt),l(e,Ee,a),g(z,e,a),l(e,Pe,a),l(e,N,a),s(N,rt),s(N,le),s(le,ot),s(N,lt),l(e,Ne,a),l(e,S,a),s(S,it),s(S,Q),s(Q,pt),s(S,ht),l(e,Se,a),l(e,b,a),s(b,C),s(C,ie),g(B,ie,null),s(b,mt),s(b,pe),s(pe,ct),l(e,Ce,a),l(e,Z,a),s(Z,ft),l(e,Ae,a),g(G,e,a),l(e,Te,a),l(e,A,a),s(A,ut),s(A,W),s(W,gt),s(A,dt),l(e,Ie,a),l(e,k,a),s(k,T),s(T,he),g(U,he,null),s(k,wt),s(k,me),s(me,_t),l(e,He,a),l(e,I,a),s(I,vt),s(I,ee),s(ee,jt),s(I,$t),l(e,qe,a),l(e,x,a),s(x,H),s(H,ce),g(D,ce,null),s(x,yt),s(x,fe),s(fe,bt),l(e,Ye,a),g(J,e,a),Re=!0},p:ts,i(e){Re||(d(q.$$.fragment,e),d(R.$$.fragment,e),d(L.$$.fragment,e),d(X.$$.fragment,e),d(M.$$.fragment,e),d(z.$$.fragment,e),d(B.$$.fragment,e),d(G.$$.fragment,e),d(U.$$.fragment,e),d(D.$$.fragment,e),d(J.$$.fragment,e),Re=!0)},o(e){w(q.$$.fragment,e),w(R.$$.fragment,e),w(L.$$.fragment,e),w(X.$$.fragment,e),w(M.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(U.$$.fragment,e),w(D.$$.fragment,e),w(J.$$.fragment,e),Re=!1},d(e){t(v),e&&t(de),e&&t(j),_(q),e&&t(we),e&&t($),e&&t(_e),e&&t(y),_(R),e&&t(ve),e&&t(O),e&&t(je),_(L,e),e&&t($e),e&&t(F),e&&t(ye),_(X,e),e&&t(be),e&&t(V),e&&t(ke),_(M,e),e&&t(xe),e&&t(K),e&&t(Ee),_(z,e),e&&t(Pe),e&&t(N),e&&t(Ne),e&&t(S),e&&t(Se),e&&t(b),_(B),e&&t(Ce),e&&t(Z),e&&t(Ae),_(G,e),e&&t(Te),e&&t(A),e&&t(Ie),e&&t(k),_(U),e&&t(He),e&&t(I),e&&t(qe),e&&t(x),_(D),e&&t(Ye),_(J,e)}}}const ns={local:"cspresnext",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"CSP-ResNeXt"};function rs(kt){return ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ps extends Kt{constructor(v){super();Qt(this,v,rs,as,Zt,{})}}export{ps as default,ns as metadata};
