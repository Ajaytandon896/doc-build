import{S as st,i as tt,s as at,e as r,k as m,w as u,t as p,M as rt,c as o,d as s,m as c,a as l,x as d,h,b as i,G as t,g as n,y as g,L as ot,q as w,o as _,B as b,v as lt}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function nt(Ps){let v,we,j,E,re,H,Oe,oe,Fe,_e,f,le,Xe,Je,C,Qe,Ve,L,es,ss,be,$,P,ne,W,ts,ie,as,ve,X,rs,je,M,$e,J,os,ye,z,ke,Q,ls,xe,B,Ee,V,ns,Pe,G,Ne,N,is,pe,ps,hs,Se,S,ms,ee,cs,fs,Ae,y,A,he,K,us,me,ds,Te,se,gs,qe,Y,Ie,T,ws,Z,_s,bs,Re,k,q,ce,D,vs,fe,js,He,I,$s,te,ys,ks,Ce,x,R,ue,U,xs,de,Es,Le,O,We;return H=new ge({}),W=new ge({}),M=new ae({props:{code:`import timm
model = timm.create_model('wide_resnet101_2', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;wide_resnet101_2&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),z=new ae({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),B=new ae({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),G=new ae({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),K=new ge({}),Y=new ae({props:{code:"model = timm.create_model('wide_resnet101_2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;wide_resnet101_2&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),D=new ge({}),U=new ge({}),O=new ae({props:{code:`@article{DBLP:journals/corr/ZagoruykoK16,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07146},
  archivePrefix = {arXiv},
  eprint    = {1605.07146},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`@article{DBLP:journals<span class="hljs-regexp">/corr/</span>ZagoruykoK16,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/<span class="hljs-number">1605.07146</span>},
  year      = {<span class="hljs-number">2016</span>},
  url       = {http:<span class="hljs-regexp">//</span>arxiv.org<span class="hljs-regexp">/abs/</span><span class="hljs-number">1605.07146</span>},
  archivePrefix = {arXiv},
  eprint    = {<span class="hljs-number">1605.07146</span>},
  timestamp = {Mon, <span class="hljs-number">13</span> Aug <span class="hljs-number">2018</span> <span class="hljs-number">16</span>:<span class="hljs-number">46</span>:<span class="hljs-number">42</span> +<span class="hljs-number">0200</span>},
  biburl    = {https:<span class="hljs-regexp">//</span>dblp.org<span class="hljs-regexp">/rec/</span>journals<span class="hljs-regexp">/corr/</span>ZagoruykoK16.bib},
  bibsource = {dblp computer science bibliography, https:<span class="hljs-regexp">//</span>dblp.org}
}`}}),{c(){v=r("meta"),we=m(),j=r("h1"),E=r("a"),re=r("span"),u(H.$$.fragment),Oe=m(),oe=r("span"),Fe=p("Wide ResNet"),_e=m(),f=r("p"),le=r("strong"),Xe=p("Wide Residual Networks"),Je=p(" are a variant on "),C=r("a"),Qe=p("ResNets"),Ve=p(" where we decrease depth and increase the width of residual networks. This is achieved through the use of "),L=r("a"),es=p("wide residual blocks"),ss=p("."),be=m(),$=r("h2"),P=r("a"),ne=r("span"),u(W.$$.fragment),ts=m(),ie=r("span"),as=p("How do I use this model on an image?"),ve=m(),X=r("p"),rs=p("To load a pretrained model:"),je=m(),u(M.$$.fragment),$e=m(),J=r("p"),os=p("To load and preprocess the image:"),ye=m(),u(z.$$.fragment),ke=m(),Q=r("p"),ls=p("To get the model predictions:"),xe=m(),u(B.$$.fragment),Ee=m(),V=r("p"),ns=p("To get the top-5 predictions class names:"),Pe=m(),u(G.$$.fragment),Ne=m(),N=r("p"),is=p("Replace the model name with the variant you want to use, e.g. "),pe=r("code"),ps=p("wide_resnet101_2"),hs=p(". You can find the IDs in the model summaries at the top of this page."),Se=m(),S=r("p"),ms=p("To extract image features with this model, follow the "),ee=r("a"),cs=p("timm feature extraction examples"),fs=p(", just change the name of the model you want to use."),Ae=m(),y=r("h2"),A=r("a"),he=r("span"),u(K.$$.fragment),us=m(),me=r("span"),ds=p("How do I finetune this model?"),Te=m(),se=r("p"),gs=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qe=m(),u(Y.$$.fragment),Ie=m(),T=r("p"),ws=p("To finetune on your own dataset, you have to write a training loop or adapt "),Z=r("a"),_s=p(`timm\u2019s training
script`),bs=p(" to use your dataset."),Re=m(),k=r("h2"),q=r("a"),ce=r("span"),u(D.$$.fragment),vs=m(),fe=r("span"),js=p("How do I train this model?"),He=m(),I=r("p"),$s=p("You can follow the "),te=r("a"),ys=p("timm recipe scripts"),ks=p(" for training a new model afresh."),Ce=m(),x=r("h2"),R=r("a"),ue=r("span"),u(U.$$.fragment),xs=m(),de=r("span"),Es=p("Citation"),Le=m(),u(O.$$.fragment),this.h()},l(e){const a=rt('[data-svelte="svelte-1phssyn"]',document.head);v=o(a,"META",{name:!0,content:!0}),a.forEach(s),we=c(e),j=o(e,"H1",{class:!0});var Me=l(j);E=o(Me,"A",{id:!0,class:!0,href:!0});var Ns=l(E);re=o(Ns,"SPAN",{});var Ss=l(re);d(H.$$.fragment,Ss),Ss.forEach(s),Ns.forEach(s),Oe=c(Me),oe=o(Me,"SPAN",{});var As=l(oe);Fe=h(As,"Wide ResNet"),As.forEach(s),Me.forEach(s),_e=c(e),f=o(e,"P",{});var F=l(f);le=o(F,"STRONG",{});var Ts=l(le);Xe=h(Ts,"Wide Residual Networks"),Ts.forEach(s),Je=h(F," are a variant on "),C=o(F,"A",{href:!0,rel:!0});var qs=l(C);Qe=h(qs,"ResNets"),qs.forEach(s),Ve=h(F," where we decrease depth and increase the width of residual networks. This is achieved through the use of "),L=o(F,"A",{href:!0,rel:!0});var Is=l(L);es=h(Is,"wide residual blocks"),Is.forEach(s),ss=h(F,"."),F.forEach(s),be=c(e),$=o(e,"H2",{class:!0});var ze=l($);P=o(ze,"A",{id:!0,class:!0,href:!0});var Rs=l(P);ne=o(Rs,"SPAN",{});var Hs=l(ne);d(W.$$.fragment,Hs),Hs.forEach(s),Rs.forEach(s),ts=c(ze),ie=o(ze,"SPAN",{});var Cs=l(ie);as=h(Cs,"How do I use this model on an image?"),Cs.forEach(s),ze.forEach(s),ve=c(e),X=o(e,"P",{});var Ls=l(X);rs=h(Ls,"To load a pretrained model:"),Ls.forEach(s),je=c(e),d(M.$$.fragment,e),$e=c(e),J=o(e,"P",{});var Ws=l(J);os=h(Ws,"To load and preprocess the image:"),Ws.forEach(s),ye=c(e),d(z.$$.fragment,e),ke=c(e),Q=o(e,"P",{});var Ms=l(Q);ls=h(Ms,"To get the model predictions:"),Ms.forEach(s),xe=c(e),d(B.$$.fragment,e),Ee=c(e),V=o(e,"P",{});var zs=l(V);ns=h(zs,"To get the top-5 predictions class names:"),zs.forEach(s),Pe=c(e),d(G.$$.fragment,e),Ne=c(e),N=o(e,"P",{});var Be=l(N);is=h(Be,"Replace the model name with the variant you want to use, e.g. "),pe=o(Be,"CODE",{});var Bs=l(pe);ps=h(Bs,"wide_resnet101_2"),Bs.forEach(s),hs=h(Be,". You can find the IDs in the model summaries at the top of this page."),Be.forEach(s),Se=c(e),S=o(e,"P",{});var Ge=l(S);ms=h(Ge,"To extract image features with this model, follow the "),ee=o(Ge,"A",{href:!0});var Gs=l(ee);cs=h(Gs,"timm feature extraction examples"),Gs.forEach(s),fs=h(Ge,", just change the name of the model you want to use."),Ge.forEach(s),Ae=c(e),y=o(e,"H2",{class:!0});var Ke=l(y);A=o(Ke,"A",{id:!0,class:!0,href:!0});var Ks=l(A);he=o(Ks,"SPAN",{});var Ys=l(he);d(K.$$.fragment,Ys),Ys.forEach(s),Ks.forEach(s),us=c(Ke),me=o(Ke,"SPAN",{});var Zs=l(me);ds=h(Zs,"How do I finetune this model?"),Zs.forEach(s),Ke.forEach(s),Te=c(e),se=o(e,"P",{});var Ds=l(se);gs=h(Ds,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ds.forEach(s),qe=c(e),d(Y.$$.fragment,e),Ie=c(e),T=o(e,"P",{});var Ye=l(T);ws=h(Ye,"To finetune on your own dataset, you have to write a training loop or adapt "),Z=o(Ye,"A",{href:!0,rel:!0});var Us=l(Z);_s=h(Us,`timm\u2019s training
script`),Us.forEach(s),bs=h(Ye," to use your dataset."),Ye.forEach(s),Re=c(e),k=o(e,"H2",{class:!0});var Ze=l(k);q=o(Ze,"A",{id:!0,class:!0,href:!0});var Os=l(q);ce=o(Os,"SPAN",{});var Fs=l(ce);d(D.$$.fragment,Fs),Fs.forEach(s),Os.forEach(s),vs=c(Ze),fe=o(Ze,"SPAN",{});var Xs=l(fe);js=h(Xs,"How do I train this model?"),Xs.forEach(s),Ze.forEach(s),He=c(e),I=o(e,"P",{});var De=l(I);$s=h(De,"You can follow the "),te=o(De,"A",{href:!0});var Js=l(te);ys=h(Js,"timm recipe scripts"),Js.forEach(s),ks=h(De," for training a new model afresh."),De.forEach(s),Ce=c(e),x=o(e,"H2",{class:!0});var Ue=l(x);R=o(Ue,"A",{id:!0,class:!0,href:!0});var Qs=l(R);ue=o(Qs,"SPAN",{});var Vs=l(ue);d(U.$$.fragment,Vs),Vs.forEach(s),Qs.forEach(s),xs=c(Ue),de=o(Ue,"SPAN",{});var et=l(de);Es=h(et,"Citation"),et.forEach(s),Ue.forEach(s),Le=c(e),d(O.$$.fragment,e),this.h()},h(){i(v,"name","hf:doc:metadata"),i(v,"content",JSON.stringify(it)),i(E,"id","wide-resnet"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#wide-resnet"),i(j,"class","relative group"),i(C,"href","https://paperswithcode.com/method/resnet"),i(C,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/wide-residual-block"),i(L,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(ee,"href","../feature_extraction"),i(A,"id","how-do-i-finetune-this-model"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(Z,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(Z,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(te,"href","../scripts"),i(R,"id","citation"),i(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(R,"href","#citation"),i(x,"class","relative group")},m(e,a){t(document.head,v),n(e,we,a),n(e,j,a),t(j,E),t(E,re),g(H,re,null),t(j,Oe),t(j,oe),t(oe,Fe),n(e,_e,a),n(e,f,a),t(f,le),t(le,Xe),t(f,Je),t(f,C),t(C,Qe),t(f,Ve),t(f,L),t(L,es),t(f,ss),n(e,be,a),n(e,$,a),t($,P),t(P,ne),g(W,ne,null),t($,ts),t($,ie),t(ie,as),n(e,ve,a),n(e,X,a),t(X,rs),n(e,je,a),g(M,e,a),n(e,$e,a),n(e,J,a),t(J,os),n(e,ye,a),g(z,e,a),n(e,ke,a),n(e,Q,a),t(Q,ls),n(e,xe,a),g(B,e,a),n(e,Ee,a),n(e,V,a),t(V,ns),n(e,Pe,a),g(G,e,a),n(e,Ne,a),n(e,N,a),t(N,is),t(N,pe),t(pe,ps),t(N,hs),n(e,Se,a),n(e,S,a),t(S,ms),t(S,ee),t(ee,cs),t(S,fs),n(e,Ae,a),n(e,y,a),t(y,A),t(A,he),g(K,he,null),t(y,us),t(y,me),t(me,ds),n(e,Te,a),n(e,se,a),t(se,gs),n(e,qe,a),g(Y,e,a),n(e,Ie,a),n(e,T,a),t(T,ws),t(T,Z),t(Z,_s),t(T,bs),n(e,Re,a),n(e,k,a),t(k,q),t(q,ce),g(D,ce,null),t(k,vs),t(k,fe),t(fe,js),n(e,He,a),n(e,I,a),t(I,$s),t(I,te),t(te,ys),t(I,ks),n(e,Ce,a),n(e,x,a),t(x,R),t(R,ue),g(U,ue,null),t(x,xs),t(x,de),t(de,Es),n(e,Le,a),g(O,e,a),We=!0},p:ot,i(e){We||(w(H.$$.fragment,e),w(W.$$.fragment,e),w(M.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(K.$$.fragment,e),w(Y.$$.fragment,e),w(D.$$.fragment,e),w(U.$$.fragment,e),w(O.$$.fragment,e),We=!0)},o(e){_(H.$$.fragment,e),_(W.$$.fragment,e),_(M.$$.fragment,e),_(z.$$.fragment,e),_(B.$$.fragment,e),_(G.$$.fragment,e),_(K.$$.fragment,e),_(Y.$$.fragment,e),_(D.$$.fragment,e),_(U.$$.fragment,e),_(O.$$.fragment,e),We=!1},d(e){s(v),e&&s(we),e&&s(j),b(H),e&&s(_e),e&&s(f),e&&s(be),e&&s($),b(W),e&&s(ve),e&&s(X),e&&s(je),b(M,e),e&&s($e),e&&s(J),e&&s(ye),b(z,e),e&&s(ke),e&&s(Q),e&&s(xe),b(B,e),e&&s(Ee),e&&s(V),e&&s(Pe),b(G,e),e&&s(Ne),e&&s(N),e&&s(Se),e&&s(S),e&&s(Ae),e&&s(y),b(K),e&&s(Te),e&&s(se),e&&s(qe),b(Y,e),e&&s(Ie),e&&s(T),e&&s(Re),e&&s(k),b(D),e&&s(He),e&&s(I),e&&s(Ce),e&&s(x),b(U),e&&s(Le),b(O,e)}}}const it={local:"wide-resnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Wide ResNet"};function pt(Ps){return lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ft extends st{constructor(v){super();tt(this,v,pt,nt,at,{})}}export{ft as default,it as metadata};
