<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;model-summaries&quot;,&quot;sections&quot;:[{&quot;local&quot;:&quot;big-transfer-resnetv2-bit&quot;,&quot;title&quot;:&quot;Big Transfer ResNetV2 (BiT)&quot;},{&quot;local&quot;:&quot;crossstage-partial-networks&quot;,&quot;title&quot;:&quot;Cross-Stage Partial Networks&quot;},{&quot;local&quot;:&quot;densenet&quot;,&quot;title&quot;:&quot;DenseNet&quot;},{&quot;local&quot;:&quot;dla&quot;,&quot;title&quot;:&quot;DLA&quot;},{&quot;local&quot;:&quot;dualpath-networks&quot;,&quot;title&quot;:&quot;Dual-Path Networks&quot;},{&quot;local&quot;:&quot;gpuefficient-networks&quot;,&quot;title&quot;:&quot;GPU-Efficient Networks&quot;},{&quot;local&quot;:&quot;hrnet&quot;,&quot;title&quot;:&quot;HRNet&quot;},{&quot;local&quot;:&quot;inceptionv3&quot;,&quot;title&quot;:&quot;Inception-V3&quot;},{&quot;local&quot;:&quot;inceptionv4&quot;,&quot;title&quot;:&quot;Inception-V4&quot;},{&quot;local&quot;:&quot;inceptionresnetv2&quot;,&quot;title&quot;:&quot;Inception-ResNet-V2&quot;},{&quot;local&quot;:&quot;nasneta&quot;,&quot;title&quot;:&quot;NASNet-A&quot;},{&quot;local&quot;:&quot;pnasnet5&quot;,&quot;title&quot;:&quot;PNasNet-5&quot;},{&quot;local&quot;:&quot;efficientnet&quot;,&quot;title&quot;:&quot;EfficientNet&quot;},{&quot;local&quot;:&quot;mobilenetv3&quot;,&quot;title&quot;:&quot;MobileNet-V3&quot;},{&quot;local&quot;:&quot;regnet&quot;,&quot;title&quot;:&quot;RegNet&quot;},{&quot;local&quot;:&quot;repvgg&quot;,&quot;title&quot;:&quot;RepVGG&quot;},{&quot;local&quot;:&quot;resnet-resnext&quot;,&quot;title&quot;:&quot;ResNet, ResNeXt&quot;},{&quot;local&quot;:&quot;res2net&quot;,&quot;title&quot;:&quot;Res2Net&quot;},{&quot;local&quot;:&quot;resnest&quot;,&quot;title&quot;:&quot;ResNeSt&quot;},{&quot;local&quot;:&quot;rexnet&quot;,&quot;title&quot;:&quot;ReXNet&quot;},{&quot;local&quot;:&quot;selectivekernel-networks&quot;,&quot;title&quot;:&quot;Selective-Kernel Networks&quot;},{&quot;local&quot;:&quot;selecsls&quot;,&quot;title&quot;:&quot;SelecSLS&quot;},{&quot;local&quot;:&quot;squeezeandexcitation-networks&quot;,&quot;title&quot;:&quot;Squeeze-and-Excitation Networks&quot;},{&quot;local&quot;:&quot;tresnet&quot;,&quot;title&quot;:&quot;TResNet&quot;},{&quot;local&quot;:&quot;vgg&quot;,&quot;title&quot;:&quot;VGG&quot;},{&quot;local&quot;:&quot;vision-transformer&quot;,&quot;title&quot;:&quot;Vision Transformer&quot;},{&quot;local&quot;:&quot;vovnet-v2-and-v1&quot;,&quot;title&quot;:&quot;VovNet V2 and V1&quot;},{&quot;local&quot;:&quot;xception&quot;,&quot;title&quot;:&quot;Xception&quot;},{&quot;local&quot;:&quot;xception-modified-aligned-gluon&quot;,&quot;title&quot;:&quot;Xception (Modified Aligned, Gluon)&quot;},{&quot;local&quot;:&quot;xception-modified-aligned-tf&quot;,&quot;title&quot;:&quot;Xception (Modified Aligned, TF)&quot;}],&quot;title&quot;:&quot;Model Summaries&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/pages/models.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/main/en/_app/chunks/IconCopyLink-hf-doc-builder.js"> 





<h1 class="relative group"><a id="model-summaries" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#model-summaries"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Model Summaries
	</span></h1>

<p>The model architectures included come from a wide variety of sources. Sources, including papers, original impl (“reference code”) that I rewrote / adapted, and PyTorch impl that I leveraged directly (“code”) are listed below.</p>
<p>Most included models have pretrained weights. The weights are either:</p>
<ol><li>from their original sources</li>
<li>ported by myself from their original impl in a different framework (e.g. Tensorflow models)</li>
<li>trained from scratch using the included training script</li></ol>
<p>The validation results for the pretrained weights are <a href="results">here</a></p>
<p>A more exciting view (with pretty pictures) of the models within <code>timm</code> can be found at <a href="https://paperswithcode.com/lib/timm" rel="nofollow">paperswithcode</a>.</p>
<h2 class="relative group"><a id="big-transfer-resnetv2-bit" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#big-transfer-resnetv2-bit"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Big Transfer ResNetV2 (BiT)
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnetv2.py" rel="nofollow">resnetv2.py</a></li>
<li>Paper: <code>Big Transfer (BiT): General Visual Representation Learning</code> - <a href="https://arxiv.org/abs/1912.11370" rel="nofollow">https://arxiv.org/abs/1912.11370</a></li>
<li>Reference code: <a href="https://github.com/google-research/big_transfer" rel="nofollow">https://github.com/google-research/big_transfer</a></li></ul>
<h2 class="relative group"><a id="crossstage-partial-networks" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#crossstage-partial-networks"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Cross-Stage Partial Networks
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cspnet.py" rel="nofollow">cspnet.py</a></li>
<li>Paper: <code>CSPNet: A New Backbone that can Enhance Learning Capability of CNN</code> - <a href="https://arxiv.org/abs/1911.11929" rel="nofollow">https://arxiv.org/abs/1911.11929</a></li>
<li>Reference impl: <a href="https://github.com/WongKinYiu/CrossStagePartialNetworks" rel="nofollow">https://github.com/WongKinYiu/CrossStagePartialNetworks</a></li></ul>
<h2 class="relative group"><a id="densenet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#densenet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>DenseNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/densenet.py" rel="nofollow">densenet.py</a></li>
<li>Paper: <code>Densely Connected Convolutional Networks</code> - <a href="https://arxiv.org/abs/1608.06993" rel="nofollow">https://arxiv.org/abs/1608.06993</a></li>
<li>Code: <a href="https://github.com/pytorch/vision/tree/master/torchvision/models" rel="nofollow">https://github.com/pytorch/vision/tree/master/torchvision/models</a></li></ul>
<h2 class="relative group"><a id="dla" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dla"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>DLA
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py" rel="nofollow">dla.py</a></li>
<li>Paper: <a href="https://arxiv.org/abs/1707.06484" rel="nofollow">https://arxiv.org/abs/1707.06484</a></li>
<li>Code: <a href="https://github.com/ucbdrive/dla" rel="nofollow">https://github.com/ucbdrive/dla</a></li></ul>
<h2 class="relative group"><a id="dualpath-networks" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dualpath-networks"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Dual-Path Networks
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dpn.py" rel="nofollow">dpn.py</a></li>
<li>Paper: <code>Dual Path Networks</code> - <a href="https://arxiv.org/abs/1707.01629" rel="nofollow">https://arxiv.org/abs/1707.01629</a></li>
<li>My PyTorch code: <a href="https://github.com/rwightman/pytorch-dpn-pretrained" rel="nofollow">https://github.com/rwightman/pytorch-dpn-pretrained</a></li>
<li>Reference code: <a href="https://github.com/cypw/DPNs" rel="nofollow">https://github.com/cypw/DPNs</a></li></ul>
<h2 class="relative group"><a id="gpuefficient-networks" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#gpuefficient-networks"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>GPU-Efficient Networks
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/byobnet.py" rel="nofollow">byobnet.py</a></li>
<li>Paper: <code>Neural Architecture Design for GPU-Efficient Networks</code> - <a href="https://arxiv.org/abs/2006.14090" rel="nofollow">https://arxiv.org/abs/2006.14090</a></li>
<li>Reference code: <a href="https://github.com/idstcv/GPU-Efficient-Networks" rel="nofollow">https://github.com/idstcv/GPU-Efficient-Networks</a></li></ul>
<h2 class="relative group"><a id="hrnet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#hrnet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>HRNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/hrnet.py" rel="nofollow">hrnet.py</a></li>
<li>Paper: <code>Deep High-Resolution Representation Learning for Visual Recognition</code> - <a href="https://arxiv.org/abs/1908.07919" rel="nofollow">https://arxiv.org/abs/1908.07919</a></li>
<li>Code: <a href="https://github.com/HRNet/HRNet-Image-Classification" rel="nofollow">https://github.com/HRNet/HRNet-Image-Classification</a></li></ul>
<h2 class="relative group"><a id="inceptionv3" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#inceptionv3"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Inception-V3
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v3.py" rel="nofollow">inception_v3.py</a></li>
<li>Paper: <code>Rethinking the Inception Architecture for Computer Vision</code> - <a href="https://arxiv.org/abs/1512.00567" rel="nofollow">https://arxiv.org/abs/1512.00567</a></li>
<li>Code: <a href="https://github.com/pytorch/vision/tree/master/torchvision/models" rel="nofollow">https://github.com/pytorch/vision/tree/master/torchvision/models</a></li></ul>
<h2 class="relative group"><a id="inceptionv4" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#inceptionv4"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Inception-V4
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v4.py" rel="nofollow">inception_v4.py</a></li>
<li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - <a href="https://arxiv.org/abs/1602.07261" rel="nofollow">https://arxiv.org/abs/1602.07261</a></li>
<li>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/slim/nets</a></li></ul>
<h2 class="relative group"><a id="inceptionresnetv2" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#inceptionresnetv2"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Inception-ResNet-V2
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_resnet_v2.py" rel="nofollow">inception_resnet_v2.py</a></li>
<li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - <a href="https://arxiv.org/abs/1602.07261" rel="nofollow">https://arxiv.org/abs/1602.07261</a></li>
<li>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/slim/nets</a></li></ul>
<h2 class="relative group"><a id="nasneta" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#nasneta"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>NASNet-A
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/nasnet.py" rel="nofollow">nasnet.py</a></li>
<li>Papers: <code>Learning Transferable Architectures for Scalable Image Recognition</code> - <a href="https://arxiv.org/abs/1707.07012" rel="nofollow">https://arxiv.org/abs/1707.07012</a></li>
<li>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</a></li></ul>
<h2 class="relative group"><a id="pnasnet5" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#pnasnet5"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>PNasNet-5
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/pnasnet.py" rel="nofollow">pnasnet.py</a></li>
<li>Papers: <code>Progressive Neural Architecture Search</code> - <a href="https://arxiv.org/abs/1712.00559" rel="nofollow">https://arxiv.org/abs/1712.00559</a></li>
<li>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</a></li></ul>
<h2 class="relative group"><a id="efficientnet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#efficientnet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>EfficientNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/efficientnet.py" rel="nofollow">efficientnet.py</a></li>
<li>Papers:<ul><li>EfficientNet NoisyStudent (B0-B7, L2) - <a href="https://arxiv.org/abs/1911.04252" rel="nofollow">https://arxiv.org/abs/1911.04252</a></li>
<li>EfficientNet AdvProp (B0-B8) - <a href="https://arxiv.org/abs/1911.09665" rel="nofollow">https://arxiv.org/abs/1911.09665</a></li>
<li>EfficientNet (B0-B7) - <a href="https://arxiv.org/abs/1905.11946" rel="nofollow">https://arxiv.org/abs/1905.11946</a></li>
<li>EfficientNet-EdgeTPU (S, M, L) - <a href="https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html" rel="nofollow">https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html</a></li>
<li>MixNet - <a href="https://arxiv.org/abs/1907.09595" rel="nofollow">https://arxiv.org/abs/1907.09595</a></li>
<li>MNASNet B1, A1 (Squeeze-Excite), and Small - <a href="https://arxiv.org/abs/1807.11626" rel="nofollow">https://arxiv.org/abs/1807.11626</a></li>
<li>MobileNet-V2 - <a href="https://arxiv.org/abs/1801.04381" rel="nofollow">https://arxiv.org/abs/1801.04381</a></li>
<li>FBNet-C - <a href="https://arxiv.org/abs/1812.03443" rel="nofollow">https://arxiv.org/abs/1812.03443</a></li>
<li>Single-Path NAS - <a href="https://arxiv.org/abs/1904.02877" rel="nofollow">https://arxiv.org/abs/1904.02877</a></li></ul></li>
<li>My PyTorch code: <a href="https://github.com/rwightman/gen-efficientnet-pytorch" rel="nofollow">https://github.com/rwightman/gen-efficientnet-pytorch</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" rel="nofollow">https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</a></li></ul>
<h2 class="relative group"><a id="mobilenetv3" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#mobilenetv3"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>MobileNet-V3
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mobilenetv3.py" rel="nofollow">mobilenetv3.py</a></li>
<li>Paper: <code>Searching for MobileNetV3</code> - <a href="https://arxiv.org/abs/1905.02244" rel="nofollow">https://arxiv.org/abs/1905.02244</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</a></li></ul>
<h2 class="relative group"><a id="regnet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#regnet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>RegNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/regnet.py" rel="nofollow">regnet.py</a></li>
<li>Paper: <code>Designing Network Design Spaces</code> - <a href="https://arxiv.org/abs/2003.13678" rel="nofollow">https://arxiv.org/abs/2003.13678</a></li>
<li>Reference code: <a href="https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py" rel="nofollow">https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py</a></li></ul>
<h2 class="relative group"><a id="repvgg" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#repvgg"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>RepVGG
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/byobnet.py" rel="nofollow">byobnet.py</a></li>
<li>Paper: <code>Making VGG-style ConvNets Great Again</code> - <a href="https://arxiv.org/abs/2101.03697" rel="nofollow">https://arxiv.org/abs/2101.03697</a></li>
<li>Reference code: <a href="https://github.com/DingXiaoH/RepVGG" rel="nofollow">https://github.com/DingXiaoH/RepVGG</a></li></ul>
<h2 class="relative group"><a id="resnet-resnext" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#resnet-resnext"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>ResNet, ResNeXt
	</span></h2>

<ul><li><p>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnet.py" rel="nofollow">resnet.py</a></p></li>
<li><p>ResNet (V1B)</p>
<ul><li>Paper: <code>Deep Residual Learning for Image Recognition</code> - <a href="https://arxiv.org/abs/1512.03385" rel="nofollow">https://arxiv.org/abs/1512.03385</a></li>
<li>Code: <a href="https://github.com/pytorch/vision/tree/master/torchvision/models" rel="nofollow">https://github.com/pytorch/vision/tree/master/torchvision/models</a></li></ul></li>
<li><p>ResNeXt</p>
<ul><li>Paper: <code>Aggregated Residual Transformations for Deep Neural Networks</code> - <a href="https://arxiv.org/abs/1611.05431" rel="nofollow">https://arxiv.org/abs/1611.05431</a></li>
<li>Code: <a href="https://github.com/pytorch/vision/tree/master/torchvision/models" rel="nofollow">https://github.com/pytorch/vision/tree/master/torchvision/models</a></li></ul></li>
<li><p>‘Bag of Tricks’ / Gluon C, D, E, S ResNet variants</p>
<ul><li>Paper: <code>Bag of Tricks for Image Classification with CNNs</code> - <a href="https://arxiv.org/abs/1812.01187" rel="nofollow">https://arxiv.org/abs/1812.01187</a></li>
<li>Code: <a href="https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py" rel="nofollow">https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py</a></li></ul></li>
<li><p>Instagram pretrained / ImageNet tuned ResNeXt101</p>
<ul><li>Paper: <code>Exploring the Limits of Weakly Supervised Pretraining</code> - <a href="https://arxiv.org/abs/1805.00932" rel="nofollow">https://arxiv.org/abs/1805.00932</a></li>
<li>Weights: <a href="https://pytorch.org/hub/facebookresearch_WSL-Images_resnext" rel="nofollow">https://pytorch.org/hub/facebookresearch_WSL-Images_resnext</a> (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li></ul></li>
<li><p>Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts</p>
<ul><li>Paper: <code>Billion-scale semi-supervised learning for image classification</code> - <a href="https://arxiv.org/abs/1905.00546" rel="nofollow">https://arxiv.org/abs/1905.00546</a></li>
<li>Weights: <a href="https://github.com/facebookresearch/semi-supervised-ImageNet1K-models" rel="nofollow">https://github.com/facebookresearch/semi-supervised-ImageNet1K-models</a> (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li></ul></li>
<li><p>Squeeze-and-Excitation Networks</p>
<ul><li>Paper: <code>Squeeze-and-Excitation Networks</code> - <a href="https://arxiv.org/abs/1709.01507" rel="nofollow">https://arxiv.org/abs/1709.01507</a></li>
<li>Code: Added to ResNet base, this is current version going forward, old <code>senet.py</code> is being deprecated</li></ul></li>
<li><p>ECAResNet (ECA-Net)</p>
<ul><li>Paper: <code>ECA-Net: Efficient Channel Attention for Deep CNN</code> - <a href="https://arxiv.org/abs/1910.03151v4" rel="nofollow">https://arxiv.org/abs/1910.03151v4</a></li>
<li>Code: Added to ResNet base, ECA module contributed by @VRandme, reference <a href="https://github.com/BangguWu/ECANet" rel="nofollow">https://github.com/BangguWu/ECANet</a></li></ul></li></ul>
<h2 class="relative group"><a id="res2net" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#res2net"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Res2Net
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/res2net.py" rel="nofollow">res2net.py</a></li>
<li>Paper: <code>Res2Net: A New Multi-scale Backbone Architecture</code> - <a href="https://arxiv.org/abs/1904.01169" rel="nofollow">https://arxiv.org/abs/1904.01169</a></li>
<li>Code: <a href="https://github.com/gasvn/Res2Net" rel="nofollow">https://github.com/gasvn/Res2Net</a></li></ul>
<h2 class="relative group"><a id="resnest" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#resnest"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>ResNeSt
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnest.py" rel="nofollow">resnest.py</a></li>
<li>Paper: <code>ResNeSt: Split-Attention Networks</code> - <a href="https://arxiv.org/abs/2004.08955" rel="nofollow">https://arxiv.org/abs/2004.08955</a></li>
<li>Code: <a href="https://github.com/zhanghang1989/ResNeSt" rel="nofollow">https://github.com/zhanghang1989/ResNeSt</a></li></ul>
<h2 class="relative group"><a id="rexnet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#rexnet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>ReXNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/rexnet.py" rel="nofollow">rexnet.py</a></li>
<li>Paper: <code>ReXNet: Diminishing Representational Bottleneck on CNN</code> - <a href="https://arxiv.org/abs/2007.00992" rel="nofollow">https://arxiv.org/abs/2007.00992</a></li>
<li>Code: <a href="https://github.com/clovaai/rexnet" rel="nofollow">https://github.com/clovaai/rexnet</a></li></ul>
<h2 class="relative group"><a id="selectivekernel-networks" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#selectivekernel-networks"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Selective-Kernel Networks
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/sknet.py" rel="nofollow">sknet.py</a></li>
<li>Paper: <code>Selective-Kernel Networks</code> - <a href="https://arxiv.org/abs/1903.06586" rel="nofollow">https://arxiv.org/abs/1903.06586</a></li>
<li>Code: <a href="https://github.com/implus/SKNet" rel="nofollow">https://github.com/implus/SKNet</a>, <a href="https://github.com/clovaai/assembled-cnn" rel="nofollow">https://github.com/clovaai/assembled-cnn</a></li></ul>
<h2 class="relative group"><a id="selecsls" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#selecsls"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>SelecSLS
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/selecsls.py" rel="nofollow">selecsls.py</a></li>
<li>Paper: <code>XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</code> - <a href="https://arxiv.org/abs/1907.00837" rel="nofollow">https://arxiv.org/abs/1907.00837</a></li>
<li>Code: <a href="https://github.com/mehtadushy/SelecSLS-Pytorch" rel="nofollow">https://github.com/mehtadushy/SelecSLS-Pytorch</a></li></ul>
<h2 class="relative group"><a id="squeezeandexcitation-networks" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#squeezeandexcitation-networks"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Squeeze-and-Excitation Networks
	</span></h2>

<ul><li><p>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/senet.py" rel="nofollow">senet.py</a>
NOTE: I am deprecating this version of the networks, the new ones are part of <code>resnet.py</code></p></li>
<li><p>Paper: <code>Squeeze-and-Excitation Networks</code> - <a href="https://arxiv.org/abs/1709.01507" rel="nofollow">https://arxiv.org/abs/1709.01507</a></p></li>
<li><p>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></p></li></ul>
<h2 class="relative group"><a id="tresnet" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#tresnet"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>TResNet
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tresnet.py" rel="nofollow">tresnet.py</a></li>
<li>Paper: <code>TResNet: High Performance GPU-Dedicated Architecture</code> - <a href="https://arxiv.org/abs/2003.13630" rel="nofollow">https://arxiv.org/abs/2003.13630</a></li>
<li>Code: <a href="https://github.com/mrT23/TResNet" rel="nofollow">https://github.com/mrT23/TResNet</a></li></ul>
<h2 class="relative group"><a id="vgg" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#vgg"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>VGG
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py" rel="nofollow">vgg.py</a></li>
<li>Paper: <code>Very Deep Convolutional Networks For Large-Scale Image Recognition</code> - <a href="https://arxiv.org/pdf/1409.1556.pdf" rel="nofollow">https://arxiv.org/pdf/1409.1556.pdf</a></li>
<li>Reference code: <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py" rel="nofollow">https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py</a></li></ul>
<h2 class="relative group"><a id="vision-transformer" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#vision-transformer"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Vision Transformer
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py" rel="nofollow">vision_transformer.py</a></li>
<li>Paper: <code>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</code> - <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">https://arxiv.org/abs/2010.11929</a></li>
<li>Reference code and pretrained weights: <a href="https://github.com/google-research/vision_transformer" rel="nofollow">https://github.com/google-research/vision_transformer</a></li></ul>
<h2 class="relative group"><a id="vovnet-v2-and-v1" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#vovnet-v2-and-v1"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>VovNet V2 and V1
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vovnet.py" rel="nofollow">vovnet.py</a></li>
<li>Paper: <code>CenterMask : Real-Time Anchor-Free Instance Segmentation</code> - <a href="https://arxiv.org/abs/1911.06667" rel="nofollow">https://arxiv.org/abs/1911.06667</a></li>
<li>Reference code: <a href="https://github.com/youngwanLEE/vovnet-detectron2" rel="nofollow">https://github.com/youngwanLEE/vovnet-detectron2</a></li></ul>
<h2 class="relative group"><a id="xception" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#xception"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Xception
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/xception.py" rel="nofollow">xception.py</a></li>
<li>Paper: <code>Xception: Deep Learning with Depthwise Separable Convolutions</code> - <a href="https://arxiv.org/abs/1610.02357" rel="nofollow">https://arxiv.org/abs/1610.02357</a></li>
<li>Code: <a href="https://github.com/Cadene/pretrained-models.pytorch" rel="nofollow">https://github.com/Cadene/pretrained-models.pytorch</a></li></ul>
<h2 class="relative group"><a id="xception-modified-aligned-gluon" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#xception-modified-aligned-gluon"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Xception (Modified Aligned, Gluon)
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/gluon_xception.py" rel="nofollow">gluon_xception.py</a></li>
<li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - <a href="https://arxiv.org/abs/1802.02611" rel="nofollow">https://arxiv.org/abs/1802.02611</a></li>
<li>Reference code: <a href="https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo" rel="nofollow">https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo</a>, <a href="https://github.com/jfzhang95/pytorch-deeplab-xception/" rel="nofollow">https://github.com/jfzhang95/pytorch-deeplab-xception/</a></li></ul>
<h2 class="relative group"><a id="xception-modified-aligned-tf" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#xception-modified-aligned-tf"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Xception (Modified Aligned, TF)
	</span></h2>

<ul><li>Implementation: <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/aligned_xception.py" rel="nofollow">aligned_xception.py</a></li>
<li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - <a href="https://arxiv.org/abs/1802.02611" rel="nofollow">https://arxiv.org/abs/1802.02611</a></li>
<li>Reference code: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="nofollow">https://github.com/tensorflow/models/tree/master/research/deeplab</a></li></ul>


		<script type="module" data-hydrate="fqfarx">
		import { start } from "/docs/timm/main/en/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="fqfarx"]').parentNode,
			paths: {"base":"/docs/timm/main/en","assets":"/docs/timm/main/en"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/timm/main/en/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/timm/main/en/_app/pages/models.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
