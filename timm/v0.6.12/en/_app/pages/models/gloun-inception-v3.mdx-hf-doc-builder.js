import{S as fs,i as us,s as gs,e as n,k as m,w as u,t as p,M as ds,c as o,d as t,m as c,a as l,x as g,h,b as i,G as s,g as r,y as d,L as vs,q as v,o as w,B as _,v as ws}from"../../chunks/vendor-hf-doc-builder.js";import{I as _e}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as le}from"../../chunks/CodeBlock-hf-doc-builder.js";function _s(Gt){let b,be,j,S,re,G,Qe,ie,et,je,f,pe,tt,st,R,at,nt,L,ot,lt,V,rt,it,$e,I,pt,M,ht,mt,ye,$,P,he,B,ct,me,ft,ke,Q,ut,xe,W,Ee,ee,gt,Se,Y,Ie,te,dt,Pe,D,Ae,se,vt,Te,U,qe,A,wt,ce,_t,bt,Ne,T,jt,ae,$t,yt,ze,y,q,fe,F,kt,ue,xt,Ce,ne,Et,He,J,Ge,N,St,O,It,Pt,Re,k,z,ge,X,At,de,Tt,Le,C,qt,oe,Nt,zt,Ve,x,H,ve,Z,Ct,we,Ht,Me,K,Be;return G=new _e({}),B=new _e({}),W=new le({props:{code:`import timm
model = timm.create_model('gluon_inception_v3', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_inception_v3&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new le({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),D=new le({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),U=new le({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),F=new _e({}),J=new le({props:{code:"model = timm.create_model('gluon_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_inception_v3&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),X=new _e({}),Z=new _e({}),K=new le({props:{code:`@article{DBLP:journals/corr/SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1512.00567},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00567},
  archivePrefix = {arXiv},
  eprint    = {1512.00567},
  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`@article{DBLP:journals<span class="hljs-regexp">/corr/</span>SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture <span class="hljs-keyword">for</span> Computer Vision},
  journal   = {CoRR},
  volume    = {abs/<span class="hljs-number">1512.00567</span>},
  year      = {<span class="hljs-number">2015</span>},
  url       = {http:<span class="hljs-regexp">//</span>arxiv.org<span class="hljs-regexp">/abs/</span><span class="hljs-number">1512.00567</span>},
  archivePrefix = {arXiv},
  eprint    = {<span class="hljs-number">1512.00567</span>},
  timestamp = {Mon, <span class="hljs-number">13</span> Aug <span class="hljs-number">2018</span> <span class="hljs-number">16</span>:<span class="hljs-number">49</span>:<span class="hljs-number">07</span> +<span class="hljs-number">0200</span>},
  biburl    = {https:<span class="hljs-regexp">//</span>dblp.org<span class="hljs-regexp">/rec/</span>journals<span class="hljs-regexp">/corr/</span>SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https:<span class="hljs-regexp">//</span>dblp.org}
}`}}),{c(){b=n("meta"),be=m(),j=n("h1"),S=n("a"),re=n("span"),u(G.$$.fragment),Qe=m(),ie=n("span"),et=p("(Gluon) Inception v3"),je=m(),f=n("p"),pe=n("strong"),tt=p("Inception v3"),st=p(" is a convolutional neural network architecture from the Inception family that makes several improvements including using "),R=n("a"),at=p("Label Smoothing"),nt=p(", Factorized 7 x 7 convolutions, and the use of an "),L=n("a"),ot=p("auxiliary classifer"),lt=p(" to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an "),V=n("a"),rt=p("Inception Module"),it=p("."),$e=m(),I=n("p"),pt=p("The weights from this model were ported from "),M=n("a"),ht=p("Gluon"),mt=p("."),ye=m(),$=n("h2"),P=n("a"),he=n("span"),u(B.$$.fragment),ct=m(),me=n("span"),ft=p("How do I use this model on an image?"),ke=m(),Q=n("p"),ut=p("To load a pretrained model:"),xe=m(),u(W.$$.fragment),Ee=m(),ee=n("p"),gt=p("To load and preprocess the image:"),Se=m(),u(Y.$$.fragment),Ie=m(),te=n("p"),dt=p("To get the model predictions:"),Pe=m(),u(D.$$.fragment),Ae=m(),se=n("p"),vt=p("To get the top-5 predictions class names:"),Te=m(),u(U.$$.fragment),qe=m(),A=n("p"),wt=p("Replace the model name with the variant you want to use, e.g. "),ce=n("code"),_t=p("gluon_inception_v3"),bt=p(". You can find the IDs in the model summaries at the top of this page."),Ne=m(),T=n("p"),jt=p("To extract image features with this model, follow the "),ae=n("a"),$t=p("timm feature extraction examples"),yt=p(", just change the name of the model you want to use."),ze=m(),y=n("h2"),q=n("a"),fe=n("span"),u(F.$$.fragment),kt=m(),ue=n("span"),xt=p("How do I finetune this model?"),Ce=m(),ne=n("p"),Et=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),He=m(),u(J.$$.fragment),Ge=m(),N=n("p"),St=p("To finetune on your own dataset, you have to write a training loop or adapt "),O=n("a"),It=p(`timm\u2019s training
script`),Pt=p(" to use your dataset."),Re=m(),k=n("h2"),z=n("a"),ge=n("span"),u(X.$$.fragment),At=m(),de=n("span"),Tt=p("How do I train this model?"),Le=m(),C=n("p"),qt=p("You can follow the "),oe=n("a"),Nt=p("timm recipe scripts"),zt=p(" for training a new model afresh."),Ve=m(),x=n("h2"),H=n("a"),ve=n("span"),u(Z.$$.fragment),Ct=m(),we=n("span"),Ht=p("Citation"),Me=m(),u(K.$$.fragment),this.h()},l(e){const a=ds('[data-svelte="svelte-1phssyn"]',document.head);b=o(a,"META",{name:!0,content:!0}),a.forEach(t),be=c(e),j=o(e,"H1",{class:!0});var We=l(j);S=o(We,"A",{id:!0,class:!0,href:!0});var Rt=l(S);re=o(Rt,"SPAN",{});var Lt=l(re);g(G.$$.fragment,Lt),Lt.forEach(t),Rt.forEach(t),Qe=c(We),ie=o(We,"SPAN",{});var Vt=l(ie);et=h(Vt,"(Gluon) Inception v3"),Vt.forEach(t),We.forEach(t),je=c(e),f=o(e,"P",{});var E=l(f);pe=o(E,"STRONG",{});var Mt=l(pe);tt=h(Mt,"Inception v3"),Mt.forEach(t),st=h(E," is a convolutional neural network architecture from the Inception family that makes several improvements including using "),R=o(E,"A",{href:!0,rel:!0});var Bt=l(R);at=h(Bt,"Label Smoothing"),Bt.forEach(t),nt=h(E,", Factorized 7 x 7 convolutions, and the use of an "),L=o(E,"A",{href:!0,rel:!0});var Wt=l(L);ot=h(Wt,"auxiliary classifer"),Wt.forEach(t),lt=h(E," to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an "),V=o(E,"A",{href:!0,rel:!0});var Yt=l(V);rt=h(Yt,"Inception Module"),Yt.forEach(t),it=h(E,"."),E.forEach(t),$e=c(e),I=o(e,"P",{});var Ye=l(I);pt=h(Ye,"The weights from this model were ported from "),M=o(Ye,"A",{href:!0,rel:!0});var Dt=l(M);ht=h(Dt,"Gluon"),Dt.forEach(t),mt=h(Ye,"."),Ye.forEach(t),ye=c(e),$=o(e,"H2",{class:!0});var De=l($);P=o(De,"A",{id:!0,class:!0,href:!0});var Ut=l(P);he=o(Ut,"SPAN",{});var Ft=l(he);g(B.$$.fragment,Ft),Ft.forEach(t),Ut.forEach(t),ct=c(De),me=o(De,"SPAN",{});var Jt=l(me);ft=h(Jt,"How do I use this model on an image?"),Jt.forEach(t),De.forEach(t),ke=c(e),Q=o(e,"P",{});var Ot=l(Q);ut=h(Ot,"To load a pretrained model:"),Ot.forEach(t),xe=c(e),g(W.$$.fragment,e),Ee=c(e),ee=o(e,"P",{});var Xt=l(ee);gt=h(Xt,"To load and preprocess the image:"),Xt.forEach(t),Se=c(e),g(Y.$$.fragment,e),Ie=c(e),te=o(e,"P",{});var Zt=l(te);dt=h(Zt,"To get the model predictions:"),Zt.forEach(t),Pe=c(e),g(D.$$.fragment,e),Ae=c(e),se=o(e,"P",{});var Kt=l(se);vt=h(Kt,"To get the top-5 predictions class names:"),Kt.forEach(t),Te=c(e),g(U.$$.fragment,e),qe=c(e),A=o(e,"P",{});var Ue=l(A);wt=h(Ue,"Replace the model name with the variant you want to use, e.g. "),ce=o(Ue,"CODE",{});var Qt=l(ce);_t=h(Qt,"gluon_inception_v3"),Qt.forEach(t),bt=h(Ue,". You can find the IDs in the model summaries at the top of this page."),Ue.forEach(t),Ne=c(e),T=o(e,"P",{});var Fe=l(T);jt=h(Fe,"To extract image features with this model, follow the "),ae=o(Fe,"A",{href:!0});var es=l(ae);$t=h(es,"timm feature extraction examples"),es.forEach(t),yt=h(Fe,", just change the name of the model you want to use."),Fe.forEach(t),ze=c(e),y=o(e,"H2",{class:!0});var Je=l(y);q=o(Je,"A",{id:!0,class:!0,href:!0});var ts=l(q);fe=o(ts,"SPAN",{});var ss=l(fe);g(F.$$.fragment,ss),ss.forEach(t),ts.forEach(t),kt=c(Je),ue=o(Je,"SPAN",{});var as=l(ue);xt=h(as,"How do I finetune this model?"),as.forEach(t),Je.forEach(t),Ce=c(e),ne=o(e,"P",{});var ns=l(ne);Et=h(ns,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),ns.forEach(t),He=c(e),g(J.$$.fragment,e),Ge=c(e),N=o(e,"P",{});var Oe=l(N);St=h(Oe,"To finetune on your own dataset, you have to write a training loop or adapt "),O=o(Oe,"A",{href:!0,rel:!0});var os=l(O);It=h(os,`timm\u2019s training
script`),os.forEach(t),Pt=h(Oe," to use your dataset."),Oe.forEach(t),Re=c(e),k=o(e,"H2",{class:!0});var Xe=l(k);z=o(Xe,"A",{id:!0,class:!0,href:!0});var ls=l(z);ge=o(ls,"SPAN",{});var rs=l(ge);g(X.$$.fragment,rs),rs.forEach(t),ls.forEach(t),At=c(Xe),de=o(Xe,"SPAN",{});var is=l(de);Tt=h(is,"How do I train this model?"),is.forEach(t),Xe.forEach(t),Le=c(e),C=o(e,"P",{});var Ze=l(C);qt=h(Ze,"You can follow the "),oe=o(Ze,"A",{href:!0});var ps=l(oe);Nt=h(ps,"timm recipe scripts"),ps.forEach(t),zt=h(Ze," for training a new model afresh."),Ze.forEach(t),Ve=c(e),x=o(e,"H2",{class:!0});var Ke=l(x);H=o(Ke,"A",{id:!0,class:!0,href:!0});var hs=l(H);ve=o(hs,"SPAN",{});var ms=l(ve);g(Z.$$.fragment,ms),ms.forEach(t),hs.forEach(t),Ct=c(Ke),we=o(Ke,"SPAN",{});var cs=l(we);Ht=h(cs,"Citation"),cs.forEach(t),Ke.forEach(t),Me=c(e),g(K.$$.fragment,e),this.h()},h(){i(b,"name","hf:doc:metadata"),i(b,"content",JSON.stringify(bs)),i(S,"id","gluon-inception-v3"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#gluon-inception-v3"),i(j,"class","relative group"),i(R,"href","https://paperswithcode.com/method/label-smoothing"),i(R,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/auxiliary-classifier"),i(L,"rel","nofollow"),i(V,"href","https://paperswithcode.com/method/inception-v3-module"),i(V,"rel","nofollow"),i(M,"href","https://cv.gluon.ai/model_zoo/classification.html"),i(M,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(ae,"href","../feature_extraction"),i(q,"id","how-do-i-finetune-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(O,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(O,"rel","nofollow"),i(z,"id","how-do-i-train-this-model"),i(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(z,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(oe,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,b),r(e,be,a),r(e,j,a),s(j,S),s(S,re),d(G,re,null),s(j,Qe),s(j,ie),s(ie,et),r(e,je,a),r(e,f,a),s(f,pe),s(pe,tt),s(f,st),s(f,R),s(R,at),s(f,nt),s(f,L),s(L,ot),s(f,lt),s(f,V),s(V,rt),s(f,it),r(e,$e,a),r(e,I,a),s(I,pt),s(I,M),s(M,ht),s(I,mt),r(e,ye,a),r(e,$,a),s($,P),s(P,he),d(B,he,null),s($,ct),s($,me),s(me,ft),r(e,ke,a),r(e,Q,a),s(Q,ut),r(e,xe,a),d(W,e,a),r(e,Ee,a),r(e,ee,a),s(ee,gt),r(e,Se,a),d(Y,e,a),r(e,Ie,a),r(e,te,a),s(te,dt),r(e,Pe,a),d(D,e,a),r(e,Ae,a),r(e,se,a),s(se,vt),r(e,Te,a),d(U,e,a),r(e,qe,a),r(e,A,a),s(A,wt),s(A,ce),s(ce,_t),s(A,bt),r(e,Ne,a),r(e,T,a),s(T,jt),s(T,ae),s(ae,$t),s(T,yt),r(e,ze,a),r(e,y,a),s(y,q),s(q,fe),d(F,fe,null),s(y,kt),s(y,ue),s(ue,xt),r(e,Ce,a),r(e,ne,a),s(ne,Et),r(e,He,a),d(J,e,a),r(e,Ge,a),r(e,N,a),s(N,St),s(N,O),s(O,It),s(N,Pt),r(e,Re,a),r(e,k,a),s(k,z),s(z,ge),d(X,ge,null),s(k,At),s(k,de),s(de,Tt),r(e,Le,a),r(e,C,a),s(C,qt),s(C,oe),s(oe,Nt),s(C,zt),r(e,Ve,a),r(e,x,a),s(x,H),s(H,ve),d(Z,ve,null),s(x,Ct),s(x,we),s(we,Ht),r(e,Me,a),d(K,e,a),Be=!0},p:vs,i(e){Be||(v(G.$$.fragment,e),v(B.$$.fragment,e),v(W.$$.fragment,e),v(Y.$$.fragment,e),v(D.$$.fragment,e),v(U.$$.fragment,e),v(F.$$.fragment,e),v(J.$$.fragment,e),v(X.$$.fragment,e),v(Z.$$.fragment,e),v(K.$$.fragment,e),Be=!0)},o(e){w(G.$$.fragment,e),w(B.$$.fragment,e),w(W.$$.fragment,e),w(Y.$$.fragment,e),w(D.$$.fragment,e),w(U.$$.fragment,e),w(F.$$.fragment,e),w(J.$$.fragment,e),w(X.$$.fragment,e),w(Z.$$.fragment,e),w(K.$$.fragment,e),Be=!1},d(e){t(b),e&&t(be),e&&t(j),_(G),e&&t(je),e&&t(f),e&&t($e),e&&t(I),e&&t(ye),e&&t($),_(B),e&&t(ke),e&&t(Q),e&&t(xe),_(W,e),e&&t(Ee),e&&t(ee),e&&t(Se),_(Y,e),e&&t(Ie),e&&t(te),e&&t(Pe),_(D,e),e&&t(Ae),e&&t(se),e&&t(Te),_(U,e),e&&t(qe),e&&t(A),e&&t(Ne),e&&t(T),e&&t(ze),e&&t(y),_(F),e&&t(Ce),e&&t(ne),e&&t(He),_(J,e),e&&t(Ge),e&&t(N),e&&t(Re),e&&t(k),_(X),e&&t(Le),e&&t(C),e&&t(Ve),e&&t(x),_(Z),e&&t(Me),_(K,e)}}}const bs={local:"gluon-inception-v3",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"(Gluon) Inception v3"};function js(Gt){return ws(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xs extends fs{constructor(b){super();us(this,b,js,_s,gs,{})}}export{xs as default,bs as metadata};
