import{S as Ve,i as Ze,s as Qe,e as n,k as i,w as u,t as p,M as st,c as o,d as e,m as h,a as r,x as f,h as m,b as c,G as t,g as l,y as g,L as et,q as d,o as w,B as v,v as tt}from"../../chunks/vendor-hf-doc-builder.js";import{I as gs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ss}from"../../chunks/CodeBlock-hf-doc-builder.js";function at(ke){let _,ds,$,E,es,R,Us,ts,Os,ws,j,as,Fs,Ks,ns,Vs,Zs,vs,y,P,os,C,Qs,rs,se,_s,U,ee,$s,L,js,O,te,ys,Y,bs,F,ae,ks,B,xs,K,ne,Es,W,Ps,S,oe,ls,re,le,Ss,T,ie,V,pe,he,Ts,b,N,is,G,me,ps,ce,Ns,Z,ue,Hs,M,Is,H,fe,z,ge,de,qs,k,I,hs,X,we,ms,ve,As,q,_e,Q,$e,je,Rs,x,A,cs,D,ye,us,be,Cs,J,Ls;return R=new gs({}),C=new gs({}),L=new ss({props:{code:`import timm
model = timm.create_model('hrnet_w18', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;hrnet_w18&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new ss({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),B=new ss({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),W=new ss({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),G=new gs({}),M=new ss({props:{code:"model = timm.create_model('hrnet_w18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;hrnet_w18&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),X=new gs({}),D=new gs({}),J=new ss({props:{code:`@misc{sun2019highresolution,
      title={High-Resolution Representations for Labeling Pixels and Regions}, 
      author={Ke Sun and Yang Zhao and Borui Jiang and Tianheng Cheng and Bin Xiao and Dong Liu and Yadong Mu and Xinggang Wang and Wenyu Liu and Jingdong Wang},
      year={2019},
      eprint={1904.04514},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{sun2019highresolution,
      title={High-Resolution Representations for Labeling Pixels <span class="hljs-keyword">and </span>Regions}, 
      author={Ke Sun <span class="hljs-keyword">and </span>Yang Zhao <span class="hljs-keyword">and </span><span class="hljs-keyword">Borui </span><span class="hljs-keyword">Jiang </span><span class="hljs-keyword">and </span>Tianheng Cheng <span class="hljs-keyword">and </span><span class="hljs-keyword">Bin </span>Xiao <span class="hljs-keyword">and </span>Dong Liu <span class="hljs-keyword">and </span>Yadong Mu <span class="hljs-keyword">and </span>Xinggang Wang <span class="hljs-keyword">and </span>Wenyu Liu <span class="hljs-keyword">and </span><span class="hljs-keyword">Jingdong </span>Wang},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1904</span>.<span class="hljs-number">04514</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){_=n("meta"),ds=i(),$=n("h1"),E=n("a"),es=n("span"),u(R.$$.fragment),Us=i(),ts=n("span"),Os=p("HRNet"),ws=i(),j=n("p"),as=n("strong"),Fs=p("HRNet"),Ks=p(", or "),ns=n("strong"),Vs=p("High-Resolution Net"),Zs=p(", is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution convolution stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over."),vs=i(),y=n("h2"),P=n("a"),os=n("span"),u(C.$$.fragment),Qs=i(),rs=n("span"),se=p("How do I use this model on an image?"),_s=i(),U=n("p"),ee=p("To load a pretrained model:"),$s=i(),u(L.$$.fragment),js=i(),O=n("p"),te=p("To load and preprocess the image:"),ys=i(),u(Y.$$.fragment),bs=i(),F=n("p"),ae=p("To get the model predictions:"),ks=i(),u(B.$$.fragment),xs=i(),K=n("p"),ne=p("To get the top-5 predictions class names:"),Es=i(),u(W.$$.fragment),Ps=i(),S=n("p"),oe=p("Replace the model name with the variant you want to use, e.g. "),ls=n("code"),re=p("hrnet_w18"),le=p(". You can find the IDs in the model summaries at the top of this page."),Ss=i(),T=n("p"),ie=p("To extract image features with this model, follow the "),V=n("a"),pe=p("timm feature extraction examples"),he=p(", just change the name of the model you want to use."),Ts=i(),b=n("h2"),N=n("a"),is=n("span"),u(G.$$.fragment),me=i(),ps=n("span"),ce=p("How do I finetune this model?"),Ns=i(),Z=n("p"),ue=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Hs=i(),u(M.$$.fragment),Is=i(),H=n("p"),fe=p("To finetune on your own dataset, you have to write a training loop or adapt "),z=n("a"),ge=p(`timm\u2019s training
script`),de=p(" to use your dataset."),qs=i(),k=n("h2"),I=n("a"),hs=n("span"),u(X.$$.fragment),we=i(),ms=n("span"),ve=p("How do I train this model?"),As=i(),q=n("p"),_e=p("You can follow the "),Q=n("a"),$e=p("timm recipe scripts"),je=p(" for training a new model afresh."),Rs=i(),x=n("h2"),A=n("a"),cs=n("span"),u(D.$$.fragment),ye=i(),us=n("span"),be=p("Citation"),Cs=i(),u(J.$$.fragment),this.h()},l(s){const a=st('[data-svelte="svelte-1phssyn"]',document.head);_=o(a,"META",{name:!0,content:!0}),a.forEach(e),ds=h(s),$=o(s,"H1",{class:!0});var Ys=r($);E=o(Ys,"A",{id:!0,class:!0,href:!0});var xe=r(E);es=o(xe,"SPAN",{});var Ee=r(es);f(R.$$.fragment,Ee),Ee.forEach(e),xe.forEach(e),Us=h(Ys),ts=o(Ys,"SPAN",{});var Pe=r(ts);Os=m(Pe,"HRNet"),Pe.forEach(e),Ys.forEach(e),ws=h(s),j=o(s,"P",{});var fs=r(j);as=o(fs,"STRONG",{});var Se=r(as);Fs=m(Se,"HRNet"),Se.forEach(e),Ks=m(fs,", or "),ns=o(fs,"STRONG",{});var Te=r(ns);Vs=m(Te,"High-Resolution Net"),Te.forEach(e),Zs=m(fs,", is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution convolution stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over."),fs.forEach(e),vs=h(s),y=o(s,"H2",{class:!0});var Bs=r(y);P=o(Bs,"A",{id:!0,class:!0,href:!0});var Ne=r(P);os=o(Ne,"SPAN",{});var He=r(os);f(C.$$.fragment,He),He.forEach(e),Ne.forEach(e),Qs=h(Bs),rs=o(Bs,"SPAN",{});var Ie=r(rs);se=m(Ie,"How do I use this model on an image?"),Ie.forEach(e),Bs.forEach(e),_s=h(s),U=o(s,"P",{});var qe=r(U);ee=m(qe,"To load a pretrained model:"),qe.forEach(e),$s=h(s),f(L.$$.fragment,s),js=h(s),O=o(s,"P",{});var Ae=r(O);te=m(Ae,"To load and preprocess the image:"),Ae.forEach(e),ys=h(s),f(Y.$$.fragment,s),bs=h(s),F=o(s,"P",{});var Re=r(F);ae=m(Re,"To get the model predictions:"),Re.forEach(e),ks=h(s),f(B.$$.fragment,s),xs=h(s),K=o(s,"P",{});var Ce=r(K);ne=m(Ce,"To get the top-5 predictions class names:"),Ce.forEach(e),Es=h(s),f(W.$$.fragment,s),Ps=h(s),S=o(s,"P",{});var Ws=r(S);oe=m(Ws,"Replace the model name with the variant you want to use, e.g. "),ls=o(Ws,"CODE",{});var Le=r(ls);re=m(Le,"hrnet_w18"),Le.forEach(e),le=m(Ws,". You can find the IDs in the model summaries at the top of this page."),Ws.forEach(e),Ss=h(s),T=o(s,"P",{});var Gs=r(T);ie=m(Gs,"To extract image features with this model, follow the "),V=o(Gs,"A",{href:!0});var Ye=r(V);pe=m(Ye,"timm feature extraction examples"),Ye.forEach(e),he=m(Gs,", just change the name of the model you want to use."),Gs.forEach(e),Ts=h(s),b=o(s,"H2",{class:!0});var Ms=r(b);N=o(Ms,"A",{id:!0,class:!0,href:!0});var Be=r(N);is=o(Be,"SPAN",{});var We=r(is);f(G.$$.fragment,We),We.forEach(e),Be.forEach(e),me=h(Ms),ps=o(Ms,"SPAN",{});var Ge=r(ps);ce=m(Ge,"How do I finetune this model?"),Ge.forEach(e),Ms.forEach(e),Ns=h(s),Z=o(s,"P",{});var Me=r(Z);ue=m(Me,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Me.forEach(e),Hs=h(s),f(M.$$.fragment,s),Is=h(s),H=o(s,"P",{});var zs=r(H);fe=m(zs,"To finetune on your own dataset, you have to write a training loop or adapt "),z=o(zs,"A",{href:!0,rel:!0});var ze=r(z);ge=m(ze,`timm\u2019s training
script`),ze.forEach(e),de=m(zs," to use your dataset."),zs.forEach(e),qs=h(s),k=o(s,"H2",{class:!0});var Xs=r(k);I=o(Xs,"A",{id:!0,class:!0,href:!0});var Xe=r(I);hs=o(Xe,"SPAN",{});var De=r(hs);f(X.$$.fragment,De),De.forEach(e),Xe.forEach(e),we=h(Xs),ms=o(Xs,"SPAN",{});var Je=r(ms);ve=m(Je,"How do I train this model?"),Je.forEach(e),Xs.forEach(e),As=h(s),q=o(s,"P",{});var Ds=r(q);_e=m(Ds,"You can follow the "),Q=o(Ds,"A",{href:!0});var Ue=r(Q);$e=m(Ue,"timm recipe scripts"),Ue.forEach(e),je=m(Ds," for training a new model afresh."),Ds.forEach(e),Rs=h(s),x=o(s,"H2",{class:!0});var Js=r(x);A=o(Js,"A",{id:!0,class:!0,href:!0});var Oe=r(A);cs=o(Oe,"SPAN",{});var Fe=r(cs);f(D.$$.fragment,Fe),Fe.forEach(e),Oe.forEach(e),ye=h(Js),us=o(Js,"SPAN",{});var Ke=r(us);be=m(Ke,"Citation"),Ke.forEach(e),Js.forEach(e),Cs=h(s),f(J.$$.fragment,s),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(nt)),c(E,"id","hrnet"),c(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E,"href","#hrnet"),c($,"class","relative group"),c(P,"id","how-do-i-use-this-model-on-an-image"),c(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P,"href","#how-do-i-use-this-model-on-an-image"),c(y,"class","relative group"),c(V,"href","../feature_extraction"),c(N,"id","how-do-i-finetune-this-model"),c(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N,"href","#how-do-i-finetune-this-model"),c(b,"class","relative group"),c(z,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),c(z,"rel","nofollow"),c(I,"id","how-do-i-train-this-model"),c(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I,"href","#how-do-i-train-this-model"),c(k,"class","relative group"),c(Q,"href","../scripts"),c(A,"id","citation"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#citation"),c(x,"class","relative group")},m(s,a){t(document.head,_),l(s,ds,a),l(s,$,a),t($,E),t(E,es),g(R,es,null),t($,Us),t($,ts),t(ts,Os),l(s,ws,a),l(s,j,a),t(j,as),t(as,Fs),t(j,Ks),t(j,ns),t(ns,Vs),t(j,Zs),l(s,vs,a),l(s,y,a),t(y,P),t(P,os),g(C,os,null),t(y,Qs),t(y,rs),t(rs,se),l(s,_s,a),l(s,U,a),t(U,ee),l(s,$s,a),g(L,s,a),l(s,js,a),l(s,O,a),t(O,te),l(s,ys,a),g(Y,s,a),l(s,bs,a),l(s,F,a),t(F,ae),l(s,ks,a),g(B,s,a),l(s,xs,a),l(s,K,a),t(K,ne),l(s,Es,a),g(W,s,a),l(s,Ps,a),l(s,S,a),t(S,oe),t(S,ls),t(ls,re),t(S,le),l(s,Ss,a),l(s,T,a),t(T,ie),t(T,V),t(V,pe),t(T,he),l(s,Ts,a),l(s,b,a),t(b,N),t(N,is),g(G,is,null),t(b,me),t(b,ps),t(ps,ce),l(s,Ns,a),l(s,Z,a),t(Z,ue),l(s,Hs,a),g(M,s,a),l(s,Is,a),l(s,H,a),t(H,fe),t(H,z),t(z,ge),t(H,de),l(s,qs,a),l(s,k,a),t(k,I),t(I,hs),g(X,hs,null),t(k,we),t(k,ms),t(ms,ve),l(s,As,a),l(s,q,a),t(q,_e),t(q,Q),t(Q,$e),t(q,je),l(s,Rs,a),l(s,x,a),t(x,A),t(A,cs),g(D,cs,null),t(x,ye),t(x,us),t(us,be),l(s,Cs,a),g(J,s,a),Ls=!0},p:et,i(s){Ls||(d(R.$$.fragment,s),d(C.$$.fragment,s),d(L.$$.fragment,s),d(Y.$$.fragment,s),d(B.$$.fragment,s),d(W.$$.fragment,s),d(G.$$.fragment,s),d(M.$$.fragment,s),d(X.$$.fragment,s),d(D.$$.fragment,s),d(J.$$.fragment,s),Ls=!0)},o(s){w(R.$$.fragment,s),w(C.$$.fragment,s),w(L.$$.fragment,s),w(Y.$$.fragment,s),w(B.$$.fragment,s),w(W.$$.fragment,s),w(G.$$.fragment,s),w(M.$$.fragment,s),w(X.$$.fragment,s),w(D.$$.fragment,s),w(J.$$.fragment,s),Ls=!1},d(s){e(_),s&&e(ds),s&&e($),v(R),s&&e(ws),s&&e(j),s&&e(vs),s&&e(y),v(C),s&&e(_s),s&&e(U),s&&e($s),v(L,s),s&&e(js),s&&e(O),s&&e(ys),v(Y,s),s&&e(bs),s&&e(F),s&&e(ks),v(B,s),s&&e(xs),s&&e(K),s&&e(Es),v(W,s),s&&e(Ps),s&&e(S),s&&e(Ss),s&&e(T),s&&e(Ts),s&&e(b),v(G),s&&e(Ns),s&&e(Z),s&&e(Hs),v(M,s),s&&e(Is),s&&e(H),s&&e(qs),s&&e(k),v(X),s&&e(As),s&&e(q),s&&e(Rs),s&&e(x),v(D),s&&e(Cs),v(J,s)}}}const nt={local:"hrnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"HRNet"};function ot(ke){return tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pt extends Ve{constructor(_){super();Ze(this,_,ot,at,Qe,{})}}export{pt as default,nt as metadata};
