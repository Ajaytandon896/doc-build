import{S as ts,i as ss,s as as,e as n,k as p,w as c,t as m,M as ns,c as r,d as t,m as h,a as l,x as d,h as f,b as i,G as s,g as o,y as u,L as rs,q as g,o as v,B as w,v as os}from"../../chunks/vendor-hf-doc-builder.js";import{I as ue}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as se}from"../../chunks/CodeBlock-hf-doc-builder.js";function ls(At){let _,ge,j,k,ae,H,Fe,ne,Ke,ve,L,re,Qe,We,we,E,Ze,G,et,tt,_e,$,P,oe,R,st,le,at,je,F,nt,$e,Y,be,K,rt,ye,M,xe,Q,ot,ke,U,Ee,W,lt,Pe,z,Ae,A,it,ie,pt,mt,Te,T,ht,Z,ft,ct,Ie,b,I,pe,B,dt,me,ut,Se,ee,gt,qe,V,Ne,S,vt,X,wt,_t,Ce,y,q,he,D,jt,fe,$t,He,N,bt,te,yt,xt,Le,x,C,ce,J,kt,de,Et,Ge,O,Re;return H=new ue({}),R=new ue({}),Y=new se({props:{code:`import timm
model = timm.create_model('tf_efficientnet_b0_ap', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;tf_efficientnet_b0_ap&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),M=new se({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),U=new se({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),z=new se({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),B=new ue({}),V=new se({props:{code:"model = timm.create_model('tf_efficientnet_b0_ap', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;tf_efficientnet_b0_ap&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),D=new ue({}),J=new ue({}),O=new se({props:{code:`@misc{xie2020adversarial,
      title={Adversarial Examples Improve Image Recognition}, 
      author={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le},
      year={2020},
      eprint={1911.09665},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{xie2020adversarial,
      title={Adversarial Examples Improve Image Recognition}, 
      author={Cihang Xie <span class="hljs-keyword">and </span>Mingxing Tan <span class="hljs-keyword">and </span><span class="hljs-keyword">Boqing </span>Gong <span class="hljs-keyword">and </span><span class="hljs-keyword">Jiang </span>Wang <span class="hljs-keyword">and </span>Alan Yuille <span class="hljs-keyword">and </span>Quoc V. Le},
      year={<span class="hljs-number">2020</span>},
      eprint={<span class="hljs-number">1911</span>.<span class="hljs-number">09665</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){_=n("meta"),ge=p(),j=n("h1"),k=n("a"),ae=n("span"),c(H.$$.fragment),Fe=p(),ne=n("span"),Ke=m("AdvProp (EfficientNet)"),ve=p(),L=n("p"),re=n("strong"),Qe=m("AdvProp"),We=m(" is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples."),we=p(),E=n("p"),Ze=m("The weights from this model were ported from "),G=n("a"),et=m("Tensorflow/TPU"),tt=m("."),_e=p(),$=n("h2"),P=n("a"),oe=n("span"),c(R.$$.fragment),st=p(),le=n("span"),at=m("How do I use this model on an image?"),je=p(),F=n("p"),nt=m("To load a pretrained model:"),$e=p(),c(Y.$$.fragment),be=p(),K=n("p"),rt=m("To load and preprocess the image:"),ye=p(),c(M.$$.fragment),xe=p(),Q=n("p"),ot=m("To get the model predictions:"),ke=p(),c(U.$$.fragment),Ee=p(),W=n("p"),lt=m("To get the top-5 predictions class names:"),Pe=p(),c(z.$$.fragment),Ae=p(),A=n("p"),it=m("Replace the model name with the variant you want to use, e.g. "),ie=n("code"),pt=m("tf_efficientnet_b0_ap"),mt=m(". You can find the IDs in the model summaries at the top of this page."),Te=p(),T=n("p"),ht=m("To extract image features with this model, follow the "),Z=n("a"),ft=m("timm feature extraction examples"),ct=m(", just change the name of the model you want to use."),Ie=p(),b=n("h2"),I=n("a"),pe=n("span"),c(B.$$.fragment),dt=p(),me=n("span"),ut=m("How do I finetune this model?"),Se=p(),ee=n("p"),gt=m("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qe=p(),c(V.$$.fragment),Ne=p(),S=n("p"),vt=m("To finetune on your own dataset, you have to write a training loop or adapt "),X=n("a"),wt=m(`timm\u2019s training
script`),_t=m(" to use your dataset."),Ce=p(),y=n("h2"),q=n("a"),he=n("span"),c(D.$$.fragment),jt=p(),fe=n("span"),$t=m("How do I train this model?"),He=p(),N=n("p"),bt=m("You can follow the "),te=n("a"),yt=m("timm recipe scripts"),xt=m(" for training a new model afresh."),Le=p(),x=n("h2"),C=n("a"),ce=n("span"),c(J.$$.fragment),kt=p(),de=n("span"),Et=m("Citation"),Ge=p(),c(O.$$.fragment),this.h()},l(e){const a=ns('[data-svelte="svelte-1phssyn"]',document.head);_=r(a,"META",{name:!0,content:!0}),a.forEach(t),ge=h(e),j=r(e,"H1",{class:!0});var Ye=l(j);k=r(Ye,"A",{id:!0,class:!0,href:!0});var Tt=l(k);ae=r(Tt,"SPAN",{});var It=l(ae);d(H.$$.fragment,It),It.forEach(t),Tt.forEach(t),Fe=h(Ye),ne=r(Ye,"SPAN",{});var St=l(ne);Ke=f(St,"AdvProp (EfficientNet)"),St.forEach(t),Ye.forEach(t),ve=h(e),L=r(e,"P",{});var Pt=l(L);re=r(Pt,"STRONG",{});var qt=l(re);Qe=f(qt,"AdvProp"),qt.forEach(t),We=f(Pt," is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples."),Pt.forEach(t),we=h(e),E=r(e,"P",{});var Me=l(E);Ze=f(Me,"The weights from this model were ported from "),G=r(Me,"A",{href:!0,rel:!0});var Nt=l(G);et=f(Nt,"Tensorflow/TPU"),Nt.forEach(t),tt=f(Me,"."),Me.forEach(t),_e=h(e),$=r(e,"H2",{class:!0});var Ue=l($);P=r(Ue,"A",{id:!0,class:!0,href:!0});var Ct=l(P);oe=r(Ct,"SPAN",{});var Ht=l(oe);d(R.$$.fragment,Ht),Ht.forEach(t),Ct.forEach(t),st=h(Ue),le=r(Ue,"SPAN",{});var Lt=l(le);at=f(Lt,"How do I use this model on an image?"),Lt.forEach(t),Ue.forEach(t),je=h(e),F=r(e,"P",{});var Gt=l(F);nt=f(Gt,"To load a pretrained model:"),Gt.forEach(t),$e=h(e),d(Y.$$.fragment,e),be=h(e),K=r(e,"P",{});var Rt=l(K);rt=f(Rt,"To load and preprocess the image:"),Rt.forEach(t),ye=h(e),d(M.$$.fragment,e),xe=h(e),Q=r(e,"P",{});var Yt=l(Q);ot=f(Yt,"To get the model predictions:"),Yt.forEach(t),ke=h(e),d(U.$$.fragment,e),Ee=h(e),W=r(e,"P",{});var Mt=l(W);lt=f(Mt,"To get the top-5 predictions class names:"),Mt.forEach(t),Pe=h(e),d(z.$$.fragment,e),Ae=h(e),A=r(e,"P",{});var ze=l(A);it=f(ze,"Replace the model name with the variant you want to use, e.g. "),ie=r(ze,"CODE",{});var Ut=l(ie);pt=f(Ut,"tf_efficientnet_b0_ap"),Ut.forEach(t),mt=f(ze,". You can find the IDs in the model summaries at the top of this page."),ze.forEach(t),Te=h(e),T=r(e,"P",{});var Be=l(T);ht=f(Be,"To extract image features with this model, follow the "),Z=r(Be,"A",{href:!0});var zt=l(Z);ft=f(zt,"timm feature extraction examples"),zt.forEach(t),ct=f(Be,", just change the name of the model you want to use."),Be.forEach(t),Ie=h(e),b=r(e,"H2",{class:!0});var Ve=l(b);I=r(Ve,"A",{id:!0,class:!0,href:!0});var Bt=l(I);pe=r(Bt,"SPAN",{});var Vt=l(pe);d(B.$$.fragment,Vt),Vt.forEach(t),Bt.forEach(t),dt=h(Ve),me=r(Ve,"SPAN",{});var Xt=l(me);ut=f(Xt,"How do I finetune this model?"),Xt.forEach(t),Ve.forEach(t),Se=h(e),ee=r(e,"P",{});var Dt=l(ee);gt=f(Dt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Dt.forEach(t),qe=h(e),d(V.$$.fragment,e),Ne=h(e),S=r(e,"P",{});var Xe=l(S);vt=f(Xe,"To finetune on your own dataset, you have to write a training loop or adapt "),X=r(Xe,"A",{href:!0,rel:!0});var Jt=l(X);wt=f(Jt,`timm\u2019s training
script`),Jt.forEach(t),_t=f(Xe," to use your dataset."),Xe.forEach(t),Ce=h(e),y=r(e,"H2",{class:!0});var De=l(y);q=r(De,"A",{id:!0,class:!0,href:!0});var Ot=l(q);he=r(Ot,"SPAN",{});var Ft=l(he);d(D.$$.fragment,Ft),Ft.forEach(t),Ot.forEach(t),jt=h(De),fe=r(De,"SPAN",{});var Kt=l(fe);$t=f(Kt,"How do I train this model?"),Kt.forEach(t),De.forEach(t),He=h(e),N=r(e,"P",{});var Je=l(N);bt=f(Je,"You can follow the "),te=r(Je,"A",{href:!0});var Qt=l(te);yt=f(Qt,"timm recipe scripts"),Qt.forEach(t),xt=f(Je," for training a new model afresh."),Je.forEach(t),Le=h(e),x=r(e,"H2",{class:!0});var Oe=l(x);C=r(Oe,"A",{id:!0,class:!0,href:!0});var Wt=l(C);ce=r(Wt,"SPAN",{});var Zt=l(ce);d(J.$$.fragment,Zt),Zt.forEach(t),Wt.forEach(t),kt=h(Oe),de=r(Oe,"SPAN",{});var es=l(de);Et=f(es,"Citation"),es.forEach(t),Oe.forEach(t),Ge=h(e),d(O.$$.fragment,e),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(is)),i(k,"id","advprop-efficientnet"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#advprop-efficientnet"),i(j,"class","relative group"),i(G,"href","https://github.com/tensorflow/tpu"),i(G,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(Z,"href","../feature_extraction"),i(I,"id","how-do-i-finetune-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-finetune-this-model"),i(b,"class","relative group"),i(X,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(X,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(y,"class","relative group"),i(te,"href","../scripts"),i(C,"id","citation"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,_),o(e,ge,a),o(e,j,a),s(j,k),s(k,ae),u(H,ae,null),s(j,Fe),s(j,ne),s(ne,Ke),o(e,ve,a),o(e,L,a),s(L,re),s(re,Qe),s(L,We),o(e,we,a),o(e,E,a),s(E,Ze),s(E,G),s(G,et),s(E,tt),o(e,_e,a),o(e,$,a),s($,P),s(P,oe),u(R,oe,null),s($,st),s($,le),s(le,at),o(e,je,a),o(e,F,a),s(F,nt),o(e,$e,a),u(Y,e,a),o(e,be,a),o(e,K,a),s(K,rt),o(e,ye,a),u(M,e,a),o(e,xe,a),o(e,Q,a),s(Q,ot),o(e,ke,a),u(U,e,a),o(e,Ee,a),o(e,W,a),s(W,lt),o(e,Pe,a),u(z,e,a),o(e,Ae,a),o(e,A,a),s(A,it),s(A,ie),s(ie,pt),s(A,mt),o(e,Te,a),o(e,T,a),s(T,ht),s(T,Z),s(Z,ft),s(T,ct),o(e,Ie,a),o(e,b,a),s(b,I),s(I,pe),u(B,pe,null),s(b,dt),s(b,me),s(me,ut),o(e,Se,a),o(e,ee,a),s(ee,gt),o(e,qe,a),u(V,e,a),o(e,Ne,a),o(e,S,a),s(S,vt),s(S,X),s(X,wt),s(S,_t),o(e,Ce,a),o(e,y,a),s(y,q),s(q,he),u(D,he,null),s(y,jt),s(y,fe),s(fe,$t),o(e,He,a),o(e,N,a),s(N,bt),s(N,te),s(te,yt),s(N,xt),o(e,Le,a),o(e,x,a),s(x,C),s(C,ce),u(J,ce,null),s(x,kt),s(x,de),s(de,Et),o(e,Ge,a),u(O,e,a),Re=!0},p:rs,i(e){Re||(g(H.$$.fragment,e),g(R.$$.fragment,e),g(Y.$$.fragment,e),g(M.$$.fragment,e),g(U.$$.fragment,e),g(z.$$.fragment,e),g(B.$$.fragment,e),g(V.$$.fragment,e),g(D.$$.fragment,e),g(J.$$.fragment,e),g(O.$$.fragment,e),Re=!0)},o(e){v(H.$$.fragment,e),v(R.$$.fragment,e),v(Y.$$.fragment,e),v(M.$$.fragment,e),v(U.$$.fragment,e),v(z.$$.fragment,e),v(B.$$.fragment,e),v(V.$$.fragment,e),v(D.$$.fragment,e),v(J.$$.fragment,e),v(O.$$.fragment,e),Re=!1},d(e){t(_),e&&t(ge),e&&t(j),w(H),e&&t(ve),e&&t(L),e&&t(we),e&&t(E),e&&t(_e),e&&t($),w(R),e&&t(je),e&&t(F),e&&t($e),w(Y,e),e&&t(be),e&&t(K),e&&t(ye),w(M,e),e&&t(xe),e&&t(Q),e&&t(ke),w(U,e),e&&t(Ee),e&&t(W),e&&t(Pe),w(z,e),e&&t(Ae),e&&t(A),e&&t(Te),e&&t(T),e&&t(Ie),e&&t(b),w(B),e&&t(Se),e&&t(ee),e&&t(qe),w(V,e),e&&t(Ne),e&&t(S),e&&t(Ce),e&&t(y),w(D),e&&t(He),e&&t(N),e&&t(Le),e&&t(x),w(J),e&&t(Ge),w(O,e)}}}const is={local:"advprop-efficientnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"AdvProp (EfficientNet)"};function ps(At){return os(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cs extends ts{constructor(_){super();ss(this,_,ps,ls,as,{})}}export{cs as default,is as metadata};
