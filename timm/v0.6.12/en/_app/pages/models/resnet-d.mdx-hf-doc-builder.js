import{S as ns,i as ls,s as rs,e as o,k as h,w as u,t as p,M as is,c as n,d as t,m as f,a as l,x as d,h as m,b as i,G as s,g as r,y as g,L as ps,q as w,o as v,B as _,v as ms}from"../../chunks/vendor-hf-doc-builder.js";import{I as we}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as oe}from"../../chunks/CodeBlock-hf-doc-builder.js";function hs(St){let j,ve,$,N,ne,R,Fe,le,Ve,_e,c,re,Ke,Qe,Z,We,et,L,tt,st,D,at,ot,je,b,P,ie,M,nt,pe,lt,$e,K,rt,be,z,ye,Q,it,ke,B,Ee,W,pt,xe,G,Ne,ee,mt,Pe,Y,Te,T,ht,me,ft,ct,Se,S,ut,te,dt,gt,Ae,y,A,he,U,wt,fe,vt,Ie,se,_t,qe,X,Ce,I,jt,J,$t,bt,He,k,q,ce,O,yt,ue,kt,Re,C,Et,ae,xt,Nt,Ze,E,H,de,F,Pt,ge,Tt,Le,V,De;return R=new we({}),M=new we({}),z=new oe({props:{code:`import timm
model = timm.create_model('resnet101d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnet101d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),B=new oe({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),G=new oe({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),Y=new oe({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),U=new we({}),X=new oe({props:{code:"model = timm.create_model('resnet101d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnet101d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),O=new we({}),F=new we({}),V=new oe({props:{code:`@misc{he2018bag,
      title={Bag of Tricks for Image Classification with Convolutional Neural Networks}, 
      author={Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li},
      year={2018},
      eprint={1812.01187},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{he<span class="hljs-symbol">2018b</span>ag,
      title={<span class="hljs-keyword">Bag </span>of Tricks for Image Classification with Convolutional Neural Networks}, 
      author={Tong He <span class="hljs-keyword">and </span>Zhi Zhang <span class="hljs-keyword">and </span>Hang Zhang <span class="hljs-keyword">and </span>Zhongyue Zhang <span class="hljs-keyword">and </span><span class="hljs-keyword">Junyuan </span>Xie <span class="hljs-keyword">and </span>Mu Li},
      year={<span class="hljs-number">2018</span>},
      eprint={<span class="hljs-number">1812</span>.<span class="hljs-number">01187</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){j=o("meta"),ve=h(),$=o("h1"),N=o("a"),ne=o("span"),u(R.$$.fragment),Fe=h(),le=o("span"),Ve=p("ResNet-D"),_e=h(),c=o("p"),re=o("strong"),Ke=p("ResNet-D"),Qe=p(" is a modification on the "),Z=o("a"),We=p("ResNet"),et=p(" architecture that utilises an "),L=o("a"),tt=p("average pooling"),st=p(" tweak for downsampling. The motivation is that in the unmodified ResNet, the "),D=o("a"),at=p("1\xD71 convolution"),ot=p(" for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored"),je=h(),b=o("h2"),P=o("a"),ie=o("span"),u(M.$$.fragment),nt=h(),pe=o("span"),lt=p("How do I use this model on an image?"),$e=h(),K=o("p"),rt=p("To load a pretrained model:"),be=h(),u(z.$$.fragment),ye=h(),Q=o("p"),it=p("To load and preprocess the image:"),ke=h(),u(B.$$.fragment),Ee=h(),W=o("p"),pt=p("To get the model predictions:"),xe=h(),u(G.$$.fragment),Ne=h(),ee=o("p"),mt=p("To get the top-5 predictions class names:"),Pe=h(),u(Y.$$.fragment),Te=h(),T=o("p"),ht=p("Replace the model name with the variant you want to use, e.g. "),me=o("code"),ft=p("resnet101d"),ct=p(". You can find the IDs in the model summaries at the top of this page."),Se=h(),S=o("p"),ut=p("To extract image features with this model, follow the "),te=o("a"),dt=p("timm feature extraction examples"),gt=p(", just change the name of the model you want to use."),Ae=h(),y=o("h2"),A=o("a"),he=o("span"),u(U.$$.fragment),wt=h(),fe=o("span"),vt=p("How do I finetune this model?"),Ie=h(),se=o("p"),_t=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qe=h(),u(X.$$.fragment),Ce=h(),I=o("p"),jt=p("To finetune on your own dataset, you have to write a training loop or adapt "),J=o("a"),$t=p(`timm\u2019s training
script`),bt=p(" to use your dataset."),He=h(),k=o("h2"),q=o("a"),ce=o("span"),u(O.$$.fragment),yt=h(),ue=o("span"),kt=p("How do I train this model?"),Re=h(),C=o("p"),Et=p("You can follow the "),ae=o("a"),xt=p("timm recipe scripts"),Nt=p(" for training a new model afresh."),Ze=h(),E=o("h2"),H=o("a"),de=o("span"),u(F.$$.fragment),Pt=h(),ge=o("span"),Tt=p("Citation"),Le=h(),u(V.$$.fragment),this.h()},l(e){const a=is('[data-svelte="svelte-1phssyn"]',document.head);j=n(a,"META",{name:!0,content:!0}),a.forEach(t),ve=f(e),$=n(e,"H1",{class:!0});var Me=l($);N=n(Me,"A",{id:!0,class:!0,href:!0});var At=l(N);ne=n(At,"SPAN",{});var It=l(ne);d(R.$$.fragment,It),It.forEach(t),At.forEach(t),Fe=f(Me),le=n(Me,"SPAN",{});var qt=l(le);Ve=m(qt,"ResNet-D"),qt.forEach(t),Me.forEach(t),_e=f(e),c=n(e,"P",{});var x=l(c);re=n(x,"STRONG",{});var Ct=l(re);Ke=m(Ct,"ResNet-D"),Ct.forEach(t),Qe=m(x," is a modification on the "),Z=n(x,"A",{href:!0,rel:!0});var Ht=l(Z);We=m(Ht,"ResNet"),Ht.forEach(t),et=m(x," architecture that utilises an "),L=n(x,"A",{href:!0,rel:!0});var Rt=l(L);tt=m(Rt,"average pooling"),Rt.forEach(t),st=m(x," tweak for downsampling. The motivation is that in the unmodified ResNet, the "),D=n(x,"A",{href:!0,rel:!0});var Zt=l(D);at=m(Zt,"1\xD71 convolution"),Zt.forEach(t),ot=m(x," for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored"),x.forEach(t),je=f(e),b=n(e,"H2",{class:!0});var ze=l(b);P=n(ze,"A",{id:!0,class:!0,href:!0});var Lt=l(P);ie=n(Lt,"SPAN",{});var Dt=l(ie);d(M.$$.fragment,Dt),Dt.forEach(t),Lt.forEach(t),nt=f(ze),pe=n(ze,"SPAN",{});var Mt=l(pe);lt=m(Mt,"How do I use this model on an image?"),Mt.forEach(t),ze.forEach(t),$e=f(e),K=n(e,"P",{});var zt=l(K);rt=m(zt,"To load a pretrained model:"),zt.forEach(t),be=f(e),d(z.$$.fragment,e),ye=f(e),Q=n(e,"P",{});var Bt=l(Q);it=m(Bt,"To load and preprocess the image:"),Bt.forEach(t),ke=f(e),d(B.$$.fragment,e),Ee=f(e),W=n(e,"P",{});var Gt=l(W);pt=m(Gt,"To get the model predictions:"),Gt.forEach(t),xe=f(e),d(G.$$.fragment,e),Ne=f(e),ee=n(e,"P",{});var Yt=l(ee);mt=m(Yt,"To get the top-5 predictions class names:"),Yt.forEach(t),Pe=f(e),d(Y.$$.fragment,e),Te=f(e),T=n(e,"P",{});var Be=l(T);ht=m(Be,"Replace the model name with the variant you want to use, e.g. "),me=n(Be,"CODE",{});var Ut=l(me);ft=m(Ut,"resnet101d"),Ut.forEach(t),ct=m(Be,". You can find the IDs in the model summaries at the top of this page."),Be.forEach(t),Se=f(e),S=n(e,"P",{});var Ge=l(S);ut=m(Ge,"To extract image features with this model, follow the "),te=n(Ge,"A",{href:!0});var Xt=l(te);dt=m(Xt,"timm feature extraction examples"),Xt.forEach(t),gt=m(Ge,", just change the name of the model you want to use."),Ge.forEach(t),Ae=f(e),y=n(e,"H2",{class:!0});var Ye=l(y);A=n(Ye,"A",{id:!0,class:!0,href:!0});var Jt=l(A);he=n(Jt,"SPAN",{});var Ot=l(he);d(U.$$.fragment,Ot),Ot.forEach(t),Jt.forEach(t),wt=f(Ye),fe=n(Ye,"SPAN",{});var Ft=l(fe);vt=m(Ft,"How do I finetune this model?"),Ft.forEach(t),Ye.forEach(t),Ie=f(e),se=n(e,"P",{});var Vt=l(se);_t=m(Vt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Vt.forEach(t),qe=f(e),d(X.$$.fragment,e),Ce=f(e),I=n(e,"P",{});var Ue=l(I);jt=m(Ue,"To finetune on your own dataset, you have to write a training loop or adapt "),J=n(Ue,"A",{href:!0,rel:!0});var Kt=l(J);$t=m(Kt,`timm\u2019s training
script`),Kt.forEach(t),bt=m(Ue," to use your dataset."),Ue.forEach(t),He=f(e),k=n(e,"H2",{class:!0});var Xe=l(k);q=n(Xe,"A",{id:!0,class:!0,href:!0});var Qt=l(q);ce=n(Qt,"SPAN",{});var Wt=l(ce);d(O.$$.fragment,Wt),Wt.forEach(t),Qt.forEach(t),yt=f(Xe),ue=n(Xe,"SPAN",{});var es=l(ue);kt=m(es,"How do I train this model?"),es.forEach(t),Xe.forEach(t),Re=f(e),C=n(e,"P",{});var Je=l(C);Et=m(Je,"You can follow the "),ae=n(Je,"A",{href:!0});var ts=l(ae);xt=m(ts,"timm recipe scripts"),ts.forEach(t),Nt=m(Je," for training a new model afresh."),Je.forEach(t),Ze=f(e),E=n(e,"H2",{class:!0});var Oe=l(E);H=n(Oe,"A",{id:!0,class:!0,href:!0});var ss=l(H);de=n(ss,"SPAN",{});var as=l(de);d(F.$$.fragment,as),as.forEach(t),ss.forEach(t),Pt=f(Oe),ge=n(Oe,"SPAN",{});var os=l(ge);Tt=m(os,"Citation"),os.forEach(t),Oe.forEach(t),Le=f(e),d(V.$$.fragment,e),this.h()},h(){i(j,"name","hf:doc:metadata"),i(j,"content",JSON.stringify(fs)),i(N,"id","resnetd"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#resnetd"),i($,"class","relative group"),i(Z,"href","https://paperswithcode.com/method/resnet"),i(Z,"rel","nofollow"),i(L,"href","https://paperswithcode.com/method/average-pooling"),i(L,"rel","nofollow"),i(D,"href","https://paperswithcode.com/method/1x1-convolution"),i(D,"rel","nofollow"),i(P,"id","how-do-i-use-this-model-on-an-image"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#how-do-i-use-this-model-on-an-image"),i(b,"class","relative group"),i(te,"href","../feature_extraction"),i(A,"id","how-do-i-finetune-this-model"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(J,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(J,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(ae,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(E,"class","relative group")},m(e,a){s(document.head,j),r(e,ve,a),r(e,$,a),s($,N),s(N,ne),g(R,ne,null),s($,Fe),s($,le),s(le,Ve),r(e,_e,a),r(e,c,a),s(c,re),s(re,Ke),s(c,Qe),s(c,Z),s(Z,We),s(c,et),s(c,L),s(L,tt),s(c,st),s(c,D),s(D,at),s(c,ot),r(e,je,a),r(e,b,a),s(b,P),s(P,ie),g(M,ie,null),s(b,nt),s(b,pe),s(pe,lt),r(e,$e,a),r(e,K,a),s(K,rt),r(e,be,a),g(z,e,a),r(e,ye,a),r(e,Q,a),s(Q,it),r(e,ke,a),g(B,e,a),r(e,Ee,a),r(e,W,a),s(W,pt),r(e,xe,a),g(G,e,a),r(e,Ne,a),r(e,ee,a),s(ee,mt),r(e,Pe,a),g(Y,e,a),r(e,Te,a),r(e,T,a),s(T,ht),s(T,me),s(me,ft),s(T,ct),r(e,Se,a),r(e,S,a),s(S,ut),s(S,te),s(te,dt),s(S,gt),r(e,Ae,a),r(e,y,a),s(y,A),s(A,he),g(U,he,null),s(y,wt),s(y,fe),s(fe,vt),r(e,Ie,a),r(e,se,a),s(se,_t),r(e,qe,a),g(X,e,a),r(e,Ce,a),r(e,I,a),s(I,jt),s(I,J),s(J,$t),s(I,bt),r(e,He,a),r(e,k,a),s(k,q),s(q,ce),g(O,ce,null),s(k,yt),s(k,ue),s(ue,kt),r(e,Re,a),r(e,C,a),s(C,Et),s(C,ae),s(ae,xt),s(C,Nt),r(e,Ze,a),r(e,E,a),s(E,H),s(H,de),g(F,de,null),s(E,Pt),s(E,ge),s(ge,Tt),r(e,Le,a),g(V,e,a),De=!0},p:ps,i(e){De||(w(R.$$.fragment,e),w(M.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(Y.$$.fragment,e),w(U.$$.fragment,e),w(X.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),w(V.$$.fragment,e),De=!0)},o(e){v(R.$$.fragment,e),v(M.$$.fragment,e),v(z.$$.fragment,e),v(B.$$.fragment,e),v(G.$$.fragment,e),v(Y.$$.fragment,e),v(U.$$.fragment,e),v(X.$$.fragment,e),v(O.$$.fragment,e),v(F.$$.fragment,e),v(V.$$.fragment,e),De=!1},d(e){t(j),e&&t(ve),e&&t($),_(R),e&&t(_e),e&&t(c),e&&t(je),e&&t(b),_(M),e&&t($e),e&&t(K),e&&t(be),_(z,e),e&&t(ye),e&&t(Q),e&&t(ke),_(B,e),e&&t(Ee),e&&t(W),e&&t(xe),_(G,e),e&&t(Ne),e&&t(ee),e&&t(Pe),_(Y,e),e&&t(Te),e&&t(T),e&&t(Se),e&&t(S),e&&t(Ae),e&&t(y),_(U),e&&t(Ie),e&&t(se),e&&t(qe),_(X,e),e&&t(Ce),e&&t(I),e&&t(He),e&&t(k),_(O),e&&t(Re),e&&t(C),e&&t(Ze),e&&t(E),_(F),e&&t(Le),_(V,e)}}}const fs={local:"resnetd",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"ResNet-D"};function cs(St){return ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ws extends ns{constructor(j){super();ls(this,j,cs,hs,rs,{})}}export{ws as default,fs as metadata};
