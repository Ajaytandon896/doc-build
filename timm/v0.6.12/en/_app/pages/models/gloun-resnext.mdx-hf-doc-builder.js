import{S as ue,i as de,s as ge,e as l,k as h,w as u,t as i,M as we,c as n,d as t,m as c,a as o,x as d,h as p,b as m,G as e,g as r,y as g,L as _e,q as w,o as _,B as j,v as je}from"../../chunks/vendor-hf-doc-builder.js";import{I as js}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ns}from"../../chunks/CodeBlock-hf-doc-builder.js";function be(Dt){let v,bs,$,P,os,C,Ws,rs,st,vs,f,tt,is,et,at,D,lt,nt,X,ot,rt,ps,it,pt,$s,A,mt,z,ht,ct,ys,y,T,ms,L,ft,hs,ut,xs,V,dt,ks,B,Es,W,gt,Ps,M,As,ss,wt,Ts,Y,Ns,ts,_t,Ss,U,qs,N,jt,cs,bt,vt,Is,S,$t,es,yt,xt,Rs,x,q,fs,O,kt,us,Et,Hs,as,Pt,Gs,F,Cs,I,At,K,Tt,Nt,Ds,k,R,ds,Z,St,gs,qt,Xs,H,It,ls,Rt,Ht,zs,E,G,ws,J,Gt,_s,Ct,Ls,Q,Bs;return C=new js({}),L=new js({}),B=new ns({props:{code:`import timm
model = timm.create_model('gluon_resnext101_32x4d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_resnext101_32x4d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),M=new ns({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),Y=new ns({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),U=new ns({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),O=new js({}),F=new ns({props:{code:"model = timm.create_model('gluon_resnext101_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_resnext101_32x4d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),Z=new js({}),J=new js({}),Q=new ns({props:{code:`@article{DBLP:journals/corr/XieGDTH16,
  author    = {Saining Xie and
               Ross B. Girshick and
               Piotr Doll{\\'{a}}r and
               Zhuowen Tu and
               Kaiming He},
  title     = {Aggregated Residual Transformations for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1611.05431},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05431},
  archivePrefix = {arXiv},
  eprint    = {1611.05431},
  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XieGDTH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`@article{DBLP:journals/corr/XieGDTH16,
  <span class="hljs-attr">author</span>    = {Saining Xie <span class="hljs-literal">and</span>
               Ross B. Girshick <span class="hljs-literal">and</span>
               Piotr Doll{\\&#x27;{a}}r <span class="hljs-literal">and</span>
               Zhuowen Tu <span class="hljs-literal">and</span>
               Kaiming He},
  <span class="hljs-attr">title</span>     = {Aggregated Residual Transformations for Deep Neural Networks},
  <span class="hljs-attr">journal</span>   = {CoRR},
  <span class="hljs-attr">volume</span>    = {abs/<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">year</span>      = {<span class="hljs-number">2016</span>},
  <span class="hljs-attr">url</span>       = {http://arxiv.org/abs/<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">archivePrefix</span> = {arXiv},
  <span class="hljs-attr">eprint</span>    = {<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">timestamp</span> = {Mon, <span class="hljs-number">13</span> Aug <span class="hljs-number">2018</span> <span class="hljs-number">16</span>:<span class="hljs-number">45</span>:<span class="hljs-number">58</span> +<span class="hljs-number">0200</span>},
  <span class="hljs-attr">biburl</span>    = {https://dblp.org/<span class="hljs-keyword">rec</span>/journals/corr/XieGDTH16.bib},
  <span class="hljs-attr">bibsource</span> = {dblp computer science bibliography, https://dblp.org}
}`}}),{c(){v=l("meta"),bs=h(),$=l("h1"),P=l("a"),os=l("span"),u(C.$$.fragment),Ws=h(),rs=l("span"),st=i("(Gluon) ResNeXt"),vs=h(),f=l("p"),tt=i("A "),is=l("strong"),et=i("ResNeXt"),at=i(" repeats a "),D=l("a"),lt=i("building block"),nt=i(" that aggregates a set of transformations with the same topology. Compared to a "),X=l("a"),ot=i("ResNet"),rt=i(", it exposes a new dimension,  "),ps=l("em"),it=i("cardinality"),pt=i(" (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),$s=h(),A=l("p"),mt=i("The weights from this model were ported from "),z=l("a"),ht=i("Gluon"),ct=i("."),ys=h(),y=l("h2"),T=l("a"),ms=l("span"),u(L.$$.fragment),ft=h(),hs=l("span"),ut=i("How do I use this model on an image?"),xs=h(),V=l("p"),dt=i("To load a pretrained model:"),ks=h(),u(B.$$.fragment),Es=h(),W=l("p"),gt=i("To load and preprocess the image:"),Ps=h(),u(M.$$.fragment),As=h(),ss=l("p"),wt=i("To get the model predictions:"),Ts=h(),u(Y.$$.fragment),Ns=h(),ts=l("p"),_t=i("To get the top-5 predictions class names:"),Ss=h(),u(U.$$.fragment),qs=h(),N=l("p"),jt=i("Replace the model name with the variant you want to use, e.g. "),cs=l("code"),bt=i("gluon_resnext101_32x4d"),vt=i(". You can find the IDs in the model summaries at the top of this page."),Is=h(),S=l("p"),$t=i("To extract image features with this model, follow the "),es=l("a"),yt=i("timm feature extraction examples"),xt=i(", just change the name of the model you want to use."),Rs=h(),x=l("h2"),q=l("a"),fs=l("span"),u(O.$$.fragment),kt=h(),us=l("span"),Et=i("How do I finetune this model?"),Hs=h(),as=l("p"),Pt=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Gs=h(),u(F.$$.fragment),Cs=h(),I=l("p"),At=i("To finetune on your own dataset, you have to write a training loop or adapt "),K=l("a"),Tt=i(`timm\u2019s training
script`),Nt=i(" to use your dataset."),Ds=h(),k=l("h2"),R=l("a"),ds=l("span"),u(Z.$$.fragment),St=h(),gs=l("span"),qt=i("How do I train this model?"),Xs=h(),H=l("p"),It=i("You can follow the "),ls=l("a"),Rt=i("timm recipe scripts"),Ht=i(" for training a new model afresh."),zs=h(),E=l("h2"),G=l("a"),ws=l("span"),u(J.$$.fragment),Gt=h(),_s=l("span"),Ct=i("Citation"),Ls=h(),u(Q.$$.fragment),this.h()},l(s){const a=we('[data-svelte="svelte-1phssyn"]',document.head);v=n(a,"META",{name:!0,content:!0}),a.forEach(t),bs=c(s),$=n(s,"H1",{class:!0});var Ms=o($);P=n(Ms,"A",{id:!0,class:!0,href:!0});var Xt=o(P);os=n(Xt,"SPAN",{});var zt=o(os);d(C.$$.fragment,zt),zt.forEach(t),Xt.forEach(t),Ws=c(Ms),rs=n(Ms,"SPAN",{});var Lt=o(rs);st=p(Lt,"(Gluon) ResNeXt"),Lt.forEach(t),Ms.forEach(t),vs=c(s),f=n(s,"P",{});var b=o(f);tt=p(b,"A "),is=n(b,"STRONG",{});var Bt=o(is);et=p(Bt,"ResNeXt"),Bt.forEach(t),at=p(b," repeats a "),D=n(b,"A",{href:!0,rel:!0});var Mt=o(D);lt=p(Mt,"building block"),Mt.forEach(t),nt=p(b," that aggregates a set of transformations with the same topology. Compared to a "),X=n(b,"A",{href:!0,rel:!0});var Yt=o(X);ot=p(Yt,"ResNet"),Yt.forEach(t),rt=p(b,", it exposes a new dimension,  "),ps=n(b,"EM",{});var Ut=o(ps);it=p(Ut,"cardinality"),Ut.forEach(t),pt=p(b," (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),b.forEach(t),$s=c(s),A=n(s,"P",{});var Ys=o(A);mt=p(Ys,"The weights from this model were ported from "),z=n(Ys,"A",{href:!0,rel:!0});var Ot=o(z);ht=p(Ot,"Gluon"),Ot.forEach(t),ct=p(Ys,"."),Ys.forEach(t),ys=c(s),y=n(s,"H2",{class:!0});var Us=o(y);T=n(Us,"A",{id:!0,class:!0,href:!0});var Ft=o(T);ms=n(Ft,"SPAN",{});var Kt=o(ms);d(L.$$.fragment,Kt),Kt.forEach(t),Ft.forEach(t),ft=c(Us),hs=n(Us,"SPAN",{});var Zt=o(hs);ut=p(Zt,"How do I use this model on an image?"),Zt.forEach(t),Us.forEach(t),xs=c(s),V=n(s,"P",{});var Jt=o(V);dt=p(Jt,"To load a pretrained model:"),Jt.forEach(t),ks=c(s),d(B.$$.fragment,s),Es=c(s),W=n(s,"P",{});var Qt=o(W);gt=p(Qt,"To load and preprocess the image:"),Qt.forEach(t),Ps=c(s),d(M.$$.fragment,s),As=c(s),ss=n(s,"P",{});var Vt=o(ss);wt=p(Vt,"To get the model predictions:"),Vt.forEach(t),Ts=c(s),d(Y.$$.fragment,s),Ns=c(s),ts=n(s,"P",{});var Wt=o(ts);_t=p(Wt,"To get the top-5 predictions class names:"),Wt.forEach(t),Ss=c(s),d(U.$$.fragment,s),qs=c(s),N=n(s,"P",{});var Os=o(N);jt=p(Os,"Replace the model name with the variant you want to use, e.g. "),cs=n(Os,"CODE",{});var se=o(cs);bt=p(se,"gluon_resnext101_32x4d"),se.forEach(t),vt=p(Os,". You can find the IDs in the model summaries at the top of this page."),Os.forEach(t),Is=c(s),S=n(s,"P",{});var Fs=o(S);$t=p(Fs,"To extract image features with this model, follow the "),es=n(Fs,"A",{href:!0});var te=o(es);yt=p(te,"timm feature extraction examples"),te.forEach(t),xt=p(Fs,", just change the name of the model you want to use."),Fs.forEach(t),Rs=c(s),x=n(s,"H2",{class:!0});var Ks=o(x);q=n(Ks,"A",{id:!0,class:!0,href:!0});var ee=o(q);fs=n(ee,"SPAN",{});var ae=o(fs);d(O.$$.fragment,ae),ae.forEach(t),ee.forEach(t),kt=c(Ks),us=n(Ks,"SPAN",{});var le=o(us);Et=p(le,"How do I finetune this model?"),le.forEach(t),Ks.forEach(t),Hs=c(s),as=n(s,"P",{});var ne=o(as);Pt=p(ne,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),ne.forEach(t),Gs=c(s),d(F.$$.fragment,s),Cs=c(s),I=n(s,"P",{});var Zs=o(I);At=p(Zs,"To finetune on your own dataset, you have to write a training loop or adapt "),K=n(Zs,"A",{href:!0,rel:!0});var oe=o(K);Tt=p(oe,`timm\u2019s training
script`),oe.forEach(t),Nt=p(Zs," to use your dataset."),Zs.forEach(t),Ds=c(s),k=n(s,"H2",{class:!0});var Js=o(k);R=n(Js,"A",{id:!0,class:!0,href:!0});var re=o(R);ds=n(re,"SPAN",{});var ie=o(ds);d(Z.$$.fragment,ie),ie.forEach(t),re.forEach(t),St=c(Js),gs=n(Js,"SPAN",{});var pe=o(gs);qt=p(pe,"How do I train this model?"),pe.forEach(t),Js.forEach(t),Xs=c(s),H=n(s,"P",{});var Qs=o(H);It=p(Qs,"You can follow the "),ls=n(Qs,"A",{href:!0});var me=o(ls);Rt=p(me,"timm recipe scripts"),me.forEach(t),Ht=p(Qs," for training a new model afresh."),Qs.forEach(t),zs=c(s),E=n(s,"H2",{class:!0});var Vs=o(E);G=n(Vs,"A",{id:!0,class:!0,href:!0});var he=o(G);ws=n(he,"SPAN",{});var ce=o(ws);d(J.$$.fragment,ce),ce.forEach(t),he.forEach(t),Gt=c(Vs),_s=n(Vs,"SPAN",{});var fe=o(_s);Ct=p(fe,"Citation"),fe.forEach(t),Vs.forEach(t),Ls=c(s),d(Q.$$.fragment,s),this.h()},h(){m(v,"name","hf:doc:metadata"),m(v,"content",JSON.stringify(ve)),m(P,"id","gluon-resnext"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#gluon-resnext"),m($,"class","relative group"),m(D,"href","https://paperswithcode.com/method/resnext-block"),m(D,"rel","nofollow"),m(X,"href","https://paperswithcode.com/method/resnet"),m(X,"rel","nofollow"),m(z,"href","https://cv.gluon.ai/model_zoo/classification.html"),m(z,"rel","nofollow"),m(T,"id","how-do-i-use-this-model-on-an-image"),m(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(T,"href","#how-do-i-use-this-model-on-an-image"),m(y,"class","relative group"),m(es,"href","../feature_extraction"),m(q,"id","how-do-i-finetune-this-model"),m(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(q,"href","#how-do-i-finetune-this-model"),m(x,"class","relative group"),m(K,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(K,"rel","nofollow"),m(R,"id","how-do-i-train-this-model"),m(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(R,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(ls,"href","../scripts"),m(G,"id","citation"),m(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(G,"href","#citation"),m(E,"class","relative group")},m(s,a){e(document.head,v),r(s,bs,a),r(s,$,a),e($,P),e(P,os),g(C,os,null),e($,Ws),e($,rs),e(rs,st),r(s,vs,a),r(s,f,a),e(f,tt),e(f,is),e(is,et),e(f,at),e(f,D),e(D,lt),e(f,nt),e(f,X),e(X,ot),e(f,rt),e(f,ps),e(ps,it),e(f,pt),r(s,$s,a),r(s,A,a),e(A,mt),e(A,z),e(z,ht),e(A,ct),r(s,ys,a),r(s,y,a),e(y,T),e(T,ms),g(L,ms,null),e(y,ft),e(y,hs),e(hs,ut),r(s,xs,a),r(s,V,a),e(V,dt),r(s,ks,a),g(B,s,a),r(s,Es,a),r(s,W,a),e(W,gt),r(s,Ps,a),g(M,s,a),r(s,As,a),r(s,ss,a),e(ss,wt),r(s,Ts,a),g(Y,s,a),r(s,Ns,a),r(s,ts,a),e(ts,_t),r(s,Ss,a),g(U,s,a),r(s,qs,a),r(s,N,a),e(N,jt),e(N,cs),e(cs,bt),e(N,vt),r(s,Is,a),r(s,S,a),e(S,$t),e(S,es),e(es,yt),e(S,xt),r(s,Rs,a),r(s,x,a),e(x,q),e(q,fs),g(O,fs,null),e(x,kt),e(x,us),e(us,Et),r(s,Hs,a),r(s,as,a),e(as,Pt),r(s,Gs,a),g(F,s,a),r(s,Cs,a),r(s,I,a),e(I,At),e(I,K),e(K,Tt),e(I,Nt),r(s,Ds,a),r(s,k,a),e(k,R),e(R,ds),g(Z,ds,null),e(k,St),e(k,gs),e(gs,qt),r(s,Xs,a),r(s,H,a),e(H,It),e(H,ls),e(ls,Rt),e(H,Ht),r(s,zs,a),r(s,E,a),e(E,G),e(G,ws),g(J,ws,null),e(E,Gt),e(E,_s),e(_s,Ct),r(s,Ls,a),g(Q,s,a),Bs=!0},p:_e,i(s){Bs||(w(C.$$.fragment,s),w(L.$$.fragment,s),w(B.$$.fragment,s),w(M.$$.fragment,s),w(Y.$$.fragment,s),w(U.$$.fragment,s),w(O.$$.fragment,s),w(F.$$.fragment,s),w(Z.$$.fragment,s),w(J.$$.fragment,s),w(Q.$$.fragment,s),Bs=!0)},o(s){_(C.$$.fragment,s),_(L.$$.fragment,s),_(B.$$.fragment,s),_(M.$$.fragment,s),_(Y.$$.fragment,s),_(U.$$.fragment,s),_(O.$$.fragment,s),_(F.$$.fragment,s),_(Z.$$.fragment,s),_(J.$$.fragment,s),_(Q.$$.fragment,s),Bs=!1},d(s){t(v),s&&t(bs),s&&t($),j(C),s&&t(vs),s&&t(f),s&&t($s),s&&t(A),s&&t(ys),s&&t(y),j(L),s&&t(xs),s&&t(V),s&&t(ks),j(B,s),s&&t(Es),s&&t(W),s&&t(Ps),j(M,s),s&&t(As),s&&t(ss),s&&t(Ts),j(Y,s),s&&t(Ns),s&&t(ts),s&&t(Ss),j(U,s),s&&t(qs),s&&t(N),s&&t(Is),s&&t(S),s&&t(Rs),s&&t(x),j(O),s&&t(Hs),s&&t(as),s&&t(Gs),j(F,s),s&&t(Cs),s&&t(I),s&&t(Ds),s&&t(k),j(Z),s&&t(Xs),s&&t(H),s&&t(zs),s&&t(E),j(J),s&&t(Ls),j(Q,s)}}}const ve={local:"gluon-resnext",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"(Gluon) ResNeXt"};function $e(Dt){return je(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ee extends ue{constructor(v){super();de(this,v,$e,be,ge,{})}}export{Ee as default,ve as metadata};
