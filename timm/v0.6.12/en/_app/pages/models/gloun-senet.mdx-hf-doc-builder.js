import{S as ls,i as rs,s as is,e as n,k as h,w as f,t as i,M as ps,c as o,d as t,m as c,a as l,x as u,h as p,b as m,G as s,g as r,y as d,L as ms,q as g,o as w,B as _,v as hs}from"../../chunks/vendor-hf-doc-builder.js";import{I as we}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";function cs(Tt){let j,_e,$,x,oe,C,Ke,le,Qe,ve,v,Ze,re,et,tt,G,st,at,je,S,nt,L,ot,lt,$e,b,P,ie,R,rt,pe,it,be,X,pt,ye,Y,ke,K,mt,Ee,M,xe,Q,ht,Se,U,Pe,Z,ct,qe,B,Ae,q,ft,me,ut,dt,Ne,A,gt,ee,wt,_t,Te,y,N,he,D,vt,ce,jt,Ie,te,$t,He,J,ze,T,bt,O,yt,kt,Ce,k,I,fe,F,Et,ue,xt,Ge,H,St,se,Pt,qt,Le,E,z,de,V,At,ge,Nt,Re,W,Ye;return C=new we({}),R=new we({}),Y=new ne({props:{code:`import timm
model = timm.create_model('gluon_senet154', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_senet154&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),M=new ne({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),U=new ne({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),B=new ne({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),D=new we({}),J=new ne({props:{code:"model = timm.create_model('gluon_senet154', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;gluon_senet154&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),F=new we({}),V=new we({}),W=new ne({props:{code:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-and-Excitation Networks}, 
      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
      year={2019},
      eprint={1709.01507},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-<span class="hljs-keyword">and-Excitation </span>Networks}, 
      author={<span class="hljs-keyword">Jie </span>Hu <span class="hljs-keyword">and </span>Li <span class="hljs-keyword">Shen </span><span class="hljs-keyword">and </span>Samuel Albanie <span class="hljs-keyword">and </span>Gang Sun <span class="hljs-keyword">and </span>Enhua Wu},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1709</span>.<span class="hljs-number">01507</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){j=n("meta"),_e=h(),$=n("h1"),x=n("a"),oe=n("span"),f(C.$$.fragment),Ke=h(),le=n("span"),Qe=i("(Gluon) SENet"),ve=h(),v=n("p"),Ze=i("A "),re=n("strong"),et=i("SENet"),tt=i(" is a convolutional neural network architecture that employs "),G=n("a"),st=i("squeeze-and-excitation blocks"),at=i(" to enable the network to perform dynamic channel-wise feature recalibration."),je=h(),S=n("p"),nt=i("The weights from this model were ported from "),L=n("a"),ot=i("Gluon"),lt=i("."),$e=h(),b=n("h2"),P=n("a"),ie=n("span"),f(R.$$.fragment),rt=h(),pe=n("span"),it=i("How do I use this model on an image?"),be=h(),X=n("p"),pt=i("To load a pretrained model:"),ye=h(),f(Y.$$.fragment),ke=h(),K=n("p"),mt=i("To load and preprocess the image:"),Ee=h(),f(M.$$.fragment),xe=h(),Q=n("p"),ht=i("To get the model predictions:"),Se=h(),f(U.$$.fragment),Pe=h(),Z=n("p"),ct=i("To get the top-5 predictions class names:"),qe=h(),f(B.$$.fragment),Ae=h(),q=n("p"),ft=i("Replace the model name with the variant you want to use, e.g. "),me=n("code"),ut=i("gluon_senet154"),dt=i(". You can find the IDs in the model summaries at the top of this page."),Ne=h(),A=n("p"),gt=i("To extract image features with this model, follow the "),ee=n("a"),wt=i("timm feature extraction examples"),_t=i(", just change the name of the model you want to use."),Te=h(),y=n("h2"),N=n("a"),he=n("span"),f(D.$$.fragment),vt=h(),ce=n("span"),jt=i("How do I finetune this model?"),Ie=h(),te=n("p"),$t=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),He=h(),f(J.$$.fragment),ze=h(),T=n("p"),bt=i("To finetune on your own dataset, you have to write a training loop or adapt "),O=n("a"),yt=i(`timm\u2019s training
script`),kt=i(" to use your dataset."),Ce=h(),k=n("h2"),I=n("a"),fe=n("span"),f(F.$$.fragment),Et=h(),ue=n("span"),xt=i("How do I train this model?"),Ge=h(),H=n("p"),St=i("You can follow the "),se=n("a"),Pt=i("timm recipe scripts"),qt=i(" for training a new model afresh."),Le=h(),E=n("h2"),z=n("a"),de=n("span"),f(V.$$.fragment),At=h(),ge=n("span"),Nt=i("Citation"),Re=h(),f(W.$$.fragment),this.h()},l(e){const a=ps('[data-svelte="svelte-1phssyn"]',document.head);j=o(a,"META",{name:!0,content:!0}),a.forEach(t),_e=c(e),$=o(e,"H1",{class:!0});var Me=l($);x=o(Me,"A",{id:!0,class:!0,href:!0});var It=l(x);oe=o(It,"SPAN",{});var Ht=l(oe);u(C.$$.fragment,Ht),Ht.forEach(t),It.forEach(t),Ke=c(Me),le=o(Me,"SPAN",{});var zt=l(le);Qe=p(zt,"(Gluon) SENet"),zt.forEach(t),Me.forEach(t),ve=c(e),v=o(e,"P",{});var ae=l(v);Ze=p(ae,"A "),re=o(ae,"STRONG",{});var Ct=l(re);et=p(Ct,"SENet"),Ct.forEach(t),tt=p(ae," is a convolutional neural network architecture that employs "),G=o(ae,"A",{href:!0,rel:!0});var Gt=l(G);st=p(Gt,"squeeze-and-excitation blocks"),Gt.forEach(t),at=p(ae," to enable the network to perform dynamic channel-wise feature recalibration."),ae.forEach(t),je=c(e),S=o(e,"P",{});var Ue=l(S);nt=p(Ue,"The weights from this model were ported from "),L=o(Ue,"A",{href:!0,rel:!0});var Lt=l(L);ot=p(Lt,"Gluon"),Lt.forEach(t),lt=p(Ue,"."),Ue.forEach(t),$e=c(e),b=o(e,"H2",{class:!0});var Be=l(b);P=o(Be,"A",{id:!0,class:!0,href:!0});var Rt=l(P);ie=o(Rt,"SPAN",{});var Yt=l(ie);u(R.$$.fragment,Yt),Yt.forEach(t),Rt.forEach(t),rt=c(Be),pe=o(Be,"SPAN",{});var Mt=l(pe);it=p(Mt,"How do I use this model on an image?"),Mt.forEach(t),Be.forEach(t),be=c(e),X=o(e,"P",{});var Ut=l(X);pt=p(Ut,"To load a pretrained model:"),Ut.forEach(t),ye=c(e),u(Y.$$.fragment,e),ke=c(e),K=o(e,"P",{});var Bt=l(K);mt=p(Bt,"To load and preprocess the image:"),Bt.forEach(t),Ee=c(e),u(M.$$.fragment,e),xe=c(e),Q=o(e,"P",{});var Dt=l(Q);ht=p(Dt,"To get the model predictions:"),Dt.forEach(t),Se=c(e),u(U.$$.fragment,e),Pe=c(e),Z=o(e,"P",{});var Jt=l(Z);ct=p(Jt,"To get the top-5 predictions class names:"),Jt.forEach(t),qe=c(e),u(B.$$.fragment,e),Ae=c(e),q=o(e,"P",{});var De=l(q);ft=p(De,"Replace the model name with the variant you want to use, e.g. "),me=o(De,"CODE",{});var Ot=l(me);ut=p(Ot,"gluon_senet154"),Ot.forEach(t),dt=p(De,". You can find the IDs in the model summaries at the top of this page."),De.forEach(t),Ne=c(e),A=o(e,"P",{});var Je=l(A);gt=p(Je,"To extract image features with this model, follow the "),ee=o(Je,"A",{href:!0});var Ft=l(ee);wt=p(Ft,"timm feature extraction examples"),Ft.forEach(t),_t=p(Je,", just change the name of the model you want to use."),Je.forEach(t),Te=c(e),y=o(e,"H2",{class:!0});var Oe=l(y);N=o(Oe,"A",{id:!0,class:!0,href:!0});var Vt=l(N);he=o(Vt,"SPAN",{});var Wt=l(he);u(D.$$.fragment,Wt),Wt.forEach(t),Vt.forEach(t),vt=c(Oe),ce=o(Oe,"SPAN",{});var Xt=l(ce);jt=p(Xt,"How do I finetune this model?"),Xt.forEach(t),Oe.forEach(t),Ie=c(e),te=o(e,"P",{});var Kt=l(te);$t=p(Kt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Kt.forEach(t),He=c(e),u(J.$$.fragment,e),ze=c(e),T=o(e,"P",{});var Fe=l(T);bt=p(Fe,"To finetune on your own dataset, you have to write a training loop or adapt "),O=o(Fe,"A",{href:!0,rel:!0});var Qt=l(O);yt=p(Qt,`timm\u2019s training
script`),Qt.forEach(t),kt=p(Fe," to use your dataset."),Fe.forEach(t),Ce=c(e),k=o(e,"H2",{class:!0});var Ve=l(k);I=o(Ve,"A",{id:!0,class:!0,href:!0});var Zt=l(I);fe=o(Zt,"SPAN",{});var es=l(fe);u(F.$$.fragment,es),es.forEach(t),Zt.forEach(t),Et=c(Ve),ue=o(Ve,"SPAN",{});var ts=l(ue);xt=p(ts,"How do I train this model?"),ts.forEach(t),Ve.forEach(t),Ge=c(e),H=o(e,"P",{});var We=l(H);St=p(We,"You can follow the "),se=o(We,"A",{href:!0});var ss=l(se);Pt=p(ss,"timm recipe scripts"),ss.forEach(t),qt=p(We," for training a new model afresh."),We.forEach(t),Le=c(e),E=o(e,"H2",{class:!0});var Xe=l(E);z=o(Xe,"A",{id:!0,class:!0,href:!0});var as=l(z);de=o(as,"SPAN",{});var ns=l(de);u(V.$$.fragment,ns),ns.forEach(t),as.forEach(t),At=c(Xe),ge=o(Xe,"SPAN",{});var os=l(ge);Nt=p(os,"Citation"),os.forEach(t),Xe.forEach(t),Re=c(e),u(W.$$.fragment,e),this.h()},h(){m(j,"name","hf:doc:metadata"),m(j,"content",JSON.stringify(fs)),m(x,"id","gluon-senet"),m(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(x,"href","#gluon-senet"),m($,"class","relative group"),m(G,"href","https://paperswithcode.com/method/squeeze-and-excitation-block"),m(G,"rel","nofollow"),m(L,"href","https://cv.gluon.ai/model_zoo/classification.html"),m(L,"rel","nofollow"),m(P,"id","how-do-i-use-this-model-on-an-image"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#how-do-i-use-this-model-on-an-image"),m(b,"class","relative group"),m(ee,"href","../feature_extraction"),m(N,"id","how-do-i-finetune-this-model"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#how-do-i-finetune-this-model"),m(y,"class","relative group"),m(O,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(O,"rel","nofollow"),m(I,"id","how-do-i-train-this-model"),m(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(I,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(se,"href","../scripts"),m(z,"id","citation"),m(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(z,"href","#citation"),m(E,"class","relative group")},m(e,a){s(document.head,j),r(e,_e,a),r(e,$,a),s($,x),s(x,oe),d(C,oe,null),s($,Ke),s($,le),s(le,Qe),r(e,ve,a),r(e,v,a),s(v,Ze),s(v,re),s(re,et),s(v,tt),s(v,G),s(G,st),s(v,at),r(e,je,a),r(e,S,a),s(S,nt),s(S,L),s(L,ot),s(S,lt),r(e,$e,a),r(e,b,a),s(b,P),s(P,ie),d(R,ie,null),s(b,rt),s(b,pe),s(pe,it),r(e,be,a),r(e,X,a),s(X,pt),r(e,ye,a),d(Y,e,a),r(e,ke,a),r(e,K,a),s(K,mt),r(e,Ee,a),d(M,e,a),r(e,xe,a),r(e,Q,a),s(Q,ht),r(e,Se,a),d(U,e,a),r(e,Pe,a),r(e,Z,a),s(Z,ct),r(e,qe,a),d(B,e,a),r(e,Ae,a),r(e,q,a),s(q,ft),s(q,me),s(me,ut),s(q,dt),r(e,Ne,a),r(e,A,a),s(A,gt),s(A,ee),s(ee,wt),s(A,_t),r(e,Te,a),r(e,y,a),s(y,N),s(N,he),d(D,he,null),s(y,vt),s(y,ce),s(ce,jt),r(e,Ie,a),r(e,te,a),s(te,$t),r(e,He,a),d(J,e,a),r(e,ze,a),r(e,T,a),s(T,bt),s(T,O),s(O,yt),s(T,kt),r(e,Ce,a),r(e,k,a),s(k,I),s(I,fe),d(F,fe,null),s(k,Et),s(k,ue),s(ue,xt),r(e,Ge,a),r(e,H,a),s(H,St),s(H,se),s(se,Pt),s(H,qt),r(e,Le,a),r(e,E,a),s(E,z),s(z,de),d(V,de,null),s(E,At),s(E,ge),s(ge,Nt),r(e,Re,a),d(W,e,a),Ye=!0},p:ms,i(e){Ye||(g(C.$$.fragment,e),g(R.$$.fragment,e),g(Y.$$.fragment,e),g(M.$$.fragment,e),g(U.$$.fragment,e),g(B.$$.fragment,e),g(D.$$.fragment,e),g(J.$$.fragment,e),g(F.$$.fragment,e),g(V.$$.fragment,e),g(W.$$.fragment,e),Ye=!0)},o(e){w(C.$$.fragment,e),w(R.$$.fragment,e),w(Y.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(B.$$.fragment,e),w(D.$$.fragment,e),w(J.$$.fragment,e),w(F.$$.fragment,e),w(V.$$.fragment,e),w(W.$$.fragment,e),Ye=!1},d(e){t(j),e&&t(_e),e&&t($),_(C),e&&t(ve),e&&t(v),e&&t(je),e&&t(S),e&&t($e),e&&t(b),_(R),e&&t(be),e&&t(X),e&&t(ye),_(Y,e),e&&t(ke),e&&t(K),e&&t(Ee),_(M,e),e&&t(xe),e&&t(Q),e&&t(Se),_(U,e),e&&t(Pe),e&&t(Z),e&&t(qe),_(B,e),e&&t(Ae),e&&t(q),e&&t(Ne),e&&t(A),e&&t(Te),e&&t(y),_(D),e&&t(Ie),e&&t(te),e&&t(He),_(J,e),e&&t(ze),e&&t(T),e&&t(Ce),e&&t(k),_(F),e&&t(Ge),e&&t(H),e&&t(Le),e&&t(E),_(V),e&&t(Re),_(W,e)}}}const fs={local:"gluon-senet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"(Gluon) SENet"};function us(Tt){return hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _s extends ls{constructor(j){super();rs(this,j,us,cs,is,{})}}export{_s as default,fs as metadata};
