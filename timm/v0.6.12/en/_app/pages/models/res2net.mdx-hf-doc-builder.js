import{S as Qs,i as Vs,s as Ws,e as l,k as p,w as f,t as m,M as ea,c as n,d as s,m as h,a as o,x as u,h as c,b as i,G as a,g as r,y as g,L as sa,q as d,o as v,B as w,v as aa}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as se}from"../../chunks/CodeBlock-hf-doc-builder.js";function ta(ks){let _,de,b,x,ae,M,Ze,te,Ke,ve,j,le,Xe,Je,R,Qe,Ve,we,$,I,ne,C,We,oe,es,_e,K,ss,be,G,je,X,as,$e,Y,ye,J,ts,ke,B,Ee,Q,ls,xe,L,Ie,N,ns,re,os,rs,Ne,P,is,V,ps,ms,Pe,y,S,ie,z,hs,pe,cs,Se,W,fs,Te,D,Ae,T,us,O,gs,ds,qe,k,A,me,U,vs,he,ws,He,q,_s,ee,bs,js,Me,E,H,ce,F,$s,fe,ys,Re,Z,Ce;return M=new ge({}),C=new ge({}),G=new se({props:{code:`import timm
model = timm.create_model('res2net101_26w_4s', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;res2net101_26w_4s&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new se({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),B=new se({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),L=new se({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),z=new ge({}),D=new se({props:{code:"model = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;res2net101_26w_4s&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),U=new ge({}),F=new ge({}),Z=new se({props:{code:`@article{Gao_2021,
   title={Res2Net: A New Multi-Scale Backbone Architecture},
   volume={43},
   ISSN={1939-3539},
   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},
   DOI={10.1109/tpami.2019.2938758},
   number={2},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},
   year={2021},
   month={Feb},
   pages={652\u2013662}
}`,highlighted:`<span class="language-xml">@article</span><span class="hljs-template-variable">{Gao_2021,
   title={Res2Net: A New Multi-Scale Backbone Architecture}</span><span class="language-xml">,
   volume=</span><span class="hljs-template-variable">{43}</span><span class="language-xml">,
   ISSN=</span><span class="hljs-template-variable">{1939-3539}</span><span class="language-xml">,
   url=</span><span class="hljs-template-variable">{http://dx.doi.org/10.1109/TPAMI.2019.2938758}</span><span class="language-xml">,
   DOI=</span><span class="hljs-template-variable">{10.1109/tpami.2019.2938758}</span><span class="language-xml">,
   number=</span><span class="hljs-template-variable">{2}</span><span class="language-xml">,
   journal=</span><span class="hljs-template-variable">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="language-xml">,
   publisher=</span><span class="hljs-template-variable">{Institute of Electrical and Electronics Engineers (IEEE)}</span><span class="language-xml">,
   author=</span><span class="hljs-template-variable">{Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip}</span><span class="language-xml">,
   year=</span><span class="hljs-template-variable">{2021}</span><span class="language-xml">,
   month=</span><span class="hljs-template-variable">{Feb}</span><span class="language-xml">,
   pages=</span><span class="hljs-template-variable">{652\u2013662}</span><span class="language-xml">
}</span>`}}),{c(){_=l("meta"),de=p(),b=l("h1"),x=l("a"),ae=l("span"),f(M.$$.fragment),Ze=p(),te=l("span"),Ke=m("Res2Net"),ve=p(),j=l("p"),le=l("strong"),Xe=m("Res2Net"),Je=m(" is an image model that employs a variation on bottleneck residual blocks, "),R=l("a"),Qe=m("Res2Net Blocks"),Ve=m(". The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer."),we=p(),$=l("h2"),I=l("a"),ne=l("span"),f(C.$$.fragment),We=p(),oe=l("span"),es=m("How do I use this model on an image?"),_e=p(),K=l("p"),ss=m("To load a pretrained model:"),be=p(),f(G.$$.fragment),je=p(),X=l("p"),as=m("To load and preprocess the image:"),$e=p(),f(Y.$$.fragment),ye=p(),J=l("p"),ts=m("To get the model predictions:"),ke=p(),f(B.$$.fragment),Ee=p(),Q=l("p"),ls=m("To get the top-5 predictions class names:"),xe=p(),f(L.$$.fragment),Ie=p(),N=l("p"),ns=m("Replace the model name with the variant you want to use, e.g. "),re=l("code"),os=m("res2net101_26w_4s"),rs=m(". You can find the IDs in the model summaries at the top of this page."),Ne=p(),P=l("p"),is=m("To extract image features with this model, follow the "),V=l("a"),ps=m("timm feature extraction examples"),ms=m(", just change the name of the model you want to use."),Pe=p(),y=l("h2"),S=l("a"),ie=l("span"),f(z.$$.fragment),hs=p(),pe=l("span"),cs=m("How do I finetune this model?"),Se=p(),W=l("p"),fs=m("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Te=p(),f(D.$$.fragment),Ae=p(),T=l("p"),us=m("To finetune on your own dataset, you have to write a training loop or adapt "),O=l("a"),gs=m(`timm\u2019s training
script`),ds=m(" to use your dataset."),qe=p(),k=l("h2"),A=l("a"),me=l("span"),f(U.$$.fragment),vs=p(),he=l("span"),ws=m("How do I train this model?"),He=p(),q=l("p"),_s=m("You can follow the "),ee=l("a"),bs=m("timm recipe scripts"),js=m(" for training a new model afresh."),Me=p(),E=l("h2"),H=l("a"),ce=l("span"),f(F.$$.fragment),$s=p(),fe=l("span"),ys=m("Citation"),Re=p(),f(Z.$$.fragment),this.h()},l(e){const t=ea('[data-svelte="svelte-1phssyn"]',document.head);_=n(t,"META",{name:!0,content:!0}),t.forEach(s),de=h(e),b=n(e,"H1",{class:!0});var Ge=o(b);x=n(Ge,"A",{id:!0,class:!0,href:!0});var Es=o(x);ae=n(Es,"SPAN",{});var xs=o(ae);u(M.$$.fragment,xs),xs.forEach(s),Es.forEach(s),Ze=h(Ge),te=n(Ge,"SPAN",{});var Is=o(te);Ke=c(Is,"Res2Net"),Is.forEach(s),Ge.forEach(s),ve=h(e),j=n(e,"P",{});var ue=o(j);le=n(ue,"STRONG",{});var Ns=o(le);Xe=c(Ns,"Res2Net"),Ns.forEach(s),Je=c(ue," is an image model that employs a variation on bottleneck residual blocks, "),R=n(ue,"A",{href:!0,rel:!0});var Ps=o(R);Qe=c(Ps,"Res2Net Blocks"),Ps.forEach(s),Ve=c(ue,". The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer."),ue.forEach(s),we=h(e),$=n(e,"H2",{class:!0});var Ye=o($);I=n(Ye,"A",{id:!0,class:!0,href:!0});var Ss=o(I);ne=n(Ss,"SPAN",{});var Ts=o(ne);u(C.$$.fragment,Ts),Ts.forEach(s),Ss.forEach(s),We=h(Ye),oe=n(Ye,"SPAN",{});var As=o(oe);es=c(As,"How do I use this model on an image?"),As.forEach(s),Ye.forEach(s),_e=h(e),K=n(e,"P",{});var qs=o(K);ss=c(qs,"To load a pretrained model:"),qs.forEach(s),be=h(e),u(G.$$.fragment,e),je=h(e),X=n(e,"P",{});var Hs=o(X);as=c(Hs,"To load and preprocess the image:"),Hs.forEach(s),$e=h(e),u(Y.$$.fragment,e),ye=h(e),J=n(e,"P",{});var Ms=o(J);ts=c(Ms,"To get the model predictions:"),Ms.forEach(s),ke=h(e),u(B.$$.fragment,e),Ee=h(e),Q=n(e,"P",{});var Rs=o(Q);ls=c(Rs,"To get the top-5 predictions class names:"),Rs.forEach(s),xe=h(e),u(L.$$.fragment,e),Ie=h(e),N=n(e,"P",{});var Be=o(N);ns=c(Be,"Replace the model name with the variant you want to use, e.g. "),re=n(Be,"CODE",{});var Cs=o(re);os=c(Cs,"res2net101_26w_4s"),Cs.forEach(s),rs=c(Be,". You can find the IDs in the model summaries at the top of this page."),Be.forEach(s),Ne=h(e),P=n(e,"P",{});var Le=o(P);is=c(Le,"To extract image features with this model, follow the "),V=n(Le,"A",{href:!0});var Gs=o(V);ps=c(Gs,"timm feature extraction examples"),Gs.forEach(s),ms=c(Le,", just change the name of the model you want to use."),Le.forEach(s),Pe=h(e),y=n(e,"H2",{class:!0});var ze=o(y);S=n(ze,"A",{id:!0,class:!0,href:!0});var Ys=o(S);ie=n(Ys,"SPAN",{});var Bs=o(ie);u(z.$$.fragment,Bs),Bs.forEach(s),Ys.forEach(s),hs=h(ze),pe=n(ze,"SPAN",{});var Ls=o(pe);cs=c(Ls,"How do I finetune this model?"),Ls.forEach(s),ze.forEach(s),Se=h(e),W=n(e,"P",{});var zs=o(W);fs=c(zs,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),zs.forEach(s),Te=h(e),u(D.$$.fragment,e),Ae=h(e),T=n(e,"P",{});var De=o(T);us=c(De,"To finetune on your own dataset, you have to write a training loop or adapt "),O=n(De,"A",{href:!0,rel:!0});var Ds=o(O);gs=c(Ds,`timm\u2019s training
script`),Ds.forEach(s),ds=c(De," to use your dataset."),De.forEach(s),qe=h(e),k=n(e,"H2",{class:!0});var Oe=o(k);A=n(Oe,"A",{id:!0,class:!0,href:!0});var Os=o(A);me=n(Os,"SPAN",{});var Us=o(me);u(U.$$.fragment,Us),Us.forEach(s),Os.forEach(s),vs=h(Oe),he=n(Oe,"SPAN",{});var Fs=o(he);ws=c(Fs,"How do I train this model?"),Fs.forEach(s),Oe.forEach(s),He=h(e),q=n(e,"P",{});var Ue=o(q);_s=c(Ue,"You can follow the "),ee=n(Ue,"A",{href:!0});var Zs=o(ee);bs=c(Zs,"timm recipe scripts"),Zs.forEach(s),js=c(Ue," for training a new model afresh."),Ue.forEach(s),Me=h(e),E=n(e,"H2",{class:!0});var Fe=o(E);H=n(Fe,"A",{id:!0,class:!0,href:!0});var Ks=o(H);ce=n(Ks,"SPAN",{});var Xs=o(ce);u(F.$$.fragment,Xs),Xs.forEach(s),Ks.forEach(s),$s=h(Fe),fe=n(Fe,"SPAN",{});var Js=o(fe);ys=c(Js,"Citation"),Js.forEach(s),Fe.forEach(s),Re=h(e),u(Z.$$.fragment,e),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(la)),i(x,"id","res2net"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#res2net"),i(b,"class","relative group"),i(R,"href","https://paperswithcode.com/method/res2net-block"),i(R,"rel","nofollow"),i(I,"id","how-do-i-use-this-model-on-an-image"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(V,"href","../feature_extraction"),i(S,"id","how-do-i-finetune-this-model"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(O,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(O,"rel","nofollow"),i(A,"id","how-do-i-train-this-model"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(ee,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(E,"class","relative group")},m(e,t){a(document.head,_),r(e,de,t),r(e,b,t),a(b,x),a(x,ae),g(M,ae,null),a(b,Ze),a(b,te),a(te,Ke),r(e,ve,t),r(e,j,t),a(j,le),a(le,Xe),a(j,Je),a(j,R),a(R,Qe),a(j,Ve),r(e,we,t),r(e,$,t),a($,I),a(I,ne),g(C,ne,null),a($,We),a($,oe),a(oe,es),r(e,_e,t),r(e,K,t),a(K,ss),r(e,be,t),g(G,e,t),r(e,je,t),r(e,X,t),a(X,as),r(e,$e,t),g(Y,e,t),r(e,ye,t),r(e,J,t),a(J,ts),r(e,ke,t),g(B,e,t),r(e,Ee,t),r(e,Q,t),a(Q,ls),r(e,xe,t),g(L,e,t),r(e,Ie,t),r(e,N,t),a(N,ns),a(N,re),a(re,os),a(N,rs),r(e,Ne,t),r(e,P,t),a(P,is),a(P,V),a(V,ps),a(P,ms),r(e,Pe,t),r(e,y,t),a(y,S),a(S,ie),g(z,ie,null),a(y,hs),a(y,pe),a(pe,cs),r(e,Se,t),r(e,W,t),a(W,fs),r(e,Te,t),g(D,e,t),r(e,Ae,t),r(e,T,t),a(T,us),a(T,O),a(O,gs),a(T,ds),r(e,qe,t),r(e,k,t),a(k,A),a(A,me),g(U,me,null),a(k,vs),a(k,he),a(he,ws),r(e,He,t),r(e,q,t),a(q,_s),a(q,ee),a(ee,bs),a(q,js),r(e,Me,t),r(e,E,t),a(E,H),a(H,ce),g(F,ce,null),a(E,$s),a(E,fe),a(fe,ys),r(e,Re,t),g(Z,e,t),Ce=!0},p:sa,i(e){Ce||(d(M.$$.fragment,e),d(C.$$.fragment,e),d(G.$$.fragment,e),d(Y.$$.fragment,e),d(B.$$.fragment,e),d(L.$$.fragment,e),d(z.$$.fragment,e),d(D.$$.fragment,e),d(U.$$.fragment,e),d(F.$$.fragment,e),d(Z.$$.fragment,e),Ce=!0)},o(e){v(M.$$.fragment,e),v(C.$$.fragment,e),v(G.$$.fragment,e),v(Y.$$.fragment,e),v(B.$$.fragment,e),v(L.$$.fragment,e),v(z.$$.fragment,e),v(D.$$.fragment,e),v(U.$$.fragment,e),v(F.$$.fragment,e),v(Z.$$.fragment,e),Ce=!1},d(e){s(_),e&&s(de),e&&s(b),w(M),e&&s(ve),e&&s(j),e&&s(we),e&&s($),w(C),e&&s(_e),e&&s(K),e&&s(be),w(G,e),e&&s(je),e&&s(X),e&&s($e),w(Y,e),e&&s(ye),e&&s(J),e&&s(ke),w(B,e),e&&s(Ee),e&&s(Q),e&&s(xe),w(L,e),e&&s(Ie),e&&s(N),e&&s(Ne),e&&s(P),e&&s(Pe),e&&s(y),w(z),e&&s(Se),e&&s(W),e&&s(Te),w(D,e),e&&s(Ae),e&&s(T),e&&s(qe),e&&s(k),w(U),e&&s(He),e&&s(q),e&&s(Me),e&&s(E),w(F),e&&s(Re),w(Z,e)}}}const la={local:"res2net",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Res2Net"};function na(ks){return aa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pa extends Qs{constructor(_){super();Vs(this,_,na,ta,Ws,{})}}export{pa as default,la as metadata};
