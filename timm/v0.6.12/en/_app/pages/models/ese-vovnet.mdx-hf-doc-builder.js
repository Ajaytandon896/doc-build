import{S as os,i as ls,s as rs,e as n,k as h,w as f,t as p,M as is,c as o,d as t,m as c,a as l,x as u,h as m,b as i,G as s,g as r,y as g,L as ps,q as d,o as w,B as v,v as ms}from"../../chunks/vendor-hf-doc-builder.js";import{I as we}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function hs(Tt){let _,ve,j,x,ne,V,Qe,oe,We,_e,b,le,Ze,et,L,tt,st,je,P,at,R,nt,ot,be,$,S,re,Y,lt,ie,rt,$e,K,it,ye,z,ke,Q,pt,Ee,B,xe,W,mt,Pe,G,Se,Z,ht,Ne,D,Ae,N,ct,pe,ft,ut,Te,A,gt,ee,dt,wt,qe,y,T,me,U,vt,he,_t,Ie,te,jt,Ce,J,He,q,bt,M,$t,yt,Ve,k,I,ce,O,kt,fe,Et,Le,C,xt,se,Pt,St,Re,E,H,ue,F,Nt,ge,At,Ye,X,ze;return V=new we({}),Y=new we({}),z=new ae({props:{code:`import timm
model = timm.create_model('ese_vovnet19b_dw', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ese_vovnet19b_dw&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),B=new ae({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),G=new ae({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),D=new ae({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),U=new we({}),J=new ae({props:{code:"model = timm.create_model('ese_vovnet19b_dw', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ese_vovnet19b_dw&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),O=new we({}),F=new we({}),X=new ae({props:{code:`@misc{lee2019energy,
      title={An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection}, 
      author={Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park},
      year={2019},
      eprint={1904.09730},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{lee2019energy,
      title={An Energy <span class="hljs-keyword">and </span>GPU-Computation Efficient <span class="hljs-keyword">Backbone </span>Network for Real-Time Object Detection}, 
      author={Youngwan Lee <span class="hljs-keyword">and </span><span class="hljs-keyword">Joong-won </span>Hwang <span class="hljs-keyword">and </span>Sangrok Lee <span class="hljs-keyword">and </span>Yuseok <span class="hljs-keyword">Bae </span><span class="hljs-keyword">and </span><span class="hljs-keyword">Jongyoul </span>Park},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1904</span>.<span class="hljs-number">09730</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){_=n("meta"),ve=h(),j=n("h1"),x=n("a"),ne=n("span"),f(V.$$.fragment),Qe=h(),oe=n("span"),We=p("ESE-VoVNet"),_e=h(),b=n("p"),le=n("strong"),Ze=p("VoVNet"),et=p(" is a convolutional neural network that seeks to make "),L=n("a"),tt=p("DenseNet"),st=p(" more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel."),je=h(),P=n("p"),at=p("Read about "),R=n("a"),nt=p("one-shot aggregation here"),ot=p("."),be=h(),$=n("h2"),S=n("a"),re=n("span"),f(Y.$$.fragment),lt=h(),ie=n("span"),rt=p("How do I use this model on an image?"),$e=h(),K=n("p"),it=p("To load a pretrained model:"),ye=h(),f(z.$$.fragment),ke=h(),Q=n("p"),pt=p("To load and preprocess the image:"),Ee=h(),f(B.$$.fragment),xe=h(),W=n("p"),mt=p("To get the model predictions:"),Pe=h(),f(G.$$.fragment),Se=h(),Z=n("p"),ht=p("To get the top-5 predictions class names:"),Ne=h(),f(D.$$.fragment),Ae=h(),N=n("p"),ct=p("Replace the model name with the variant you want to use, e.g. "),pe=n("code"),ft=p("ese_vovnet19b_dw"),ut=p(". You can find the IDs in the model summaries at the top of this page."),Te=h(),A=n("p"),gt=p("To extract image features with this model, follow the "),ee=n("a"),dt=p("timm feature extraction examples"),wt=p(", just change the name of the model you want to use."),qe=h(),y=n("h2"),T=n("a"),me=n("span"),f(U.$$.fragment),vt=h(),he=n("span"),_t=p("How do I finetune this model?"),Ie=h(),te=n("p"),jt=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ce=h(),f(J.$$.fragment),He=h(),q=n("p"),bt=p("To finetune on your own dataset, you have to write a training loop or adapt "),M=n("a"),$t=p(`timm\u2019s training
script`),yt=p(" to use your dataset."),Ve=h(),k=n("h2"),I=n("a"),ce=n("span"),f(O.$$.fragment),kt=h(),fe=n("span"),Et=p("How do I train this model?"),Le=h(),C=n("p"),xt=p("You can follow the "),se=n("a"),Pt=p("timm recipe scripts"),St=p(" for training a new model afresh."),Re=h(),E=n("h2"),H=n("a"),ue=n("span"),f(F.$$.fragment),Nt=h(),ge=n("span"),At=p("Citation"),Ye=h(),f(X.$$.fragment),this.h()},l(e){const a=is('[data-svelte="svelte-1phssyn"]',document.head);_=o(a,"META",{name:!0,content:!0}),a.forEach(t),ve=c(e),j=o(e,"H1",{class:!0});var Be=l(j);x=o(Be,"A",{id:!0,class:!0,href:!0});var qt=l(x);ne=o(qt,"SPAN",{});var It=l(ne);u(V.$$.fragment,It),It.forEach(t),qt.forEach(t),Qe=c(Be),oe=o(Be,"SPAN",{});var Ct=l(oe);We=m(Ct,"ESE-VoVNet"),Ct.forEach(t),Be.forEach(t),_e=c(e),b=o(e,"P",{});var de=l(b);le=o(de,"STRONG",{});var Ht=l(le);Ze=m(Ht,"VoVNet"),Ht.forEach(t),et=m(de," is a convolutional neural network that seeks to make "),L=o(de,"A",{href:!0,rel:!0});var Vt=l(L);tt=m(Vt,"DenseNet"),Vt.forEach(t),st=m(de," more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel."),de.forEach(t),je=c(e),P=o(e,"P",{});var Ge=l(P);at=m(Ge,"Read about "),R=o(Ge,"A",{href:!0,rel:!0});var Lt=l(R);nt=m(Lt,"one-shot aggregation here"),Lt.forEach(t),ot=m(Ge,"."),Ge.forEach(t),be=c(e),$=o(e,"H2",{class:!0});var De=l($);S=o(De,"A",{id:!0,class:!0,href:!0});var Rt=l(S);re=o(Rt,"SPAN",{});var Yt=l(re);u(Y.$$.fragment,Yt),Yt.forEach(t),Rt.forEach(t),lt=c(De),ie=o(De,"SPAN",{});var zt=l(ie);rt=m(zt,"How do I use this model on an image?"),zt.forEach(t),De.forEach(t),$e=c(e),K=o(e,"P",{});var Bt=l(K);it=m(Bt,"To load a pretrained model:"),Bt.forEach(t),ye=c(e),u(z.$$.fragment,e),ke=c(e),Q=o(e,"P",{});var Gt=l(Q);pt=m(Gt,"To load and preprocess the image:"),Gt.forEach(t),Ee=c(e),u(B.$$.fragment,e),xe=c(e),W=o(e,"P",{});var Dt=l(W);mt=m(Dt,"To get the model predictions:"),Dt.forEach(t),Pe=c(e),u(G.$$.fragment,e),Se=c(e),Z=o(e,"P",{});var Ut=l(Z);ht=m(Ut,"To get the top-5 predictions class names:"),Ut.forEach(t),Ne=c(e),u(D.$$.fragment,e),Ae=c(e),N=o(e,"P",{});var Ue=l(N);ct=m(Ue,"Replace the model name with the variant you want to use, e.g. "),pe=o(Ue,"CODE",{});var Jt=l(pe);ft=m(Jt,"ese_vovnet19b_dw"),Jt.forEach(t),ut=m(Ue,". You can find the IDs in the model summaries at the top of this page."),Ue.forEach(t),Te=c(e),A=o(e,"P",{});var Je=l(A);gt=m(Je,"To extract image features with this model, follow the "),ee=o(Je,"A",{href:!0});var Mt=l(ee);dt=m(Mt,"timm feature extraction examples"),Mt.forEach(t),wt=m(Je,", just change the name of the model you want to use."),Je.forEach(t),qe=c(e),y=o(e,"H2",{class:!0});var Me=l(y);T=o(Me,"A",{id:!0,class:!0,href:!0});var Ot=l(T);me=o(Ot,"SPAN",{});var Ft=l(me);u(U.$$.fragment,Ft),Ft.forEach(t),Ot.forEach(t),vt=c(Me),he=o(Me,"SPAN",{});var Xt=l(he);_t=m(Xt,"How do I finetune this model?"),Xt.forEach(t),Me.forEach(t),Ie=c(e),te=o(e,"P",{});var Kt=l(te);jt=m(Kt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Kt.forEach(t),Ce=c(e),u(J.$$.fragment,e),He=c(e),q=o(e,"P",{});var Oe=l(q);bt=m(Oe,"To finetune on your own dataset, you have to write a training loop or adapt "),M=o(Oe,"A",{href:!0,rel:!0});var Qt=l(M);$t=m(Qt,`timm\u2019s training
script`),Qt.forEach(t),yt=m(Oe," to use your dataset."),Oe.forEach(t),Ve=c(e),k=o(e,"H2",{class:!0});var Fe=l(k);I=o(Fe,"A",{id:!0,class:!0,href:!0});var Wt=l(I);ce=o(Wt,"SPAN",{});var Zt=l(ce);u(O.$$.fragment,Zt),Zt.forEach(t),Wt.forEach(t),kt=c(Fe),fe=o(Fe,"SPAN",{});var es=l(fe);Et=m(es,"How do I train this model?"),es.forEach(t),Fe.forEach(t),Le=c(e),C=o(e,"P",{});var Xe=l(C);xt=m(Xe,"You can follow the "),se=o(Xe,"A",{href:!0});var ts=l(se);Pt=m(ts,"timm recipe scripts"),ts.forEach(t),St=m(Xe," for training a new model afresh."),Xe.forEach(t),Re=c(e),E=o(e,"H2",{class:!0});var Ke=l(E);H=o(Ke,"A",{id:!0,class:!0,href:!0});var ss=l(H);ue=o(ss,"SPAN",{});var as=l(ue);u(F.$$.fragment,as),as.forEach(t),ss.forEach(t),Nt=c(Ke),ge=o(Ke,"SPAN",{});var ns=l(ge);At=m(ns,"Citation"),ns.forEach(t),Ke.forEach(t),Ye=c(e),u(X.$$.fragment,e),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(cs)),i(x,"id","esevovnet"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#esevovnet"),i(j,"class","relative group"),i(L,"href","https://paperswithcode.com/method/densenet"),i(L,"rel","nofollow"),i(R,"href","https://paperswithcode.com/method/one-shot-aggregation"),i(R,"rel","nofollow"),i(S,"id","how-do-i-use-this-model-on-an-image"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(ee,"href","../feature_extraction"),i(T,"id","how-do-i-finetune-this-model"),i(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(T,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(M,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(M,"rel","nofollow"),i(I,"id","how-do-i-train-this-model"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(se,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(E,"class","relative group")},m(e,a){s(document.head,_),r(e,ve,a),r(e,j,a),s(j,x),s(x,ne),g(V,ne,null),s(j,Qe),s(j,oe),s(oe,We),r(e,_e,a),r(e,b,a),s(b,le),s(le,Ze),s(b,et),s(b,L),s(L,tt),s(b,st),r(e,je,a),r(e,P,a),s(P,at),s(P,R),s(R,nt),s(P,ot),r(e,be,a),r(e,$,a),s($,S),s(S,re),g(Y,re,null),s($,lt),s($,ie),s(ie,rt),r(e,$e,a),r(e,K,a),s(K,it),r(e,ye,a),g(z,e,a),r(e,ke,a),r(e,Q,a),s(Q,pt),r(e,Ee,a),g(B,e,a),r(e,xe,a),r(e,W,a),s(W,mt),r(e,Pe,a),g(G,e,a),r(e,Se,a),r(e,Z,a),s(Z,ht),r(e,Ne,a),g(D,e,a),r(e,Ae,a),r(e,N,a),s(N,ct),s(N,pe),s(pe,ft),s(N,ut),r(e,Te,a),r(e,A,a),s(A,gt),s(A,ee),s(ee,dt),s(A,wt),r(e,qe,a),r(e,y,a),s(y,T),s(T,me),g(U,me,null),s(y,vt),s(y,he),s(he,_t),r(e,Ie,a),r(e,te,a),s(te,jt),r(e,Ce,a),g(J,e,a),r(e,He,a),r(e,q,a),s(q,bt),s(q,M),s(M,$t),s(q,yt),r(e,Ve,a),r(e,k,a),s(k,I),s(I,ce),g(O,ce,null),s(k,kt),s(k,fe),s(fe,Et),r(e,Le,a),r(e,C,a),s(C,xt),s(C,se),s(se,Pt),s(C,St),r(e,Re,a),r(e,E,a),s(E,H),s(H,ue),g(F,ue,null),s(E,Nt),s(E,ge),s(ge,At),r(e,Ye,a),g(X,e,a),ze=!0},p:ps,i(e){ze||(d(V.$$.fragment,e),d(Y.$$.fragment,e),d(z.$$.fragment,e),d(B.$$.fragment,e),d(G.$$.fragment,e),d(D.$$.fragment,e),d(U.$$.fragment,e),d(J.$$.fragment,e),d(O.$$.fragment,e),d(F.$$.fragment,e),d(X.$$.fragment,e),ze=!0)},o(e){w(V.$$.fragment,e),w(Y.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(D.$$.fragment,e),w(U.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),w(X.$$.fragment,e),ze=!1},d(e){t(_),e&&t(ve),e&&t(j),v(V),e&&t(_e),e&&t(b),e&&t(je),e&&t(P),e&&t(be),e&&t($),v(Y),e&&t($e),e&&t(K),e&&t(ye),v(z,e),e&&t(ke),e&&t(Q),e&&t(Ee),v(B,e),e&&t(xe),e&&t(W),e&&t(Pe),v(G,e),e&&t(Se),e&&t(Z),e&&t(Ne),v(D,e),e&&t(Ae),e&&t(N),e&&t(Te),e&&t(A),e&&t(qe),e&&t(y),v(U),e&&t(Ie),e&&t(te),e&&t(Ce),v(J,e),e&&t(He),e&&t(q),e&&t(Ve),e&&t(k),v(O),e&&t(Le),e&&t(C),e&&t(Re),e&&t(E),v(F),e&&t(Ye),v(X,e)}}}const cs={local:"esevovnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"ESE-VoVNet"};function fs(Tt){return ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ws extends os{constructor(_){super();ls(this,_,fs,hs,rs,{})}}export{ws as default,cs as metadata};
