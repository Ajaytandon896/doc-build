import{S as Oe,i as Fe,s as Qe,e as n,k as i,w as f,t as h,M as Xe,c as l,d as e,m as p,a as r,x as u,h as c,b as m,G as t,g as o,y as g,L as Ze,q as d,o as w,B as v,v as Ke}from"../../chunks/vendor-hf-doc-builder.js";import{I as fs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ss}from"../../chunks/CodeBlock-hf-doc-builder.js";function We($e){let _,us,j,E,es,C,Ds,ts,Js,gs,H,as,Os,Fs,ds,$,x,ns,L,Qs,ls,Xs,ws,O,Zs,vs,z,_s,F,Ks,js,R,$s,Q,Ws,bs,V,ys,X,se,ks,B,Es,S,ee,os,te,ae,xs,P,ne,Z,le,oe,Ss,b,N,rs,G,re,is,ie,Ps,K,pe,Ns,Y,As,A,me,M,he,ce,Ts,y,T,ps,U,fe,ms,ue,Is,I,ge,W,de,we,qs,k,q,hs,D,ve,cs,_e,Cs,J,Hs;return C=new fs({}),L=new fs({}),z=new ss({props:{code:`import timm
model = timm.create_model('nasnetalarge', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;nasnetalarge&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),R=new ss({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),V=new ss({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),B=new ss({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),G=new fs({}),Y=new ss({props:{code:"model = timm.create_model('nasnetalarge', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;nasnetalarge&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),U=new fs({}),D=new fs({}),J=new ss({props:{code:`@misc{zoph2018learning,
      title={Learning Transferable Architectures for Scalable Image Recognition}, 
      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
      year={2018},
      eprint={1707.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{zoph2018learning,
      title={Learning Transferable Architectures for <span class="hljs-keyword">Scalable </span>Image Recognition}, 
      author={<span class="hljs-keyword">Barret </span>Zoph <span class="hljs-keyword">and </span>Vijay Vasudevan <span class="hljs-keyword">and </span><span class="hljs-keyword">Jonathon </span><span class="hljs-keyword">Shlens </span><span class="hljs-keyword">and </span>Quoc V. Le},
      year={<span class="hljs-number">2018</span>},
      eprint={<span class="hljs-number">1707</span>.<span class="hljs-number">07012</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){_=n("meta"),us=i(),j=n("h1"),E=n("a"),es=n("span"),f(C.$$.fragment),Ds=i(),ts=n("span"),Js=h("NASNet"),gs=i(),H=n("p"),as=n("strong"),Os=h("NASNet"),Fs=h(" is a type of convolutional neural network discovered through neural architecture search. The building blocks consist of normal and reduction cells."),ds=i(),$=n("h2"),x=n("a"),ns=n("span"),f(L.$$.fragment),Qs=i(),ls=n("span"),Xs=h("How do I use this model on an image?"),ws=i(),O=n("p"),Zs=h("To load a pretrained model:"),vs=i(),f(z.$$.fragment),_s=i(),F=n("p"),Ks=h("To load and preprocess the image:"),js=i(),f(R.$$.fragment),$s=i(),Q=n("p"),Ws=h("To get the model predictions:"),bs=i(),f(V.$$.fragment),ys=i(),X=n("p"),se=h("To get the top-5 predictions class names:"),ks=i(),f(B.$$.fragment),Es=i(),S=n("p"),ee=h("Replace the model name with the variant you want to use, e.g. "),os=n("code"),te=h("nasnetalarge"),ae=h(". You can find the IDs in the model summaries at the top of this page."),xs=i(),P=n("p"),ne=h("To extract image features with this model, follow the "),Z=n("a"),le=h("timm feature extraction examples"),oe=h(", just change the name of the model you want to use."),Ss=i(),b=n("h2"),N=n("a"),rs=n("span"),f(G.$$.fragment),re=i(),is=n("span"),ie=h("How do I finetune this model?"),Ps=i(),K=n("p"),pe=h("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ns=i(),f(Y.$$.fragment),As=i(),A=n("p"),me=h("To finetune on your own dataset, you have to write a training loop or adapt "),M=n("a"),he=h(`timm\u2019s training
script`),ce=h(" to use your dataset."),Ts=i(),y=n("h2"),T=n("a"),ps=n("span"),f(U.$$.fragment),fe=i(),ms=n("span"),ue=h("How do I train this model?"),Is=i(),I=n("p"),ge=h("You can follow the "),W=n("a"),de=h("timm recipe scripts"),we=h(" for training a new model afresh."),qs=i(),k=n("h2"),q=n("a"),hs=n("span"),f(D.$$.fragment),ve=i(),cs=n("span"),_e=h("Citation"),Cs=i(),f(J.$$.fragment),this.h()},l(s){const a=Xe('[data-svelte="svelte-1phssyn"]',document.head);_=l(a,"META",{name:!0,content:!0}),a.forEach(e),us=p(s),j=l(s,"H1",{class:!0});var Ls=r(j);E=l(Ls,"A",{id:!0,class:!0,href:!0});var be=r(E);es=l(be,"SPAN",{});var ye=r(es);u(C.$$.fragment,ye),ye.forEach(e),be.forEach(e),Ds=p(Ls),ts=l(Ls,"SPAN",{});var ke=r(ts);Js=c(ke,"NASNet"),ke.forEach(e),Ls.forEach(e),gs=p(s),H=l(s,"P",{});var je=r(H);as=l(je,"STRONG",{});var Ee=r(as);Os=c(Ee,"NASNet"),Ee.forEach(e),Fs=c(je," is a type of convolutional neural network discovered through neural architecture search. The building blocks consist of normal and reduction cells."),je.forEach(e),ds=p(s),$=l(s,"H2",{class:!0});var zs=r($);x=l(zs,"A",{id:!0,class:!0,href:!0});var xe=r(x);ns=l(xe,"SPAN",{});var Se=r(ns);u(L.$$.fragment,Se),Se.forEach(e),xe.forEach(e),Qs=p(zs),ls=l(zs,"SPAN",{});var Pe=r(ls);Xs=c(Pe,"How do I use this model on an image?"),Pe.forEach(e),zs.forEach(e),ws=p(s),O=l(s,"P",{});var Ne=r(O);Zs=c(Ne,"To load a pretrained model:"),Ne.forEach(e),vs=p(s),u(z.$$.fragment,s),_s=p(s),F=l(s,"P",{});var Ae=r(F);Ks=c(Ae,"To load and preprocess the image:"),Ae.forEach(e),js=p(s),u(R.$$.fragment,s),$s=p(s),Q=l(s,"P",{});var Te=r(Q);Ws=c(Te,"To get the model predictions:"),Te.forEach(e),bs=p(s),u(V.$$.fragment,s),ys=p(s),X=l(s,"P",{});var Ie=r(X);se=c(Ie,"To get the top-5 predictions class names:"),Ie.forEach(e),ks=p(s),u(B.$$.fragment,s),Es=p(s),S=l(s,"P",{});var Rs=r(S);ee=c(Rs,"Replace the model name with the variant you want to use, e.g. "),os=l(Rs,"CODE",{});var qe=r(os);te=c(qe,"nasnetalarge"),qe.forEach(e),ae=c(Rs,". You can find the IDs in the model summaries at the top of this page."),Rs.forEach(e),xs=p(s),P=l(s,"P",{});var Vs=r(P);ne=c(Vs,"To extract image features with this model, follow the "),Z=l(Vs,"A",{href:!0});var Ce=r(Z);le=c(Ce,"timm feature extraction examples"),Ce.forEach(e),oe=c(Vs,", just change the name of the model you want to use."),Vs.forEach(e),Ss=p(s),b=l(s,"H2",{class:!0});var Bs=r(b);N=l(Bs,"A",{id:!0,class:!0,href:!0});var He=r(N);rs=l(He,"SPAN",{});var Le=r(rs);u(G.$$.fragment,Le),Le.forEach(e),He.forEach(e),re=p(Bs),is=l(Bs,"SPAN",{});var ze=r(is);ie=c(ze,"How do I finetune this model?"),ze.forEach(e),Bs.forEach(e),Ps=p(s),K=l(s,"P",{});var Re=r(K);pe=c(Re,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Re.forEach(e),Ns=p(s),u(Y.$$.fragment,s),As=p(s),A=l(s,"P",{});var Gs=r(A);me=c(Gs,"To finetune on your own dataset, you have to write a training loop or adapt "),M=l(Gs,"A",{href:!0,rel:!0});var Ve=r(M);he=c(Ve,`timm\u2019s training
script`),Ve.forEach(e),ce=c(Gs," to use your dataset."),Gs.forEach(e),Ts=p(s),y=l(s,"H2",{class:!0});var Ys=r(y);T=l(Ys,"A",{id:!0,class:!0,href:!0});var Be=r(T);ps=l(Be,"SPAN",{});var Ge=r(ps);u(U.$$.fragment,Ge),Ge.forEach(e),Be.forEach(e),fe=p(Ys),ms=l(Ys,"SPAN",{});var Ye=r(ms);ue=c(Ye,"How do I train this model?"),Ye.forEach(e),Ys.forEach(e),Is=p(s),I=l(s,"P",{});var Ms=r(I);ge=c(Ms,"You can follow the "),W=l(Ms,"A",{href:!0});var Me=r(W);de=c(Me,"timm recipe scripts"),Me.forEach(e),we=c(Ms," for training a new model afresh."),Ms.forEach(e),qs=p(s),k=l(s,"H2",{class:!0});var Us=r(k);q=l(Us,"A",{id:!0,class:!0,href:!0});var Ue=r(q);hs=l(Ue,"SPAN",{});var De=r(hs);u(D.$$.fragment,De),De.forEach(e),Ue.forEach(e),ve=p(Us),cs=l(Us,"SPAN",{});var Je=r(cs);_e=c(Je,"Citation"),Je.forEach(e),Us.forEach(e),Cs=p(s),u(J.$$.fragment,s),this.h()},h(){m(_,"name","hf:doc:metadata"),m(_,"content",JSON.stringify(st)),m(E,"id","nasnet"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#nasnet"),m(j,"class","relative group"),m(x,"id","how-do-i-use-this-model-on-an-image"),m(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(x,"href","#how-do-i-use-this-model-on-an-image"),m($,"class","relative group"),m(Z,"href","../feature_extraction"),m(N,"id","how-do-i-finetune-this-model"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#how-do-i-finetune-this-model"),m(b,"class","relative group"),m(M,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(M,"rel","nofollow"),m(T,"id","how-do-i-train-this-model"),m(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(T,"href","#how-do-i-train-this-model"),m(y,"class","relative group"),m(W,"href","../scripts"),m(q,"id","citation"),m(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(q,"href","#citation"),m(k,"class","relative group")},m(s,a){t(document.head,_),o(s,us,a),o(s,j,a),t(j,E),t(E,es),g(C,es,null),t(j,Ds),t(j,ts),t(ts,Js),o(s,gs,a),o(s,H,a),t(H,as),t(as,Os),t(H,Fs),o(s,ds,a),o(s,$,a),t($,x),t(x,ns),g(L,ns,null),t($,Qs),t($,ls),t(ls,Xs),o(s,ws,a),o(s,O,a),t(O,Zs),o(s,vs,a),g(z,s,a),o(s,_s,a),o(s,F,a),t(F,Ks),o(s,js,a),g(R,s,a),o(s,$s,a),o(s,Q,a),t(Q,Ws),o(s,bs,a),g(V,s,a),o(s,ys,a),o(s,X,a),t(X,se),o(s,ks,a),g(B,s,a),o(s,Es,a),o(s,S,a),t(S,ee),t(S,os),t(os,te),t(S,ae),o(s,xs,a),o(s,P,a),t(P,ne),t(P,Z),t(Z,le),t(P,oe),o(s,Ss,a),o(s,b,a),t(b,N),t(N,rs),g(G,rs,null),t(b,re),t(b,is),t(is,ie),o(s,Ps,a),o(s,K,a),t(K,pe),o(s,Ns,a),g(Y,s,a),o(s,As,a),o(s,A,a),t(A,me),t(A,M),t(M,he),t(A,ce),o(s,Ts,a),o(s,y,a),t(y,T),t(T,ps),g(U,ps,null),t(y,fe),t(y,ms),t(ms,ue),o(s,Is,a),o(s,I,a),t(I,ge),t(I,W),t(W,de),t(I,we),o(s,qs,a),o(s,k,a),t(k,q),t(q,hs),g(D,hs,null),t(k,ve),t(k,cs),t(cs,_e),o(s,Cs,a),g(J,s,a),Hs=!0},p:Ze,i(s){Hs||(d(C.$$.fragment,s),d(L.$$.fragment,s),d(z.$$.fragment,s),d(R.$$.fragment,s),d(V.$$.fragment,s),d(B.$$.fragment,s),d(G.$$.fragment,s),d(Y.$$.fragment,s),d(U.$$.fragment,s),d(D.$$.fragment,s),d(J.$$.fragment,s),Hs=!0)},o(s){w(C.$$.fragment,s),w(L.$$.fragment,s),w(z.$$.fragment,s),w(R.$$.fragment,s),w(V.$$.fragment,s),w(B.$$.fragment,s),w(G.$$.fragment,s),w(Y.$$.fragment,s),w(U.$$.fragment,s),w(D.$$.fragment,s),w(J.$$.fragment,s),Hs=!1},d(s){e(_),s&&e(us),s&&e(j),v(C),s&&e(gs),s&&e(H),s&&e(ds),s&&e($),v(L),s&&e(ws),s&&e(O),s&&e(vs),v(z,s),s&&e(_s),s&&e(F),s&&e(js),v(R,s),s&&e($s),s&&e(Q),s&&e(bs),v(V,s),s&&e(ys),s&&e(X),s&&e(ks),v(B,s),s&&e(Es),s&&e(S),s&&e(xs),s&&e(P),s&&e(Ss),s&&e(b),v(G),s&&e(Ps),s&&e(K),s&&e(Ns),v(Y,s),s&&e(As),s&&e(A),s&&e(Ts),s&&e(y),v(U),s&&e(Is),s&&e(I),s&&e(qs),s&&e(k),v(D),s&&e(Cs),v(J,s)}}}const st={local:"nasnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"NASNet"};function et($e){return Ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lt extends Oe{constructor(_){super();Fe(this,_,et,We,Qe,{})}}export{lt as default,st as metadata};
