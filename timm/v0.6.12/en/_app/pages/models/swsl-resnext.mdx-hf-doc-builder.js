import{S as ua,i as ga,s as da,e as l,k as h,w as u,t as i,M as va,c as n,d as s,m as c,a as o,x as g,h as p,b as m,G as a,g as r,y as d,L as wa,q as v,o as w,B as b,v as ba}from"../../chunks/vendor-hf-doc-builder.js";import{I as be}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";function _a(Ls){let j,_e,$,P,oe,H,Ve,re,es,je,f,ss,ie,as,ts,L,ls,ns,M,os,rs,pe,is,ps,$e,K,ms,xe,Z,hs,ye,x,S,me,z,cs,he,fs,ke,Q,us,Ee,B,Pe,V,gs,Se,Y,Ne,ee,ds,Ae,D,Ce,se,vs,Te,X,Ie,N,ws,ce,bs,_s,qe,A,js,ae,$s,xs,Re,y,C,fe,G,ys,ue,ks,He,te,Es,Le,U,Me,T,Ps,J,Ss,Ns,ze,k,I,ge,O,As,de,Cs,Be,q,Ts,le,Is,qs,Ye,E,R,ve,W,Rs,we,Hs,De,F,Xe;return H=new be({}),z=new be({}),B=new ne({props:{code:`import timm
model = timm.create_model('swsl_resnext101_32x16d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;swsl_resnext101_32x16d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new ne({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),D=new ne({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),X=new ne({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),G=new be({}),U=new ne({props:{code:"model = timm.create_model('swsl_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;swsl_resnext101_32x16d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),O=new be({}),W=new be({}),F=new ne({props:{code:`@article{DBLP:journals/corr/abs-1905-00546,
  author    = {I. Zeki Yalniz and
               Herv{\\'{e}} J{\\'{e}}gou and
               Kan Chen and
               Manohar Paluri and
               Dhruv Mahajan},
  title     = {Billion-scale semi-supervised learning for image classification},
  journal   = {CoRR},
  volume    = {abs/1905.00546},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00546},
  archivePrefix = {arXiv},
  eprint    = {1905.00546},
  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`<span class="language-xml">@article</span><span class="hljs-template-variable">{DBLP:journals/corr/abs-1905-00546,
  author    = {I. Zeki Yalniz and
               Herv{\\&#x27;{e}</span><span class="language-xml">} J</span><span class="hljs-template-variable">{\\&#x27;{e}</span><span class="language-xml">}gou and
               Kan Chen and
               Manohar Paluri and
               Dhruv Mahajan},
  title     = </span><span class="hljs-template-variable">{Billion-scale semi-supervised learning for image classification}</span><span class="language-xml">,
  journal   = </span><span class="hljs-template-variable">{CoRR}</span><span class="language-xml">,
  volume    = </span><span class="hljs-template-variable">{abs/1905.00546}</span><span class="language-xml">,
  year      = </span><span class="hljs-template-variable">{2019}</span><span class="language-xml">,
  url       = </span><span class="hljs-template-variable">{http://arxiv.org/abs/1905.00546}</span><span class="language-xml">,
  archivePrefix = </span><span class="hljs-template-variable">{arXiv}</span><span class="language-xml">,
  eprint    = </span><span class="hljs-template-variable">{1905.00546}</span><span class="language-xml">,
  timestamp = </span><span class="hljs-template-variable">{Mon, 28 <span class="hljs-keyword">Sep</span> 2020 08:19:37 +0200}</span><span class="language-xml">,
  biburl    = </span><span class="hljs-template-variable">{https://dblp.org/rec/journals/corr/abs-1905-00546.bib}</span><span class="language-xml">,
  bibsource = </span><span class="hljs-template-variable">{dblp computer science bibliography, https://dblp.org}</span><span class="language-xml">
}</span>`}}),{c(){j=l("meta"),_e=h(),$=l("h1"),P=l("a"),oe=l("span"),u(H.$$.fragment),Ve=h(),re=l("span"),es=i("SWSL ResNeXt"),je=h(),f=l("p"),ss=i("A "),ie=l("strong"),as=i("ResNeXt"),ts=i(" repeats a "),L=l("a"),ls=i("building block"),ns=i(" that aggregates a set of transformations with the same topology. Compared to a "),M=l("a"),os=i("ResNet"),rs=i(", it exposes a new dimension,  "),pe=l("em"),is=i("cardinality"),ps=i(" (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),$e=h(),K=l("p"),ms=i("The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification."),xe=h(),Z=l("p"),hs=i("Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only."),ye=h(),x=l("h2"),S=l("a"),me=l("span"),u(z.$$.fragment),cs=h(),he=l("span"),fs=i("How do I use this model on an image?"),ke=h(),Q=l("p"),us=i("To load a pretrained model:"),Ee=h(),u(B.$$.fragment),Pe=h(),V=l("p"),gs=i("To load and preprocess the image:"),Se=h(),u(Y.$$.fragment),Ne=h(),ee=l("p"),ds=i("To get the model predictions:"),Ae=h(),u(D.$$.fragment),Ce=h(),se=l("p"),vs=i("To get the top-5 predictions class names:"),Te=h(),u(X.$$.fragment),Ie=h(),N=l("p"),ws=i("Replace the model name with the variant you want to use, e.g. "),ce=l("code"),bs=i("swsl_resnext101_32x16d"),_s=i(". You can find the IDs in the model summaries at the top of this page."),qe=h(),A=l("p"),js=i("To extract image features with this model, follow the "),ae=l("a"),$s=i("timm feature extraction examples"),xs=i(", just change the name of the model you want to use."),Re=h(),y=l("h2"),C=l("a"),fe=l("span"),u(G.$$.fragment),ys=h(),ue=l("span"),ks=i("How do I finetune this model?"),He=h(),te=l("p"),Es=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Le=h(),u(U.$$.fragment),Me=h(),T=l("p"),Ps=i("To finetune on your own dataset, you have to write a training loop or adapt "),J=l("a"),Ss=i(`timm\u2019s training
script`),Ns=i(" to use your dataset."),ze=h(),k=l("h2"),I=l("a"),ge=l("span"),u(O.$$.fragment),As=h(),de=l("span"),Cs=i("How do I train this model?"),Be=h(),q=l("p"),Ts=i("You can follow the "),le=l("a"),Is=i("timm recipe scripts"),qs=i(" for training a new model afresh."),Ye=h(),E=l("h2"),R=l("a"),ve=l("span"),u(W.$$.fragment),Rs=h(),we=l("span"),Hs=i("Citation"),De=h(),u(F.$$.fragment),this.h()},l(e){const t=va('[data-svelte="svelte-1phssyn"]',document.head);j=n(t,"META",{name:!0,content:!0}),t.forEach(s),_e=c(e),$=n(e,"H1",{class:!0});var Ge=o($);P=n(Ge,"A",{id:!0,class:!0,href:!0});var Ms=o(P);oe=n(Ms,"SPAN",{});var zs=o(oe);g(H.$$.fragment,zs),zs.forEach(s),Ms.forEach(s),Ve=c(Ge),re=n(Ge,"SPAN",{});var Bs=o(re);es=p(Bs,"SWSL ResNeXt"),Bs.forEach(s),Ge.forEach(s),je=c(e),f=n(e,"P",{});var _=o(f);ss=p(_,"A "),ie=n(_,"STRONG",{});var Ys=o(ie);as=p(Ys,"ResNeXt"),Ys.forEach(s),ts=p(_," repeats a "),L=n(_,"A",{href:!0,rel:!0});var Ds=o(L);ls=p(Ds,"building block"),Ds.forEach(s),ns=p(_," that aggregates a set of transformations with the same topology. Compared to a "),M=n(_,"A",{href:!0,rel:!0});var Xs=o(M);os=p(Xs,"ResNet"),Xs.forEach(s),rs=p(_,", it exposes a new dimension,  "),pe=n(_,"EM",{});var Gs=o(pe);is=p(Gs,"cardinality"),Gs.forEach(s),ps=p(_," (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),_.forEach(s),$e=c(e),K=n(e,"P",{});var Us=o(K);ms=p(Us,"The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification."),Us.forEach(s),xe=c(e),Z=n(e,"P",{});var Js=o(Z);hs=p(Js,"Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only."),Js.forEach(s),ye=c(e),x=n(e,"H2",{class:!0});var Ue=o(x);S=n(Ue,"A",{id:!0,class:!0,href:!0});var Os=o(S);me=n(Os,"SPAN",{});var Ws=o(me);g(z.$$.fragment,Ws),Ws.forEach(s),Os.forEach(s),cs=c(Ue),he=n(Ue,"SPAN",{});var Fs=o(he);fs=p(Fs,"How do I use this model on an image?"),Fs.forEach(s),Ue.forEach(s),ke=c(e),Q=n(e,"P",{});var Ks=o(Q);us=p(Ks,"To load a pretrained model:"),Ks.forEach(s),Ee=c(e),g(B.$$.fragment,e),Pe=c(e),V=n(e,"P",{});var Zs=o(V);gs=p(Zs,"To load and preprocess the image:"),Zs.forEach(s),Se=c(e),g(Y.$$.fragment,e),Ne=c(e),ee=n(e,"P",{});var Qs=o(ee);ds=p(Qs,"To get the model predictions:"),Qs.forEach(s),Ae=c(e),g(D.$$.fragment,e),Ce=c(e),se=n(e,"P",{});var Vs=o(se);vs=p(Vs,"To get the top-5 predictions class names:"),Vs.forEach(s),Te=c(e),g(X.$$.fragment,e),Ie=c(e),N=n(e,"P",{});var Je=o(N);ws=p(Je,"Replace the model name with the variant you want to use, e.g. "),ce=n(Je,"CODE",{});var ea=o(ce);bs=p(ea,"swsl_resnext101_32x16d"),ea.forEach(s),_s=p(Je,". You can find the IDs in the model summaries at the top of this page."),Je.forEach(s),qe=c(e),A=n(e,"P",{});var Oe=o(A);js=p(Oe,"To extract image features with this model, follow the "),ae=n(Oe,"A",{href:!0});var sa=o(ae);$s=p(sa,"timm feature extraction examples"),sa.forEach(s),xs=p(Oe,", just change the name of the model you want to use."),Oe.forEach(s),Re=c(e),y=n(e,"H2",{class:!0});var We=o(y);C=n(We,"A",{id:!0,class:!0,href:!0});var aa=o(C);fe=n(aa,"SPAN",{});var ta=o(fe);g(G.$$.fragment,ta),ta.forEach(s),aa.forEach(s),ys=c(We),ue=n(We,"SPAN",{});var la=o(ue);ks=p(la,"How do I finetune this model?"),la.forEach(s),We.forEach(s),He=c(e),te=n(e,"P",{});var na=o(te);Es=p(na,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),na.forEach(s),Le=c(e),g(U.$$.fragment,e),Me=c(e),T=n(e,"P",{});var Fe=o(T);Ps=p(Fe,"To finetune on your own dataset, you have to write a training loop or adapt "),J=n(Fe,"A",{href:!0,rel:!0});var oa=o(J);Ss=p(oa,`timm\u2019s training
script`),oa.forEach(s),Ns=p(Fe," to use your dataset."),Fe.forEach(s),ze=c(e),k=n(e,"H2",{class:!0});var Ke=o(k);I=n(Ke,"A",{id:!0,class:!0,href:!0});var ra=o(I);ge=n(ra,"SPAN",{});var ia=o(ge);g(O.$$.fragment,ia),ia.forEach(s),ra.forEach(s),As=c(Ke),de=n(Ke,"SPAN",{});var pa=o(de);Cs=p(pa,"How do I train this model?"),pa.forEach(s),Ke.forEach(s),Be=c(e),q=n(e,"P",{});var Ze=o(q);Ts=p(Ze,"You can follow the "),le=n(Ze,"A",{href:!0});var ma=o(le);Is=p(ma,"timm recipe scripts"),ma.forEach(s),qs=p(Ze," for training a new model afresh."),Ze.forEach(s),Ye=c(e),E=n(e,"H2",{class:!0});var Qe=o(E);R=n(Qe,"A",{id:!0,class:!0,href:!0});var ha=o(R);ve=n(ha,"SPAN",{});var ca=o(ve);g(W.$$.fragment,ca),ca.forEach(s),ha.forEach(s),Rs=c(Qe),we=n(Qe,"SPAN",{});var fa=o(we);Hs=p(fa,"Citation"),fa.forEach(s),Qe.forEach(s),De=c(e),g(F.$$.fragment,e),this.h()},h(){m(j,"name","hf:doc:metadata"),m(j,"content",JSON.stringify(ja)),m(P,"id","swsl-resnext"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#swsl-resnext"),m($,"class","relative group"),m(L,"href","https://paperswithcode.com/method/resnext-block"),m(L,"rel","nofollow"),m(M,"href","https://paperswithcode.com/method/resnet"),m(M,"rel","nofollow"),m(S,"id","how-do-i-use-this-model-on-an-image"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#how-do-i-use-this-model-on-an-image"),m(x,"class","relative group"),m(ae,"href","../feature_extraction"),m(C,"id","how-do-i-finetune-this-model"),m(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(C,"href","#how-do-i-finetune-this-model"),m(y,"class","relative group"),m(J,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(J,"rel","nofollow"),m(I,"id","how-do-i-train-this-model"),m(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(I,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(le,"href","../scripts"),m(R,"id","citation"),m(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(R,"href","#citation"),m(E,"class","relative group")},m(e,t){a(document.head,j),r(e,_e,t),r(e,$,t),a($,P),a(P,oe),d(H,oe,null),a($,Ve),a($,re),a(re,es),r(e,je,t),r(e,f,t),a(f,ss),a(f,ie),a(ie,as),a(f,ts),a(f,L),a(L,ls),a(f,ns),a(f,M),a(M,os),a(f,rs),a(f,pe),a(pe,is),a(f,ps),r(e,$e,t),r(e,K,t),a(K,ms),r(e,xe,t),r(e,Z,t),a(Z,hs),r(e,ye,t),r(e,x,t),a(x,S),a(S,me),d(z,me,null),a(x,cs),a(x,he),a(he,fs),r(e,ke,t),r(e,Q,t),a(Q,us),r(e,Ee,t),d(B,e,t),r(e,Pe,t),r(e,V,t),a(V,gs),r(e,Se,t),d(Y,e,t),r(e,Ne,t),r(e,ee,t),a(ee,ds),r(e,Ae,t),d(D,e,t),r(e,Ce,t),r(e,se,t),a(se,vs),r(e,Te,t),d(X,e,t),r(e,Ie,t),r(e,N,t),a(N,ws),a(N,ce),a(ce,bs),a(N,_s),r(e,qe,t),r(e,A,t),a(A,js),a(A,ae),a(ae,$s),a(A,xs),r(e,Re,t),r(e,y,t),a(y,C),a(C,fe),d(G,fe,null),a(y,ys),a(y,ue),a(ue,ks),r(e,He,t),r(e,te,t),a(te,Es),r(e,Le,t),d(U,e,t),r(e,Me,t),r(e,T,t),a(T,Ps),a(T,J),a(J,Ss),a(T,Ns),r(e,ze,t),r(e,k,t),a(k,I),a(I,ge),d(O,ge,null),a(k,As),a(k,de),a(de,Cs),r(e,Be,t),r(e,q,t),a(q,Ts),a(q,le),a(le,Is),a(q,qs),r(e,Ye,t),r(e,E,t),a(E,R),a(R,ve),d(W,ve,null),a(E,Rs),a(E,we),a(we,Hs),r(e,De,t),d(F,e,t),Xe=!0},p:wa,i(e){Xe||(v(H.$$.fragment,e),v(z.$$.fragment,e),v(B.$$.fragment,e),v(Y.$$.fragment,e),v(D.$$.fragment,e),v(X.$$.fragment,e),v(G.$$.fragment,e),v(U.$$.fragment,e),v(O.$$.fragment,e),v(W.$$.fragment,e),v(F.$$.fragment,e),Xe=!0)},o(e){w(H.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(Y.$$.fragment,e),w(D.$$.fragment,e),w(X.$$.fragment,e),w(G.$$.fragment,e),w(U.$$.fragment,e),w(O.$$.fragment,e),w(W.$$.fragment,e),w(F.$$.fragment,e),Xe=!1},d(e){s(j),e&&s(_e),e&&s($),b(H),e&&s(je),e&&s(f),e&&s($e),e&&s(K),e&&s(xe),e&&s(Z),e&&s(ye),e&&s(x),b(z),e&&s(ke),e&&s(Q),e&&s(Ee),b(B,e),e&&s(Pe),e&&s(V),e&&s(Se),b(Y,e),e&&s(Ne),e&&s(ee),e&&s(Ae),b(D,e),e&&s(Ce),e&&s(se),e&&s(Te),b(X,e),e&&s(Ie),e&&s(N),e&&s(qe),e&&s(A),e&&s(Re),e&&s(y),b(G),e&&s(He),e&&s(te),e&&s(Le),b(U,e),e&&s(Me),e&&s(T),e&&s(ze),e&&s(k),b(O),e&&s(Be),e&&s(q),e&&s(Ye),e&&s(E),b(W),e&&s(De),b(F,e)}}}const ja={local:"swsl-resnext",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"SWSL ResNeXt"};function $a(Ls){return ba(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ea extends ua{constructor(j){super();ga(this,j,$a,_a,da,{})}}export{Ea as default,ja as metadata};
