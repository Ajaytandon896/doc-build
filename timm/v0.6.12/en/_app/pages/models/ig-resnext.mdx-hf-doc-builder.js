import{S as dt,i as ut,s as gt,e as n,k as h,w as d,t as i,M as wt,c as o,d as s,m as f,a as r,x as u,h as p,b as m,G as t,g as l,y as g,L as _t,q as w,o as _,B as v,v as vt}from"../../chunks/vendor-hf-doc-builder.js";import{I as ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as oe}from"../../chunks/CodeBlock-hf-doc-builder.js";function jt(Ls){let $,je,b,P,re,H,Ze,le,es,$e,c,ss,ie,ts,as,L,ns,os,M,rs,ls,pe,is,ps,be,K,ms,ye,J,hs,xe,y,S,me,Y,fs,he,cs,ke,Q,ds,Ee,z,Pe,Z,us,Se,B,Ae,ee,gs,Ie,G,Ne,se,ws,Ce,X,Te,A,_s,fe,vs,js,qe,I,$s,te,bs,ys,Re,x,N,ce,D,xs,de,ks,He,ae,Es,Le,U,Me,C,Ps,W,Ss,As,Ye,k,T,ue,V,Is,ge,Ns,ze,q,Cs,ne,Ts,qs,Be,E,R,we,O,Rs,_e,Hs,Ge,F,Xe;return H=new ve({}),Y=new ve({}),z=new oe({props:{code:`import timm
model = timm.create_model('ig_resnext101_32x16d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ig_resnext101_32x16d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),B=new oe({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),G=new oe({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),X=new oe({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),D=new ve({}),U=new oe({props:{code:"model = timm.create_model('ig_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;ig_resnext101_32x16d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),V=new ve({}),O=new ve({}),F=new oe({props:{code:`@misc{mahajan2018exploring,
      title={Exploring the Limits of Weakly Supervised Pretraining}, 
      author={Dhruv Mahajan and Ross Girshick and Vignesh Ramanathan and Kaiming He and Manohar Paluri and Yixuan Li and Ashwin Bharambe and Laurens van der Maaten},
      year={2018},
      eprint={1805.00932},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{mahajan2018exploring,
      title={Exploring the Limits of Weakly Supervised Pretraining}, 
      author={Dhruv Mahajan <span class="hljs-keyword">and </span>Ross Girshick <span class="hljs-keyword">and </span>Vignesh Ramanathan <span class="hljs-keyword">and </span>Kaiming He <span class="hljs-keyword">and </span>Manohar Paluri <span class="hljs-keyword">and </span>Yixuan Li <span class="hljs-keyword">and </span>Ashwin <span class="hljs-keyword">Bharambe </span><span class="hljs-keyword">and </span>Laurens van der Maaten},
      year={<span class="hljs-number">2018</span>},
      eprint={<span class="hljs-number">1805</span>.<span class="hljs-number">00932</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){$=n("meta"),je=h(),b=n("h1"),P=n("a"),re=n("span"),d(H.$$.fragment),Ze=h(),le=n("span"),es=i("Instagram ResNeXt WSL"),$e=h(),c=n("p"),ss=i("A "),ie=n("strong"),ts=i("ResNeXt"),as=i(" repeats a "),L=n("a"),ns=i("building block"),os=i(" that aggregates a set of transformations with the same topology. Compared to a "),M=n("a"),rs=i("ResNet"),ls=i(", it exposes a new dimension,  "),pe=n("em"),is=i("cardinality"),ps=i(" (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),be=h(),K=n("p"),ms=i("This model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance."),ye=h(),J=n("p"),hs=i("Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only."),xe=h(),y=n("h2"),S=n("a"),me=n("span"),d(Y.$$.fragment),fs=h(),he=n("span"),cs=i("How do I use this model on an image?"),ke=h(),Q=n("p"),ds=i("To load a pretrained model:"),Ee=h(),d(z.$$.fragment),Pe=h(),Z=n("p"),us=i("To load and preprocess the image:"),Se=h(),d(B.$$.fragment),Ae=h(),ee=n("p"),gs=i("To get the model predictions:"),Ie=h(),d(G.$$.fragment),Ne=h(),se=n("p"),ws=i("To get the top-5 predictions class names:"),Ce=h(),d(X.$$.fragment),Te=h(),A=n("p"),_s=i("Replace the model name with the variant you want to use, e.g. "),fe=n("code"),vs=i("ig_resnext101_32x16d"),js=i(". You can find the IDs in the model summaries at the top of this page."),qe=h(),I=n("p"),$s=i("To extract image features with this model, follow the "),te=n("a"),bs=i("timm feature extraction examples"),ys=i(", just change the name of the model you want to use."),Re=h(),x=n("h2"),N=n("a"),ce=n("span"),d(D.$$.fragment),xs=h(),de=n("span"),ks=i("How do I finetune this model?"),He=h(),ae=n("p"),Es=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Le=h(),d(U.$$.fragment),Me=h(),C=n("p"),Ps=i("To finetune on your own dataset, you have to write a training loop or adapt "),W=n("a"),Ss=i(`timm\u2019s training
script`),As=i(" to use your dataset."),Ye=h(),k=n("h2"),T=n("a"),ue=n("span"),d(V.$$.fragment),Is=h(),ge=n("span"),Ns=i("How do I train this model?"),ze=h(),q=n("p"),Cs=i("You can follow the "),ne=n("a"),Ts=i("timm recipe scripts"),qs=i(" for training a new model afresh."),Be=h(),E=n("h2"),R=n("a"),we=n("span"),d(O.$$.fragment),Rs=h(),_e=n("span"),Hs=i("Citation"),Ge=h(),d(F.$$.fragment),this.h()},l(e){const a=wt('[data-svelte="svelte-1phssyn"]',document.head);$=o(a,"META",{name:!0,content:!0}),a.forEach(s),je=f(e),b=o(e,"H1",{class:!0});var De=r(b);P=o(De,"A",{id:!0,class:!0,href:!0});var Ms=r(P);re=o(Ms,"SPAN",{});var Ys=r(re);u(H.$$.fragment,Ys),Ys.forEach(s),Ms.forEach(s),Ze=f(De),le=o(De,"SPAN",{});var zs=r(le);es=p(zs,"Instagram ResNeXt WSL"),zs.forEach(s),De.forEach(s),$e=f(e),c=o(e,"P",{});var j=r(c);ss=p(j,"A "),ie=o(j,"STRONG",{});var Bs=r(ie);ts=p(Bs,"ResNeXt"),Bs.forEach(s),as=p(j," repeats a "),L=o(j,"A",{href:!0,rel:!0});var Gs=r(L);ns=p(Gs,"building block"),Gs.forEach(s),os=p(j," that aggregates a set of transformations with the same topology. Compared to a "),M=o(j,"A",{href:!0,rel:!0});var Xs=r(M);rs=p(Xs,"ResNet"),Xs.forEach(s),ls=p(j,", it exposes a new dimension,  "),pe=o(j,"EM",{});var Ds=r(pe);is=p(Ds,"cardinality"),Ds.forEach(s),ps=p(j," (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),j.forEach(s),be=f(e),K=o(e,"P",{});var Us=r(K);ms=p(Us,"This model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance."),Us.forEach(s),ye=f(e),J=o(e,"P",{});var Ws=r(J);hs=p(Ws,"Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only."),Ws.forEach(s),xe=f(e),y=o(e,"H2",{class:!0});var Ue=r(y);S=o(Ue,"A",{id:!0,class:!0,href:!0});var Vs=r(S);me=o(Vs,"SPAN",{});var Os=r(me);u(Y.$$.fragment,Os),Os.forEach(s),Vs.forEach(s),fs=f(Ue),he=o(Ue,"SPAN",{});var Fs=r(he);cs=p(Fs,"How do I use this model on an image?"),Fs.forEach(s),Ue.forEach(s),ke=f(e),Q=o(e,"P",{});var Ks=r(Q);ds=p(Ks,"To load a pretrained model:"),Ks.forEach(s),Ee=f(e),u(z.$$.fragment,e),Pe=f(e),Z=o(e,"P",{});var Js=r(Z);us=p(Js,"To load and preprocess the image:"),Js.forEach(s),Se=f(e),u(B.$$.fragment,e),Ae=f(e),ee=o(e,"P",{});var Qs=r(ee);gs=p(Qs,"To get the model predictions:"),Qs.forEach(s),Ie=f(e),u(G.$$.fragment,e),Ne=f(e),se=o(e,"P",{});var Zs=r(se);ws=p(Zs,"To get the top-5 predictions class names:"),Zs.forEach(s),Ce=f(e),u(X.$$.fragment,e),Te=f(e),A=o(e,"P",{});var We=r(A);_s=p(We,"Replace the model name with the variant you want to use, e.g. "),fe=o(We,"CODE",{});var et=r(fe);vs=p(et,"ig_resnext101_32x16d"),et.forEach(s),js=p(We,". You can find the IDs in the model summaries at the top of this page."),We.forEach(s),qe=f(e),I=o(e,"P",{});var Ve=r(I);$s=p(Ve,"To extract image features with this model, follow the "),te=o(Ve,"A",{href:!0});var st=r(te);bs=p(st,"timm feature extraction examples"),st.forEach(s),ys=p(Ve,", just change the name of the model you want to use."),Ve.forEach(s),Re=f(e),x=o(e,"H2",{class:!0});var Oe=r(x);N=o(Oe,"A",{id:!0,class:!0,href:!0});var tt=r(N);ce=o(tt,"SPAN",{});var at=r(ce);u(D.$$.fragment,at),at.forEach(s),tt.forEach(s),xs=f(Oe),de=o(Oe,"SPAN",{});var nt=r(de);ks=p(nt,"How do I finetune this model?"),nt.forEach(s),Oe.forEach(s),He=f(e),ae=o(e,"P",{});var ot=r(ae);Es=p(ot,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),ot.forEach(s),Le=f(e),u(U.$$.fragment,e),Me=f(e),C=o(e,"P",{});var Fe=r(C);Ps=p(Fe,"To finetune on your own dataset, you have to write a training loop or adapt "),W=o(Fe,"A",{href:!0,rel:!0});var rt=r(W);Ss=p(rt,`timm\u2019s training
script`),rt.forEach(s),As=p(Fe," to use your dataset."),Fe.forEach(s),Ye=f(e),k=o(e,"H2",{class:!0});var Ke=r(k);T=o(Ke,"A",{id:!0,class:!0,href:!0});var lt=r(T);ue=o(lt,"SPAN",{});var it=r(ue);u(V.$$.fragment,it),it.forEach(s),lt.forEach(s),Is=f(Ke),ge=o(Ke,"SPAN",{});var pt=r(ge);Ns=p(pt,"How do I train this model?"),pt.forEach(s),Ke.forEach(s),ze=f(e),q=o(e,"P",{});var Je=r(q);Cs=p(Je,"You can follow the "),ne=o(Je,"A",{href:!0});var mt=r(ne);Ts=p(mt,"timm recipe scripts"),mt.forEach(s),qs=p(Je," for training a new model afresh."),Je.forEach(s),Be=f(e),E=o(e,"H2",{class:!0});var Qe=r(E);R=o(Qe,"A",{id:!0,class:!0,href:!0});var ht=r(R);we=o(ht,"SPAN",{});var ft=r(we);u(O.$$.fragment,ft),ft.forEach(s),ht.forEach(s),Rs=f(Qe),_e=o(Qe,"SPAN",{});var ct=r(_e);Hs=p(ct,"Citation"),ct.forEach(s),Qe.forEach(s),Ge=f(e),u(F.$$.fragment,e),this.h()},h(){m($,"name","hf:doc:metadata"),m($,"content",JSON.stringify($t)),m(P,"id","instagram-resnext-wsl"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#instagram-resnext-wsl"),m(b,"class","relative group"),m(L,"href","https://paperswithcode.com/method/resnext-block"),m(L,"rel","nofollow"),m(M,"href","https://paperswithcode.com/method/resnet"),m(M,"rel","nofollow"),m(S,"id","how-do-i-use-this-model-on-an-image"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#how-do-i-use-this-model-on-an-image"),m(y,"class","relative group"),m(te,"href","../feature_extraction"),m(N,"id","how-do-i-finetune-this-model"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#how-do-i-finetune-this-model"),m(x,"class","relative group"),m(W,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(W,"rel","nofollow"),m(T,"id","how-do-i-train-this-model"),m(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(T,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(ne,"href","../scripts"),m(R,"id","citation"),m(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(R,"href","#citation"),m(E,"class","relative group")},m(e,a){t(document.head,$),l(e,je,a),l(e,b,a),t(b,P),t(P,re),g(H,re,null),t(b,Ze),t(b,le),t(le,es),l(e,$e,a),l(e,c,a),t(c,ss),t(c,ie),t(ie,ts),t(c,as),t(c,L),t(L,ns),t(c,os),t(c,M),t(M,rs),t(c,ls),t(c,pe),t(pe,is),t(c,ps),l(e,be,a),l(e,K,a),t(K,ms),l(e,ye,a),l(e,J,a),t(J,hs),l(e,xe,a),l(e,y,a),t(y,S),t(S,me),g(Y,me,null),t(y,fs),t(y,he),t(he,cs),l(e,ke,a),l(e,Q,a),t(Q,ds),l(e,Ee,a),g(z,e,a),l(e,Pe,a),l(e,Z,a),t(Z,us),l(e,Se,a),g(B,e,a),l(e,Ae,a),l(e,ee,a),t(ee,gs),l(e,Ie,a),g(G,e,a),l(e,Ne,a),l(e,se,a),t(se,ws),l(e,Ce,a),g(X,e,a),l(e,Te,a),l(e,A,a),t(A,_s),t(A,fe),t(fe,vs),t(A,js),l(e,qe,a),l(e,I,a),t(I,$s),t(I,te),t(te,bs),t(I,ys),l(e,Re,a),l(e,x,a),t(x,N),t(N,ce),g(D,ce,null),t(x,xs),t(x,de),t(de,ks),l(e,He,a),l(e,ae,a),t(ae,Es),l(e,Le,a),g(U,e,a),l(e,Me,a),l(e,C,a),t(C,Ps),t(C,W),t(W,Ss),t(C,As),l(e,Ye,a),l(e,k,a),t(k,T),t(T,ue),g(V,ue,null),t(k,Is),t(k,ge),t(ge,Ns),l(e,ze,a),l(e,q,a),t(q,Cs),t(q,ne),t(ne,Ts),t(q,qs),l(e,Be,a),l(e,E,a),t(E,R),t(R,we),g(O,we,null),t(E,Rs),t(E,_e),t(_e,Hs),l(e,Ge,a),g(F,e,a),Xe=!0},p:_t,i(e){Xe||(w(H.$$.fragment,e),w(Y.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(G.$$.fragment,e),w(X.$$.fragment,e),w(D.$$.fragment,e),w(U.$$.fragment,e),w(V.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),Xe=!0)},o(e){_(H.$$.fragment,e),_(Y.$$.fragment,e),_(z.$$.fragment,e),_(B.$$.fragment,e),_(G.$$.fragment,e),_(X.$$.fragment,e),_(D.$$.fragment,e),_(U.$$.fragment,e),_(V.$$.fragment,e),_(O.$$.fragment,e),_(F.$$.fragment,e),Xe=!1},d(e){s($),e&&s(je),e&&s(b),v(H),e&&s($e),e&&s(c),e&&s(be),e&&s(K),e&&s(ye),e&&s(J),e&&s(xe),e&&s(y),v(Y),e&&s(ke),e&&s(Q),e&&s(Ee),v(z,e),e&&s(Pe),e&&s(Z),e&&s(Se),v(B,e),e&&s(Ae),e&&s(ee),e&&s(Ie),v(G,e),e&&s(Ne),e&&s(se),e&&s(Ce),v(X,e),e&&s(Te),e&&s(A),e&&s(qe),e&&s(I),e&&s(Re),e&&s(x),v(D),e&&s(He),e&&s(ae),e&&s(Le),v(U,e),e&&s(Me),e&&s(C),e&&s(Ye),e&&s(k),v(V),e&&s(ze),e&&s(q),e&&s(Be),e&&s(E),v(O),e&&s(Ge),v(F,e)}}}const $t={local:"instagram-resnext-wsl",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Instagram ResNeXt WSL"};function bt(Ls){return vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Et extends dt{constructor($){super();ut(this,$,bt,jt,gt,{})}}export{Et as default,$t as metadata};
