<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;recent-changes&quot;,&quot;sections&quot;:[{&quot;local&quot;:&quot;july-27-2022&quot;,&quot;title&quot;:&quot;July 27, 2022&quot;},{&quot;local&quot;:&quot;july-8-2022&quot;,&quot;title&quot;:&quot;July 8, 2022&quot;},{&quot;local&quot;:&quot;may-13-2022&quot;,&quot;title&quot;:&quot;May 13, 2022&quot;},{&quot;local&quot;:&quot;may-2-2022&quot;,&quot;title&quot;:&quot;May 2, 2022&quot;},{&quot;local&quot;:&quot;april-22-2022&quot;,&quot;title&quot;:&quot;April 22, 2022&quot;},{&quot;local&quot;:&quot;march-23-2022&quot;,&quot;title&quot;:&quot;March 23, 2022&quot;},{&quot;local&quot;:&quot;march-21-2022&quot;,&quot;title&quot;:&quot;March 21, 2022&quot;},{&quot;local&quot;:&quot;feb-2-2022&quot;,&quot;title&quot;:&quot;Feb 2, 2022&quot;},{&quot;local&quot;:&quot;jan-14-2022&quot;,&quot;title&quot;:&quot;Jan 14, 2022&quot;},{&quot;local&quot;:&quot;nov-22-2021&quot;,&quot;title&quot;:&quot;Nov 22, 2021&quot;},{&quot;local&quot;:&quot;oct-19-2021&quot;,&quot;title&quot;:&quot;Oct 19, 2021&quot;},{&quot;local&quot;:&quot;aug-18-2021&quot;,&quot;title&quot;:&quot;Aug 18, 2021&quot;}],&quot;title&quot;:&quot;Recent Changes&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/pages/changes.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/timm/master/en/_app/chunks/IconCopyLink-hf-doc-builder.js"> 





<h1 class="relative group"><a id="recent-changes" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#recent-changes"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Recent Changes
	</span></h1>

<h3 class="relative group"><a id="july-27-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#july-27-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>July 27, 2022
	</span></h3>

<ul><li>All runtime benchmark and validation result csv files are up-to-date!</li>
<li>A few more weights &amp; model defs added:<ul><li><code>darknetaa53</code> -  79.8 @ 256, 80.5 @ 288</li>
<li><code>convnext_nano</code> - 80.8 @ 224, 81.5 @ 288</li>
<li><code>cs3sedarknet_l</code> - 81.2 @ 256, 81.8 @ 288</li>
<li><code>cs3darknet_x</code> - 81.8 @ 256, 82.2 @ 288</li>
<li><code>cs3sedarknet_x</code> - 82.2 @ 256, 82.7 @ 288</li>
<li><code>cs3edgenet_x</code> - 82.2 @ 256, 82.7 @ 288</li>
<li><code>cs3se_edgenet_x</code> - 82.8 @ 256, 83.5 @ 320</li></ul></li>
<li><code>cs3*</code> weights above all trained on TPU w/ <code>bits_and_tpu</code> branch. Thanks to TRC program!</li>
<li>Add output_stride=8 and 16 support to ConvNeXt (dilation)</li>
<li>deit3 models not being able to resize pos_emb fixed</li>
<li>Version 0.6.7 PyPi release (/w above bug fixes and new weighs since 0.6.5)</li></ul>
<h3 class="relative group"><a id="july-8-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#july-8-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>July 8, 2022
	</span></h3>

<p>More models, more fixes</p>
<ul><li>Official research models (w/ weights) added:<ul><li>EdgeNeXt from (<a href="https://github.com/mmaaz60/EdgeNeXt" rel="nofollow">https://github.com/mmaaz60/EdgeNeXt</a>)</li>
<li>MobileViT-V2 from (<a href="https://github.com/apple/ml-cvnets" rel="nofollow">https://github.com/apple/ml-cvnets</a>)</li>
<li>DeiT III (Revenge of the ViT) from (<a href="https://github.com/facebookresearch/deit" rel="nofollow">https://github.com/facebookresearch/deit</a>)</li></ul></li>
<li>My own models:<ul><li>Small <code>ResNet</code> defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14)</li>
<li><code>CspNet</code> refactored with dataclass config, simplified CrossStage3 (<code>cs3</code>) option. These are closer to YOLO-v5+ backbone defs.</li>
<li>More relative position vit fiddling. Two <code>srelpos</code> (shared relative position) models trained, and a medium w/ class token.</li>
<li>Add an alternate downsample mode to EdgeNeXt and train a <code>small</code> model. Better than original small, but not their new USI trained weights.</li></ul></li>
<li>My own model weight results (all ImageNet-1k training)<ul><li><code>resnet10t</code> - 66.5 @ 176, 68.3 @ 224</li>
<li><code>resnet14t</code> - 71.3 @ 176, 72.3 @ 224</li>
<li><code>resnetaa50</code> - 80.6 @ 224 , 81.6 @ 288</li>
<li><code>darknet53</code> -  80.0 @ 256, 80.5 @ 288</li>
<li><code>cs3darknet_m</code> - 77.0 @ 256, 77.6 @ 288</li>
<li><code>cs3darknet_focus_m</code> - 76.7 @ 256, 77.3 @ 288</li>
<li><code>cs3darknet_l</code> - 80.4 @ 256, 80.9 @ 288</li>
<li><code>cs3darknet_focus_l</code> - 80.3 @ 256, 80.9 @ 288</li>
<li><code>vit_srelpos_small_patch16_224</code> - 81.1 @ 224, 82.1 @ 320</li>
<li><code>vit_srelpos_medium_patch16_224</code> - 82.3 @ 224, 83.1 @ 320</li>
<li><code>vit_relpos_small_patch16_cls_224</code> - 82.6 @ 224, 83.6 @ 320</li>
<li><code>edgnext_small_rw</code> - 79.6 @ 224, 80.4 @ 320</li></ul></li>
<li><code>cs3</code>, <code>darknet</code>, and <code>vit_*relpos</code> weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs.</li>
<li>Hugging Face Hub support fixes verified, demo notebook TBA</li>
<li>Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation.</li>
<li>Add support to change image extensions scanned by <code>timm</code> datasets/parsers. See (<a href="https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103" rel="nofollow">https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103</a>)</li>
<li>Default ConvNeXt LayerNorm impl to use <code>F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)</code> via <code>LayerNorm2d</code> in all cases. <ul><li>a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges. </li>
<li>previous impl exists as <code>LayerNormExp2d</code> in <code>models/layers/norm.py</code></li></ul></li>
<li>Numerous bug fixes</li>
<li>Currently testing for imminent PyPi 0.6.x release</li>
<li>LeViT pretraining of larger models still a WIP, they don’t train well / easily without distillation. Time to add distill support (finally)?</li>
<li>ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) …</li></ul>
<h3 class="relative group"><a id="may-13-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#may-13-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>May 13, 2022
	</span></h3>

<ul><li>Official Swin-V2 models and weights added from (<a href="https://github.com/microsoft/Swin-Transformer" rel="nofollow">https://github.com/microsoft/Swin-Transformer</a>). Cleaned up to support torchscript.</li>
<li>Some refactoring for existing <code>timm</code> Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects.</li>
<li>More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)<ul><li><code>vit_relpos_small_patch16_224</code> - 81.5 @ 224, 82.5 @ 320 — rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_relpos_medium_patch16_rpn_224</code> - 82.3 @ 224, 83.1 @ 320 — rel pos + res-post-norm, no class token, avg pool</li>
<li><code>vit_relpos_medium_patch16_224</code> - 82.5 @ 224, 83.3 @ 320 — rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_relpos_base_patch16_gapcls_224</code> - 82.8 @ 224, 83.9 @ 320 — rel pos, layer scale, class token, avg pool (by mistake)</li></ul></li>
<li>Bring 512 dim, 8-head ‘medium’ ViT model variant back to life (after using in a pre DeiT ‘small’ model for first ViT impl back in 2020)</li>
<li>Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials</li>
<li>Sequencer2D impl (<a href="https://arxiv.org/abs/2205.01972" rel="nofollow">https://arxiv.org/abs/2205.01972</a>), added via PR from author (<a href="https://github.com/okojoalg" rel="nofollow">https://github.com/okojoalg</a>)</li></ul>
<h3 class="relative group"><a id="may-2-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#may-2-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>May 2, 2022
	</span></h3>

<ul><li>Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (<code>vision_transformer_relpos.py</code>) and Residual Post-Norm branches (from Swin-V2) (<code>vision_transformer*.py</code>)<ul><li><code>vit_relpos_base_patch32_plus_rpn_256</code> - 79.5 @ 256, 80.6 @ 320 — rel pos + extended width + res-post-norm, no class token, avg pool</li>
<li><code>vit_relpos_base_patch16_224</code> - 82.5 @ 224, 83.6 @ 320 — rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_base_patch16_rpn_224</code> - 82.3 @ 224 — rel pos + res-post-norm, no class token, avg pool</li></ul></li>
<li>Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie <code>How to Train Your ViT</code>)</li>
<li><code>vit_*</code> models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae).</li></ul>
<h3 class="relative group"><a id="april-22-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#april-22-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>April 22, 2022
	</span></h3>

<ul><li><code>timm</code> models are now officially supported in <a href="https://www.fast.ai/" rel="nofollow">fast.ai</a>! Just in time for the new Practical Deep Learning course. <code>timmdocs</code> documentation link updated to <a href="http://timm.fast.ai/" rel="nofollow">timm.fast.ai</a>.</li>
<li>Two more model weights added in the TPU trained <a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights" rel="nofollow">series</a>. Some In22k pretrain still in progress.<ul><li><code>seresnext101d_32x8d</code> - 83.69 @ 224, 84.35 @ 288</li>
<li><code>seresnextaa101d_32x8d</code> (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288</li></ul></li></ul>
<h3 class="relative group"><a id="march-23-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#march-23-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>March 23, 2022
	</span></h3>

<ul><li>Add <code>ParallelBlock</code> and <code>LayerScale</code> option to base vit models to support model configs in <a href="https://arxiv.org/abs/2203.09795" rel="nofollow">Three things everyone should know about ViT</a></li>
<li><code>convnext_tiny_hnf</code> (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.</li></ul>
<h3 class="relative group"><a id="march-21-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#march-21-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>March 21, 2022
	</span></h3>

<ul><li>Merge <code>norm_norm_norm</code>. <strong>IMPORTANT</strong> this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch <a href="https://github.com/rwightman/pytorch-image-models/tree/0.5.x" rel="nofollow"><code>0.5.x</code></a> or a previous 0.5.x release can be used if stability is required.</li>
<li>Significant weights update (all TPU trained) as described in this <a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights" rel="nofollow">release</a><ul><li><code>regnety_040</code> - 82.3 @ 224, 82.96 @ 288</li>
<li><code>regnety_064</code> - 83.0 @ 224, 83.65 @ 288</li>
<li><code>regnety_080</code> - 83.17 @ 224, 83.86 @ 288</li>
<li><code>regnetv_040</code> - 82.44 @ 224, 83.18 @ 288   (timm pre-act)</li>
<li><code>regnetv_064</code> - 83.1 @ 224, 83.71 @ 288   (timm pre-act)</li>
<li><code>regnetz_040</code> - 83.67 @ 256, 84.25 @ 320</li>
<li><code>regnetz_040h</code> - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)</li>
<li><code>resnetv2_50d_gn</code> - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)</li>
<li><code>resnetv2_50d_evos</code> 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)</li>
<li><code>regnetz_c16_evos</code>  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)</li>
<li><code>regnetz_d8_evos</code>  - 83.42 @ 256, 84.04 @ 320 (EvoNormS)</li>
<li><code>xception41p</code> - 82 @ 299   (timm pre-act)</li>
<li><code>xception65</code> -  83.17 @ 299</li>
<li><code>xception65p</code> -  83.14 @ 299   (timm pre-act)</li>
<li><code>resnext101_64x4d</code> - 82.46 @ 224, 83.16 @ 288</li>
<li><code>seresnext101_32x8d</code> - 83.57 @ 224, 84.270 @ 288</li>
<li><code>resnetrs200</code> - 83.85 @ 256, 84.44 @ 320</li></ul></li>
<li>HuggingFace hub support fixed w/ initial groundwork for allowing alternative ‘config sources’ for pretrained model definitions and weights (generic local file / remote url support soon)</li>
<li>SwinTransformer-V2 implementation added. Submitted by <a href="https://github.com/ChristophReich1996" rel="nofollow">Christoph Reich</a>. Training experiments and model changes by myself are ongoing so expect compat breaks.</li>
<li>Swin-S3 (AutoFormerV2) models / weights added from <a href="https://github.com/microsoft/Cream/tree/main/AutoFormerV2" rel="nofollow">https://github.com/microsoft/Cream/tree/main/AutoFormerV2</a></li>
<li>MobileViT models w/ weights adapted from <a href="https://github.com/apple/ml-cvnets" rel="nofollow">https://github.com/apple/ml-cvnets</a></li>
<li>PoolFormer models w/ weights adapted from <a href="https://github.com/sail-sg/poolformer" rel="nofollow">https://github.com/sail-sg/poolformer</a></li>
<li>VOLO models w/ weights adapted from <a href="https://github.com/sail-sg/volo" rel="nofollow">https://github.com/sail-sg/volo</a></li>
<li>Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc</li>
<li>Enhance support for alternate norm + act (‘NormAct’) layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception</li>
<li>Grouped conv support added to EfficientNet family</li>
<li>Add ‘group matching’ API to all models to allow grouping model parameters for application of ‘layer-wise’ LR decay, lr scale added to LR scheduler</li>
<li>Gradient checkpointing support added to many models</li>
<li><code>forward_head(x, pre_logits=False)</code> fn added to all models to allow separate calls of <code>forward_features</code> + <code>forward_head</code></li>
<li>All vision transformer and vision MLP models update to return non-pooled / non-token selected features from <code>foward_features</code>, for consistency with CNN models, token selection or pooling now applied in <code>forward_head</code></li></ul>
<h3 class="relative group"><a id="feb-2-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#feb-2-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Feb 2, 2022
	</span></h3>

<ul><li><a href="https://github.com/Chris-hughes10" rel="nofollow">Chris Hughes</a> posted an exhaustive run through of <code>timm</code> on his blog yesterday. Well worth a read. <a href="https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055" rel="nofollow">Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide</a></li>
<li>I’m currently prepping to merge the <code>norm_norm_norm</code> branch back to master (ver 0.6.x) in next week or so.<ul><li>The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware <code>pip install git+https://github.com/rwightman/pytorch-image-models</code> installs!</li>
<li><code>0.5.x</code> releases and a <code>0.5.x</code> branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable.</li></ul></li></ul>
<h3 class="relative group"><a id="jan-14-2022" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#jan-14-2022"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Jan 14, 2022
	</span></h3>

<ul><li>Version 0.5.4 w/ release to be pushed to pypi. It’s been a while since last pypi update and riskier changes will be merged to main branch soon…</li>
<li>Add ConvNeXT models /w weights from official impl (<a href="https://github.com/facebookresearch/ConvNeXt" rel="nofollow">https://github.com/facebookresearch/ConvNeXt</a>), a few perf tweaks, compatible with timm features</li>
<li>Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way…<ul><li><code>mnasnet_small</code> - 65.6 top-1</li>
<li><code>mobilenetv2_050</code> - 65.9</li>
<li><code>lcnet_100/075/050</code> - 72.1 / 68.8 / 63.1</li>
<li><code>semnasnet_075</code> - 73</li>
<li><code>fbnetv3_b/d/g</code> - 79.1 / 79.7 / 82.0</li></ul></li>
<li>TinyNet models added by <a href="https://github.com/rsomani95" rel="nofollow">rsomani95</a></li>
<li>LCNet added via MobileNetV3 architecture</li></ul>
<h3 class="relative group"><a id="nov-22-2021" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#nov-22-2021"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Nov 22, 2021
	</span></h3>

<ul><li>A number of updated weights anew new model defs<ul><li><code>eca_halonext26ts</code> - 79.5 @ 256</li>
<li><code>resnet50_gn</code> (new) - 80.1 @ 224, 81.3 @ 288</li>
<li><code>resnet50</code> - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don’t scale as well to higher res, <a href="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1h2_176-001a1197.pth" rel="nofollow">weights</a>)</li>
<li><code>resnext50_32x4d</code> - 81.1 @ 224, 82.0 @ 288</li>
<li><code>sebotnet33ts_256</code> (new) - 81.2 @ 224</li>
<li><code>lamhalobotnet50ts_256</code> - 81.5 @ 256</li>
<li><code>halonet50ts</code> - 81.7 @ 256</li>
<li><code>halo2botnet50ts_256</code> - 82.0 @ 256</li>
<li><code>resnet101</code> - 82.0 @ 224, 82.8 @ 288</li>
<li><code>resnetv2_101</code> (new) - 82.1 @ 224, 83.0 @ 288</li>
<li><code>resnet152</code> - 82.8 @ 224, 83.5 @ 288</li>
<li><code>regnetz_d8</code> (new) - 83.5 @ 256, 84.0 @ 320</li>
<li><code>regnetz_e8</code> (new) - 84.5 @ 256, 85.0 @ 320</li></ul></li>
<li><code>vit_base_patch8_224</code> (85.8 top-1) &amp; <code>in21k</code> variant weights added thanks <a href="https://github.com/martinsbruveris" rel="nofollow">Martins Bruveris</a></li>
<li>Groundwork in for FX feature extraction thanks to <a href="https://github.com/alexander-soare" rel="nofollow">Alexander Soare</a><ul><li>models updated for tracing compatibility (almost full support with some distlled transformer exceptions)</li></ul></li></ul>
<h3 class="relative group"><a id="oct-19-2021" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#oct-19-2021"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Oct 19, 2021
	</span></h3>

<ul><li>ResNet strikes back (<a href="https://arxiv.org/abs/2110.00476" rel="nofollow">https://arxiv.org/abs/2110.00476</a>) weights added, plus any extra training components used. Model weights and some more details here (<a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights" rel="nofollow">https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights</a>)</li>
<li>BCE loss and Repeated Augmentation support for RSB paper</li>
<li>4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here (<a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights" rel="nofollow">https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights</a>)</li>
<li>Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl):<ul><li>Halo (<a href="https://arxiv.org/abs/2103.12731" rel="nofollow">https://arxiv.org/abs/2103.12731</a>)</li>
<li>Bottleneck Transformer (<a href="https://arxiv.org/abs/2101.11605" rel="nofollow">https://arxiv.org/abs/2101.11605</a>)</li>
<li>LambdaNetworks (<a href="https://arxiv.org/abs/2102.08602" rel="nofollow">https://arxiv.org/abs/2102.08602</a>)</li></ul></li>
<li>A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper (<a href="https://arxiv.org/abs/2103.06877" rel="nofollow">https://arxiv.org/abs/2103.06877</a>) in any way other than block architecture, details of official models are not available. See more here (<a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights" rel="nofollow">https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights</a>)</li>
<li>ConvMixer (<a href="https://openreview.net/forum?id=TVHS5Y4dNvM" rel="nofollow">https://openreview.net/forum?id=TVHS5Y4dNvM</a>), CrossVit (<a href="https://arxiv.org/abs/2103.14899" rel="nofollow">https://arxiv.org/abs/2103.14899</a>), and BeiT (<a href="https://arxiv.org/abs/2106.08254" rel="nofollow">https://arxiv.org/abs/2106.08254</a>) architectures + weights added</li>
<li>freeze/unfreeze helpers by <a href="https://github.com/alexander-soare" rel="nofollow">Alexander Soare</a></li></ul>
<h3 class="relative group"><a id="aug-18-2021" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#aug-18-2021"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Aug 18, 2021
	</span></h3>

<ul><li>Optimizer bonanza!<ul><li>Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ <code>timm bits</code> <a href="https://github.com/rwightman/pytorch-image-models/tree/bits_and_tpu/timm/bits" rel="nofollow">branch</a>)</li>
<li>Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)</li>
<li>Some cleanup on all optimizers and factory. No more <code>.data</code>, a bit more consistency, unit tests for all!</li>
<li>SGDP and AdamP still won’t work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself).</li></ul></li>
<li>EfficientNet-V2 XL TF ported weights added, but they don’t validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -&gt; 1k weights are very sensitive and less robust than the 1k weights.</li>
<li>Added PyTorch trained EfficientNet-V2 ‘Tiny’ w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested.</li></ul>


		<script type="module" data-hydrate="13jxojr">
		import { start } from "/docs/timm/master/en/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="13jxojr"]').parentNode,
			paths: {"base":"/docs/timm/master/en","assets":"/docs/timm/master/en"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/timm/master/en/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/timm/master/en/_app/pages/changes.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
