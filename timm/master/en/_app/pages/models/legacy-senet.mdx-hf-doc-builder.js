import{S as ss,i as as,s as ns,e as n,k as h,w as f,t as i,M as ls,c as l,d as t,m as c,a as r,x as u,h as p,b as m,G as s,g as o,y as d,L as os,q as g,o as w,B as _,v as rs}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function is(Pt){let j,we,y,x,ne,C,We,le,Xe,_e,v,Ke,oe,Qe,Ze,z,et,tt,ve,V,st,je,$,S,re,L,at,ie,nt,ye,W,lt,$e,G,be,X,ot,ke,R,Ee,K,rt,xe,Y,Se,Q,it,Pe,M,qe,P,pt,pe,mt,ht,Ae,q,ct,Z,ft,ut,Ne,b,A,me,U,dt,he,gt,Te,ee,wt,Ie,B,He,N,_t,D,vt,jt,Ce,k,T,ce,J,yt,fe,$t,ze,I,bt,te,kt,Et,Le,E,H,ue,O,xt,de,St,Ge,F,Re;return C=new ge({}),L=new ge({}),G=new ae({props:{code:`import timm
model = timm.create_model('legacy_senet154', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;legacy_senet154&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),R=new ae({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),Y=new ae({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),M=new ae({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),U=new ge({}),B=new ae({props:{code:"model = timm.create_model('legacy_senet154', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;legacy_senet154&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),J=new ge({}),O=new ge({}),F=new ae({props:{code:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-and-Excitation Networks}, 
      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
      year={2019},
      eprint={1709.01507},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{hu2019squeezeandexcitation,
      title={Squeeze-<span class="hljs-keyword">and-Excitation </span>Networks}, 
      author={<span class="hljs-keyword">Jie </span>Hu <span class="hljs-keyword">and </span>Li <span class="hljs-keyword">Shen </span><span class="hljs-keyword">and </span>Samuel Albanie <span class="hljs-keyword">and </span>Gang Sun <span class="hljs-keyword">and </span>Enhua Wu},
      year={<span class="hljs-number">2019</span>},
      eprint={<span class="hljs-number">1709</span>.<span class="hljs-number">01507</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){j=n("meta"),we=h(),y=n("h1"),x=n("a"),ne=n("span"),f(C.$$.fragment),We=h(),le=n("span"),Xe=i("(Legacy) SENet"),_e=h(),v=n("p"),Ke=i("A "),oe=n("strong"),Qe=i("SENet"),Ze=i(" is a convolutional neural network architecture that employs "),z=n("a"),et=i("squeeze-and-excitation blocks"),tt=i(" to enable the network to perform dynamic channel-wise feature recalibration."),ve=h(),V=n("p"),st=i("The weights from this model were ported from Gluon."),je=h(),$=n("h2"),S=n("a"),re=n("span"),f(L.$$.fragment),at=h(),ie=n("span"),nt=i("How do I use this model on an image?"),ye=h(),W=n("p"),lt=i("To load a pretrained model:"),$e=h(),f(G.$$.fragment),be=h(),X=n("p"),ot=i("To load and preprocess the image:"),ke=h(),f(R.$$.fragment),Ee=h(),K=n("p"),rt=i("To get the model predictions:"),xe=h(),f(Y.$$.fragment),Se=h(),Q=n("p"),it=i("To get the top-5 predictions class names:"),Pe=h(),f(M.$$.fragment),qe=h(),P=n("p"),pt=i("Replace the model name with the variant you want to use, e.g. "),pe=n("code"),mt=i("legacy_senet154"),ht=i(". You can find the IDs in the model summaries at the top of this page."),Ae=h(),q=n("p"),ct=i("To extract image features with this model, follow the "),Z=n("a"),ft=i("timm feature extraction examples"),ut=i(", just change the name of the model you want to use."),Ne=h(),b=n("h2"),A=n("a"),me=n("span"),f(U.$$.fragment),dt=h(),he=n("span"),gt=i("How do I finetune this model?"),Te=h(),ee=n("p"),wt=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ie=h(),f(B.$$.fragment),He=h(),N=n("p"),_t=i("To finetune on your own dataset, you have to write a training loop or adapt "),D=n("a"),vt=i(`timm\u2019s training
script`),jt=i(" to use your dataset."),Ce=h(),k=n("h2"),T=n("a"),ce=n("span"),f(J.$$.fragment),yt=h(),fe=n("span"),$t=i("How do I train this model?"),ze=h(),I=n("p"),bt=i("You can follow the "),te=n("a"),kt=i("timm recipe scripts"),Et=i(" for training a new model afresh."),Le=h(),E=n("h2"),H=n("a"),ue=n("span"),f(O.$$.fragment),xt=h(),de=n("span"),St=i("Citation"),Ge=h(),f(F.$$.fragment),this.h()},l(e){const a=ls('[data-svelte="svelte-1phssyn"]',document.head);j=l(a,"META",{name:!0,content:!0}),a.forEach(t),we=c(e),y=l(e,"H1",{class:!0});var Ye=r(y);x=l(Ye,"A",{id:!0,class:!0,href:!0});var qt=r(x);ne=l(qt,"SPAN",{});var At=r(ne);u(C.$$.fragment,At),At.forEach(t),qt.forEach(t),We=c(Ye),le=l(Ye,"SPAN",{});var Nt=r(le);Xe=p(Nt,"(Legacy) SENet"),Nt.forEach(t),Ye.forEach(t),_e=c(e),v=l(e,"P",{});var se=r(v);Ke=p(se,"A "),oe=l(se,"STRONG",{});var Tt=r(oe);Qe=p(Tt,"SENet"),Tt.forEach(t),Ze=p(se," is a convolutional neural network architecture that employs "),z=l(se,"A",{href:!0,rel:!0});var It=r(z);et=p(It,"squeeze-and-excitation blocks"),It.forEach(t),tt=p(se," to enable the network to perform dynamic channel-wise feature recalibration."),se.forEach(t),ve=c(e),V=l(e,"P",{});var Ht=r(V);st=p(Ht,"The weights from this model were ported from Gluon."),Ht.forEach(t),je=c(e),$=l(e,"H2",{class:!0});var Me=r($);S=l(Me,"A",{id:!0,class:!0,href:!0});var Ct=r(S);re=l(Ct,"SPAN",{});var zt=r(re);u(L.$$.fragment,zt),zt.forEach(t),Ct.forEach(t),at=c(Me),ie=l(Me,"SPAN",{});var Lt=r(ie);nt=p(Lt,"How do I use this model on an image?"),Lt.forEach(t),Me.forEach(t),ye=c(e),W=l(e,"P",{});var Gt=r(W);lt=p(Gt,"To load a pretrained model:"),Gt.forEach(t),$e=c(e),u(G.$$.fragment,e),be=c(e),X=l(e,"P",{});var Rt=r(X);ot=p(Rt,"To load and preprocess the image:"),Rt.forEach(t),ke=c(e),u(R.$$.fragment,e),Ee=c(e),K=l(e,"P",{});var Yt=r(K);rt=p(Yt,"To get the model predictions:"),Yt.forEach(t),xe=c(e),u(Y.$$.fragment,e),Se=c(e),Q=l(e,"P",{});var Mt=r(Q);it=p(Mt,"To get the top-5 predictions class names:"),Mt.forEach(t),Pe=c(e),u(M.$$.fragment,e),qe=c(e),P=l(e,"P",{});var Ue=r(P);pt=p(Ue,"Replace the model name with the variant you want to use, e.g. "),pe=l(Ue,"CODE",{});var Ut=r(pe);mt=p(Ut,"legacy_senet154"),Ut.forEach(t),ht=p(Ue,". You can find the IDs in the model summaries at the top of this page."),Ue.forEach(t),Ae=c(e),q=l(e,"P",{});var Be=r(q);ct=p(Be,"To extract image features with this model, follow the "),Z=l(Be,"A",{href:!0});var Bt=r(Z);ft=p(Bt,"timm feature extraction examples"),Bt.forEach(t),ut=p(Be,", just change the name of the model you want to use."),Be.forEach(t),Ne=c(e),b=l(e,"H2",{class:!0});var De=r(b);A=l(De,"A",{id:!0,class:!0,href:!0});var Dt=r(A);me=l(Dt,"SPAN",{});var Jt=r(me);u(U.$$.fragment,Jt),Jt.forEach(t),Dt.forEach(t),dt=c(De),he=l(De,"SPAN",{});var Ot=r(he);gt=p(Ot,"How do I finetune this model?"),Ot.forEach(t),De.forEach(t),Te=c(e),ee=l(e,"P",{});var Ft=r(ee);wt=p(Ft,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ft.forEach(t),Ie=c(e),u(B.$$.fragment,e),He=c(e),N=l(e,"P",{});var Je=r(N);_t=p(Je,"To finetune on your own dataset, you have to write a training loop or adapt "),D=l(Je,"A",{href:!0,rel:!0});var Vt=r(D);vt=p(Vt,`timm\u2019s training
script`),Vt.forEach(t),jt=p(Je," to use your dataset."),Je.forEach(t),Ce=c(e),k=l(e,"H2",{class:!0});var Oe=r(k);T=l(Oe,"A",{id:!0,class:!0,href:!0});var Wt=r(T);ce=l(Wt,"SPAN",{});var Xt=r(ce);u(J.$$.fragment,Xt),Xt.forEach(t),Wt.forEach(t),yt=c(Oe),fe=l(Oe,"SPAN",{});var Kt=r(fe);$t=p(Kt,"How do I train this model?"),Kt.forEach(t),Oe.forEach(t),ze=c(e),I=l(e,"P",{});var Fe=r(I);bt=p(Fe,"You can follow the "),te=l(Fe,"A",{href:!0});var Qt=r(te);kt=p(Qt,"timm recipe scripts"),Qt.forEach(t),Et=p(Fe," for training a new model afresh."),Fe.forEach(t),Le=c(e),E=l(e,"H2",{class:!0});var Ve=r(E);H=l(Ve,"A",{id:!0,class:!0,href:!0});var Zt=r(H);ue=l(Zt,"SPAN",{});var es=r(ue);u(O.$$.fragment,es),es.forEach(t),Zt.forEach(t),xt=c(Ve),de=l(Ve,"SPAN",{});var ts=r(de);St=p(ts,"Citation"),ts.forEach(t),Ve.forEach(t),Ge=c(e),u(F.$$.fragment,e),this.h()},h(){m(j,"name","hf:doc:metadata"),m(j,"content",JSON.stringify(ps)),m(x,"id","legacy-senet"),m(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(x,"href","#legacy-senet"),m(y,"class","relative group"),m(z,"href","https://paperswithcode.com/method/squeeze-and-excitation-block"),m(z,"rel","nofollow"),m(S,"id","how-do-i-use-this-model-on-an-image"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#how-do-i-use-this-model-on-an-image"),m($,"class","relative group"),m(Z,"href","../feature_extraction"),m(A,"id","how-do-i-finetune-this-model"),m(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(A,"href","#how-do-i-finetune-this-model"),m(b,"class","relative group"),m(D,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(D,"rel","nofollow"),m(T,"id","how-do-i-train-this-model"),m(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(T,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(te,"href","../scripts"),m(H,"id","citation"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#citation"),m(E,"class","relative group")},m(e,a){s(document.head,j),o(e,we,a),o(e,y,a),s(y,x),s(x,ne),d(C,ne,null),s(y,We),s(y,le),s(le,Xe),o(e,_e,a),o(e,v,a),s(v,Ke),s(v,oe),s(oe,Qe),s(v,Ze),s(v,z),s(z,et),s(v,tt),o(e,ve,a),o(e,V,a),s(V,st),o(e,je,a),o(e,$,a),s($,S),s(S,re),d(L,re,null),s($,at),s($,ie),s(ie,nt),o(e,ye,a),o(e,W,a),s(W,lt),o(e,$e,a),d(G,e,a),o(e,be,a),o(e,X,a),s(X,ot),o(e,ke,a),d(R,e,a),o(e,Ee,a),o(e,K,a),s(K,rt),o(e,xe,a),d(Y,e,a),o(e,Se,a),o(e,Q,a),s(Q,it),o(e,Pe,a),d(M,e,a),o(e,qe,a),o(e,P,a),s(P,pt),s(P,pe),s(pe,mt),s(P,ht),o(e,Ae,a),o(e,q,a),s(q,ct),s(q,Z),s(Z,ft),s(q,ut),o(e,Ne,a),o(e,b,a),s(b,A),s(A,me),d(U,me,null),s(b,dt),s(b,he),s(he,gt),o(e,Te,a),o(e,ee,a),s(ee,wt),o(e,Ie,a),d(B,e,a),o(e,He,a),o(e,N,a),s(N,_t),s(N,D),s(D,vt),s(N,jt),o(e,Ce,a),o(e,k,a),s(k,T),s(T,ce),d(J,ce,null),s(k,yt),s(k,fe),s(fe,$t),o(e,ze,a),o(e,I,a),s(I,bt),s(I,te),s(te,kt),s(I,Et),o(e,Le,a),o(e,E,a),s(E,H),s(H,ue),d(O,ue,null),s(E,xt),s(E,de),s(de,St),o(e,Ge,a),d(F,e,a),Re=!0},p:os,i(e){Re||(g(C.$$.fragment,e),g(L.$$.fragment,e),g(G.$$.fragment,e),g(R.$$.fragment,e),g(Y.$$.fragment,e),g(M.$$.fragment,e),g(U.$$.fragment,e),g(B.$$.fragment,e),g(J.$$.fragment,e),g(O.$$.fragment,e),g(F.$$.fragment,e),Re=!0)},o(e){w(C.$$.fragment,e),w(L.$$.fragment,e),w(G.$$.fragment,e),w(R.$$.fragment,e),w(Y.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(B.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(F.$$.fragment,e),Re=!1},d(e){t(j),e&&t(we),e&&t(y),_(C),e&&t(_e),e&&t(v),e&&t(ve),e&&t(V),e&&t(je),e&&t($),_(L),e&&t(ye),e&&t(W),e&&t($e),_(G,e),e&&t(be),e&&t(X),e&&t(ke),_(R,e),e&&t(Ee),e&&t(K),e&&t(xe),_(Y,e),e&&t(Se),e&&t(Q),e&&t(Pe),_(M,e),e&&t(qe),e&&t(P),e&&t(Ae),e&&t(q),e&&t(Ne),e&&t(b),_(U),e&&t(Te),e&&t(ee),e&&t(Ie),_(B,e),e&&t(He),e&&t(N),e&&t(Ce),e&&t(k),_(J),e&&t(ze),e&&t(I),e&&t(Le),e&&t(E),_(O),e&&t(Ge),_(F,e)}}}const ps={local:"legacy-senet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"(Legacy) SENet"};function ms(Pt){return rs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class us extends ss{constructor(j){super();as(this,j,ms,is,ns,{})}}export{us as default,ps as metadata};
