import{S as ls,i as os,s as is,e as n,k as h,w as d,t as p,M as ps,c as r,d as t,m as c,a as l,x as u,h as m,b as i,G as s,g as o,y as g,L as ms,q as w,o as _,B as v,v as hs}from"../../chunks/vendor-hf-doc-builder.js";import{I as we}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";function cs(St){let $,_e,y,P,re,C,Je,le,Ke,ve,f,Qe,oe,We,Ze,B,et,tt,G,st,at,z,nt,rt,je,b,A,ie,L,lt,pe,ot,$e,Q,it,ye,U,be,W,pt,ke,D,Ee,Z,mt,xe,Y,Pe,ee,ht,Ae,M,Te,T,ct,me,ft,dt,Ne,N,ut,te,gt,wt,Se,k,S,he,F,_t,ce,vt,Ie,se,jt,qe,O,He,I,$t,V,yt,bt,Re,E,q,fe,X,kt,de,Et,Ce,H,xt,ae,Pt,At,Be,x,R,ue,J,Tt,ge,Nt,Ge,K,ze;return C=new we({}),L=new we({}),U=new ne({props:{code:`import timm
model = timm.create_model('tresnet_l', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;tresnet_l&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),D=new ne({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),Y=new ne({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),M=new ne({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),F=new we({}),O=new ne({props:{code:"model = timm.create_model('tresnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;tresnet_l&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),X=new we({}),J=new we({}),K=new ne({props:{code:`@misc{ridnik2020tresnet,
      title={TResNet: High Performance GPU-Dedicated Architecture}, 
      author={Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman},
      year={2020},
      eprint={2003.13630},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{ridnik2020tresnet,
      title={TResNet: High Performance GPU-Dedicated Architecture}, 
      author={Tal Ridnik <span class="hljs-keyword">and </span>Hussam Lawen <span class="hljs-keyword">and </span>Asaf Noy <span class="hljs-keyword">and </span>Emanuel <span class="hljs-keyword">Ben </span><span class="hljs-keyword">Baruch </span><span class="hljs-keyword">and </span>Gilad <span class="hljs-keyword">Sharir </span><span class="hljs-keyword">and </span>Itamar Friedman},
      year={<span class="hljs-number">2020</span>},
      eprint={<span class="hljs-number">2003</span>.<span class="hljs-number">13630</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){$=n("meta"),_e=h(),y=n("h1"),P=n("a"),re=n("span"),d(C.$$.fragment),Je=h(),le=n("span"),Ke=p("TResNet"),ve=h(),f=n("p"),Qe=p("A "),oe=n("strong"),We=p("TResNet"),Ze=p(" is a variant on a "),B=n("a"),et=p("ResNet"),tt=p(" that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, "),G=n("a"),st=p("Anti-Alias downsampling"),at=p(", In-Place Activated BatchNorm, Blocks selection and "),z=n("a"),nt=p("squeeze-and-excitation layers"),rt=p("."),je=h(),b=n("h2"),A=n("a"),ie=n("span"),d(L.$$.fragment),lt=h(),pe=n("span"),ot=p("How do I use this model on an image?"),$e=h(),Q=n("p"),it=p("To load a pretrained model:"),ye=h(),d(U.$$.fragment),be=h(),W=n("p"),pt=p("To load and preprocess the image:"),ke=h(),d(D.$$.fragment),Ee=h(),Z=n("p"),mt=p("To get the model predictions:"),xe=h(),d(Y.$$.fragment),Pe=h(),ee=n("p"),ht=p("To get the top-5 predictions class names:"),Ae=h(),d(M.$$.fragment),Te=h(),T=n("p"),ct=p("Replace the model name with the variant you want to use, e.g. "),me=n("code"),ft=p("tresnet_l"),dt=p(". You can find the IDs in the model summaries at the top of this page."),Ne=h(),N=n("p"),ut=p("To extract image features with this model, follow the "),te=n("a"),gt=p("timm feature extraction examples"),wt=p(", just change the name of the model you want to use."),Se=h(),k=n("h2"),S=n("a"),he=n("span"),d(F.$$.fragment),_t=h(),ce=n("span"),vt=p("How do I finetune this model?"),Ie=h(),se=n("p"),jt=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qe=h(),d(O.$$.fragment),He=h(),I=n("p"),$t=p("To finetune on your own dataset, you have to write a training loop or adapt "),V=n("a"),yt=p(`timm\u2019s training
script`),bt=p(" to use your dataset."),Re=h(),E=n("h2"),q=n("a"),fe=n("span"),d(X.$$.fragment),kt=h(),de=n("span"),Et=p("How do I train this model?"),Ce=h(),H=n("p"),xt=p("You can follow the "),ae=n("a"),Pt=p("timm recipe scripts"),At=p(" for training a new model afresh."),Be=h(),x=n("h2"),R=n("a"),ue=n("span"),d(J.$$.fragment),Tt=h(),ge=n("span"),Nt=p("Citation"),Ge=h(),d(K.$$.fragment),this.h()},l(e){const a=ps('[data-svelte="svelte-1phssyn"]',document.head);$=r(a,"META",{name:!0,content:!0}),a.forEach(t),_e=c(e),y=r(e,"H1",{class:!0});var Le=l(y);P=r(Le,"A",{id:!0,class:!0,href:!0});var It=l(P);re=r(It,"SPAN",{});var qt=l(re);u(C.$$.fragment,qt),qt.forEach(t),It.forEach(t),Je=c(Le),le=r(Le,"SPAN",{});var Ht=l(le);Ke=m(Ht,"TResNet"),Ht.forEach(t),Le.forEach(t),ve=c(e),f=r(e,"P",{});var j=l(f);Qe=m(j,"A "),oe=r(j,"STRONG",{});var Rt=l(oe);We=m(Rt,"TResNet"),Rt.forEach(t),Ze=m(j," is a variant on a "),B=r(j,"A",{href:!0,rel:!0});var Ct=l(B);et=m(Ct,"ResNet"),Ct.forEach(t),tt=m(j," that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, "),G=r(j,"A",{href:!0,rel:!0});var Bt=l(G);st=m(Bt,"Anti-Alias downsampling"),Bt.forEach(t),at=m(j,", In-Place Activated BatchNorm, Blocks selection and "),z=r(j,"A",{href:!0,rel:!0});var Gt=l(z);nt=m(Gt,"squeeze-and-excitation layers"),Gt.forEach(t),rt=m(j,"."),j.forEach(t),je=c(e),b=r(e,"H2",{class:!0});var Ue=l(b);A=r(Ue,"A",{id:!0,class:!0,href:!0});var zt=l(A);ie=r(zt,"SPAN",{});var Lt=l(ie);u(L.$$.fragment,Lt),Lt.forEach(t),zt.forEach(t),lt=c(Ue),pe=r(Ue,"SPAN",{});var Ut=l(pe);ot=m(Ut,"How do I use this model on an image?"),Ut.forEach(t),Ue.forEach(t),$e=c(e),Q=r(e,"P",{});var Dt=l(Q);it=m(Dt,"To load a pretrained model:"),Dt.forEach(t),ye=c(e),u(U.$$.fragment,e),be=c(e),W=r(e,"P",{});var Yt=l(W);pt=m(Yt,"To load and preprocess the image:"),Yt.forEach(t),ke=c(e),u(D.$$.fragment,e),Ee=c(e),Z=r(e,"P",{});var Mt=l(Z);mt=m(Mt,"To get the model predictions:"),Mt.forEach(t),xe=c(e),u(Y.$$.fragment,e),Pe=c(e),ee=r(e,"P",{});var Ft=l(ee);ht=m(Ft,"To get the top-5 predictions class names:"),Ft.forEach(t),Ae=c(e),u(M.$$.fragment,e),Te=c(e),T=r(e,"P",{});var De=l(T);ct=m(De,"Replace the model name with the variant you want to use, e.g. "),me=r(De,"CODE",{});var Ot=l(me);ft=m(Ot,"tresnet_l"),Ot.forEach(t),dt=m(De,". You can find the IDs in the model summaries at the top of this page."),De.forEach(t),Ne=c(e),N=r(e,"P",{});var Ye=l(N);ut=m(Ye,"To extract image features with this model, follow the "),te=r(Ye,"A",{href:!0});var Vt=l(te);gt=m(Vt,"timm feature extraction examples"),Vt.forEach(t),wt=m(Ye,", just change the name of the model you want to use."),Ye.forEach(t),Se=c(e),k=r(e,"H2",{class:!0});var Me=l(k);S=r(Me,"A",{id:!0,class:!0,href:!0});var Xt=l(S);he=r(Xt,"SPAN",{});var Jt=l(he);u(F.$$.fragment,Jt),Jt.forEach(t),Xt.forEach(t),_t=c(Me),ce=r(Me,"SPAN",{});var Kt=l(ce);vt=m(Kt,"How do I finetune this model?"),Kt.forEach(t),Me.forEach(t),Ie=c(e),se=r(e,"P",{});var Qt=l(se);jt=m(Qt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Qt.forEach(t),qe=c(e),u(O.$$.fragment,e),He=c(e),I=r(e,"P",{});var Fe=l(I);$t=m(Fe,"To finetune on your own dataset, you have to write a training loop or adapt "),V=r(Fe,"A",{href:!0,rel:!0});var Wt=l(V);yt=m(Wt,`timm\u2019s training
script`),Wt.forEach(t),bt=m(Fe," to use your dataset."),Fe.forEach(t),Re=c(e),E=r(e,"H2",{class:!0});var Oe=l(E);q=r(Oe,"A",{id:!0,class:!0,href:!0});var Zt=l(q);fe=r(Zt,"SPAN",{});var es=l(fe);u(X.$$.fragment,es),es.forEach(t),Zt.forEach(t),kt=c(Oe),de=r(Oe,"SPAN",{});var ts=l(de);Et=m(ts,"How do I train this model?"),ts.forEach(t),Oe.forEach(t),Ce=c(e),H=r(e,"P",{});var Ve=l(H);xt=m(Ve,"You can follow the "),ae=r(Ve,"A",{href:!0});var ss=l(ae);Pt=m(ss,"timm recipe scripts"),ss.forEach(t),At=m(Ve," for training a new model afresh."),Ve.forEach(t),Be=c(e),x=r(e,"H2",{class:!0});var Xe=l(x);R=r(Xe,"A",{id:!0,class:!0,href:!0});var as=l(R);ue=r(as,"SPAN",{});var ns=l(ue);u(J.$$.fragment,ns),ns.forEach(t),as.forEach(t),Tt=c(Xe),ge=r(Xe,"SPAN",{});var rs=l(ge);Nt=m(rs,"Citation"),rs.forEach(t),Xe.forEach(t),Ge=c(e),u(K.$$.fragment,e),this.h()},h(){i($,"name","hf:doc:metadata"),i($,"content",JSON.stringify(fs)),i(P,"id","tresnet"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#tresnet"),i(y,"class","relative group"),i(B,"href","https://paperswithcode.com/method/resnet"),i(B,"rel","nofollow"),i(G,"href","https://paperswithcode.com/method/anti-alias-downsampling"),i(G,"rel","nofollow"),i(z,"href","https://paperswithcode.com/method/squeeze-and-excitation-block"),i(z,"rel","nofollow"),i(A,"id","how-do-i-use-this-model-on-an-image"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-use-this-model-on-an-image"),i(b,"class","relative group"),i(te,"href","../feature_extraction"),i(S,"id","how-do-i-finetune-this-model"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-finetune-this-model"),i(k,"class","relative group"),i(V,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(V,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(E,"class","relative group"),i(ae,"href","../scripts"),i(R,"id","citation"),i(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(R,"href","#citation"),i(x,"class","relative group")},m(e,a){s(document.head,$),o(e,_e,a),o(e,y,a),s(y,P),s(P,re),g(C,re,null),s(y,Je),s(y,le),s(le,Ke),o(e,ve,a),o(e,f,a),s(f,Qe),s(f,oe),s(oe,We),s(f,Ze),s(f,B),s(B,et),s(f,tt),s(f,G),s(G,st),s(f,at),s(f,z),s(z,nt),s(f,rt),o(e,je,a),o(e,b,a),s(b,A),s(A,ie),g(L,ie,null),s(b,lt),s(b,pe),s(pe,ot),o(e,$e,a),o(e,Q,a),s(Q,it),o(e,ye,a),g(U,e,a),o(e,be,a),o(e,W,a),s(W,pt),o(e,ke,a),g(D,e,a),o(e,Ee,a),o(e,Z,a),s(Z,mt),o(e,xe,a),g(Y,e,a),o(e,Pe,a),o(e,ee,a),s(ee,ht),o(e,Ae,a),g(M,e,a),o(e,Te,a),o(e,T,a),s(T,ct),s(T,me),s(me,ft),s(T,dt),o(e,Ne,a),o(e,N,a),s(N,ut),s(N,te),s(te,gt),s(N,wt),o(e,Se,a),o(e,k,a),s(k,S),s(S,he),g(F,he,null),s(k,_t),s(k,ce),s(ce,vt),o(e,Ie,a),o(e,se,a),s(se,jt),o(e,qe,a),g(O,e,a),o(e,He,a),o(e,I,a),s(I,$t),s(I,V),s(V,yt),s(I,bt),o(e,Re,a),o(e,E,a),s(E,q),s(q,fe),g(X,fe,null),s(E,kt),s(E,de),s(de,Et),o(e,Ce,a),o(e,H,a),s(H,xt),s(H,ae),s(ae,Pt),s(H,At),o(e,Be,a),o(e,x,a),s(x,R),s(R,ue),g(J,ue,null),s(x,Tt),s(x,ge),s(ge,Nt),o(e,Ge,a),g(K,e,a),ze=!0},p:ms,i(e){ze||(w(C.$$.fragment,e),w(L.$$.fragment,e),w(U.$$.fragment,e),w(D.$$.fragment,e),w(Y.$$.fragment,e),w(M.$$.fragment,e),w(F.$$.fragment,e),w(O.$$.fragment,e),w(X.$$.fragment,e),w(J.$$.fragment,e),w(K.$$.fragment,e),ze=!0)},o(e){_(C.$$.fragment,e),_(L.$$.fragment,e),_(U.$$.fragment,e),_(D.$$.fragment,e),_(Y.$$.fragment,e),_(M.$$.fragment,e),_(F.$$.fragment,e),_(O.$$.fragment,e),_(X.$$.fragment,e),_(J.$$.fragment,e),_(K.$$.fragment,e),ze=!1},d(e){t($),e&&t(_e),e&&t(y),v(C),e&&t(ve),e&&t(f),e&&t(je),e&&t(b),v(L),e&&t($e),e&&t(Q),e&&t(ye),v(U,e),e&&t(be),e&&t(W),e&&t(ke),v(D,e),e&&t(Ee),e&&t(Z),e&&t(xe),v(Y,e),e&&t(Pe),e&&t(ee),e&&t(Ae),v(M,e),e&&t(Te),e&&t(T),e&&t(Ne),e&&t(N),e&&t(Se),e&&t(k),v(F),e&&t(Ie),e&&t(se),e&&t(qe),v(O,e),e&&t(He),e&&t(I),e&&t(Re),e&&t(E),v(X),e&&t(Ce),e&&t(H),e&&t(Be),e&&t(x),v(J),e&&t(Ge),v(K,e)}}}const fs={local:"tresnet",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"TResNet"};function ds(St){return hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _s extends ls{constructor($){super();os(this,$,ds,cs,is,{})}}export{_s as default,fs as metadata};
