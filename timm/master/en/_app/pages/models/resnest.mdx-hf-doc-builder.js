import{S as ss,i as as,s as ns,e as n,k as m,w as u,t as p,M as os,c as o,d as e,m as c,a as l,x as d,h,b as i,G as s,g as r,y as g,L as ls,q as w,o as $,B as v,v as rs}from"../../chunks/vendor-hf-doc-builder.js";import{I as gt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as at}from"../../chunks/CodeBlock-hf-doc-builder.js";function is(Ae){let _,wt,j,E,nt,R,Dt,ot,Jt,$t,f,Ot,lt,Kt,Wt,Y,Qt,te,V,ee,se,vt,b,S,rt,Z,ae,it,ne,_t,O,oe,jt,L,bt,K,le,yt,M,kt,W,re,xt,z,Et,Q,ie,St,F,At,A,pe,pt,he,me,Pt,P,ce,tt,fe,ue,Tt,y,T,ht,G,de,mt,ge,Nt,et,we,qt,X,It,N,$e,U,ve,_e,Ht,k,q,ct,B,je,ft,be,Ct,I,ye,st,ke,xe,Rt,x,H,ut,D,Ee,dt,Se,Yt,J,Vt;return R=new gt({}),Z=new gt({}),L=new at({props:{code:`import timm
model = timm.create_model('resnest101e', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnest101e&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),M=new at({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),z=new at({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),F=new at({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),G=new gt({}),X=new at({props:{code:"model = timm.create_model('resnest101e', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnest101e&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),B=new gt({}),D=new gt({}),J=new at({props:{code:`@misc{zhang2020resnest,
      title={ResNeSt: Split-Attention Networks}, 
      author={Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola},
      year={2020},
      eprint={2004.08955},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`@misc{zhang2020resnest,
      title={ResNeSt: Split-Attention Networks}, 
      author={Hang Zhang <span class="hljs-keyword">and </span>Chongruo Wu <span class="hljs-keyword">and </span>Zhongyue Zhang <span class="hljs-keyword">and </span>Yi Zhu <span class="hljs-keyword">and </span>Haibin Lin <span class="hljs-keyword">and </span>Zhi Zhang <span class="hljs-keyword">and </span>Yue Sun <span class="hljs-keyword">and </span>Tong He <span class="hljs-keyword">and </span><span class="hljs-keyword">Jonas </span>Mueller <span class="hljs-keyword">and </span>R. Manmatha <span class="hljs-keyword">and </span>Mu Li <span class="hljs-keyword">and </span>Alexander Smola},
      year={<span class="hljs-number">2020</span>},
      eprint={<span class="hljs-number">2004</span>.<span class="hljs-number">08955</span>},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`}}),{c(){_=n("meta"),wt=m(),j=n("h1"),E=n("a"),nt=n("span"),u(R.$$.fragment),Dt=m(),ot=n("span"),Jt=p("ResNeSt"),$t=m(),f=n("p"),Ot=p("A "),lt=n("strong"),Kt=p("ResNeSt"),Wt=p(" is a variant on a "),Y=n("a"),Qt=p("ResNet"),te=p(", which instead stacks "),V=n("a"),ee=p("Split-Attention blocks"),se=p(". The cardinal group representations are then concatenated along the channel dimension: $V = \\text{Concat}${$V^{1},V^{2},\\cdots{V}^{K}$}. As in standard residual blocks, the final output $Y$ of otheur Split-Attention block is produced using a shortcut connection: $Y=V+X$, if the input and output feature-map share the same shape.  For blocks with a stride, an appropriate transformation $\\mathcal{T}$ is applied to the shortcut connection to align the output shapes:  $Y=V+\\mathcal{T}(X)$. For example, $\\mathcal{T}$ can be strided convolution or combined convolution-with-pooling."),vt=m(),b=n("h2"),S=n("a"),rt=n("span"),u(Z.$$.fragment),ae=m(),it=n("span"),ne=p("How do I use this model on an image?"),_t=m(),O=n("p"),oe=p("To load a pretrained model:"),jt=m(),u(L.$$.fragment),bt=m(),K=n("p"),le=p("To load and preprocess the image:"),yt=m(),u(M.$$.fragment),kt=m(),W=n("p"),re=p("To get the model predictions:"),xt=m(),u(z.$$.fragment),Et=m(),Q=n("p"),ie=p("To get the top-5 predictions class names:"),St=m(),u(F.$$.fragment),At=m(),A=n("p"),pe=p("Replace the model name with the variant you want to use, e.g. "),pt=n("code"),he=p("resnest101e"),me=p(". You can find the IDs in the model summaries at the top of this page."),Pt=m(),P=n("p"),ce=p("To extract image features with this model, follow the "),tt=n("a"),fe=p("timm feature extraction examples"),ue=p(", just change the name of the model you want to use."),Tt=m(),y=n("h2"),T=n("a"),ht=n("span"),u(G.$$.fragment),de=m(),mt=n("span"),ge=p("How do I finetune this model?"),Nt=m(),et=n("p"),we=p("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qt=m(),u(X.$$.fragment),It=m(),N=n("p"),$e=p("To finetune on your own dataset, you have to write a training loop or adapt "),U=n("a"),ve=p(`timm\u2019s training
script`),_e=p(" to use your dataset."),Ht=m(),k=n("h2"),q=n("a"),ct=n("span"),u(B.$$.fragment),je=m(),ft=n("span"),be=p("How do I train this model?"),Ct=m(),I=n("p"),ye=p("You can follow the "),st=n("a"),ke=p("timm recipe scripts"),xe=p(" for training a new model afresh."),Rt=m(),x=n("h2"),H=n("a"),ut=n("span"),u(D.$$.fragment),Ee=m(),dt=n("span"),Se=p("Citation"),Yt=m(),u(J.$$.fragment),this.h()},l(t){const a=os('[data-svelte="svelte-1phssyn"]',document.head);_=o(a,"META",{name:!0,content:!0}),a.forEach(e),wt=c(t),j=o(t,"H1",{class:!0});var Zt=l(j);E=o(Zt,"A",{id:!0,class:!0,href:!0});var Pe=l(E);nt=o(Pe,"SPAN",{});var Te=l(nt);d(R.$$.fragment,Te),Te.forEach(e),Pe.forEach(e),Dt=c(Zt),ot=o(Zt,"SPAN",{});var Ne=l(ot);Jt=h(Ne,"ResNeSt"),Ne.forEach(e),Zt.forEach(e),$t=c(t),f=o(t,"P",{});var C=l(f);Ot=h(C,"A "),lt=o(C,"STRONG",{});var qe=l(lt);Kt=h(qe,"ResNeSt"),qe.forEach(e),Wt=h(C," is a variant on a "),Y=o(C,"A",{href:!0,rel:!0});var Ie=l(Y);Qt=h(Ie,"ResNet"),Ie.forEach(e),te=h(C,", which instead stacks "),V=o(C,"A",{href:!0,rel:!0});var He=l(V);ee=h(He,"Split-Attention blocks"),He.forEach(e),se=h(C,". The cardinal group representations are then concatenated along the channel dimension: $V = \\text{Concat}${$V^{1},V^{2},\\cdots{V}^{K}$}. As in standard residual blocks, the final output $Y$ of otheur Split-Attention block is produced using a shortcut connection: $Y=V+X$, if the input and output feature-map share the same shape.  For blocks with a stride, an appropriate transformation $\\mathcal{T}$ is applied to the shortcut connection to align the output shapes:  $Y=V+\\mathcal{T}(X)$. For example, $\\mathcal{T}$ can be strided convolution or combined convolution-with-pooling."),C.forEach(e),vt=c(t),b=o(t,"H2",{class:!0});var Lt=l(b);S=o(Lt,"A",{id:!0,class:!0,href:!0});var Ce=l(S);rt=o(Ce,"SPAN",{});var Re=l(rt);d(Z.$$.fragment,Re),Re.forEach(e),Ce.forEach(e),ae=c(Lt),it=o(Lt,"SPAN",{});var Ye=l(it);ne=h(Ye,"How do I use this model on an image?"),Ye.forEach(e),Lt.forEach(e),_t=c(t),O=o(t,"P",{});var Ve=l(O);oe=h(Ve,"To load a pretrained model:"),Ve.forEach(e),jt=c(t),d(L.$$.fragment,t),bt=c(t),K=o(t,"P",{});var Ze=l(K);le=h(Ze,"To load and preprocess the image:"),Ze.forEach(e),yt=c(t),d(M.$$.fragment,t),kt=c(t),W=o(t,"P",{});var Le=l(W);re=h(Le,"To get the model predictions:"),Le.forEach(e),xt=c(t),d(z.$$.fragment,t),Et=c(t),Q=o(t,"P",{});var Me=l(Q);ie=h(Me,"To get the top-5 predictions class names:"),Me.forEach(e),St=c(t),d(F.$$.fragment,t),At=c(t),A=o(t,"P",{});var Mt=l(A);pe=h(Mt,"Replace the model name with the variant you want to use, e.g. "),pt=o(Mt,"CODE",{});var ze=l(pt);he=h(ze,"resnest101e"),ze.forEach(e),me=h(Mt,". You can find the IDs in the model summaries at the top of this page."),Mt.forEach(e),Pt=c(t),P=o(t,"P",{});var zt=l(P);ce=h(zt,"To extract image features with this model, follow the "),tt=o(zt,"A",{href:!0});var Fe=l(tt);fe=h(Fe,"timm feature extraction examples"),Fe.forEach(e),ue=h(zt,", just change the name of the model you want to use."),zt.forEach(e),Tt=c(t),y=o(t,"H2",{class:!0});var Ft=l(y);T=o(Ft,"A",{id:!0,class:!0,href:!0});var Ge=l(T);ht=o(Ge,"SPAN",{});var Xe=l(ht);d(G.$$.fragment,Xe),Xe.forEach(e),Ge.forEach(e),de=c(Ft),mt=o(Ft,"SPAN",{});var Ue=l(mt);ge=h(Ue,"How do I finetune this model?"),Ue.forEach(e),Ft.forEach(e),Nt=c(t),et=o(t,"P",{});var Be=l(et);we=h(Be,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Be.forEach(e),qt=c(t),d(X.$$.fragment,t),It=c(t),N=o(t,"P",{});var Gt=l(N);$e=h(Gt,"To finetune on your own dataset, you have to write a training loop or adapt "),U=o(Gt,"A",{href:!0,rel:!0});var De=l(U);ve=h(De,`timm\u2019s training
script`),De.forEach(e),_e=h(Gt," to use your dataset."),Gt.forEach(e),Ht=c(t),k=o(t,"H2",{class:!0});var Xt=l(k);q=o(Xt,"A",{id:!0,class:!0,href:!0});var Je=l(q);ct=o(Je,"SPAN",{});var Oe=l(ct);d(B.$$.fragment,Oe),Oe.forEach(e),Je.forEach(e),je=c(Xt),ft=o(Xt,"SPAN",{});var Ke=l(ft);be=h(Ke,"How do I train this model?"),Ke.forEach(e),Xt.forEach(e),Ct=c(t),I=o(t,"P",{});var Ut=l(I);ye=h(Ut,"You can follow the "),st=o(Ut,"A",{href:!0});var We=l(st);ke=h(We,"timm recipe scripts"),We.forEach(e),xe=h(Ut," for training a new model afresh."),Ut.forEach(e),Rt=c(t),x=o(t,"H2",{class:!0});var Bt=l(x);H=o(Bt,"A",{id:!0,class:!0,href:!0});var Qe=l(H);ut=o(Qe,"SPAN",{});var ts=l(ut);d(D.$$.fragment,ts),ts.forEach(e),Qe.forEach(e),Ee=c(Bt),dt=o(Bt,"SPAN",{});var es=l(dt);Se=h(es,"Citation"),es.forEach(e),Bt.forEach(e),Yt=c(t),d(J.$$.fragment,t),this.h()},h(){i(_,"name","hf:doc:metadata"),i(_,"content",JSON.stringify(ps)),i(E,"id","resnest"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#resnest"),i(j,"class","relative group"),i(Y,"href","https://paperswithcode.com/method/resnet"),i(Y,"rel","nofollow"),i(V,"href","https://paperswithcode.com/method/split-attention"),i(V,"rel","nofollow"),i(S,"id","how-do-i-use-this-model-on-an-image"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-use-this-model-on-an-image"),i(b,"class","relative group"),i(tt,"href","../feature_extraction"),i(T,"id","how-do-i-finetune-this-model"),i(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(T,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(U,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(U,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(k,"class","relative group"),i(st,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(x,"class","relative group")},m(t,a){s(document.head,_),r(t,wt,a),r(t,j,a),s(j,E),s(E,nt),g(R,nt,null),s(j,Dt),s(j,ot),s(ot,Jt),r(t,$t,a),r(t,f,a),s(f,Ot),s(f,lt),s(lt,Kt),s(f,Wt),s(f,Y),s(Y,Qt),s(f,te),s(f,V),s(V,ee),s(f,se),r(t,vt,a),r(t,b,a),s(b,S),s(S,rt),g(Z,rt,null),s(b,ae),s(b,it),s(it,ne),r(t,_t,a),r(t,O,a),s(O,oe),r(t,jt,a),g(L,t,a),r(t,bt,a),r(t,K,a),s(K,le),r(t,yt,a),g(M,t,a),r(t,kt,a),r(t,W,a),s(W,re),r(t,xt,a),g(z,t,a),r(t,Et,a),r(t,Q,a),s(Q,ie),r(t,St,a),g(F,t,a),r(t,At,a),r(t,A,a),s(A,pe),s(A,pt),s(pt,he),s(A,me),r(t,Pt,a),r(t,P,a),s(P,ce),s(P,tt),s(tt,fe),s(P,ue),r(t,Tt,a),r(t,y,a),s(y,T),s(T,ht),g(G,ht,null),s(y,de),s(y,mt),s(mt,ge),r(t,Nt,a),r(t,et,a),s(et,we),r(t,qt,a),g(X,t,a),r(t,It,a),r(t,N,a),s(N,$e),s(N,U),s(U,ve),s(N,_e),r(t,Ht,a),r(t,k,a),s(k,q),s(q,ct),g(B,ct,null),s(k,je),s(k,ft),s(ft,be),r(t,Ct,a),r(t,I,a),s(I,ye),s(I,st),s(st,ke),s(I,xe),r(t,Rt,a),r(t,x,a),s(x,H),s(H,ut),g(D,ut,null),s(x,Ee),s(x,dt),s(dt,Se),r(t,Yt,a),g(J,t,a),Vt=!0},p:ls,i(t){Vt||(w(R.$$.fragment,t),w(Z.$$.fragment,t),w(L.$$.fragment,t),w(M.$$.fragment,t),w(z.$$.fragment,t),w(F.$$.fragment,t),w(G.$$.fragment,t),w(X.$$.fragment,t),w(B.$$.fragment,t),w(D.$$.fragment,t),w(J.$$.fragment,t),Vt=!0)},o(t){$(R.$$.fragment,t),$(Z.$$.fragment,t),$(L.$$.fragment,t),$(M.$$.fragment,t),$(z.$$.fragment,t),$(F.$$.fragment,t),$(G.$$.fragment,t),$(X.$$.fragment,t),$(B.$$.fragment,t),$(D.$$.fragment,t),$(J.$$.fragment,t),Vt=!1},d(t){e(_),t&&e(wt),t&&e(j),v(R),t&&e($t),t&&e(f),t&&e(vt),t&&e(b),v(Z),t&&e(_t),t&&e(O),t&&e(jt),v(L,t),t&&e(bt),t&&e(K),t&&e(yt),v(M,t),t&&e(kt),t&&e(W),t&&e(xt),v(z,t),t&&e(Et),t&&e(Q),t&&e(St),v(F,t),t&&e(At),t&&e(A),t&&e(Pt),t&&e(P),t&&e(Tt),t&&e(y),v(G),t&&e(Nt),t&&e(et),t&&e(qt),v(X,t),t&&e(It),t&&e(N),t&&e(Ht),t&&e(k),v(B),t&&e(Ct),t&&e(I),t&&e(Rt),t&&e(x),v(D),t&&e(Yt),v(J,t)}}}const ps={local:"resnest",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"ResNeSt"};function hs(Ae){return rs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class us extends ss{constructor(_){super();as(this,_,hs,is,ns,{})}}export{us as default,ps as metadata};
