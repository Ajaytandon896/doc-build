import{S as le,i as oe,s as ie,e as n,k as h,w as u,t as i,M as pe,c as r,d as t,m as c,a as l,x as d,h as p,b as m,G as e,g as o,y as g,L as me,q as w,o as j,B as b,v as he}from"../../chunks/vendor-hf-doc-builder.js";import{I as ws}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as as}from"../../chunks/CodeBlock-hf-doc-builder.js";function ce(St){let v,js,$,P,ns,C,Zs,rs,Js,bs,f,Qs,ls,Vs,Ws,D,st,tt,X,et,at,os,nt,rt,_s,y,A,is,G,lt,ps,ot,vs,J,it,$s,L,ys,Q,pt,xs,z,ks,V,mt,Es,B,Ps,W,ht,As,M,Ns,N,ct,ms,ft,ut,Ts,T,dt,ss,gt,wt,Ss,x,S,hs,Y,jt,cs,bt,Rs,ts,_t,qs,U,Is,R,vt,O,$t,yt,Hs,k,q,fs,F,xt,us,kt,Cs,I,Et,es,Pt,At,Ds,E,H,ds,K,Nt,gs,Tt,Xs,Z,Gs;return C=new ws({}),G=new ws({}),L=new as({props:{code:`import timm
model = timm.create_model('resnext101_32x8d', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnext101_32x8d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),z=new as({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),B=new as({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),M=new as({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),Y=new ws({}),U=new as({props:{code:"model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;resnext101_32x8d&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),F=new ws({}),K=new ws({}),Z=new as({props:{code:`@article{DBLP:journals/corr/XieGDTH16,
  author    = {Saining Xie and
               Ross B. Girshick and
               Piotr Doll{\\'{a}}r and
               Zhuowen Tu and
               Kaiming He},
  title     = {Aggregated Residual Transformations for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1611.05431},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05431},
  archivePrefix = {arXiv},
  eprint    = {1611.05431},
  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XieGDTH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}`,highlighted:`@article{DBLP:journals/corr/XieGDTH16,
  <span class="hljs-attr">author</span>    = {Saining Xie <span class="hljs-literal">and</span>
               Ross B. Girshick <span class="hljs-literal">and</span>
               Piotr Doll{\\&#x27;{a}}r <span class="hljs-literal">and</span>
               Zhuowen Tu <span class="hljs-literal">and</span>
               Kaiming He},
  <span class="hljs-attr">title</span>     = {Aggregated Residual Transformations for Deep Neural Networks},
  <span class="hljs-attr">journal</span>   = {CoRR},
  <span class="hljs-attr">volume</span>    = {abs/<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">year</span>      = {<span class="hljs-number">2016</span>},
  <span class="hljs-attr">url</span>       = {http://arxiv.org/abs/<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">archivePrefix</span> = {arXiv},
  <span class="hljs-attr">eprint</span>    = {<span class="hljs-number">1611.05431</span>},
  <span class="hljs-attr">timestamp</span> = {Mon, <span class="hljs-number">13</span> Aug <span class="hljs-number">2018</span> <span class="hljs-number">16</span>:<span class="hljs-number">45</span>:<span class="hljs-number">58</span> +<span class="hljs-number">0200</span>},
  <span class="hljs-attr">biburl</span>    = {https://dblp.org/<span class="hljs-keyword">rec</span>/journals/corr/XieGDTH16.bib},
  <span class="hljs-attr">bibsource</span> = {dblp computer science bibliography, https://dblp.org}
}`}}),{c(){v=n("meta"),js=h(),$=n("h1"),P=n("a"),ns=n("span"),u(C.$$.fragment),Zs=h(),rs=n("span"),Js=i("ResNeXt"),bs=h(),f=n("p"),Qs=i("A "),ls=n("strong"),Vs=i("ResNeXt"),Ws=i(" repeats a "),D=n("a"),st=i("building block"),tt=i(" that aggregates a set of transformations with the same topology. Compared to a "),X=n("a"),et=i("ResNet"),at=i(", it exposes a new dimension,  "),os=n("em"),nt=i("cardinality"),rt=i(" (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),_s=h(),y=n("h2"),A=n("a"),is=n("span"),u(G.$$.fragment),lt=h(),ps=n("span"),ot=i("How do I use this model on an image?"),vs=h(),J=n("p"),it=i("To load a pretrained model:"),$s=h(),u(L.$$.fragment),ys=h(),Q=n("p"),pt=i("To load and preprocess the image:"),xs=h(),u(z.$$.fragment),ks=h(),V=n("p"),mt=i("To get the model predictions:"),Es=h(),u(B.$$.fragment),Ps=h(),W=n("p"),ht=i("To get the top-5 predictions class names:"),As=h(),u(M.$$.fragment),Ns=h(),N=n("p"),ct=i("Replace the model name with the variant you want to use, e.g. "),ms=n("code"),ft=i("resnext101_32x8d"),ut=i(". You can find the IDs in the model summaries at the top of this page."),Ts=h(),T=n("p"),dt=i("To extract image features with this model, follow the "),ss=n("a"),gt=i("timm feature extraction examples"),wt=i(", just change the name of the model you want to use."),Ss=h(),x=n("h2"),S=n("a"),hs=n("span"),u(Y.$$.fragment),jt=h(),cs=n("span"),bt=i("How do I finetune this model?"),Rs=h(),ts=n("p"),_t=i("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),qs=h(),u(U.$$.fragment),Is=h(),R=n("p"),vt=i("To finetune on your own dataset, you have to write a training loop or adapt "),O=n("a"),$t=i(`timm\u2019s training
script`),yt=i(" to use your dataset."),Hs=h(),k=n("h2"),q=n("a"),fs=n("span"),u(F.$$.fragment),xt=h(),us=n("span"),kt=i("How do I train this model?"),Cs=h(),I=n("p"),Et=i("You can follow the "),es=n("a"),Pt=i("timm recipe scripts"),At=i(" for training a new model afresh."),Ds=h(),E=n("h2"),H=n("a"),ds=n("span"),u(K.$$.fragment),Nt=h(),gs=n("span"),Tt=i("Citation"),Xs=h(),u(Z.$$.fragment),this.h()},l(s){const a=pe('[data-svelte="svelte-1phssyn"]',document.head);v=r(a,"META",{name:!0,content:!0}),a.forEach(t),js=c(s),$=r(s,"H1",{class:!0});var Ls=l($);P=r(Ls,"A",{id:!0,class:!0,href:!0});var Rt=l(P);ns=r(Rt,"SPAN",{});var qt=l(ns);d(C.$$.fragment,qt),qt.forEach(t),Rt.forEach(t),Zs=c(Ls),rs=r(Ls,"SPAN",{});var It=l(rs);Js=p(It,"ResNeXt"),It.forEach(t),Ls.forEach(t),bs=c(s),f=r(s,"P",{});var _=l(f);Qs=p(_,"A "),ls=r(_,"STRONG",{});var Ht=l(ls);Vs=p(Ht,"ResNeXt"),Ht.forEach(t),Ws=p(_," repeats a "),D=r(_,"A",{href:!0,rel:!0});var Ct=l(D);st=p(Ct,"building block"),Ct.forEach(t),tt=p(_," that aggregates a set of transformations with the same topology. Compared to a "),X=r(_,"A",{href:!0,rel:!0});var Dt=l(X);et=p(Dt,"ResNet"),Dt.forEach(t),at=p(_,", it exposes a new dimension,  "),os=r(_,"EM",{});var Xt=l(os);nt=p(Xt,"cardinality"),Xt.forEach(t),rt=p(_," (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width."),_.forEach(t),_s=c(s),y=r(s,"H2",{class:!0});var zs=l(y);A=r(zs,"A",{id:!0,class:!0,href:!0});var Gt=l(A);is=r(Gt,"SPAN",{});var Lt=l(is);d(G.$$.fragment,Lt),Lt.forEach(t),Gt.forEach(t),lt=c(zs),ps=r(zs,"SPAN",{});var zt=l(ps);ot=p(zt,"How do I use this model on an image?"),zt.forEach(t),zs.forEach(t),vs=c(s),J=r(s,"P",{});var Bt=l(J);it=p(Bt,"To load a pretrained model:"),Bt.forEach(t),$s=c(s),d(L.$$.fragment,s),ys=c(s),Q=r(s,"P",{});var Mt=l(Q);pt=p(Mt,"To load and preprocess the image:"),Mt.forEach(t),xs=c(s),d(z.$$.fragment,s),ks=c(s),V=r(s,"P",{});var Yt=l(V);mt=p(Yt,"To get the model predictions:"),Yt.forEach(t),Es=c(s),d(B.$$.fragment,s),Ps=c(s),W=r(s,"P",{});var Ut=l(W);ht=p(Ut,"To get the top-5 predictions class names:"),Ut.forEach(t),As=c(s),d(M.$$.fragment,s),Ns=c(s),N=r(s,"P",{});var Bs=l(N);ct=p(Bs,"Replace the model name with the variant you want to use, e.g. "),ms=r(Bs,"CODE",{});var Ot=l(ms);ft=p(Ot,"resnext101_32x8d"),Ot.forEach(t),ut=p(Bs,". You can find the IDs in the model summaries at the top of this page."),Bs.forEach(t),Ts=c(s),T=r(s,"P",{});var Ms=l(T);dt=p(Ms,"To extract image features with this model, follow the "),ss=r(Ms,"A",{href:!0});var Ft=l(ss);gt=p(Ft,"timm feature extraction examples"),Ft.forEach(t),wt=p(Ms,", just change the name of the model you want to use."),Ms.forEach(t),Ss=c(s),x=r(s,"H2",{class:!0});var Ys=l(x);S=r(Ys,"A",{id:!0,class:!0,href:!0});var Kt=l(S);hs=r(Kt,"SPAN",{});var Zt=l(hs);d(Y.$$.fragment,Zt),Zt.forEach(t),Kt.forEach(t),jt=c(Ys),cs=r(Ys,"SPAN",{});var Jt=l(cs);bt=p(Jt,"How do I finetune this model?"),Jt.forEach(t),Ys.forEach(t),Rs=c(s),ts=r(s,"P",{});var Qt=l(ts);_t=p(Qt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Qt.forEach(t),qs=c(s),d(U.$$.fragment,s),Is=c(s),R=r(s,"P",{});var Us=l(R);vt=p(Us,"To finetune on your own dataset, you have to write a training loop or adapt "),O=r(Us,"A",{href:!0,rel:!0});var Vt=l(O);$t=p(Vt,`timm\u2019s training
script`),Vt.forEach(t),yt=p(Us," to use your dataset."),Us.forEach(t),Hs=c(s),k=r(s,"H2",{class:!0});var Os=l(k);q=r(Os,"A",{id:!0,class:!0,href:!0});var Wt=l(q);fs=r(Wt,"SPAN",{});var se=l(fs);d(F.$$.fragment,se),se.forEach(t),Wt.forEach(t),xt=c(Os),us=r(Os,"SPAN",{});var te=l(us);kt=p(te,"How do I train this model?"),te.forEach(t),Os.forEach(t),Cs=c(s),I=r(s,"P",{});var Fs=l(I);Et=p(Fs,"You can follow the "),es=r(Fs,"A",{href:!0});var ee=l(es);Pt=p(ee,"timm recipe scripts"),ee.forEach(t),At=p(Fs," for training a new model afresh."),Fs.forEach(t),Ds=c(s),E=r(s,"H2",{class:!0});var Ks=l(E);H=r(Ks,"A",{id:!0,class:!0,href:!0});var ae=l(H);ds=r(ae,"SPAN",{});var ne=l(ds);d(K.$$.fragment,ne),ne.forEach(t),ae.forEach(t),Nt=c(Ks),gs=r(Ks,"SPAN",{});var re=l(gs);Tt=p(re,"Citation"),re.forEach(t),Ks.forEach(t),Xs=c(s),d(Z.$$.fragment,s),this.h()},h(){m(v,"name","hf:doc:metadata"),m(v,"content",JSON.stringify(fe)),m(P,"id","resnext"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#resnext"),m($,"class","relative group"),m(D,"href","https://paperswithcode.com/method/resnext-block"),m(D,"rel","nofollow"),m(X,"href","https://paperswithcode.com/method/resnet"),m(X,"rel","nofollow"),m(A,"id","how-do-i-use-this-model-on-an-image"),m(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(A,"href","#how-do-i-use-this-model-on-an-image"),m(y,"class","relative group"),m(ss,"href","../feature_extraction"),m(S,"id","how-do-i-finetune-this-model"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#how-do-i-finetune-this-model"),m(x,"class","relative group"),m(O,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),m(O,"rel","nofollow"),m(q,"id","how-do-i-train-this-model"),m(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(q,"href","#how-do-i-train-this-model"),m(k,"class","relative group"),m(es,"href","../scripts"),m(H,"id","citation"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#citation"),m(E,"class","relative group")},m(s,a){e(document.head,v),o(s,js,a),o(s,$,a),e($,P),e(P,ns),g(C,ns,null),e($,Zs),e($,rs),e(rs,Js),o(s,bs,a),o(s,f,a),e(f,Qs),e(f,ls),e(ls,Vs),e(f,Ws),e(f,D),e(D,st),e(f,tt),e(f,X),e(X,et),e(f,at),e(f,os),e(os,nt),e(f,rt),o(s,_s,a),o(s,y,a),e(y,A),e(A,is),g(G,is,null),e(y,lt),e(y,ps),e(ps,ot),o(s,vs,a),o(s,J,a),e(J,it),o(s,$s,a),g(L,s,a),o(s,ys,a),o(s,Q,a),e(Q,pt),o(s,xs,a),g(z,s,a),o(s,ks,a),o(s,V,a),e(V,mt),o(s,Es,a),g(B,s,a),o(s,Ps,a),o(s,W,a),e(W,ht),o(s,As,a),g(M,s,a),o(s,Ns,a),o(s,N,a),e(N,ct),e(N,ms),e(ms,ft),e(N,ut),o(s,Ts,a),o(s,T,a),e(T,dt),e(T,ss),e(ss,gt),e(T,wt),o(s,Ss,a),o(s,x,a),e(x,S),e(S,hs),g(Y,hs,null),e(x,jt),e(x,cs),e(cs,bt),o(s,Rs,a),o(s,ts,a),e(ts,_t),o(s,qs,a),g(U,s,a),o(s,Is,a),o(s,R,a),e(R,vt),e(R,O),e(O,$t),e(R,yt),o(s,Hs,a),o(s,k,a),e(k,q),e(q,fs),g(F,fs,null),e(k,xt),e(k,us),e(us,kt),o(s,Cs,a),o(s,I,a),e(I,Et),e(I,es),e(es,Pt),e(I,At),o(s,Ds,a),o(s,E,a),e(E,H),e(H,ds),g(K,ds,null),e(E,Nt),e(E,gs),e(gs,Tt),o(s,Xs,a),g(Z,s,a),Gs=!0},p:me,i(s){Gs||(w(C.$$.fragment,s),w(G.$$.fragment,s),w(L.$$.fragment,s),w(z.$$.fragment,s),w(B.$$.fragment,s),w(M.$$.fragment,s),w(Y.$$.fragment,s),w(U.$$.fragment,s),w(F.$$.fragment,s),w(K.$$.fragment,s),w(Z.$$.fragment,s),Gs=!0)},o(s){j(C.$$.fragment,s),j(G.$$.fragment,s),j(L.$$.fragment,s),j(z.$$.fragment,s),j(B.$$.fragment,s),j(M.$$.fragment,s),j(Y.$$.fragment,s),j(U.$$.fragment,s),j(F.$$.fragment,s),j(K.$$.fragment,s),j(Z.$$.fragment,s),Gs=!1},d(s){t(v),s&&t(js),s&&t($),b(C),s&&t(bs),s&&t(f),s&&t(_s),s&&t(y),b(G),s&&t(vs),s&&t(J),s&&t($s),b(L,s),s&&t(ys),s&&t(Q),s&&t(xs),b(z,s),s&&t(ks),s&&t(V),s&&t(Es),b(B,s),s&&t(Ps),s&&t(W),s&&t(As),b(M,s),s&&t(Ns),s&&t(N),s&&t(Ts),s&&t(T),s&&t(Ss),s&&t(x),b(Y),s&&t(Rs),s&&t(ts),s&&t(qs),b(U,s),s&&t(Is),s&&t(R),s&&t(Hs),s&&t(k),b(F),s&&t(Cs),s&&t(I),s&&t(Ds),s&&t(E),b(K),s&&t(Xs),b(Z,s)}}}const fe={local:"resnext",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"ResNeXt"};function ue(St){return he(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class je extends le{constructor(v){super();oe(this,v,ue,ce,ie,{})}}export{je as default,fe as metadata};
