import{S as Qt,i as Wt,s as Zt,e as n,k as p,w as f,t as m,M as es,c as o,d as t,m as h,a as l,x as u,h as c,b as i,G as s,g as r,y as g,L as ts,q as d,o as v,B as _,v as ss}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as te}from"../../chunks/CodeBlock-hf-doc-builder.js";function as(xt){let w,de,$,E,se,R,Fe,ae,Xe,ve,j,ne,Je,Ke,H,Qe,We,_e,b,I,oe,z,Ze,le,et,we,X,tt,$e,L,je,J,st,be,G,ye,K,at,xe,V,ke,Q,nt,Ee,Y,Ie,P,ot,re,lt,rt,Pe,S,it,W,pt,mt,Se,y,A,ie,M,ht,pe,ct,Ae,Z,ft,Ne,U,qe,N,ut,B,gt,dt,Te,x,q,me,D,vt,he,_t,Ce,T,wt,ee,$t,jt,Re,k,C,ce,O,bt,fe,yt,He,F,ze;return R=new ge({}),z=new ge({}),L=new te({props:{code:`import timm
model = timm.create_model('inception_resnet_v2', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;inception_resnet_v2&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),G=new te({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),V=new te({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),Y=new te({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),M=new ge({}),U=new te({props:{code:"model = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;inception_resnet_v2&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),D=new ge({}),O=new ge({}),F=new te({props:{code:`@misc{szegedy2016inceptionv4,
      title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}, 
      author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
      year={2016},
      eprint={1602.07261},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}`,highlighted:`<span class="language-xml">@misc</span><span class="hljs-template-variable">{szegedy2016inceptionv4,
      title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}</span><span class="language-xml">, 
      author=</span><span class="hljs-template-variable">{Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi}</span><span class="language-xml">,
      year=</span><span class="hljs-template-variable">{2016}</span><span class="language-xml">,
      eprint=</span><span class="hljs-template-variable">{1602.07261}</span><span class="language-xml">,
      archivePrefix=</span><span class="hljs-template-variable">{arXiv}</span><span class="language-xml">,
      primaryClass=</span><span class="hljs-template-variable">{cs.CV}</span><span class="language-xml">
}</span>`}}),{c(){w=n("meta"),de=p(),$=n("h1"),E=n("a"),se=n("span"),f(R.$$.fragment),Fe=p(),ae=n("span"),Xe=m("Inception ResNet v2"),ve=p(),j=n("p"),ne=n("strong"),Je=m("Inception-ResNet-v2"),Ke=m(" is a convolutional neural architecture that builds on the Inception family of architectures but incorporates "),H=n("a"),Qe=m("residual connections"),We=m(" (replacing the filter concatenation stage of the Inception architecture)."),_e=p(),b=n("h2"),I=n("a"),oe=n("span"),f(z.$$.fragment),Ze=p(),le=n("span"),et=m("How do I use this model on an image?"),we=p(),X=n("p"),tt=m("To load a pretrained model:"),$e=p(),f(L.$$.fragment),je=p(),J=n("p"),st=m("To load and preprocess the image:"),be=p(),f(G.$$.fragment),ye=p(),K=n("p"),at=m("To get the model predictions:"),xe=p(),f(V.$$.fragment),ke=p(),Q=n("p"),nt=m("To get the top-5 predictions class names:"),Ee=p(),f(Y.$$.fragment),Ie=p(),P=n("p"),ot=m("Replace the model name with the variant you want to use, e.g. "),re=n("code"),lt=m("inception_resnet_v2"),rt=m(". You can find the IDs in the model summaries at the top of this page."),Pe=p(),S=n("p"),it=m("To extract image features with this model, follow the "),W=n("a"),pt=m("timm feature extraction examples"),mt=m(", just change the name of the model you want to use."),Se=p(),y=n("h2"),A=n("a"),ie=n("span"),f(M.$$.fragment),ht=p(),pe=n("span"),ct=m("How do I finetune this model?"),Ae=p(),Z=n("p"),ft=m("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Ne=p(),f(U.$$.fragment),qe=p(),N=n("p"),ut=m("To finetune on your own dataset, you have to write a training loop or adapt "),B=n("a"),gt=m(`timm\u2019s training
script`),dt=m(" to use your dataset."),Te=p(),x=n("h2"),q=n("a"),me=n("span"),f(D.$$.fragment),vt=p(),he=n("span"),_t=m("How do I train this model?"),Ce=p(),T=n("p"),wt=m("You can follow the "),ee=n("a"),$t=m("timm recipe scripts"),jt=m(" for training a new model afresh."),Re=p(),k=n("h2"),C=n("a"),ce=n("span"),f(O.$$.fragment),bt=p(),fe=n("span"),yt=m("Citation"),He=p(),f(F.$$.fragment),this.h()},l(e){const a=es('[data-svelte="svelte-1phssyn"]',document.head);w=o(a,"META",{name:!0,content:!0}),a.forEach(t),de=h(e),$=o(e,"H1",{class:!0});var Le=l($);E=o(Le,"A",{id:!0,class:!0,href:!0});var kt=l(E);se=o(kt,"SPAN",{});var Et=l(se);u(R.$$.fragment,Et),Et.forEach(t),kt.forEach(t),Fe=h(Le),ae=o(Le,"SPAN",{});var It=l(ae);Xe=c(It,"Inception ResNet v2"),It.forEach(t),Le.forEach(t),ve=h(e),j=o(e,"P",{});var ue=l(j);ne=o(ue,"STRONG",{});var Pt=l(ne);Je=c(Pt,"Inception-ResNet-v2"),Pt.forEach(t),Ke=c(ue," is a convolutional neural architecture that builds on the Inception family of architectures but incorporates "),H=o(ue,"A",{href:!0,rel:!0});var St=l(H);Qe=c(St,"residual connections"),St.forEach(t),We=c(ue," (replacing the filter concatenation stage of the Inception architecture)."),ue.forEach(t),_e=h(e),b=o(e,"H2",{class:!0});var Ge=l(b);I=o(Ge,"A",{id:!0,class:!0,href:!0});var At=l(I);oe=o(At,"SPAN",{});var Nt=l(oe);u(z.$$.fragment,Nt),Nt.forEach(t),At.forEach(t),Ze=h(Ge),le=o(Ge,"SPAN",{});var qt=l(le);et=c(qt,"How do I use this model on an image?"),qt.forEach(t),Ge.forEach(t),we=h(e),X=o(e,"P",{});var Tt=l(X);tt=c(Tt,"To load a pretrained model:"),Tt.forEach(t),$e=h(e),u(L.$$.fragment,e),je=h(e),J=o(e,"P",{});var Ct=l(J);st=c(Ct,"To load and preprocess the image:"),Ct.forEach(t),be=h(e),u(G.$$.fragment,e),ye=h(e),K=o(e,"P",{});var Rt=l(K);at=c(Rt,"To get the model predictions:"),Rt.forEach(t),xe=h(e),u(V.$$.fragment,e),ke=h(e),Q=o(e,"P",{});var Ht=l(Q);nt=c(Ht,"To get the top-5 predictions class names:"),Ht.forEach(t),Ee=h(e),u(Y.$$.fragment,e),Ie=h(e),P=o(e,"P",{});var Ve=l(P);ot=c(Ve,"Replace the model name with the variant you want to use, e.g. "),re=o(Ve,"CODE",{});var zt=l(re);lt=c(zt,"inception_resnet_v2"),zt.forEach(t),rt=c(Ve,". You can find the IDs in the model summaries at the top of this page."),Ve.forEach(t),Pe=h(e),S=o(e,"P",{});var Ye=l(S);it=c(Ye,"To extract image features with this model, follow the "),W=o(Ye,"A",{href:!0});var Lt=l(W);pt=c(Lt,"timm feature extraction examples"),Lt.forEach(t),mt=c(Ye,", just change the name of the model you want to use."),Ye.forEach(t),Se=h(e),y=o(e,"H2",{class:!0});var Me=l(y);A=o(Me,"A",{id:!0,class:!0,href:!0});var Gt=l(A);ie=o(Gt,"SPAN",{});var Vt=l(ie);u(M.$$.fragment,Vt),Vt.forEach(t),Gt.forEach(t),ht=h(Me),pe=o(Me,"SPAN",{});var Yt=l(pe);ct=c(Yt,"How do I finetune this model?"),Yt.forEach(t),Me.forEach(t),Ae=h(e),Z=o(e,"P",{});var Mt=l(Z);ft=c(Mt,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Mt.forEach(t),Ne=h(e),u(U.$$.fragment,e),qe=h(e),N=o(e,"P",{});var Ue=l(N);ut=c(Ue,"To finetune on your own dataset, you have to write a training loop or adapt "),B=o(Ue,"A",{href:!0,rel:!0});var Ut=l(B);gt=c(Ut,`timm\u2019s training
script`),Ut.forEach(t),dt=c(Ue," to use your dataset."),Ue.forEach(t),Te=h(e),x=o(e,"H2",{class:!0});var Be=l(x);q=o(Be,"A",{id:!0,class:!0,href:!0});var Bt=l(q);me=o(Bt,"SPAN",{});var Dt=l(me);u(D.$$.fragment,Dt),Dt.forEach(t),Bt.forEach(t),vt=h(Be),he=o(Be,"SPAN",{});var Ot=l(he);_t=c(Ot,"How do I train this model?"),Ot.forEach(t),Be.forEach(t),Ce=h(e),T=o(e,"P",{});var De=l(T);wt=c(De,"You can follow the "),ee=o(De,"A",{href:!0});var Ft=l(ee);$t=c(Ft,"timm recipe scripts"),Ft.forEach(t),jt=c(De," for training a new model afresh."),De.forEach(t),Re=h(e),k=o(e,"H2",{class:!0});var Oe=l(k);C=o(Oe,"A",{id:!0,class:!0,href:!0});var Xt=l(C);ce=o(Xt,"SPAN",{});var Jt=l(ce);u(O.$$.fragment,Jt),Jt.forEach(t),Xt.forEach(t),bt=h(Oe),fe=o(Oe,"SPAN",{});var Kt=l(fe);yt=c(Kt,"Citation"),Kt.forEach(t),Oe.forEach(t),He=h(e),u(F.$$.fragment,e),this.h()},h(){i(w,"name","hf:doc:metadata"),i(w,"content",JSON.stringify(ns)),i(E,"id","inception-resnet-v2"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#inception-resnet-v2"),i($,"class","relative group"),i(H,"href","https://paperswithcode.com/method/residual-connection"),i(H,"rel","nofollow"),i(I,"id","how-do-i-use-this-model-on-an-image"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-use-this-model-on-an-image"),i(b,"class","relative group"),i(W,"href","../feature_extraction"),i(A,"id","how-do-i-finetune-this-model"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(B,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(B,"rel","nofollow"),i(q,"id","how-do-i-train-this-model"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-do-i-train-this-model"),i(x,"class","relative group"),i(ee,"href","../scripts"),i(C,"id","citation"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#citation"),i(k,"class","relative group")},m(e,a){s(document.head,w),r(e,de,a),r(e,$,a),s($,E),s(E,se),g(R,se,null),s($,Fe),s($,ae),s(ae,Xe),r(e,ve,a),r(e,j,a),s(j,ne),s(ne,Je),s(j,Ke),s(j,H),s(H,Qe),s(j,We),r(e,_e,a),r(e,b,a),s(b,I),s(I,oe),g(z,oe,null),s(b,Ze),s(b,le),s(le,et),r(e,we,a),r(e,X,a),s(X,tt),r(e,$e,a),g(L,e,a),r(e,je,a),r(e,J,a),s(J,st),r(e,be,a),g(G,e,a),r(e,ye,a),r(e,K,a),s(K,at),r(e,xe,a),g(V,e,a),r(e,ke,a),r(e,Q,a),s(Q,nt),r(e,Ee,a),g(Y,e,a),r(e,Ie,a),r(e,P,a),s(P,ot),s(P,re),s(re,lt),s(P,rt),r(e,Pe,a),r(e,S,a),s(S,it),s(S,W),s(W,pt),s(S,mt),r(e,Se,a),r(e,y,a),s(y,A),s(A,ie),g(M,ie,null),s(y,ht),s(y,pe),s(pe,ct),r(e,Ae,a),r(e,Z,a),s(Z,ft),r(e,Ne,a),g(U,e,a),r(e,qe,a),r(e,N,a),s(N,ut),s(N,B),s(B,gt),s(N,dt),r(e,Te,a),r(e,x,a),s(x,q),s(q,me),g(D,me,null),s(x,vt),s(x,he),s(he,_t),r(e,Ce,a),r(e,T,a),s(T,wt),s(T,ee),s(ee,$t),s(T,jt),r(e,Re,a),r(e,k,a),s(k,C),s(C,ce),g(O,ce,null),s(k,bt),s(k,fe),s(fe,yt),r(e,He,a),g(F,e,a),ze=!0},p:ts,i(e){ze||(d(R.$$.fragment,e),d(z.$$.fragment,e),d(L.$$.fragment,e),d(G.$$.fragment,e),d(V.$$.fragment,e),d(Y.$$.fragment,e),d(M.$$.fragment,e),d(U.$$.fragment,e),d(D.$$.fragment,e),d(O.$$.fragment,e),d(F.$$.fragment,e),ze=!0)},o(e){v(R.$$.fragment,e),v(z.$$.fragment,e),v(L.$$.fragment,e),v(G.$$.fragment,e),v(V.$$.fragment,e),v(Y.$$.fragment,e),v(M.$$.fragment,e),v(U.$$.fragment,e),v(D.$$.fragment,e),v(O.$$.fragment,e),v(F.$$.fragment,e),ze=!1},d(e){t(w),e&&t(de),e&&t($),_(R),e&&t(ve),e&&t(j),e&&t(_e),e&&t(b),_(z),e&&t(we),e&&t(X),e&&t($e),_(L,e),e&&t(je),e&&t(J),e&&t(be),_(G,e),e&&t(ye),e&&t(K),e&&t(xe),_(V,e),e&&t(ke),e&&t(Q),e&&t(Ee),_(Y,e),e&&t(Ie),e&&t(P),e&&t(Pe),e&&t(S),e&&t(Se),e&&t(y),_(M),e&&t(Ae),e&&t(Z),e&&t(Ne),_(U,e),e&&t(qe),e&&t(N),e&&t(Te),e&&t(x),_(D),e&&t(Ce),e&&t(T),e&&t(Re),e&&t(k),_(O),e&&t(He),_(F,e)}}}const ns={local:"inception-resnet-v2",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Inception ResNet v2"};function os(xt){return ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ps extends Qt{constructor(w){super();Wt(this,w,os,as,Zt,{})}}export{ps as default,ns as metadata};
