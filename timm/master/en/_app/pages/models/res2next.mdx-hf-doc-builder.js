import{S as Qa,i as Va,s as Wa,e as n,k as p,w as f,t as m,M as es,c as l,d as a,m as h,a as r,x as u,h as c,b as i,G as s,g as o,y as g,L as as,q as d,o as v,B as w,v as ss}from"../../chunks/vendor-hf-doc-builder.js";import{I as ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function ts(xa){let b,de,_,k,se,M,Fe,te,Ze,ve,j,ne,Ke,Je,R,Qe,Ve,we,$,I,le,C,We,re,ea,be,Z,aa,_e,G,je,K,sa,$e,Y,ye,J,ta,xe,X,Ee,Q,na,ke,L,Ie,N,la,oe,ra,oa,Ne,P,ia,V,pa,ma,Pe,y,S,ie,z,ha,pe,ca,Se,W,fa,Te,B,Ae,T,ua,D,ga,da,qe,x,A,me,O,va,he,wa,He,q,ba,ee,_a,ja,Me,E,H,ce,U,$a,fe,ya,Re,F,Ce;return M=new ge({}),C=new ge({}),G=new ae({props:{code:`import timm
model = timm.create_model('res2next50', pretrained=True)
model.eval()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> timm
<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;res2next50&#x27;</span>, pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()`}}),Y=new ae({props:{code:`import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> urllib
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data <span class="hljs-keyword">import</span> resolve_data_config
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> timm.data.transforms_factory <span class="hljs-keyword">import</span> create_transform

<span class="hljs-meta">&gt;&gt;&gt; </span>config = resolve_data_config({}, model=model)
<span class="hljs-meta">&gt;&gt;&gt; </span>transform = create_transform(**config)

<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;</span>, <span class="hljs-string">&quot;dog.jpg&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename)
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(filename).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tensor = transform(img).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># transform and add batch dimension</span>`}}),X=new ae({props:{code:`import torch
with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)
# prints: torch.Size([1000])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    out = model(tensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.nn.functional.softmax(out[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(probabilities.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints: torch.Size([1000])</span>`}}),L=new ae({props:{code:`# Get imagenet class mappings
url, filename = ("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt", "imagenet_classes.txt")
urllib.request.urlretrieve(url, filename) 
with open("imagenet_classes.txt", "r") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get imagenet class mappings</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url, filename = (<span class="hljs-string">&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>urllib.request.urlretrieve(url, filename) 
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;imagenet_classes.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>    categories = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Print top categories per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_prob, top5_catid = torch.topk(probabilities, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top5_prob.size(<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(categories[top5_catid[i]], top5_prob[i].item())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prints class names and probabilities like:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [(&#x27;Samoyed&#x27;, 0.6425196528434753), (&#x27;Pomeranian&#x27;, 0.04062102362513542), (&#x27;keeshond&#x27;, 0.03186424449086189), (&#x27;white wolf&#x27;, 0.01739676296710968), (&#x27;Eskimo dog&#x27;, 0.011717947199940681)]</span>`}}),z=new ge({}),B=new ae({props:{code:"model = timm.create_model('res2next50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = timm.create_model(<span class="hljs-string">&#x27;res2next50&#x27;</span>, pretrained=<span class="hljs-literal">True</span>, num_classes=NUM_FINETUNE_CLASSES)'}}),O=new ge({}),U=new ge({}),F=new ae({props:{code:`@article{Gao_2021,
   title={Res2Net: A New Multi-Scale Backbone Architecture},
   volume={43},
   ISSN={1939-3539},
   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},
   DOI={10.1109/tpami.2019.2938758},
   number={2},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},
   year={2021},
   month={Feb},
   pages={652\u2013662}
}`,highlighted:`<span class="language-xml">@article</span><span class="hljs-template-variable">{Gao_2021,
   title={Res2Net: A New Multi-Scale Backbone Architecture}</span><span class="language-xml">,
   volume=</span><span class="hljs-template-variable">{43}</span><span class="language-xml">,
   ISSN=</span><span class="hljs-template-variable">{1939-3539}</span><span class="language-xml">,
   url=</span><span class="hljs-template-variable">{http://dx.doi.org/10.1109/TPAMI.2019.2938758}</span><span class="language-xml">,
   DOI=</span><span class="hljs-template-variable">{10.1109/tpami.2019.2938758}</span><span class="language-xml">,
   number=</span><span class="hljs-template-variable">{2}</span><span class="language-xml">,
   journal=</span><span class="hljs-template-variable">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="language-xml">,
   publisher=</span><span class="hljs-template-variable">{Institute of Electrical and Electronics Engineers (IEEE)}</span><span class="language-xml">,
   author=</span><span class="hljs-template-variable">{Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip}</span><span class="language-xml">,
   year=</span><span class="hljs-template-variable">{2021}</span><span class="language-xml">,
   month=</span><span class="hljs-template-variable">{Feb}</span><span class="language-xml">,
   pages=</span><span class="hljs-template-variable">{652\u2013662}</span><span class="language-xml">
}</span>`}}),{c(){b=n("meta"),de=p(),_=n("h1"),k=n("a"),se=n("span"),f(M.$$.fragment),Fe=p(),te=n("span"),Ze=m("Res2NeXt"),ve=p(),j=n("p"),ne=n("strong"),Ke=m("Res2NeXt"),Je=m(" is an image model that employs a variation on "),R=n("a"),Qe=m("ResNeXt"),Ve=m(" bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer."),we=p(),$=n("h2"),I=n("a"),le=n("span"),f(C.$$.fragment),We=p(),re=n("span"),ea=m("How do I use this model on an image?"),be=p(),Z=n("p"),aa=m("To load a pretrained model:"),_e=p(),f(G.$$.fragment),je=p(),K=n("p"),sa=m("To load and preprocess the image:"),$e=p(),f(Y.$$.fragment),ye=p(),J=n("p"),ta=m("To get the model predictions:"),xe=p(),f(X.$$.fragment),Ee=p(),Q=n("p"),na=m("To get the top-5 predictions class names:"),ke=p(),f(L.$$.fragment),Ie=p(),N=n("p"),la=m("Replace the model name with the variant you want to use, e.g. "),oe=n("code"),ra=m("res2next50"),oa=m(". You can find the IDs in the model summaries at the top of this page."),Ne=p(),P=n("p"),ia=m("To extract image features with this model, follow the "),V=n("a"),pa=m("timm feature extraction examples"),ma=m(", just change the name of the model you want to use."),Pe=p(),y=n("h2"),S=n("a"),ie=n("span"),f(z.$$.fragment),ha=p(),pe=n("span"),ca=m("How do I finetune this model?"),Se=p(),W=n("p"),fa=m("You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),Te=p(),f(B.$$.fragment),Ae=p(),T=n("p"),ua=m("To finetune on your own dataset, you have to write a training loop or adapt "),D=n("a"),ga=m(`timm\u2019s training
script`),da=m(" to use your dataset."),qe=p(),x=n("h2"),A=n("a"),me=n("span"),f(O.$$.fragment),va=p(),he=n("span"),wa=m("How do I train this model?"),He=p(),q=n("p"),ba=m("You can follow the "),ee=n("a"),_a=m("timm recipe scripts"),ja=m(" for training a new model afresh."),Me=p(),E=n("h2"),H=n("a"),ce=n("span"),f(U.$$.fragment),$a=p(),fe=n("span"),ya=m("Citation"),Re=p(),f(F.$$.fragment),this.h()},l(e){const t=es('[data-svelte="svelte-1phssyn"]',document.head);b=l(t,"META",{name:!0,content:!0}),t.forEach(a),de=h(e),_=l(e,"H1",{class:!0});var Ge=r(_);k=l(Ge,"A",{id:!0,class:!0,href:!0});var Ea=r(k);se=l(Ea,"SPAN",{});var ka=r(se);u(M.$$.fragment,ka),ka.forEach(a),Ea.forEach(a),Fe=h(Ge),te=l(Ge,"SPAN",{});var Ia=r(te);Ze=c(Ia,"Res2NeXt"),Ia.forEach(a),Ge.forEach(a),ve=h(e),j=l(e,"P",{});var ue=r(j);ne=l(ue,"STRONG",{});var Na=r(ne);Ke=c(Na,"Res2NeXt"),Na.forEach(a),Je=c(ue," is an image model that employs a variation on "),R=l(ue,"A",{href:!0,rel:!0});var Pa=r(R);Qe=c(Pa,"ResNeXt"),Pa.forEach(a),Ve=c(ue," bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer."),ue.forEach(a),we=h(e),$=l(e,"H2",{class:!0});var Ye=r($);I=l(Ye,"A",{id:!0,class:!0,href:!0});var Sa=r(I);le=l(Sa,"SPAN",{});var Ta=r(le);u(C.$$.fragment,Ta),Ta.forEach(a),Sa.forEach(a),We=h(Ye),re=l(Ye,"SPAN",{});var Aa=r(re);ea=c(Aa,"How do I use this model on an image?"),Aa.forEach(a),Ye.forEach(a),be=h(e),Z=l(e,"P",{});var qa=r(Z);aa=c(qa,"To load a pretrained model:"),qa.forEach(a),_e=h(e),u(G.$$.fragment,e),je=h(e),K=l(e,"P",{});var Ha=r(K);sa=c(Ha,"To load and preprocess the image:"),Ha.forEach(a),$e=h(e),u(Y.$$.fragment,e),ye=h(e),J=l(e,"P",{});var Ma=r(J);ta=c(Ma,"To get the model predictions:"),Ma.forEach(a),xe=h(e),u(X.$$.fragment,e),Ee=h(e),Q=l(e,"P",{});var Ra=r(Q);na=c(Ra,"To get the top-5 predictions class names:"),Ra.forEach(a),ke=h(e),u(L.$$.fragment,e),Ie=h(e),N=l(e,"P",{});var Xe=r(N);la=c(Xe,"Replace the model name with the variant you want to use, e.g. "),oe=l(Xe,"CODE",{});var Ca=r(oe);ra=c(Ca,"res2next50"),Ca.forEach(a),oa=c(Xe,". You can find the IDs in the model summaries at the top of this page."),Xe.forEach(a),Ne=h(e),P=l(e,"P",{});var Le=r(P);ia=c(Le,"To extract image features with this model, follow the "),V=l(Le,"A",{href:!0});var Ga=r(V);pa=c(Ga,"timm feature extraction examples"),Ga.forEach(a),ma=c(Le,", just change the name of the model you want to use."),Le.forEach(a),Pe=h(e),y=l(e,"H2",{class:!0});var ze=r(y);S=l(ze,"A",{id:!0,class:!0,href:!0});var Ya=r(S);ie=l(Ya,"SPAN",{});var Xa=r(ie);u(z.$$.fragment,Xa),Xa.forEach(a),Ya.forEach(a),ha=h(ze),pe=l(ze,"SPAN",{});var La=r(pe);ca=c(La,"How do I finetune this model?"),La.forEach(a),ze.forEach(a),Se=h(e),W=l(e,"P",{});var za=r(W);fa=c(za,"You can finetune any of the pre-trained models just by changing the classifier (the last layer)."),za.forEach(a),Te=h(e),u(B.$$.fragment,e),Ae=h(e),T=l(e,"P",{});var Be=r(T);ua=c(Be,"To finetune on your own dataset, you have to write a training loop or adapt "),D=l(Be,"A",{href:!0,rel:!0});var Ba=r(D);ga=c(Ba,`timm\u2019s training
script`),Ba.forEach(a),da=c(Be," to use your dataset."),Be.forEach(a),qe=h(e),x=l(e,"H2",{class:!0});var De=r(x);A=l(De,"A",{id:!0,class:!0,href:!0});var Da=r(A);me=l(Da,"SPAN",{});var Oa=r(me);u(O.$$.fragment,Oa),Oa.forEach(a),Da.forEach(a),va=h(De),he=l(De,"SPAN",{});var Ua=r(he);wa=c(Ua,"How do I train this model?"),Ua.forEach(a),De.forEach(a),He=h(e),q=l(e,"P",{});var Oe=r(q);ba=c(Oe,"You can follow the "),ee=l(Oe,"A",{href:!0});var Fa=r(ee);_a=c(Fa,"timm recipe scripts"),Fa.forEach(a),ja=c(Oe," for training a new model afresh."),Oe.forEach(a),Me=h(e),E=l(e,"H2",{class:!0});var Ue=r(E);H=l(Ue,"A",{id:!0,class:!0,href:!0});var Za=r(H);ce=l(Za,"SPAN",{});var Ka=r(ce);u(U.$$.fragment,Ka),Ka.forEach(a),Za.forEach(a),$a=h(Ue),fe=l(Ue,"SPAN",{});var Ja=r(fe);ya=c(Ja,"Citation"),Ja.forEach(a),Ue.forEach(a),Re=h(e),u(F.$$.fragment,e),this.h()},h(){i(b,"name","hf:doc:metadata"),i(b,"content",JSON.stringify(ns)),i(k,"id","res2next"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#res2next"),i(_,"class","relative group"),i(R,"href","https://paperswithcode.com/method/resnext"),i(R,"rel","nofollow"),i(I,"id","how-do-i-use-this-model-on-an-image"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#how-do-i-use-this-model-on-an-image"),i($,"class","relative group"),i(V,"href","../feature_extraction"),i(S,"id","how-do-i-finetune-this-model"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#how-do-i-finetune-this-model"),i(y,"class","relative group"),i(D,"href","https://github.com/rwightman/pytorch-image-models/blob/master/train.py"),i(D,"rel","nofollow"),i(A,"id","how-do-i-train-this-model"),i(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(A,"href","#how-do-i-train-this-model"),i(x,"class","relative group"),i(ee,"href","../scripts"),i(H,"id","citation"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#citation"),i(E,"class","relative group")},m(e,t){s(document.head,b),o(e,de,t),o(e,_,t),s(_,k),s(k,se),g(M,se,null),s(_,Fe),s(_,te),s(te,Ze),o(e,ve,t),o(e,j,t),s(j,ne),s(ne,Ke),s(j,Je),s(j,R),s(R,Qe),s(j,Ve),o(e,we,t),o(e,$,t),s($,I),s(I,le),g(C,le,null),s($,We),s($,re),s(re,ea),o(e,be,t),o(e,Z,t),s(Z,aa),o(e,_e,t),g(G,e,t),o(e,je,t),o(e,K,t),s(K,sa),o(e,$e,t),g(Y,e,t),o(e,ye,t),o(e,J,t),s(J,ta),o(e,xe,t),g(X,e,t),o(e,Ee,t),o(e,Q,t),s(Q,na),o(e,ke,t),g(L,e,t),o(e,Ie,t),o(e,N,t),s(N,la),s(N,oe),s(oe,ra),s(N,oa),o(e,Ne,t),o(e,P,t),s(P,ia),s(P,V),s(V,pa),s(P,ma),o(e,Pe,t),o(e,y,t),s(y,S),s(S,ie),g(z,ie,null),s(y,ha),s(y,pe),s(pe,ca),o(e,Se,t),o(e,W,t),s(W,fa),o(e,Te,t),g(B,e,t),o(e,Ae,t),o(e,T,t),s(T,ua),s(T,D),s(D,ga),s(T,da),o(e,qe,t),o(e,x,t),s(x,A),s(A,me),g(O,me,null),s(x,va),s(x,he),s(he,wa),o(e,He,t),o(e,q,t),s(q,ba),s(q,ee),s(ee,_a),s(q,ja),o(e,Me,t),o(e,E,t),s(E,H),s(H,ce),g(U,ce,null),s(E,$a),s(E,fe),s(fe,ya),o(e,Re,t),g(F,e,t),Ce=!0},p:as,i(e){Ce||(d(M.$$.fragment,e),d(C.$$.fragment,e),d(G.$$.fragment,e),d(Y.$$.fragment,e),d(X.$$.fragment,e),d(L.$$.fragment,e),d(z.$$.fragment,e),d(B.$$.fragment,e),d(O.$$.fragment,e),d(U.$$.fragment,e),d(F.$$.fragment,e),Ce=!0)},o(e){v(M.$$.fragment,e),v(C.$$.fragment,e),v(G.$$.fragment,e),v(Y.$$.fragment,e),v(X.$$.fragment,e),v(L.$$.fragment,e),v(z.$$.fragment,e),v(B.$$.fragment,e),v(O.$$.fragment,e),v(U.$$.fragment,e),v(F.$$.fragment,e),Ce=!1},d(e){a(b),e&&a(de),e&&a(_),w(M),e&&a(ve),e&&a(j),e&&a(we),e&&a($),w(C),e&&a(be),e&&a(Z),e&&a(_e),w(G,e),e&&a(je),e&&a(K),e&&a($e),w(Y,e),e&&a(ye),e&&a(J),e&&a(xe),w(X,e),e&&a(Ee),e&&a(Q),e&&a(ke),w(L,e),e&&a(Ie),e&&a(N),e&&a(Ne),e&&a(P),e&&a(Pe),e&&a(y),w(z),e&&a(Se),e&&a(W),e&&a(Te),w(B,e),e&&a(Ae),e&&a(T),e&&a(qe),e&&a(x),w(O),e&&a(He),e&&a(q),e&&a(Me),e&&a(E),w(U),e&&a(Re),w(F,e)}}}const ns={local:"res2next",sections:[{local:"how-do-i-use-this-model-on-an-image",title:"How do I use this model on an image?"},{local:"how-do-i-finetune-this-model",title:"How do I finetune this model?"},{local:"how-do-i-train-this-model",title:"How do I train this model?"},{local:"citation",title:"Citation"}],title:"Res2NeXt"};function ls(xa){return ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ps extends Qa{constructor(b){super();Va(this,b,ls,ts,Wa,{})}}export{ps as default,ns as metadata};
