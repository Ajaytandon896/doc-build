import{S as sr,i as er,s as ar,e as o,t as h,k as m,w,c as p,a as u,h as c,d as t,m as g,x as q,g as l,G as s,y as x,q as y,o as v,B as E,b as O,l as Jh,M as hr,N as Oc,p as ge,v as cr,n as _e}from"../../chunks/vendor-hf-doc-builder.js";import{T as de}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Nc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ka}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as D}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as tr}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as lr}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function rr(A){let a,_;return a=new tr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_tf.ipynb"}]}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function ir(A){let a,_;return a=new tr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_pt.ipynb"}]}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function or(A){let a,_;return{c(){a=o("p"),_=h("Vi\u1EC7c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\xF4 h\xECnh ng\xF4n ng\u1EEF s\u1EBD m\u1EA5t m\u1ED9t l\xFAc. Ch\xFAng t\xF4i khuy\xEAn b\u1EA1n tr\u01B0\u1EDBc ti\xEAn n\xEAn ch\u1EA1y v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n tr\xEAn m\u1ED9t m\u1EABu d\u1EEF li\u1EC7u b\u1EB1ng c\xE1ch b\u1ECF ch\xFA th\xEDch hai d\xF2ng m\u1ED9t ph\u1EA7n \u1EDF tr\xEAn v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n ho\xE0n t\u1EA5t th\xE0nh c\xF4ng v\xE0 c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF. Kh\xF4ng c\xF3 g\xEC kh\xF3 ch\u1ECBu h\u01A1n l\xE0 m\u1ED9t l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n kh\xF4ng th\xE0nh c\xF4ng \u1EDF b\u01B0\u1EDBc cu\u1ED1i c\xF9ng v\xEC b\u1EA1n qu\xEAn t\u1EA1o m\u1ED9t th\u01B0 m\u1EE5c ho\u1EB7c v\xEC c\xF3 l\u1ED7i \u0111\xE1nh m\xE1y \u1EDF cu\u1ED1i v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n!")},l(r){a=p(r,"P",{});var d=u(a);_=c(d,"Vi\u1EC7c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\xF4 h\xECnh ng\xF4n ng\u1EEF s\u1EBD m\u1EA5t m\u1ED9t l\xFAc. Ch\xFAng t\xF4i khuy\xEAn b\u1EA1n tr\u01B0\u1EDBc ti\xEAn n\xEAn ch\u1EA1y v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n tr\xEAn m\u1ED9t m\u1EABu d\u1EEF li\u1EC7u b\u1EB1ng c\xE1ch b\u1ECF ch\xFA th\xEDch hai d\xF2ng m\u1ED9t ph\u1EA7n \u1EDF tr\xEAn v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n ho\xE0n t\u1EA5t th\xE0nh c\xF4ng v\xE0 c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF. Kh\xF4ng c\xF3 g\xEC kh\xF3 ch\u1ECBu h\u01A1n l\xE0 m\u1ED9t l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n kh\xF4ng th\xE0nh c\xF4ng \u1EDF b\u01B0\u1EDBc cu\u1ED1i c\xF9ng v\xEC b\u1EA1n qu\xEAn t\u1EA1o m\u1ED9t th\u01B0 m\u1EE5c ho\u1EB7c v\xEC c\xF3 l\u1ED7i \u0111\xE1nh m\xE1y \u1EDF cu\u1ED1i v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n!"),d.forEach(t)},m(r,d){l(r,a,d),s(a,_)},d(r){r&&t(a)}}}function pr(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P;return{c(){a=o("p"),_=h("\u270F\uFE0F "),r=o("strong"),d=h("Th\u1EED nghi\u1EC7m th\xF4i!"),$=h(" Lo\u1EA1i b\u1ECF t\u1EA5t c\u1EA3 c\xE1c ph\u1EA7n nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh kh\xF4ng ph\u1EA3i l\xE0 v\u1EA5n \u0111\u1EC1 l\u1EDBn \u1EDF \u0111\xE2y v\xEC ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng c\xE1c c\u1EEDa s\u1ED5 ng\u1EEF c\u1EA3nh nh\u1ECF. Khi b\u1EA1n t\u0103ng k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh (ho\u1EB7c n\u1EBFu b\u1EA1n c\xF3 m\u1ED9t kho t\xE0i li\u1EC7u ng\u1EAFn), ph\u1EA7n nh\u1ECF c\xE1c ph\u1EA7n b\u1ECB v\u1EE9t b\u1ECF c\u0169ng s\u1EBD t\u0103ng l\xEAn. M\u1ED9t c\xE1ch hi\u1EC7u qu\u1EA3 h\u01A1n \u0111\u1EC3 chu\u1EA9n b\u1ECB d\u1EEF li\u1EC7u l\xE0 k\u1EBFt h\u1EE3p t\u1EA5t c\u1EA3 c\xE1c m\u1EABu \u0111\u01B0\u1EE3c tokenize trong m\u1ED9t l\xF4 v\u1EDBi token "),b=o("code"),j=h("eos_token_id"),z=h(" \u1EDF gi\u1EEFa, v\xE0 sau \u0111\xF3 th\u1EF1c hi\u1EC7n ph\xE2n \u0111o\u1EA1n tr\xEAn c\xE1c chu\u1ED7i \u0111\u01B0\u1EE3c n\u1ED1i. Nh\u01B0 m\u1ED9t b\xE0i t\u1EADp, h\xE3y s\u1EEDa \u0111\u1ED5i h\xE0m "),T=o("code"),H=h("tokenize()"),M=h(" \u0111\u1EC3 s\u1EED d\u1EE5ng c\xE1ch ti\u1EBFp c\u1EADn \u0111\xF3. L\u01B0u \xFD r\u1EB1ng b\u1EA1n s\u1EBD mu\u1ED1n \u0111\u1EB7t "),L=o("code"),f=h("truncation=False"),P=h(" v\xE0 x\xF3a c\xE1c tham s\u1ED1 kh\xE1c kh\u1ECFi tokenizer \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c chu\u1ED7i \u0111\u1EA7y \u0111\u1EE7 c\u1EE7a token ID.")},l(F){a=p(F,"P",{});var S=u(a);_=c(S,"\u270F\uFE0F "),r=p(S,"STRONG",{});var G=u(r);d=c(G,"Th\u1EED nghi\u1EC7m th\xF4i!"),G.forEach(t),$=c(S," Lo\u1EA1i b\u1ECF t\u1EA5t c\u1EA3 c\xE1c ph\u1EA7n nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh kh\xF4ng ph\u1EA3i l\xE0 v\u1EA5n \u0111\u1EC1 l\u1EDBn \u1EDF \u0111\xE2y v\xEC ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng c\xE1c c\u1EEDa s\u1ED5 ng\u1EEF c\u1EA3nh nh\u1ECF. Khi b\u1EA1n t\u0103ng k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh (ho\u1EB7c n\u1EBFu b\u1EA1n c\xF3 m\u1ED9t kho t\xE0i li\u1EC7u ng\u1EAFn), ph\u1EA7n nh\u1ECF c\xE1c ph\u1EA7n b\u1ECB v\u1EE9t b\u1ECF c\u0169ng s\u1EBD t\u0103ng l\xEAn. M\u1ED9t c\xE1ch hi\u1EC7u qu\u1EA3 h\u01A1n \u0111\u1EC3 chu\u1EA9n b\u1ECB d\u1EEF li\u1EC7u l\xE0 k\u1EBFt h\u1EE3p t\u1EA5t c\u1EA3 c\xE1c m\u1EABu \u0111\u01B0\u1EE3c tokenize trong m\u1ED9t l\xF4 v\u1EDBi token "),b=p(S,"CODE",{});var V=u(b);j=c(V,"eos_token_id"),V.forEach(t),z=c(S," \u1EDF gi\u1EEFa, v\xE0 sau \u0111\xF3 th\u1EF1c hi\u1EC7n ph\xE2n \u0111o\u1EA1n tr\xEAn c\xE1c chu\u1ED7i \u0111\u01B0\u1EE3c n\u1ED1i. Nh\u01B0 m\u1ED9t b\xE0i t\u1EADp, h\xE3y s\u1EEDa \u0111\u1ED5i h\xE0m "),T=p(S,"CODE",{});var U=u(T);H=c(U,"tokenize()"),U.forEach(t),M=c(S," \u0111\u1EC3 s\u1EED d\u1EE5ng c\xE1ch ti\u1EBFp c\u1EADn \u0111\xF3. L\u01B0u \xFD r\u1EB1ng b\u1EA1n s\u1EBD mu\u1ED1n \u0111\u1EB7t "),L=p(S,"CODE",{});var N=u(L);f=c(N,"truncation=False"),N.forEach(t),P=c(S," v\xE0 x\xF3a c\xE1c tham s\u1ED1 kh\xE1c kh\u1ECFi tokenizer \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c chu\u1ED7i \u0111\u1EA7y \u0111\u1EE7 c\u1EE7a token ID."),S.forEach(t)},m(F,S){l(F,a,S),s(a,_),s(a,r),s(r,d),s(a,$),s(a,b),s(b,j),s(a,z),s(a,T),s(T,H),s(a,M),s(a,L),s(L,f),s(a,P)},d(F){F&&t(a)}}}function ur(A){let a,_,r,d,$,b,j,z,T,H,M,L;return a=new D({props:{code:`from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),T=new D({props:{code:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # X\xE2y m\xF4 h\xECnh
model.summary()`,highlighted:`model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  <span class="hljs-comment"># X\xE2y m\xF4 h\xECnh</span>
model.summary()`}}),M=new D({props:{code:`_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________`,highlighted:`_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#</span>
=================================================================
transformer (TFGPT2MainLayer multiple                  <span class="hljs-number">124242432</span>
=================================================================
Total params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Trainable params: <span class="hljs-number">124</span>,<span class="hljs-number">242</span>,<span class="hljs-number">432</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){w(a.$$.fragment),_=m(),r=o("p"),d=h("V\u1EDBi c\u1EA5u h\xECnh \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i m\u1ED9t m\xF4 h\xECnh m\u1EDBi. L\u01B0u \xFD r\u1EB1ng \u0111\xE2y l\xE0 l\u1EA7n \u0111\u1EA7u ti\xEAn ch\xFAng ta kh\xF4ng s\u1EED d\u1EE5ng h\xE0m "),$=o("code"),b=h("from_pretrained()"),j=h(", v\xEC ch\xFAng ta th\u1EF1c s\u1EF1 \u0111ang kh\u1EDFi t\u1EA1o m\u1ED9t m\xF4 h\xECnh c\u1EE7a ch\xEDnh ch\xFAng ta:"),z=m(),w(T.$$.fragment),H=m(),w(M.$$.fragment)},l(f){q(a.$$.fragment,f),_=g(f),r=p(f,"P",{});var P=u(r);d=c(P,"V\u1EDBi c\u1EA5u h\xECnh \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i m\u1ED9t m\xF4 h\xECnh m\u1EDBi. L\u01B0u \xFD r\u1EB1ng \u0111\xE2y l\xE0 l\u1EA7n \u0111\u1EA7u ti\xEAn ch\xFAng ta kh\xF4ng s\u1EED d\u1EE5ng h\xE0m "),$=p(P,"CODE",{});var F=u($);b=c(F,"from_pretrained()"),F.forEach(t),j=c(P,", v\xEC ch\xFAng ta th\u1EF1c s\u1EF1 \u0111ang kh\u1EDFi t\u1EA1o m\u1ED9t m\xF4 h\xECnh c\u1EE7a ch\xEDnh ch\xFAng ta:"),P.forEach(t),z=g(f),q(T.$$.fragment,f),H=g(f),q(M.$$.fragment,f)},m(f,P){x(a,f,P),l(f,_,P),l(f,r,P),s(r,d),s(r,$),s($,b),s(r,j),l(f,z,P),x(T,f,P),l(f,H,P),x(M,f,P),L=!0},i(f){L||(y(a.$$.fragment,f),y(T.$$.fragment,f),y(M.$$.fragment,f),L=!0)},o(f){v(a.$$.fragment,f),v(T.$$.fragment,f),v(M.$$.fragment,f),L=!1},d(f){E(a,f),f&&t(_),f&&t(r),f&&t(z),E(T,f),f&&t(H),E(M,f)}}}function mr(A){let a,_,r,d,$,b,j,z,T,H,M,L;return a=new D({props:{code:`from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>,
    vocab_size=<span class="hljs-built_in">len</span>(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)`}}),T=new D({props:{code:`model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")`,highlighted:`model = GPT2LMHeadModel(config)
model_size = <span class="hljs-built_in">sum</span>(t.numel() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> model.parameters())
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;GPT-2 size: <span class="hljs-subst">{model_size/<span class="hljs-number">1000</span>**<span class="hljs-number">2</span>:<span class="hljs-number">.1</span>f}</span>M parameters&quot;</span>)`}}),M=new D({props:{code:"GPT-2 size: 124.2M parameters",highlighted:'GPT-<span class="hljs-number">2</span> size: <span class="hljs-number">124.2</span>M parameters'}}),{c(){w(a.$$.fragment),_=m(),r=o("p"),d=h("V\u1EDBi c\u1EA5u h\xECnh \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i m\u1ED9t m\xF4 h\xECnh m\u1EDBi. L\u01B0u \xFD r\u1EB1ng \u0111\xE2y l\xE0 l\u1EA7n \u0111\u1EA7u ti\xEAn ch\xFAng ta kh\xF4ng s\u1EED d\u1EE5ng h\xE0m "),$=o("code"),b=h("from_pretrained()"),j=h(", v\xEC ch\xFAng ta th\u1EF1c s\u1EF1 \u0111ang kh\u1EDFi t\u1EA1o m\u1ED9t m\xF4 h\xECnh c\u1EE7a ch\xEDnh ch\xFAng ta:"),z=m(),w(T.$$.fragment),H=m(),w(M.$$.fragment)},l(f){q(a.$$.fragment,f),_=g(f),r=p(f,"P",{});var P=u(r);d=c(P,"V\u1EDBi c\u1EA5u h\xECnh \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i m\u1ED9t m\xF4 h\xECnh m\u1EDBi. L\u01B0u \xFD r\u1EB1ng \u0111\xE2y l\xE0 l\u1EA7n \u0111\u1EA7u ti\xEAn ch\xFAng ta kh\xF4ng s\u1EED d\u1EE5ng h\xE0m "),$=p(P,"CODE",{});var F=u($);b=c(F,"from_pretrained()"),F.forEach(t),j=c(P,", v\xEC ch\xFAng ta th\u1EF1c s\u1EF1 \u0111ang kh\u1EDFi t\u1EA1o m\u1ED9t m\xF4 h\xECnh c\u1EE7a ch\xEDnh ch\xFAng ta:"),P.forEach(t),z=g(f),q(T.$$.fragment,f),H=g(f),q(M.$$.fragment,f)},m(f,P){x(a,f,P),l(f,_,P),l(f,r,P),s(r,d),s(r,$),s($,b),s(r,j),l(f,z,P),x(T,f,P),l(f,H,P),x(M,f,P),L=!0},i(f){L||(y(a.$$.fragment,f),y(T.$$.fragment,f),y(M.$$.fragment,f),L=!0)},o(f){v(a.$$.fragment,f),v(T.$$.fragment,f),v(M.$$.fragment,f),L=!1},d(f){E(a,f),f&&t(_),f&&t(r),f&&t(z),E(T,f),f&&t(H),E(M,f)}}}function gr(A){let a,_;return a=new D({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function _r(A){let a,_;return a=new D({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=<span class="hljs-literal">False</span>)`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function dr(A){let a,_;return a=new D({props:{code:`input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)`,highlighted:`input_ids shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
attention_mask shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)
labels shape: (<span class="hljs-number">5</span>, <span class="hljs-number">128</span>)`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function fr(A){let a,_;return a=new D({props:{code:`input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])`,highlighted:`input_ids shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
attention_mask shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])
labels shape: torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">128</span>])`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function Ql(A){let a,_,r,d,$,b,j,z;return j=new D({props:{code:`tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)
tf_eval_dataset = tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),{c(){a=o("p"),_=h("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng ph\u01B0\u01A1ng th\u1EE9c "),r=o("code"),d=h("to_tf_dataset()"),$=h(" \u0111\u1EC3 chuy\u1EC3n \u0111\u1ED5i t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh th\xE0nh t\u1EADp d\u1EEF li\u1EC7u TensorFlow b\u1EB1ng c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\xE3 t\u1EA1o \u1EDF tr\xEAn:"),b=m(),w(j.$$.fragment)},l(T){a=p(T,"P",{});var H=u(a);_=c(H,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng ph\u01B0\u01A1ng th\u1EE9c "),r=p(H,"CODE",{});var M=u(r);d=c(M,"to_tf_dataset()"),M.forEach(t),$=c(H," \u0111\u1EC3 chuy\u1EC3n \u0111\u1ED5i t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh th\xE0nh t\u1EADp d\u1EEF li\u1EC7u TensorFlow b\u1EB1ng c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\xE3 t\u1EA1o \u1EDF tr\xEAn:"),H.forEach(t),b=g(T),q(j.$$.fragment,T)},m(T,H){l(T,a,H),s(a,_),s(a,r),s(r,d),s(a,$),l(T,b,H),x(j,T,H),z=!0},i(T){z||(y(j.$$.fragment,T),z=!0)},o(T){v(j.$$.fragment,T),z=!1},d(T){T&&t(a),T&&t(b),E(j,T)}}}function yr(A){let a,_;return{c(){a=o("p"),_=h("\u26A0\uFE0F Vi\u1EC7c d\u1ECBch chuy\u1EC3n c\xE1c \u0111\u1EA7u v\xE0o v\xE0 nh\xE3n \u0111\u1EC3 c\u0103n ch\u1EC9nh ch\xFAng x\u1EA3y ra b\xEAn trong m\xF4 h\xECnh, do \u0111\xF3, b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ch\u1EC9 c\u1EA7n sao ch\xE9p c\xE1c \u0111\u1EA7u v\xE0o \u0111\u1EC3 t\u1EA1o nh\xE3n.")},l(r){a=p(r,"P",{});var d=u(a);_=c(d,"\u26A0\uFE0F Vi\u1EC7c d\u1ECBch chuy\u1EC3n c\xE1c \u0111\u1EA7u v\xE0o v\xE0 nh\xE3n \u0111\u1EC3 c\u0103n ch\u1EC9nh ch\xFAng x\u1EA3y ra b\xEAn trong m\xF4 h\xECnh, do \u0111\xF3, b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ch\u1EC9 c\u1EA7n sao ch\xE9p c\xE1c \u0111\u1EA7u v\xE0o \u0111\u1EC3 t\u1EA1o nh\xE3n."),d.forEach(t)},m(r,d){l(r,a,d),s(a,_)},d(r){r&&t(a)}}}function vr(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P,F,S,G,V,U;return H=new D({props:{code:`from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Hu\u1EA5n luy\u1EC7n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Hu\u1EA5n luy\u1EC7n trong mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),V=new D({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>, tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])`}}),{c(){a=o("p"),_=h("T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC c\xF2n l\u1EA1i c\u1EA7n l\xE0m l\xE0 \u0111\u1ECBnh c\u1EA5u h\xECnh c\xE1c si\xEAu tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 g\u1ECDi "),r=o("code"),d=h("compile()"),$=h("v\xE0 "),b=o("code"),j=h("fit()"),z=h(". Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng l\u1ECBch tr\xECnh t\u1ED1c \u0111\u1ED9 h\u1ECDc v\u1EDBi m\u1ED9t s\u1ED1 kh\u1EDFi \u0111\u1ED9ng \u0111\u1EC3 c\u1EA3i thi\u1EC7n t\xEDnh \u1ED5n \u0111\u1ECBnh c\u1EE7a qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n:"),T=m(),w(H.$$.fragment),M=m(),L=o("p"),f=h("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 g\u1ECDi "),P=o("code"),F=h("model.fit()"),S=h(" v\xE0 \u0111\u1EE3i qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc. T\xF9y thu\u1ED9c v\xE0o vi\u1EC7c b\u1EA1n ch\u1EA1y n\xF3 tr\xEAn to\xE0n b\u1ED9 hay m\u1ED9t t\u1EADp h\u1EE3p con c\u1EE7a b\u1ED9 hu\u1EA5n luy\u1EC7n, t\u01B0\u01A1ng \u1EE9ng s\u1EBD m\u1EA5t 20 ho\u1EB7c 2 gi\u1EDD, v\xEC v\u1EADy h\xE3y l\u1EA5y m\u1ED9t \xEDt c\xE0 ph\xEA v\xE0 m\u1ED9t cu\u1ED1n s\xE1ch hay \u0111\u1EC3 \u0111\u1ECDc! Sau khi hu\u1EA5n luy\u1EC7n xong, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0 tr\xECnh tokenize v\xE0o Hub:"),G=m(),w(V.$$.fragment)},l(N){a=p(N,"P",{});var B=u(a);_=c(B,"T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC c\xF2n l\u1EA1i c\u1EA7n l\xE0m l\xE0 \u0111\u1ECBnh c\u1EA5u h\xECnh c\xE1c si\xEAu tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 g\u1ECDi "),r=p(B,"CODE",{});var I=u(r);d=c(I,"compile()"),I.forEach(t),$=c(B,"v\xE0 "),b=p(B,"CODE",{});var W=u(b);j=c(W,"fit()"),W.forEach(t),z=c(B,". Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng l\u1ECBch tr\xECnh t\u1ED1c \u0111\u1ED9 h\u1ECDc v\u1EDBi m\u1ED9t s\u1ED1 kh\u1EDFi \u0111\u1ED9ng \u0111\u1EC3 c\u1EA3i thi\u1EC7n t\xEDnh \u1ED5n \u0111\u1ECBnh c\u1EE7a qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n:"),B.forEach(t),T=g(N),q(H.$$.fragment,N),M=g(N),L=p(N,"P",{});var X=u(L);f=c(X,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 g\u1ECDi "),P=p(X,"CODE",{});var Y=u(P);F=c(Y,"model.fit()"),Y.forEach(t),S=c(X," v\xE0 \u0111\u1EE3i qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc. T\xF9y thu\u1ED9c v\xE0o vi\u1EC7c b\u1EA1n ch\u1EA1y n\xF3 tr\xEAn to\xE0n b\u1ED9 hay m\u1ED9t t\u1EADp h\u1EE3p con c\u1EE7a b\u1ED9 hu\u1EA5n luy\u1EC7n, t\u01B0\u01A1ng \u1EE9ng s\u1EBD m\u1EA5t 20 ho\u1EB7c 2 gi\u1EDD, v\xEC v\u1EADy h\xE3y l\u1EA5y m\u1ED9t \xEDt c\xE0 ph\xEA v\xE0 m\u1ED9t cu\u1ED1n s\xE1ch hay \u0111\u1EC3 \u0111\u1ECDc! Sau khi hu\u1EA5n luy\u1EC7n xong, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0 tr\xECnh tokenize v\xE0o Hub:"),X.forEach(t),G=g(N),q(V.$$.fragment,N)},m(N,B){l(N,a,B),s(a,_),s(a,r),s(r,d),s(a,$),s(a,b),s(b,j),s(a,z),l(N,T,B),x(H,N,B),l(N,M,B),l(N,L,B),s(L,f),s(L,P),s(P,F),s(L,S),l(N,G,B),x(V,N,B),U=!0},i(N){U||(y(H.$$.fragment,N),y(V.$$.fragment,N),U=!0)},o(N){v(H.$$.fragment,N),v(V.$$.fragment,N),U=!1},d(N){N&&t(a),N&&t(T),E(H,N),N&&t(M),N&&t(L),N&&t(G),E(V,N)}}}function br(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P,F,S,G,V,U,N,B,I,W,X,Y,Q,cn;return f=new D({props:{code:`from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=<span class="hljs-string">&quot;codeparrot-ds&quot;</span>,
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
    eval_steps=<span class="hljs-number">5_000</span>,
    logging_steps=<span class="hljs-number">5_000</span>,
    gradient_accumulation_steps=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">1</span>,
    weight_decay=<span class="hljs-number">0.1</span>,
    warmup_steps=<span class="hljs-number">1_000</span>,
    lr_scheduler_type=<span class="hljs-string">&quot;cosine&quot;</span>,
    learning_rate=<span class="hljs-number">5e-4</span>,
    save_steps=<span class="hljs-number">5_000</span>,
    fp16=<span class="hljs-literal">True</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;valid&quot;</span>],
)`}}),B=new D({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),Q=new D({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){a=o("p"),_=h("T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC c\xF2n l\u1EA1i c\u1EA7n l\xE0m l\xE0 c\u1EA5u h\xECnh c\xE1c tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 k\xEDch ho\u1EA1t "),r=o("code"),d=h("Trainer"),$=h(". Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng l\u1ECBch tr\xECnh t\u1ED1c \u0111\u1ED9 h\u1ECDc cosine v\u1EDBi m\u1ED9t s\u1ED1 l\u1EA7n kh\u1EDFi \u0111\u1ED9ng v\xE0 k\xEDch th\u01B0\u1EDBc l\xF4 hi\u1EC7u qu\u1EA3 l\xE0 256 ("),b=o("code"),j=h("per_device_train_batch_size"),z=h(" * "),T=o("code"),H=h("gradient_accumulation_steps"),M=h("). T\xEDch l\u0169y gradient \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng khi m\u1ED9t lo\u1EA1t l\xF4 duy nh\u1EA5t kh\xF4ng v\u1EEBa v\u1EDBi b\u1ED9 nh\u1EDB v\xE0 d\u1EA7n d\u1EA7n t\xEDch l\u0169y gradient th\xF4ng qua m\u1ED9t s\u1ED1 l\u1EA7n truy\u1EC1n xu\xF4i/ng\u01B0\u1EE3c. Ch\xFAng ta s\u1EBD th\u1EA5y \u0111i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng khi ch\xFAng ta t\u1EA1o v\xF2ng hu\u1EA5n luy\u1EC7n v\u1EDBi \u{1F917} Accelerate."),L=m(),w(f.$$.fragment),P=m(),F=o("p"),S=h("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 kh\u1EDFi \u0111\u1ED9ng "),G=o("code"),V=h("Trainer"),U=h(" v\xE0 \u0111\u1EE3i qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc. T\xF9y thu\u1ED9c v\xE0o vi\u1EC7c b\u1EA1n ch\u1EA1y n\xF3 tr\xEAn to\xE0n b\u1ED9 hay m\u1ED9t t\u1EADp h\u1EE3p con c\u1EE7a b\u1ED9 hu\u1EA5n luy\u1EC7n, t\u01B0\u01A1ng \u1EE9ng s\u1EBD m\u1EA5t 20 ho\u1EB7c 2 gi\u1EDD, v\xEC v\u1EADy h\xE3y l\u1EA5y m\u1ED9t \xEDt c\xE0 ph\xEA v\xE0 m\u1ED9t cu\u1ED1n s\xE1ch hay \u0111\u1EC3 \u0111\u1ECDc!"),N=m(),w(B.$$.fragment),I=m(),W=o("p"),X=h("Sau khi qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n ho\xE0n t\u1EA5t, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0 tr\xECnh tokenizer v\xE0o Hub:"),Y=m(),w(Q.$$.fragment)},l(C){a=p(C,"P",{});var K=u(a);_=c(K,"T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC c\xF2n l\u1EA1i c\u1EA7n l\xE0m l\xE0 c\u1EA5u h\xECnh c\xE1c tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 k\xEDch ho\u1EA1t "),r=p(K,"CODE",{});var An=u(r);d=c(An,"Trainer"),An.forEach(t),$=c(K,". Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng l\u1ECBch tr\xECnh t\u1ED1c \u0111\u1ED9 h\u1ECDc cosine v\u1EDBi m\u1ED9t s\u1ED1 l\u1EA7n kh\u1EDFi \u0111\u1ED9ng v\xE0 k\xEDch th\u01B0\u1EDBc l\xF4 hi\u1EC7u qu\u1EA3 l\xE0 256 ("),b=p(K,"CODE",{});var On=u(b);j=c(On,"per_device_train_batch_size"),On.forEach(t),z=c(K," * "),T=p(K,"CODE",{});var At=u(T);H=c(At,"gradient_accumulation_steps"),At.forEach(t),M=c(K,"). T\xEDch l\u0169y gradient \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng khi m\u1ED9t lo\u1EA1t l\xF4 duy nh\u1EA5t kh\xF4ng v\u1EEBa v\u1EDBi b\u1ED9 nh\u1EDB v\xE0 d\u1EA7n d\u1EA7n t\xEDch l\u0169y gradient th\xF4ng qua m\u1ED9t s\u1ED1 l\u1EA7n truy\u1EC1n xu\xF4i/ng\u01B0\u1EE3c. Ch\xFAng ta s\u1EBD th\u1EA5y \u0111i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng khi ch\xFAng ta t\u1EA1o v\xF2ng hu\u1EA5n luy\u1EC7n v\u1EDBi \u{1F917} Accelerate."),K.forEach(t),L=g(C),q(f.$$.fragment,C),P=g(C),F=p(C,"P",{});var bn=u(F);S=c(bn,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 kh\u1EDFi \u0111\u1ED9ng "),G=p(bn,"CODE",{});var Nn=u(G);V=c(Nn,"Trainer"),Nn.forEach(t),U=c(bn," v\xE0 \u0111\u1EE3i qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc. T\xF9y thu\u1ED9c v\xE0o vi\u1EC7c b\u1EA1n ch\u1EA1y n\xF3 tr\xEAn to\xE0n b\u1ED9 hay m\u1ED9t t\u1EADp h\u1EE3p con c\u1EE7a b\u1ED9 hu\u1EA5n luy\u1EC7n, t\u01B0\u01A1ng \u1EE9ng s\u1EBD m\u1EA5t 20 ho\u1EB7c 2 gi\u1EDD, v\xEC v\u1EADy h\xE3y l\u1EA5y m\u1ED9t \xEDt c\xE0 ph\xEA v\xE0 m\u1ED9t cu\u1ED1n s\xE1ch hay \u0111\u1EC3 \u0111\u1ECDc!"),bn.forEach(t),N=g(C),q(B.$$.fragment,C),I=g(C),W=p(C,"P",{});var Ot=u(W);X=c(Ot,"Sau khi qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n ho\xE0n t\u1EA5t, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0 tr\xECnh tokenizer v\xE0o Hub:"),Ot.forEach(t),Y=g(C),q(Q.$$.fragment,C)},m(C,K){l(C,a,K),s(a,_),s(a,r),s(r,d),s(a,$),s(a,b),s(b,j),s(a,z),s(a,T),s(T,H),s(a,M),l(C,L,K),x(f,C,K),l(C,P,K),l(C,F,K),s(F,S),s(F,G),s(G,V),s(F,U),l(C,N,K),x(B,C,K),l(C,I,K),l(C,W,K),s(W,X),l(C,Y,K),x(Q,C,K),cn=!0},i(C){cn||(y(f.$$.fragment,C),y(B.$$.fragment,C),y(Q.$$.fragment,C),cn=!0)},o(C){v(f.$$.fragment,C),v(B.$$.fragment,C),v(Q.$$.fragment,C),cn=!1},d(C){C&&t(a),C&&t(L),E(f,C),C&&t(P),C&&t(F),C&&t(N),E(B,C),C&&t(I),C&&t(W),C&&t(Y),E(Q,C)}}}function kr(A){let a,_,r,d,$,b,j,z;return{c(){a=o("p"),_=h("\u270F\uFE0F "),r=o("strong"),d=h("Th\u1EED nghi\u1EC7m th\xF4i!"),$=h(" Ch\u1EC9 m\u1EA5t kho\u1EA3ng 30 d\xF2ng m\xE3 ngo\xE0i "),b=o("code"),j=h("TrainingArguments"),z=h(" \u0111\u1EC3 t\u1EEB v\u0103n b\u1EA3n th\xF4 \u0111\u1EBFn hu\u1EA5n luy\u1EC7n GPT-2. H\xE3y d\xF9ng th\u1EED v\u1EDBi t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ri\xEAng b\u1EA1n v\xE0 xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t hay kh\xF4ng!")},l(T){a=p(T,"P",{});var H=u(a);_=c(H,"\u270F\uFE0F "),r=p(H,"STRONG",{});var M=u(r);d=c(M,"Th\u1EED nghi\u1EC7m th\xF4i!"),M.forEach(t),$=c(H," Ch\u1EC9 m\u1EA5t kho\u1EA3ng 30 d\xF2ng m\xE3 ngo\xE0i "),b=p(H,"CODE",{});var L=u(b);j=c(L,"TrainingArguments"),L.forEach(t),z=c(H," \u0111\u1EC3 t\u1EEB v\u0103n b\u1EA3n th\xF4 \u0111\u1EBFn hu\u1EA5n luy\u1EC7n GPT-2. H\xE3y d\xF9ng th\u1EED v\u1EDBi t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ri\xEAng b\u1EA1n v\xE0 xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t hay kh\xF4ng!"),H.forEach(t)},m(T,H){l(T,a,H),s(a,_),s(a,r),s(r,d),s(a,$),s(a,b),s(b,j),s(a,z)},d(T){T&&t(a)}}}function $r(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P,F,S,G,V,U,N;return{c(){a=o("p"),_=h("\u{1F4A1} N\u1EBFu b\u1EA1n c\xF3 quy\u1EC1n truy c\u1EADp v\xE0o m\u1ED9t m\xE1y c\xF3 nhi\u1EC1u GPU, b\u1EA1n c\xF3 th\u1EC3 th\u1EED s\u1EED d\u1EE5ng ng\u1EEF c\u1EA3nh "),r=o("code"),d=h("MirroredStrategy"),$=h(" \u0111\u1EC3 t\u0103ng t\u1ED1c \u0111\xE1ng k\u1EC3 cho qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n. B\u1EA1n s\u1EBD c\u1EA7n t\u1EA1o m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),b=o("code"),j=h("tf.distribute.MirroredStrategy"),z=h(" v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng c\xE1c l\u1EC7nh "),T=o("code"),H=h("to_tf_dataset"),M=h(" c\u0169ng nh\u01B0 t\u1EA1o m\xF4 h\xECnh v\xE0 l\u1EC7nh g\u1ECDi \u0111\u1EBFn "),L=o("code"),f=h("fit()"),P=h(" \u0111\u1EC1u \u0111\u01B0\u1EE3c ch\u1EA1y trong ng\u1EEF c\u1EA3nh "),F=o("code"),S=h("scope()"),G=h(" c\u1EE7a n\xF3. B\u1EA1n c\xF3 th\u1EC3 xem t\xE0i li\u1EC7u v\u1EC1 \u0111i\u1EC1u n\xE0y "),V=o("a"),U=h("t\u1EA1i \u0111\xE2y"),N=h("."),this.h()},l(B){a=p(B,"P",{});var I=u(a);_=c(I,"\u{1F4A1} N\u1EBFu b\u1EA1n c\xF3 quy\u1EC1n truy c\u1EADp v\xE0o m\u1ED9t m\xE1y c\xF3 nhi\u1EC1u GPU, b\u1EA1n c\xF3 th\u1EC3 th\u1EED s\u1EED d\u1EE5ng ng\u1EEF c\u1EA3nh "),r=p(I,"CODE",{});var W=u(r);d=c(W,"MirroredStrategy"),W.forEach(t),$=c(I," \u0111\u1EC3 t\u0103ng t\u1ED1c \u0111\xE1ng k\u1EC3 cho qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n. B\u1EA1n s\u1EBD c\u1EA7n t\u1EA1o m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),b=p(I,"CODE",{});var X=u(b);j=c(X,"tf.distribute.MirroredStrategy"),X.forEach(t),z=c(I," v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng c\xE1c l\u1EC7nh "),T=p(I,"CODE",{});var Y=u(T);H=c(Y,"to_tf_dataset"),Y.forEach(t),M=c(I," c\u0169ng nh\u01B0 t\u1EA1o m\xF4 h\xECnh v\xE0 l\u1EC7nh g\u1ECDi \u0111\u1EBFn "),L=p(I,"CODE",{});var Q=u(L);f=c(Q,"fit()"),Q.forEach(t),P=c(I," \u0111\u1EC1u \u0111\u01B0\u1EE3c ch\u1EA1y trong ng\u1EEF c\u1EA3nh "),F=p(I,"CODE",{});var cn=u(F);S=c(cn,"scope()"),cn.forEach(t),G=c(I," c\u1EE7a n\xF3. B\u1EA1n c\xF3 th\u1EC3 xem t\xE0i li\u1EC7u v\u1EC1 \u0111i\u1EC1u n\xE0y "),V=p(I,"A",{href:!0,rel:!0});var C=u(V);U=c(C,"t\u1EA1i \u0111\xE2y"),C.forEach(t),N=c(I,"."),I.forEach(t),this.h()},h(){O(V,"href","https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit"),O(V,"rel","nofollow")},m(B,I){l(B,a,I),s(a,_),s(a,r),s(r,d),s(a,$),s(a,b),s(b,j),s(a,z),s(a,T),s(T,H),s(a,M),s(a,L),s(L,f),s(a,P),s(a,F),s(F,S),s(a,G),s(a,V),s(V,U),s(a,N)},d(B){B&&t(a)}}}function jr(A){let a,_,r,d,$;return{c(){a=o("p"),_=h("\u{1F4A1} N\u1EBFu b\u1EA1n c\xF3 quy\u1EC1n truy c\u1EADp v\xE0o m\u1ED9t m\xE1y c\xF3 nhi\u1EC1u GPU, h\xE3y th\u1EED ch\u1EA1y m\xE3 \u1EDF \u0111\xF3. "),r=o("code"),d=h("Trainer"),$=h(" t\u1EF1 \u0111\u1ED9ng qu\u1EA3n l\xFD nhi\u1EC1u m\xE1y v\xE0 \u0111i\u1EC1u n\xE0y c\xF3 th\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn r\u1EA5t nhi\u1EC1u.")},l(b){a=p(b,"P",{});var j=u(a);_=c(j,"\u{1F4A1} N\u1EBFu b\u1EA1n c\xF3 quy\u1EC1n truy c\u1EADp v\xE0o m\u1ED9t m\xE1y c\xF3 nhi\u1EC1u GPU, h\xE3y th\u1EED ch\u1EA1y m\xE3 \u1EDF \u0111\xF3. "),r=p(j,"CODE",{});var z=u(r);d=c(z,"Trainer"),z.forEach(t),$=c(j," t\u1EF1 \u0111\u1ED9ng qu\u1EA3n l\xFD nhi\u1EC1u m\xE1y v\xE0 \u0111i\u1EC1u n\xE0y c\xF3 th\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn r\u1EA5t nhi\u1EC1u."),j.forEach(t)},m(b,j){l(b,a,j),s(a,_),s(a,r),s(r,d),s(a,$)},d(b){b&&t(a)}}}function wr(A){let a;function _($,b){return $[0]==="pt"?jr:$r}let r=_(A),d=r(A);return{c(){d.c(),a=Jh()},l($){d.l($),a=Jh()},m($,b){d.m($,b),l($,a,b)},p($,b){r!==(r=_($))&&(d.d(1),d=r($),d&&(d.c(),d.m(a.parentNode,a)))},d($){d.d($),$&&t(a)}}}function qr(A){let a,_;return a=new D({props:{code:`from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

course_model = TFGPT2LMHeadModel.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
course_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=course_model, tokenizer=course_tokenizer, device=<span class="hljs-number">0</span>
)`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function xr(A){let a,_;return a=new D({props:{code:`import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/codeparrot-ds&quot;</span>, device=device
)`}}),{c(){w(a.$$.fragment)},l(r){q(a.$$.fragment,r)},m(r,d){x(a,r,d),_=!0},i(r){_||(y(a.$$.fragment,r),_=!0)},o(r){v(a.$$.fragment,r),_=!1},d(r){E(a,r)}}}function Er(A){let a,_,r,d,$;return{c(){a=o("p"),_=h("Nh\xECn v\xE0o m\u1ED9t v\xE0i v\xED d\u1EE5 n\xE0y, c\xF3 v\u1EBB nh\u01B0 m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c m\u1ED9t s\u1ED1 c\xFA ph\xE1p c\u1EE7a b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python (t\u1EA5t nhi\xEAn, ch\xFAng t\xF4i s\u1EBD c\u1EA7n \u0111\xE1nh gi\xE1 k\u1EF9 l\u01B0\u1EE1ng h\u01A1n tr\u01B0\u1EDBc khi tri\u1EC3n khai m\xF4 h\xECnh trong th\u1EBF gi\u1EDBi th\u1EF1c). Tuy nhi\xEAn, \u0111\xF4i khi n\xF3 \u0111\xF2i h\u1ECFi ph\u1EA3i t\xF9y ch\u1EC9nh nhi\u1EC1u h\u01A1n vi\u1EC7c hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh \u0111\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c hi\u1EC7u su\u1EA5t c\u1EA7n thi\u1EBFt cho m\u1ED9t tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng nh\u1EA5t \u0111\u1ECBnh. V\xED d\u1EE5: \u0111i\u1EC1u g\xEC s\u1EBD x\u1EA3y ra n\u1EBFu ch\xFAng ta mu\u1ED1n c\u1EADp nh\u1EADt \u0111\u1ED9ng k\xEDch th\u01B0\u1EDBc l\xF4 ho\u1EB7c c\xF3 m\u1ED9t v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n c\xF3 \u0111i\u1EC1u ki\u1EC7n \u0111\u1EC3 b\u1ECF qua c\xE1c v\xED d\u1EE5 x\u1EA5u m\u1ED9t c\xE1ch nhanh ch\xF3ng? M\u1ED9t t\xF9y ch\u1ECDn s\u1EBD l\xE0 ph\xE2n l\u1EDBp "),r=o("code"),d=h("Trainer"),$=h(" v\xE0 th\xEAm c\xE1c thay \u0111\u1ED5i c\u1EA7n thi\u1EBFt, nh\u01B0ng \u0111\xF4i khi vi\u1EC7c vi\u1EBFt v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n t\u1EEB \u0111\u1EA7u s\u1EBD \u0111\u01A1n gi\u1EA3n h\u01A1n. \u0110\xF3 l\xE0 l\xFAc \u{1F917} Accelerate xu\u1EA5t hi\u1EC7n.")},l(b){a=p(b,"P",{});var j=u(a);_=c(j,"Nh\xECn v\xE0o m\u1ED9t v\xE0i v\xED d\u1EE5 n\xE0y, c\xF3 v\u1EBB nh\u01B0 m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c m\u1ED9t s\u1ED1 c\xFA ph\xE1p c\u1EE7a b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python (t\u1EA5t nhi\xEAn, ch\xFAng t\xF4i s\u1EBD c\u1EA7n \u0111\xE1nh gi\xE1 k\u1EF9 l\u01B0\u1EE1ng h\u01A1n tr\u01B0\u1EDBc khi tri\u1EC3n khai m\xF4 h\xECnh trong th\u1EBF gi\u1EDBi th\u1EF1c). Tuy nhi\xEAn, \u0111\xF4i khi n\xF3 \u0111\xF2i h\u1ECFi ph\u1EA3i t\xF9y ch\u1EC9nh nhi\u1EC1u h\u01A1n vi\u1EC7c hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh \u0111\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c hi\u1EC7u su\u1EA5t c\u1EA7n thi\u1EBFt cho m\u1ED9t tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng nh\u1EA5t \u0111\u1ECBnh. V\xED d\u1EE5: \u0111i\u1EC1u g\xEC s\u1EBD x\u1EA3y ra n\u1EBFu ch\xFAng ta mu\u1ED1n c\u1EADp nh\u1EADt \u0111\u1ED9ng k\xEDch th\u01B0\u1EDBc l\xF4 ho\u1EB7c c\xF3 m\u1ED9t v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n c\xF3 \u0111i\u1EC1u ki\u1EC7n \u0111\u1EC3 b\u1ECF qua c\xE1c v\xED d\u1EE5 x\u1EA5u m\u1ED9t c\xE1ch nhanh ch\xF3ng? M\u1ED9t t\xF9y ch\u1ECDn s\u1EBD l\xE0 ph\xE2n l\u1EDBp "),r=p(j,"CODE",{});var z=u(r);d=c(z,"Trainer"),z.forEach(t),$=c(j," v\xE0 th\xEAm c\xE1c thay \u0111\u1ED5i c\u1EA7n thi\u1EBFt, nh\u01B0ng \u0111\xF4i khi vi\u1EC7c vi\u1EBFt v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n t\u1EEB \u0111\u1EA7u s\u1EBD \u0111\u01A1n gi\u1EA3n h\u01A1n. \u0110\xF3 l\xE0 l\xFAc \u{1F917} Accelerate xu\u1EA5t hi\u1EC7n."),j.forEach(t)},m(b,j){l(b,a,j),s(a,_),s(a,r),s(r,d),s(a,$)},d(b){b&&t(a)}}}function Tr(A){let a,_;return{c(){a=o("p"),_=h("Nh\xECn v\xE0o m\u1ED9t v\xE0i v\xED d\u1EE5 n\xE0y, c\xF3 v\u1EBB nh\u01B0 m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c m\u1ED9t s\u1ED1 c\xFA ph\xE1p c\u1EE7a b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python. T\u1EA5t nhi\xEAn, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i \u0111\xE1nh gi\xE1 m\xF4 h\xECnh k\u1EF9 l\u01B0\u1EE1ng h\u01A1n tr\u01B0\u1EDBc khi tri\u1EC3n khai n\xF3 trong th\u1EBF gi\u1EDBi th\u1EF1c, nh\u01B0ng \u0111\xE2y v\u1EABn l\xE0 m\u1ED9t nguy\xEAn m\u1EABu \u1EA5n t\u01B0\u1EE3ng.")},l(r){a=p(r,"P",{});var d=u(a);_=c(d,"Nh\xECn v\xE0o m\u1ED9t v\xE0i v\xED d\u1EE5 n\xE0y, c\xF3 v\u1EBB nh\u01B0 m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c m\u1ED9t s\u1ED1 c\xFA ph\xE1p c\u1EE7a b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python. T\u1EA5t nhi\xEAn, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i \u0111\xE1nh gi\xE1 m\xF4 h\xECnh k\u1EF9 l\u01B0\u1EE1ng h\u01A1n tr\u01B0\u1EDBc khi tri\u1EC3n khai n\xF3 trong th\u1EBF gi\u1EDBi th\u1EF1c, nh\u01B0ng \u0111\xE2y v\u1EABn l\xE0 m\u1ED9t nguy\xEAn m\u1EABu \u1EA5n t\u01B0\u1EE3ng."),d.forEach(t)},m(r,d){l(r,a,d),s(a,_)},d(r){r&&t(a)}}}function nr(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P,F,S,G,V,U,N,B,I,W,X,Y,Q,cn,C,K,An,On,At,bn,Nn,Ot,is,os,fe,ps,us,ye,Cs,Mn,Ln,rn,nt,tt,Nt,ve,Mt,st,zs,et,sn,be,on,kn,ke,at,ms,$e,Ps,pn,Fn,Lt,nn,je,ht,we,Ds,tn,qe,ct,Hs,lt,Ft,Gt,xe,Gn,rt,As,it,Sn,Os,qn,Ns,$n,Ms,xn,Ls,St,Bt,Fs,ot,gs,un,_s,En,Ee,ds,mn,fs,Tn,Te,ys,gn,vs,_n,bs,en,Ce,It,Vt,ze,Rt,pt,Gs,Bn,In,Cn,an,Kt,ks,Ss,ut,Ut,mt,Bs,jn,Pe,$s,Is,gt,js,hn,zn,Vn,_t,Vs,Pn,Rs,dn,dt,Xt,Wt,De,Yt,ln,He,Zt,Jt,Ae,ft,yt,Ks,Dn,Us,Rn,Xs,fn,vt,la,Ws,yn,Oe,bt,Qt,Ys,Kn,Un,Xn,Wn;return d=new Ka({}),F=new Nc({props:{id:"Hm8_PgVTFuc"}}),Ln=new D({props:{code:`keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")`,highlighted:`keytoken_ids = []
<span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> [
    <span class="hljs-string">&quot;plt&quot;</span>,
    <span class="hljs-string">&quot;pd&quot;</span>,
    <span class="hljs-string">&quot;sk&quot;</span>,
    <span class="hljs-string">&quot;fit&quot;</span>,
    <span class="hljs-string">&quot;predict&quot;</span>,
    <span class="hljs-string">&quot; plt&quot;</span>,
    <span class="hljs-string">&quot; pd&quot;</span>,
    <span class="hljs-string">&quot; sk&quot;</span>,
    <span class="hljs-string">&quot; fit&quot;</span>,
    <span class="hljs-string">&quot; predict&quot;</span>,
    <span class="hljs-string">&quot;testtest&quot;</span>,
]:
    ids = tokenizer([keyword]).input_ids[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ids) == <span class="hljs-number">1</span>:
        keytoken_ids.append(ids[<span class="hljs-number">0</span>])
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keyword has not single token: <span class="hljs-subst">{keyword}</span>&quot;</span>)`}}),nt=new D({props:{code:"'Keyword has not single token: testtest'",highlighted:'<span class="hljs-string">&#x27;Keyword has not single token: testtest&#x27;</span>'}}),st=new D({props:{code:`from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # D\u1ECBch chuy\u1EC3n \u0111\u1EC3 token < n d\u1EF1 \u0111o\xE1n n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # T\xEDnh \u0111\u1ED9 m\u1EA5t m\xE1t t\u1EEBng token
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Thay \u0111\u1ED5i k\xEDch th\u01B0\u1EDBc v\xE0 m\u1EA5t m\xE1t trung b\xECnh tr\xEAn m\u1ED7i m\u1EABu
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # T\xEDnh to\xE1n v\xE0 chia t\u1EF7 tr\u1ECDng
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # T\xEDnh gi\xE1 tr\u1ECB trung b\xECnh c\xF3 tr\u1ECDng s\u1ED1
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss`,highlighted:`<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> CrossEntropyLoss
<span class="hljs-keyword">import</span> torch


<span class="hljs-keyword">def</span> <span class="hljs-title function_">keytoken_weighted_loss</span>(<span class="hljs-params">inputs, logits, keytoken_ids, alpha=<span class="hljs-number">1.0</span></span>):
    <span class="hljs-comment"># D\u1ECBch chuy\u1EC3n \u0111\u1EC3 token &lt; n d\u1EF1 \u0111o\xE1n n</span>
    shift_labels = inputs[..., <span class="hljs-number">1</span>:].contiguous()
    shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()
    <span class="hljs-comment"># T\xEDnh \u0111\u1ED9 m\u1EA5t m\xE1t t\u1EEBng token</span>
    loss_fct = CrossEntropyLoss(reduce=<span class="hljs-literal">False</span>)
    loss = loss_fct(shift_logits.view(-<span class="hljs-number">1</span>, shift_logits.size(-<span class="hljs-number">1</span>)), shift_labels.view(-<span class="hljs-number">1</span>))
    <span class="hljs-comment"># Thay \u0111\u1ED5i k\xEDch th\u01B0\u1EDBc v\xE0 m\u1EA5t m\xE1t trung b\xECnh tr\xEAn m\u1ED7i m\u1EABu</span>
    loss_per_sample = loss.view(shift_logits.size(<span class="hljs-number">0</span>), shift_logits.size(<span class="hljs-number">1</span>)).mean(axis=<span class="hljs-number">1</span>)
    <span class="hljs-comment"># T\xEDnh to\xE1n v\xE0 chia t\u1EF7 tr\u1ECDng</span>
    weights = torch.stack([(inputs == kt).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">for</span> kt <span class="hljs-keyword">in</span> keytoken_ids]).<span class="hljs-built_in">sum</span>(
        axis=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]
    )
    weights = alpha * (<span class="hljs-number">1.0</span> + weights)
    <span class="hljs-comment"># T\xEDnh gi\xE1 tr\u1ECB trung b\xECnh c\xF3 tr\u1ECDng s\u1ED1</span>
    weighted_loss = (loss_per_sample * weights).mean()
    <span class="hljs-keyword">return</span> weighted_loss`}}),lt=new D({props:{code:`from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader

tokenized_dataset.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
train_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>], batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)
eval_dataloader = DataLoader(tokenized_dataset[<span class="hljs-string">&quot;valid&quot;</span>], batch_size=<span class="hljs-number">32</span>)`}}),rt=new D({props:{code:`weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]`,highlighted:`weight_decay = <span class="hljs-number">0.1</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_grouped_params</span>(<span class="hljs-params">model, no_decay=[<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-string">&quot;LayerNorm.weight&quot;</span>]</span>):
    params_with_wd, params_without_wd = [], []
    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay):
            params_without_wd.append(p)
        <span class="hljs-keyword">else</span>:
            params_with_wd.append(p)
    <span class="hljs-keyword">return</span> [
        {<span class="hljs-string">&quot;params&quot;</span>: params_with_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: weight_decay},
        {<span class="hljs-string">&quot;params&quot;</span>: params_without_wd, <span class="hljs-string">&quot;weight_decay&quot;</span>: <span class="hljs-number">0.0</span>},
    ]`}}),qn=new D({props:{code:`def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>():
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], labels=batch[<span class="hljs-string">&quot;input_ids&quot;</span>])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    <span class="hljs-keyword">try</span>:
        perplexity = torch.exp(loss)
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)
    <span class="hljs-keyword">return</span> loss.item(), perplexity.item()`}}),un=new D({props:{code:"model = GPT2LMHeadModel(config)",highlighted:"model = GPT2LMHeadModel(config)"}}),mn=new D({props:{code:`from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(get_grouped_params(model), lr=<span class="hljs-number">5e-4</span>)`}}),gn=new D({props:{code:`from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator(fp16=<span class="hljs-literal">True</span>)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),_n=new de({props:{$$slots:{default:[Cr]},$$scope:{ctx:A}}}),In=new D({props:{code:`from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">1</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name=<span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_training_steps=num_training_steps,
)`}}),gt=new D({props:{code:`from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, get_full_repo_name

model_name = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),hn=new D({props:{code:"'sgugger/codeparrot-ds-accelerate'",highlighted:'<span class="hljs-string">&#x27;sgugger/codeparrot-ds-accelerate&#x27;</span>'}}),Pn=new D({props:{code:`output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`output_dir = <span class="hljs-string">&quot;codeparrot-ds-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),yt=new D({props:{code:"evaluate()",highlighted:"evaluate()"}}),Dn=new D({props:{code:"(10.934126853942871, 56057.14453125)",highlighted:'(<span class="hljs-number">10.934126853942871</span>, <span class="hljs-number">56057.14453125</span>)'}}),yn=new D({props:{code:`from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm

gradient_accumulation_steps = <span class="hljs-number">8</span>
eval_steps = <span class="hljs-number">5_000</span>

model.train()
completed_steps = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> tqdm(
        <span class="hljs-built_in">enumerate</span>(train_dataloader, start=<span class="hljs-number">1</span>), total=num_training_steps
    ):
        logits = model(batch[<span class="hljs-string">&quot;input_ids&quot;</span>]).logits
        loss = keytoken_weighted_loss(batch[<span class="hljs-string">&quot;input_ids&quot;</span>], logits, keytoken_ids)
        <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            accelerator.<span class="hljs-built_in">print</span>(
                {
                    <span class="hljs-string">&quot;lr&quot;</span>: get_lr(),
                    <span class="hljs-string">&quot;samples&quot;</span>: step * samples_per_step,
                    <span class="hljs-string">&quot;steps&quot;</span>: completed_steps,
                    <span class="hljs-string">&quot;loss/train&quot;</span>: loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        <span class="hljs-keyword">if</span> step % gradient_accumulation_steps == <span class="hljs-number">0</span>:
            accelerator.clip_grad_norm_(model.parameters(), <span class="hljs-number">1.0</span>)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> (step % (eval_steps * gradient_accumulation_steps)) == <span class="hljs-number">0</span>:
            eval_loss, perplexity = evaluate()
            accelerator.<span class="hljs-built_in">print</span>({<span class="hljs-string">&quot;loss/eval&quot;</span>: eval_loss, <span class="hljs-string">&quot;perplexity&quot;</span>: perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            <span class="hljs-keyword">if</span> accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=<span class="hljs-string">f&quot;Training in progress step <span class="hljs-subst">{step}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
                )`}}),Kn=new de({props:{$$slots:{default:[zr]},$$scope:{ctx:A}}}),Xn=new de({props:{$$slots:{default:[Pr]},$$scope:{ctx:A}}}),{c(){a=o("h2"),_=o("a"),r=o("span"),w(d.$$.fragment),$=m(),b=o("span"),j=h("Hu\u1EA5n luy\u1EC7n v\u1EDBi \u{1F917} Accelerate"),z=m(),T=o("p"),H=h("Ch\xFAng ta \u0111\xE3 th\u1EA5y c\xE1ch hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh v\u1EDBi "),M=o("code"),L=h("Trainer"),f=h(", c\xF3 th\u1EC3 cho ph\xE9p m\u1ED9t s\u1ED1 t\xF9y ch\u1EC9nh. Tuy nhi\xEAn, \u0111\xF4i khi ch\xFAng ta mu\u1ED1n to\xE0n quy\u1EC1n ki\u1EC3m so\xE1t v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n ho\u1EB7c ch\xFAng ta mu\u1ED1n th\u1EF1c hi\u1EC7n m\u1ED9t s\u1ED1 thay \u0111\u1ED5i k\u1EF3 l\u1EA1. Trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y \u{1F917} Accelerate l\xE0 m\u1ED9t l\u1EF1a ch\u1ECDn tuy\u1EC7t v\u1EDDi v\xE0 trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD xem x\xE9t c\xE1c b\u01B0\u1EDBc s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh c\u1EE7a m\xECnh. \u0110\u1EC3 l\xE0m cho m\u1ECDi th\u1EE9 th\xFA v\u1ECB h\u01A1n, ch\xFAng ta c\u0169ng s\u1EBD th\xEAm m\u1ED9t s\u1ED1 \u0111i\u1EC1u ch\u1EC9nh v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n."),P=m(),w(F.$$.fragment),S=m(),G=o("p"),V=h("V\xEC ch\xFAng ta ch\u1EE7 y\u1EBFu quan t\xE2m \u0111\u1EBFn t\xEDnh n\u0103ng t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh h\u1EE3p l\xFD cho c\xE1c th\u01B0 vi\u1EC7n khoa h\u1ECDc d\u1EEF li\u1EC7u, n\xEAn vi\u1EC7c \u0111\u01B0a ra nhi\u1EC1u tr\u1ECDng s\u1ED1 h\u01A1n cho c\xE1c m\u1EABu hu\u1EA5n luy\u1EC7n s\u1EED d\u1EE5ng nhi\u1EC1u h\u01A1n c\xE1c th\u01B0 vi\u1EC7n n\xE0y. Ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng x\xE1c \u0111\u1ECBnh nh\u1EEFng v\xED d\u1EE5 n\xE0y th\xF4ng qua vi\u1EC7c s\u1EED d\u1EE5ng c\xE1c t\u1EEB kh\xF3a nh\u01B0 "),U=o("code"),N=h("plt"),B=h(", "),I=o("code"),W=h("pd"),X=h(", "),Y=o("code"),Q=h("sk"),cn=h(", "),C=o("code"),K=h("fit"),An=h(" v\xE0 "),On=o("code"),At=h("predict"),bn=h(", l\xE0 nh\u1EEFng t\xEAn nh\u1EADp th\u01B0\u1EDDng g\u1EB7p nh\u1EA5t cho "),Nn=o("code"),Ot=h("matplotlib.pyplot"),is=h(", "),os=o("code"),fe=h("pandas"),ps=h(", v\xE0 "),us=o("code"),ye=h("sklearn"),Cs=h(" c\u0169ng nh\u01B0 c\xE1c h\xE0nh vi sau \u0111\xF3. N\u1EBFu ch\xFAng \u0111\u01B0\u1EE3c bi\u1EC3u di\u1EC5n d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t token duy nh\u1EA5t, ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng ki\u1EC3m tra xem ch\xFAng c\xF3 xu\u1EA5t hi\u1EC7n trong chu\u1ED7i \u0111\u1EA7u v\xE0o hay kh\xF4ng. C\xE1c token c\xF3 th\u1EC3 c\xF3 ti\u1EC1n t\u1ED1 kho\u1EA3ng tr\u1EAFng, v\xEC v\u1EADy ch\xFAng ta c\u0169ng s\u1EBD ki\u1EC3m tra c\xE1c phi\xEAn b\u1EA3n \u0111\xF3 trong t\u1EEB v\u1EF1ng b\u1ED9 tokenizer. \u0110\u1EC3 x\xE1c minh r\u1EB1ng n\xF3 ho\u1EA1t \u0111\u1ED9ng, ch\xFAng ta s\u1EBD th\xEAm m\u1ED9t token ki\u1EC3m th\u1EED s\u1EBD \u0111\u01B0\u1EE3c chia th\xE0nh nhi\u1EC1u token:"),Mn=m(),w(Ln.$$.fragment),rn=m(),w(nt.$$.fragment),tt=m(),Nt=o("p"),ve=h("Tuy\u1EC7t v\u1EDDi, \u0111i\u1EC1u \u0111\xF3 c\xF3 v\u1EBB ho\u1EA1t \u0111\u1ED9ng t\u1ED1t! B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 vi\u1EBFt m\u1ED9t h\xE0m m\u1EA5t m\xE1t t\xF9y ch\u1EC9nh l\u1EA5y chu\u1ED7i \u0111\u1EA7u v\xE0o, logits v\xE0 token kh\xF3a m\xE0 ch\xFAng ta v\u1EEBa ch\u1ECDn l\xE0m \u0111\u1EA7u v\xE0o. Tr\u01B0\u1EDBc ti\xEAn, ch\xFAng ta c\u1EA7n c\u0103n ch\u1EC9nh logits v\xE0 \u0111\u1EA7u v\xE0o: chu\u1ED7i \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c d\u1ECBch chuy\u1EC3n m\u1ED9t \u0111\u01A1n v\u1ECB sang b\xEAn ph\u1EA3i t\u1EA1o th\xE0nh c\xE1c nh\xE3n, v\xEC token ti\u1EBFp theo l\xE0 nh\xE3n cho token hi\u1EC7n t\u1EA1i. Ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch b\u1EAFt \u0111\u1EA7u c\xE1c nh\xE3n t\u1EEB token th\u1EE9 hai c\u1EE7a chu\u1ED7i \u0111\u1EA7u v\xE0o, v\xEC d\xF9 sao th\xEC m\xF4 h\xECnh c\u0169ng kh\xF4ng \u0111\u01B0a ra d\u1EF1 \u0111o\xE1n cho token \u0111\u1EA7u ti\xEAn. Sau \u0111\xF3, ch\xFAng ta c\u1EAFt logit cu\u1ED1i c\xF9ng, v\xEC ch\xFAng ta kh\xF4ng c\xF3 nh\xE3n cho token theo tr\xECnh t\u1EF1 \u0111\u1EA7u v\xE0o \u0111\u1EA7y \u0111\u1EE7. Nh\u1EDD \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n s\u1EF1 m\u1EA5t m\xE1t tr\xEAn m\u1ED7i m\u1EABu v\xE0 \u0111\u1EBFm s\u1ED1 l\u1EA7n xu\u1EA5t hi\u1EC7n c\u1EE7a t\u1EA5t c\u1EA3 c\xE1c t\u1EEB kh\xF3a trong m\u1ED7i m\u1EABu. Cu\u1ED1i c\xF9ng, ch\xFAng ta t\xEDnh gi\xE1 tr\u1ECB trung b\xECnh c\xF3 tr\u1ECDng s\u1ED1 tr\xEAn t\u1EA5t c\u1EA3 c\xE1c m\u1EABu b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c l\u1EA7n xu\u1EA5t hi\u1EC7n d\u01B0\u1EDBi d\u1EA1ng tr\u1ECDng s\u1ED1. V\xEC ch\xFAng ta kh\xF4ng mu\u1ED1n lo\u1EA1i b\u1ECF t\u1EA5t c\u1EA3 c\xE1c m\u1EABu kh\xF4ng c\xF3 t\u1EEB kh\xF3a, ch\xFAng ta th\xEAm 1 v\xE0o c\xE1c tr\u1ECDng s\u1ED1:"),Mt=m(),w(st.$$.fragment),zs=m(),et=o("p"),sn=h("Tr\u01B0\u1EDBc khi c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n v\u1EDBi h\xE0m m\u1EA5t m\xE1t m\u1EDBi tuy\u1EC7t v\u1EDDi n\xE0y, ch\xFAng ta c\u1EA7n chu\u1EA9n b\u1ECB m\u1ED9t s\u1ED1 th\u1EE9:"),be=m(),on=o("ul"),kn=o("li"),ke=h("Ch\xFAng ta c\u1EA7n b\u1ED9 ghi d\u1EEF li\u1EC7u \u0111\u1EC3 t\u1EA3i d\u1EEF li\u1EC7u theo l\xF4."),at=m(),ms=o("li"),$e=h("Ch\xFAng ta c\u1EA7n thi\u1EBFt l\u1EADp c\xE1c th\xF4ng s\u1ED1 ph\xE2n r\xE3 tr\u1ECDng s\u1ED1."),Ps=m(),pn=o("li"),Fn=h("Theo th\u1EDDi gian, ch\xFAng ta mu\u1ED1n \u0111\xE1nh gi\xE1, v\xEC v\u1EADy s\u1EBD h\u1EE3p l\xFD khi bao m\xE3 \u0111\xE1nh gi\xE1 trong m\u1ED9t h\xE0m."),Lt=m(),nn=o("p"),je=h("H\xE3y b\u1EAFt \u0111\u1EA7u v\u1EDBi b\u1ED9 d\u1EEF li\u1EC7u. Ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EB7t \u0111\u1ECBnh d\u1EA1ng c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u th\xE0nh "),ht=o("code"),we=h('"torch"'),Ds=h(", v\xE0 sau \u0111\xF3 c\xF3 th\u1EC3 chuy\u1EC3n n\xF3 \u0111\u1EBFn PyTorch "),tn=o("code"),qe=h("DataLoader"),ct=h(" v\u1EDBi k\xEDch th\u01B0\u1EDBc l\xF4 th\xEDch h\u1EE3p:"),Hs=m(),w(lt.$$.fragment),Ft=m(),Gt=o("p"),xe=h("Ti\u1EBFp theo, ch\xFAng ta nh\xF3m c\xE1c tham s\u1ED1 \u0111\u1EC3 tr\xECnh t\u1ED1i \u01B0u h\xF3a bi\u1EBFt nh\u1EEFng th\xF4ng s\u1ED1 n\xE0o s\u1EBD b\u1ECB gi\u1EA3m tr\u1ECDng s\u1ED1 b\u1ED5 sung. Th\xF4ng th\u01B0\u1EDDng, t\u1EA5t c\u1EA3 c\xE1c \u0111i\u1EC1u kho\u1EA3n thi\xEAn v\u1ECB v\xE0 tr\u1ECDng s\u1ED1 LayerNorm \u0111\u1EC1u \u0111\u01B0\u1EE3c mi\u1EC5n tr\u1EEB; \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y:"),Gn=m(),w(rt.$$.fragment),As=m(),it=o("p"),Sn=h("V\xEC ch\xFAng ta mu\u1ED1n \u0111\xE1nh gi\xE1 m\xF4 h\xECnh th\u01B0\u1EDDng xuy\xEAn tr\xEAn b\u1ED9 x\xE1c nh\u1EADn trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n, ch\xFAng ta h\xE3y vi\u1EBFt m\u1ED9t h\xE0m cho \u0111i\u1EC1u \u0111\xF3. N\xF3 ch\u1EC9 ch\u1EA1y qua b\u1ED9 d\u1EEF li\u1EC7u \u0111\xE1nh gi\xE1 v\xE0 t\u1EADp h\u1EE3p t\u1EA5t c\u1EA3 c\xE1c m\u1EA5t m\xE1t qua c\xE1c quy tr\xECnh:"),Os=m(),w(qn.$$.fragment),Ns=m(),$n=o("p"),Ms=h("V\u1EDBi h\xE0m "),xn=o("code"),Ls=h("evaluate()"),St=h(", ch\xFAng ta c\xF3 th\u1EC3 b\xE1o c\xE1o m\u1EA5t m\xE1t v\xE0 "),Bt=o("a"),Fs=h("perplexity"),ot=h(" theo kho\u1EA3ng th\u1EDDi gian \u0111\u1EC1u \u0111\u1EB7n. Ti\u1EBFp theo, ch\xFAng ta x\xE1c \u0111\u1ECBnh l\u1EA1i m\xF4 h\xECnh c\u1EE7a m\xECnh \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng ta hu\u1EA5n luy\u1EC7n l\u1EA1i t\u1EEB \u0111\u1EA7u:"),gs=m(),w(un.$$.fragment),_s=m(),En=o("p"),Ee=h("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 x\xE1c \u0111\u1ECBnh tr\xECnh t\u1ED1i \u01B0u h\xF3a c\u1EE7a m\xECnh, s\u1EED d\u1EE5ng h\xE0m t\u1EEB tr\u01B0\u1EDBc \u0111\u1EC3 ph\xE2n chia c\xE1c tham s\u1ED1 cho ph\xE2n r\xE3 tr\u1ECDng s\u1ED1:"),ds=m(),w(mn.$$.fragment),fs=m(),Tn=o("p"),Te=h("B\xE2y gi\u1EDD, h\xE3y chu\u1EA9n b\u1ECB m\xF4 h\xECnh, tr\xECnh t\u1ED1i \u01B0u h\xF3a v\xE0 b\u1ED9 ghi d\u1EEF li\u1EC7u \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n:"),ys=m(),w(gn.$$.fragment),vs=m(),w(_n.$$.fragment),bs=m(),en=o("p"),Ce=h("B\xE2y gi\u1EDD, ch\xFAng ta \u0111\xE3 g\u1EEDi "),It=o("code"),Vt=h("train_dataloader"),ze=h(" c\u1EE7a m\xECnh t\u1EDBi "),Rt=o("code"),pt=h("accelerator.prepare()"),Gs=h(", ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3 \u0111\u1EC3 t\xEDnh s\u1ED1 b\u01B0\u1EDBc hu\u1EA5n luy\u1EC7n. H\xE3y nh\u1EDB r\u1EB1ng ch\xFAng ta ph\u1EA3i lu\xF4n l\xE0m \u0111i\u1EC1u n\xE0y sau khi chu\u1EA9n b\u1ECB dataloader, v\xEC ph\u01B0\u01A1ng th\u1EE9c \u0111\xF3 s\u1EBD thay \u0111\u1ED5i \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3. Ch\xFAng ta s\u1EED d\u1EE5ng m\u1ED9t l\u1ECBch tr\xECnh tuy\u1EBFn t\xEDnh c\u1ED5 \u0111i\u1EC3n t\u1EEB t\u1ED1c \u0111\u1ED9 h\u1ECDc \u0111\u1EBFn 0:"),Bn=m(),w(In.$$.fragment),Cn=m(),an=o("p"),Kt=h("Cu\u1ED1i c\xF9ng, \u0111\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh l\xEAn Hub, ch\xFAng ta s\u1EBD c\u1EA7n t\u1EA1o m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),ks=o("code"),Ss=h("Repository"),ut=h(" trong m\u1ED9t th\u01B0 m\u1EE5c \u0111ang l\xE0m vi\u1EC7c. Tr\u01B0\u1EDBc ti\xEAn, h\xE3y \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face Hub, n\u1EBFu b\u1EA1n ch\u01B0a \u0111\u0103ng nh\u1EADp. Ch\xFAng ta s\u1EBD x\xE1c \u0111\u1ECBnh t\xEAn kho l\u01B0u tr\u1EEF t\u1EEB ID m\xF4 h\xECnh m\xE0 ta mu\u1ED1n cung c\u1EA5p cho m\xF4 h\xECnh c\u1EE7a m\xECnh (vui l\xF2ng thay th\u1EBF "),Ut=o("code"),mt=h("repo_name"),Bs=h(" b\u1EB1ng s\u1EF1 l\u1EF1a ch\u1ECDn c\u1EE7a ri\xEAng b\u1EA1n; n\xF3 ch\u1EC9 c\u1EA7n ch\u1EE9a t\xEAn ng\u01B0\u1EDDi d\xF9ng c\u1EE7a b\u1EA1n, \u0111\xF3 l\xE0 nh\u1EEFng g\xEC h\xE0m "),jn=o("code"),Pe=h("get_full_repo_name()"),$s=h(" th\u1EF1c hi\u1EC7n ):"),Is=m(),w(gt.$$.fragment),js=m(),w(hn.$$.fragment),zn=m(),Vn=o("p"),_t=h("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 sao ch\xE9p kho l\u01B0u tr\u1EEF \u0111\xF3 trong m\u1ED9t th\u01B0 m\u1EE5c c\u1EE5c b\u1ED9. N\u1EBFu n\xF3 \u0111\xE3 t\u1ED3n t\u1EA1i, th\u01B0 m\u1EE5c c\u1EE5c b\u1ED9 n\xE0y ph\u1EA3i l\xE0 b\u1EA3n sao hi\u1EC7n c\xF3 c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 ch\xFAng ta \u0111ang l\xE0m vi\u1EC7c:"),Vs=m(),w(Pn.$$.fragment),Rs=m(),dn=o("p"),dt=h("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i l\xEAn b\u1EA5t c\u1EE9 th\u1EE9 g\xEC ch\xFAng ta l\u01B0u trong "),Xt=o("code"),Wt=h("output_dir"),De=h(" b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),Yt=o("code"),ln=h("repo.push_to_hub()"),He=h(". \u0110i\u1EC1u n\xE0y s\u1EBD gi\xFAp ch\xFAng ta t\u1EA3i l\xEAn c\xE1c m\xF4 h\xECnh trung gian \u1EDF cu\u1ED1i m\u1ED7i epoch."),Zt=m(),Jt=o("p"),Ae=h("Tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n, h\xE3y ch\u1EA1y th\u1EED nhanh \u0111\u1EC3 xem ch\u1EE9c n\u0103ng \u0111\xE1nh gi\xE1 c\xF3 ho\u1EA1t \u0111\u1ED9ng b\xECnh th\u01B0\u1EDDng kh\xF4ng:"),ft=m(),w(yt.$$.fragment),Ks=m(),w(Dn.$$.fragment),Us=m(),Rn=o("p"),Xs=h("\u0110\xF3 l\xE0 nh\u1EEFng gi\xE1 tr\u1ECB r\u1EA5t cao v\u1EC1 m\u1EE9c m\u1EA5t m\xE1t v\xE0 perplexity, nh\u01B0ng \u0111i\u1EC1u \u0111\xF3 kh\xF4ng \u0111\xE1ng ng\u1EA1c nhi\xEAn v\xEC ch\xFAng ta ch\u01B0a hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh. C\xF9ng v\u1EDBi \u0111\xF3, ch\xFAng ta \u0111\xE3 chu\u1EA9n b\u1ECB m\u1ECDi th\u1EE9 \u0111\u1EC3 vi\u1EBFt ph\u1EA7n c\u1ED1t l\xF5i c\u1EE7a k\u1ECBch b\u1EA3n hu\u1EA5n luy\u1EC7n: v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n. Trong v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n, ch\xFAng ta l\u1EB7p qua dataloader v\xE0 truy\u1EC1n c\xE1c l\xF4 v\xE0o m\xF4 h\xECnh. V\u1EDBi nh\u1EADt k\xFD, sau \u0111\xF3 ch\xFAng ta c\xF3 th\u1EC3 \u0111\xE1nh gi\xE1 h\xE0m m\u1EA5t m\xE1t t\xF9y ch\u1EC9nh c\u1EE7a m\xECnh. Ch\xFAng ta chia t\u1EF7 l\u1EC7 m\u1EA5t m\xE1t theo s\u1ED1 b\u01B0\u1EDBc t\xEDch l\u0169y gradient \u0111\u1EC3 kh\xF4ng t\u1EA1o ra m\u1EA5t m\xE1t l\u1EDBn h\u01A1n khi t\u1ED5ng h\u1EE3p nhi\u1EC1u b\u01B0\u1EDBc h\u01A1n. Tr\u01B0\u1EDBc khi t\u1ED1i \u01B0u h\xF3a, ch\xFAng ta c\u0169ng c\u1EAFt c\xE1c gradient \u0111\u1EC3 h\u1ED9i t\u1EE5 t\u1ED1t h\u01A1n. Cu\u1ED1i c\xF9ng, c\u1EE9 sau v\xE0i b\u01B0\u1EDBc, ch\xFAng ta \u0111\xE1nh gi\xE1 m\xF4 h\xECnh tr\xEAn t\u1EADp h\u1EE3p \u0111\xE1nh gi\xE1 v\u1EDBi h\xE0m "),fn=o("code"),vt=h("eval()"),la=h(" m\u1EDBi c\u1EE7a m\xECnh:"),Ws=m(),w(yn.$$.fragment),Oe=m(),bt=o("p"),Qt=h("V\u1EADy l\xE0 xong - b\xE2y gi\u1EDD b\u1EA1n c\xF3 v\xF2ng hu\u1EA5n luy\u1EC7n t\xF9y ch\u1EC9nh c\u1EE7a ri\xEAng m\xECnh cho c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 ch\u1EB3ng h\u1EA1n nh\u01B0 GPT-2 m\xE0 b\u1EA1n c\xF3 th\u1EC3 t\xF9y ch\u1EC9nh th\xEAm theo nhu c\u1EA7u c\u1EE7a m\xECnh."),Ys=m(),w(Kn.$$.fragment),Un=m(),w(Xn.$$.fragment),this.h()},l(e){a=p(e,"H2",{class:!0});var k=u(a);_=p(k,"A",{id:!0,class:!0,href:!0});var ra=u(_);r=p(ra,"SPAN",{});var Zs=u(r);q(d.$$.fragment,Zs),Zs.forEach(t),ra.forEach(t),$=g(k),b=p(k,"SPAN",{});var ia=u(b);j=c(ia,"Hu\u1EA5n luy\u1EC7n v\u1EDBi \u{1F917} Accelerate"),ia.forEach(t),k.forEach(t),z=g(e),T=p(e,"P",{});var Js=u(T);H=c(Js,"Ch\xFAng ta \u0111\xE3 th\u1EA5y c\xE1ch hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh v\u1EDBi "),M=p(Js,"CODE",{});var Ne=u(M);L=c(Ne,"Trainer"),Ne.forEach(t),f=c(Js,", c\xF3 th\u1EC3 cho ph\xE9p m\u1ED9t s\u1ED1 t\xF9y ch\u1EC9nh. Tuy nhi\xEAn, \u0111\xF4i khi ch\xFAng ta mu\u1ED1n to\xE0n quy\u1EC1n ki\u1EC3m so\xE1t v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n ho\u1EB7c ch\xFAng ta mu\u1ED1n th\u1EF1c hi\u1EC7n m\u1ED9t s\u1ED1 thay \u0111\u1ED5i k\u1EF3 l\u1EA1. Trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y \u{1F917} Accelerate l\xE0 m\u1ED9t l\u1EF1a ch\u1ECDn tuy\u1EC7t v\u1EDDi v\xE0 trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD xem x\xE9t c\xE1c b\u01B0\u1EDBc s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh c\u1EE7a m\xECnh. \u0110\u1EC3 l\xE0m cho m\u1ECDi th\u1EE9 th\xFA v\u1ECB h\u01A1n, ch\xFAng ta c\u0169ng s\u1EBD th\xEAm m\u1ED9t s\u1ED1 \u0111i\u1EC1u ch\u1EC9nh v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n."),Js.forEach(t),P=g(e),q(F.$$.fragment,e),S=g(e),G=p(e,"P",{});var R=u(G);V=c(R,"V\xEC ch\xFAng ta ch\u1EE7 y\u1EBFu quan t\xE2m \u0111\u1EBFn t\xEDnh n\u0103ng t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh h\u1EE3p l\xFD cho c\xE1c th\u01B0 vi\u1EC7n khoa h\u1ECDc d\u1EEF li\u1EC7u, n\xEAn vi\u1EC7c \u0111\u01B0a ra nhi\u1EC1u tr\u1ECDng s\u1ED1 h\u01A1n cho c\xE1c m\u1EABu hu\u1EA5n luy\u1EC7n s\u1EED d\u1EE5ng nhi\u1EC1u h\u01A1n c\xE1c th\u01B0 vi\u1EC7n n\xE0y. Ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng x\xE1c \u0111\u1ECBnh nh\u1EEFng v\xED d\u1EE5 n\xE0y th\xF4ng qua vi\u1EC7c s\u1EED d\u1EE5ng c\xE1c t\u1EEB kh\xF3a nh\u01B0 "),U=p(R,"CODE",{});var oa=u(U);N=c(oa,"plt"),oa.forEach(t),B=c(R,", "),I=p(R,"CODE",{});var Qs=u(I);W=c(Qs,"pd"),Qs.forEach(t),X=c(R,", "),Y=p(R,"CODE",{});var pa=u(Y);Q=c(pa,"sk"),pa.forEach(t),cn=c(R,", "),C=p(R,"CODE",{});var ua=u(C);K=c(ua,"fit"),ua.forEach(t),An=c(R," v\xE0 "),On=p(R,"CODE",{});var ws=u(On);At=c(ws,"predict"),ws.forEach(t),bn=c(R,", l\xE0 nh\u1EEFng t\xEAn nh\u1EADp th\u01B0\u1EDDng g\u1EB7p nh\u1EA5t cho "),Nn=p(R,"CODE",{});var ma=u(Nn);Ot=c(ma,"matplotlib.pyplot"),ma.forEach(t),is=c(R,", "),os=p(R,"CODE",{});var ga=u(os);fe=c(ga,"pandas"),ga.forEach(t),ps=c(R,", v\xE0 "),us=p(R,"CODE",{});var ne=u(us);ye=c(ne,"sklearn"),ne.forEach(t),Cs=c(R," c\u0169ng nh\u01B0 c\xE1c h\xE0nh vi sau \u0111\xF3. N\u1EBFu ch\xFAng \u0111\u01B0\u1EE3c bi\u1EC3u di\u1EC5n d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t token duy nh\u1EA5t, ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng ki\u1EC3m tra xem ch\xFAng c\xF3 xu\u1EA5t hi\u1EC7n trong chu\u1ED7i \u0111\u1EA7u v\xE0o hay kh\xF4ng. C\xE1c token c\xF3 th\u1EC3 c\xF3 ti\u1EC1n t\u1ED1 kho\u1EA3ng tr\u1EAFng, v\xEC v\u1EADy ch\xFAng ta c\u0169ng s\u1EBD ki\u1EC3m tra c\xE1c phi\xEAn b\u1EA3n \u0111\xF3 trong t\u1EEB v\u1EF1ng b\u1ED9 tokenizer. \u0110\u1EC3 x\xE1c minh r\u1EB1ng n\xF3 ho\u1EA1t \u0111\u1ED9ng, ch\xFAng ta s\u1EBD th\xEAm m\u1ED9t token ki\u1EC3m th\u1EED s\u1EBD \u0111\u01B0\u1EE3c chia th\xE0nh nhi\u1EC1u token:"),R.forEach(t),Mn=g(e),q(Ln.$$.fragment,e),rn=g(e),q(nt.$$.fragment,e),tt=g(e),Nt=p(e,"P",{});var _a=u(Nt);ve=c(_a,"Tuy\u1EC7t v\u1EDDi, \u0111i\u1EC1u \u0111\xF3 c\xF3 v\u1EBB ho\u1EA1t \u0111\u1ED9ng t\u1ED1t! B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 vi\u1EBFt m\u1ED9t h\xE0m m\u1EA5t m\xE1t t\xF9y ch\u1EC9nh l\u1EA5y chu\u1ED7i \u0111\u1EA7u v\xE0o, logits v\xE0 token kh\xF3a m\xE0 ch\xFAng ta v\u1EEBa ch\u1ECDn l\xE0m \u0111\u1EA7u v\xE0o. Tr\u01B0\u1EDBc ti\xEAn, ch\xFAng ta c\u1EA7n c\u0103n ch\u1EC9nh logits v\xE0 \u0111\u1EA7u v\xE0o: chu\u1ED7i \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c d\u1ECBch chuy\u1EC3n m\u1ED9t \u0111\u01A1n v\u1ECB sang b\xEAn ph\u1EA3i t\u1EA1o th\xE0nh c\xE1c nh\xE3n, v\xEC token ti\u1EBFp theo l\xE0 nh\xE3n cho token hi\u1EC7n t\u1EA1i. Ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch b\u1EAFt \u0111\u1EA7u c\xE1c nh\xE3n t\u1EEB token th\u1EE9 hai c\u1EE7a chu\u1ED7i \u0111\u1EA7u v\xE0o, v\xEC d\xF9 sao th\xEC m\xF4 h\xECnh c\u0169ng kh\xF4ng \u0111\u01B0a ra d\u1EF1 \u0111o\xE1n cho token \u0111\u1EA7u ti\xEAn. Sau \u0111\xF3, ch\xFAng ta c\u1EAFt logit cu\u1ED1i c\xF9ng, v\xEC ch\xFAng ta kh\xF4ng c\xF3 nh\xE3n cho token theo tr\xECnh t\u1EF1 \u0111\u1EA7u v\xE0o \u0111\u1EA7y \u0111\u1EE7. Nh\u1EDD \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n s\u1EF1 m\u1EA5t m\xE1t tr\xEAn m\u1ED7i m\u1EABu v\xE0 \u0111\u1EBFm s\u1ED1 l\u1EA7n xu\u1EA5t hi\u1EC7n c\u1EE7a t\u1EA5t c\u1EA3 c\xE1c t\u1EEB kh\xF3a trong m\u1ED7i m\u1EABu. Cu\u1ED1i c\xF9ng, ch\xFAng ta t\xEDnh gi\xE1 tr\u1ECB trung b\xECnh c\xF3 tr\u1ECDng s\u1ED1 tr\xEAn t\u1EA5t c\u1EA3 c\xE1c m\u1EABu b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c l\u1EA7n xu\u1EA5t hi\u1EC7n d\u01B0\u1EDBi d\u1EA1ng tr\u1ECDng s\u1ED1. V\xEC ch\xFAng ta kh\xF4ng mu\u1ED1n lo\u1EA1i b\u1ECF t\u1EA5t c\u1EA3 c\xE1c m\u1EABu kh\xF4ng c\xF3 t\u1EEB kh\xF3a, ch\xFAng ta th\xEAm 1 v\xE0o c\xE1c tr\u1ECDng s\u1ED1:"),_a.forEach(t),Mt=g(e),q(st.$$.fragment,e),zs=g(e),et=p(e,"P",{});var da=u(et);sn=c(da,"Tr\u01B0\u1EDBc khi c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n v\u1EDBi h\xE0m m\u1EA5t m\xE1t m\u1EDBi tuy\u1EC7t v\u1EDDi n\xE0y, ch\xFAng ta c\u1EA7n chu\u1EA9n b\u1ECB m\u1ED9t s\u1ED1 th\u1EE9:"),da.forEach(t),be=g(e),on=p(e,"UL",{});var kt=u(on);kn=p(kt,"LI",{});var ns=u(kn);ke=c(ns,"Ch\xFAng ta c\u1EA7n b\u1ED9 ghi d\u1EEF li\u1EC7u \u0111\u1EC3 t\u1EA3i d\u1EEF li\u1EC7u theo l\xF4."),ns.forEach(t),at=g(kt),ms=p(kt,"LI",{});var Me=u(ms);$e=c(Me,"Ch\xFAng ta c\u1EA7n thi\u1EBFt l\u1EADp c\xE1c th\xF4ng s\u1ED1 ph\xE2n r\xE3 tr\u1ECDng s\u1ED1."),Me.forEach(t),Ps=g(kt),pn=p(kt,"LI",{});var ts=u(pn);Fn=c(ts,"Theo th\u1EDDi gian, ch\xFAng ta mu\u1ED1n \u0111\xE1nh gi\xE1, v\xEC v\u1EADy s\u1EBD h\u1EE3p l\xFD khi bao m\xE3 \u0111\xE1nh gi\xE1 trong m\u1ED9t h\xE0m."),ts.forEach(t),kt.forEach(t),Lt=g(e),nn=p(e,"P",{});var $t=u(nn);je=c($t,"H\xE3y b\u1EAFt \u0111\u1EA7u v\u1EDBi b\u1ED9 d\u1EEF li\u1EC7u. Ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EB7t \u0111\u1ECBnh d\u1EA1ng c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u th\xE0nh "),ht=p($t,"CODE",{});var qs=u(ht);we=c(qs,'"torch"'),qs.forEach(t),Ds=c($t,", v\xE0 sau \u0111\xF3 c\xF3 th\u1EC3 chuy\u1EC3n n\xF3 \u0111\u1EBFn PyTorch "),tn=p($t,"CODE",{});var fa=u(tn);qe=c(fa,"DataLoader"),fa.forEach(t),ct=c($t," v\u1EDBi k\xEDch th\u01B0\u1EDBc l\xF4 th\xEDch h\u1EE3p:"),$t.forEach(t),Hs=g(e),q(lt.$$.fragment,e),Ft=g(e),Gt=p(e,"P",{});var Le=u(Gt);xe=c(Le,"Ti\u1EBFp theo, ch\xFAng ta nh\xF3m c\xE1c tham s\u1ED1 \u0111\u1EC3 tr\xECnh t\u1ED1i \u01B0u h\xF3a bi\u1EBFt nh\u1EEFng th\xF4ng s\u1ED1 n\xE0o s\u1EBD b\u1ECB gi\u1EA3m tr\u1ECDng s\u1ED1 b\u1ED5 sung. Th\xF4ng th\u01B0\u1EDDng, t\u1EA5t c\u1EA3 c\xE1c \u0111i\u1EC1u kho\u1EA3n thi\xEAn v\u1ECB v\xE0 tr\u1ECDng s\u1ED1 LayerNorm \u0111\u1EC1u \u0111\u01B0\u1EE3c mi\u1EC5n tr\u1EEB; \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y:"),Le.forEach(t),Gn=g(e),q(rt.$$.fragment,e),As=g(e),it=p(e,"P",{});var xs=u(it);Sn=c(xs,"V\xEC ch\xFAng ta mu\u1ED1n \u0111\xE1nh gi\xE1 m\xF4 h\xECnh th\u01B0\u1EDDng xuy\xEAn tr\xEAn b\u1ED9 x\xE1c nh\u1EADn trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n, ch\xFAng ta h\xE3y vi\u1EBFt m\u1ED9t h\xE0m cho \u0111i\u1EC1u \u0111\xF3. N\xF3 ch\u1EC9 ch\u1EA1y qua b\u1ED9 d\u1EEF li\u1EC7u \u0111\xE1nh gi\xE1 v\xE0 t\u1EADp h\u1EE3p t\u1EA5t c\u1EA3 c\xE1c m\u1EA5t m\xE1t qua c\xE1c quy tr\xECnh:"),xs.forEach(t),Os=g(e),q(qn.$$.fragment,e),Ns=g(e),$n=p(e,"P",{});var ss=u($n);Ms=c(ss,"V\u1EDBi h\xE0m "),xn=p(ss,"CODE",{});var Fe=u(xn);Ls=c(Fe,"evaluate()"),Fe.forEach(t),St=c(ss,", ch\xFAng ta c\xF3 th\u1EC3 b\xE1o c\xE1o m\u1EA5t m\xE1t v\xE0 "),Bt=p(ss,"A",{href:!0});var jt=u(Bt);Fs=c(jt,"perplexity"),jt.forEach(t),ot=c(ss," theo kho\u1EA3ng th\u1EDDi gian \u0111\u1EC1u \u0111\u1EB7n. Ti\u1EBFp theo, ch\xFAng ta x\xE1c \u0111\u1ECBnh l\u1EA1i m\xF4 h\xECnh c\u1EE7a m\xECnh \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng ta hu\u1EA5n luy\u1EC7n l\u1EA1i t\u1EEB \u0111\u1EA7u:"),ss.forEach(t),gs=g(e),q(un.$$.fragment,e),_s=g(e),En=p(e,"P",{});var Ge=u(En);Ee=c(Ge,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 x\xE1c \u0111\u1ECBnh tr\xECnh t\u1ED1i \u01B0u h\xF3a c\u1EE7a m\xECnh, s\u1EED d\u1EE5ng h\xE0m t\u1EEB tr\u01B0\u1EDBc \u0111\u1EC3 ph\xE2n chia c\xE1c tham s\u1ED1 cho ph\xE2n r\xE3 tr\u1ECDng s\u1ED1:"),Ge.forEach(t),ds=g(e),q(mn.$$.fragment,e),fs=g(e),Tn=p(e,"P",{});var Yn=u(Tn);Te=c(Yn,"B\xE2y gi\u1EDD, h\xE3y chu\u1EA9n b\u1ECB m\xF4 h\xECnh, tr\xECnh t\u1ED1i \u01B0u h\xF3a v\xE0 b\u1ED9 ghi d\u1EEF li\u1EC7u \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n:"),Yn.forEach(t),ys=g(e),q(gn.$$.fragment,e),vs=g(e),q(_n.$$.fragment,e),bs=g(e),en=p(e,"P",{});var vn=u(en);Ce=c(vn,"B\xE2y gi\u1EDD, ch\xFAng ta \u0111\xE3 g\u1EEDi "),It=p(vn,"CODE",{});var te=u(It);Vt=c(te,"train_dataloader"),te.forEach(t),ze=c(vn," c\u1EE7a m\xECnh t\u1EDBi "),Rt=p(vn,"CODE",{});var es=u(Rt);pt=c(es,"accelerator.prepare()"),es.forEach(t),Gs=c(vn,", ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3 \u0111\u1EC3 t\xEDnh s\u1ED1 b\u01B0\u1EDBc hu\u1EA5n luy\u1EC7n. H\xE3y nh\u1EDB r\u1EB1ng ch\xFAng ta ph\u1EA3i lu\xF4n l\xE0m \u0111i\u1EC1u n\xE0y sau khi chu\u1EA9n b\u1ECB dataloader, v\xEC ph\u01B0\u01A1ng th\u1EE9c \u0111\xF3 s\u1EBD thay \u0111\u1ED5i \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3. Ch\xFAng ta s\u1EED d\u1EE5ng m\u1ED9t l\u1ECBch tr\xECnh tuy\u1EBFn t\xEDnh c\u1ED5 \u0111i\u1EC3n t\u1EEB t\u1ED1c \u0111\u1ED9 h\u1ECDc \u0111\u1EBFn 0:"),vn.forEach(t),Bn=g(e),q(In.$$.fragment,e),Cn=g(e),an=p(e,"P",{});var Zn=u(an);Kt=c(Zn,"Cu\u1ED1i c\xF9ng, \u0111\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh l\xEAn Hub, ch\xFAng ta s\u1EBD c\u1EA7n t\u1EA1o m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),ks=p(Zn,"CODE",{});var se=u(ks);Ss=c(se,"Repository"),se.forEach(t),ut=c(Zn," trong m\u1ED9t th\u01B0 m\u1EE5c \u0111ang l\xE0m vi\u1EC7c. Tr\u01B0\u1EDBc ti\xEAn, h\xE3y \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face Hub, n\u1EBFu b\u1EA1n ch\u01B0a \u0111\u0103ng nh\u1EADp. Ch\xFAng ta s\u1EBD x\xE1c \u0111\u1ECBnh t\xEAn kho l\u01B0u tr\u1EEF t\u1EEB ID m\xF4 h\xECnh m\xE0 ta mu\u1ED1n cung c\u1EA5p cho m\xF4 h\xECnh c\u1EE7a m\xECnh (vui l\xF2ng thay th\u1EBF "),Ut=p(Zn,"CODE",{});var ya=u(Ut);mt=c(ya,"repo_name"),ya.forEach(t),Bs=c(Zn," b\u1EB1ng s\u1EF1 l\u1EF1a ch\u1ECDn c\u1EE7a ri\xEAng b\u1EA1n; n\xF3 ch\u1EC9 c\u1EA7n ch\u1EE9a t\xEAn ng\u01B0\u1EDDi d\xF9ng c\u1EE7a b\u1EA1n, \u0111\xF3 l\xE0 nh\u1EEFng g\xEC h\xE0m "),jn=p(Zn,"CODE",{});var Se=u(jn);Pe=c(Se,"get_full_repo_name()"),Se.forEach(t),$s=c(Zn," th\u1EF1c hi\u1EC7n ):"),Zn.forEach(t),Is=g(e),q(gt.$$.fragment,e),js=g(e),q(hn.$$.fragment,e),zn=g(e),Vn=p(e,"P",{});var Hn=u(Vn);_t=c(Hn,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 sao ch\xE9p kho l\u01B0u tr\u1EEF \u0111\xF3 trong m\u1ED9t th\u01B0 m\u1EE5c c\u1EE5c b\u1ED9. N\u1EBFu n\xF3 \u0111\xE3 t\u1ED3n t\u1EA1i, th\u01B0 m\u1EE5c c\u1EE5c b\u1ED9 n\xE0y ph\u1EA3i l\xE0 b\u1EA3n sao hi\u1EC7n c\xF3 c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 ch\xFAng ta \u0111ang l\xE0m vi\u1EC7c:"),Hn.forEach(t),Vs=g(e),q(Pn.$$.fragment,e),Rs=g(e),dn=p(e,"P",{});var as=u(dn);dt=c(as,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i l\xEAn b\u1EA5t c\u1EE9 th\u1EE9 g\xEC ch\xFAng ta l\u01B0u trong "),Xt=p(as,"CODE",{});var ee=u(Xt);Wt=c(ee,"output_dir"),ee.forEach(t),De=c(as," b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),Yt=p(as,"CODE",{});var va=u(Yt);ln=c(va,"repo.push_to_hub()"),va.forEach(t),He=c(as,". \u0110i\u1EC1u n\xE0y s\u1EBD gi\xFAp ch\xFAng ta t\u1EA3i l\xEAn c\xE1c m\xF4 h\xECnh trung gian \u1EDF cu\u1ED1i m\u1ED7i epoch."),as.forEach(t),Zt=g(e),Jt=p(e,"P",{});var ba=u(Jt);Ae=c(ba,"Tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n, h\xE3y ch\u1EA1y th\u1EED nhanh \u0111\u1EC3 xem ch\u1EE9c n\u0103ng \u0111\xE1nh gi\xE1 c\xF3 ho\u1EA1t \u0111\u1ED9ng b\xECnh th\u01B0\u1EDDng kh\xF4ng:"),ba.forEach(t),ft=g(e),q(yt.$$.fragment,e),Ks=g(e),q(Dn.$$.fragment,e),Us=g(e),Rn=p(e,"P",{});var hs=u(Rn);Xs=c(hs,"\u0110\xF3 l\xE0 nh\u1EEFng gi\xE1 tr\u1ECB r\u1EA5t cao v\u1EC1 m\u1EE9c m\u1EA5t m\xE1t v\xE0 perplexity, nh\u01B0ng \u0111i\u1EC1u \u0111\xF3 kh\xF4ng \u0111\xE1ng ng\u1EA1c nhi\xEAn v\xEC ch\xFAng ta ch\u01B0a hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh. C\xF9ng v\u1EDBi \u0111\xF3, ch\xFAng ta \u0111\xE3 chu\u1EA9n b\u1ECB m\u1ECDi th\u1EE9 \u0111\u1EC3 vi\u1EBFt ph\u1EA7n c\u1ED1t l\xF5i c\u1EE7a k\u1ECBch b\u1EA3n hu\u1EA5n luy\u1EC7n: v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n. Trong v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n, ch\xFAng ta l\u1EB7p qua dataloader v\xE0 truy\u1EC1n c\xE1c l\xF4 v\xE0o m\xF4 h\xECnh. V\u1EDBi nh\u1EADt k\xFD, sau \u0111\xF3 ch\xFAng ta c\xF3 th\u1EC3 \u0111\xE1nh gi\xE1 h\xE0m m\u1EA5t m\xE1t t\xF9y ch\u1EC9nh c\u1EE7a m\xECnh. Ch\xFAng ta chia t\u1EF7 l\u1EC7 m\u1EA5t m\xE1t theo s\u1ED1 b\u01B0\u1EDBc t\xEDch l\u0169y gradient \u0111\u1EC3 kh\xF4ng t\u1EA1o ra m\u1EA5t m\xE1t l\u1EDBn h\u01A1n khi t\u1ED5ng h\u1EE3p nhi\u1EC1u b\u01B0\u1EDBc h\u01A1n. Tr\u01B0\u1EDBc khi t\u1ED1i \u01B0u h\xF3a, ch\xFAng ta c\u0169ng c\u1EAFt c\xE1c gradient \u0111\u1EC3 h\u1ED9i t\u1EE5 t\u1ED1t h\u01A1n. Cu\u1ED1i c\xF9ng, c\u1EE9 sau v\xE0i b\u01B0\u1EDBc, ch\xFAng ta \u0111\xE1nh gi\xE1 m\xF4 h\xECnh tr\xEAn t\u1EADp h\u1EE3p \u0111\xE1nh gi\xE1 v\u1EDBi h\xE0m "),fn=p(hs,"CODE",{});var ka=u(fn);vt=c(ka,"eval()"),ka.forEach(t),la=c(hs," m\u1EDBi c\u1EE7a m\xECnh:"),hs.forEach(t),Ws=g(e),q(yn.$$.fragment,e),Oe=g(e),bt=p(e,"P",{});var $a=u(bt);Qt=c($a,"V\u1EADy l\xE0 xong - b\xE2y gi\u1EDD b\u1EA1n c\xF3 v\xF2ng hu\u1EA5n luy\u1EC7n t\xF9y ch\u1EC9nh c\u1EE7a ri\xEAng m\xECnh cho c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 ch\u1EB3ng h\u1EA1n nh\u01B0 GPT-2 m\xE0 b\u1EA1n c\xF3 th\u1EC3 t\xF9y ch\u1EC9nh th\xEAm theo nhu c\u1EA7u c\u1EE7a m\xECnh."),$a.forEach(t),Ys=g(e),q(Kn.$$.fragment,e),Un=g(e),q(Xn.$$.fragment,e),this.h()},h(){O(_,"id","hun-luyn-vi-accelerate"),O(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(_,"href","#hun-luyn-vi-accelerate"),O(a,"class","relative group"),O(Bt,"href","/course/chapter7/3")},m(e,k){l(e,a,k),s(a,_),s(_,r),x(d,r,null),s(a,$),s(a,b),s(b,j),l(e,z,k),l(e,T,k),s(T,H),s(T,M),s(M,L),s(T,f),l(e,P,k),x(F,e,k),l(e,S,k),l(e,G,k),s(G,V),s(G,U),s(U,N),s(G,B),s(G,I),s(I,W),s(G,X),s(G,Y),s(Y,Q),s(G,cn),s(G,C),s(C,K),s(G,An),s(G,On),s(On,At),s(G,bn),s(G,Nn),s(Nn,Ot),s(G,is),s(G,os),s(os,fe),s(G,ps),s(G,us),s(us,ye),s(G,Cs),l(e,Mn,k),x(Ln,e,k),l(e,rn,k),x(nt,e,k),l(e,tt,k),l(e,Nt,k),s(Nt,ve),l(e,Mt,k),x(st,e,k),l(e,zs,k),l(e,et,k),s(et,sn),l(e,be,k),l(e,on,k),s(on,kn),s(kn,ke),s(on,at),s(on,ms),s(ms,$e),s(on,Ps),s(on,pn),s(pn,Fn),l(e,Lt,k),l(e,nn,k),s(nn,je),s(nn,ht),s(ht,we),s(nn,Ds),s(nn,tn),s(tn,qe),s(nn,ct),l(e,Hs,k),x(lt,e,k),l(e,Ft,k),l(e,Gt,k),s(Gt,xe),l(e,Gn,k),x(rt,e,k),l(e,As,k),l(e,it,k),s(it,Sn),l(e,Os,k),x(qn,e,k),l(e,Ns,k),l(e,$n,k),s($n,Ms),s($n,xn),s(xn,Ls),s($n,St),s($n,Bt),s(Bt,Fs),s($n,ot),l(e,gs,k),x(un,e,k),l(e,_s,k),l(e,En,k),s(En,Ee),l(e,ds,k),x(mn,e,k),l(e,fs,k),l(e,Tn,k),s(Tn,Te),l(e,ys,k),x(gn,e,k),l(e,vs,k),x(_n,e,k),l(e,bs,k),l(e,en,k),s(en,Ce),s(en,It),s(It,Vt),s(en,ze),s(en,Rt),s(Rt,pt),s(en,Gs),l(e,Bn,k),x(In,e,k),l(e,Cn,k),l(e,an,k),s(an,Kt),s(an,ks),s(ks,Ss),s(an,ut),s(an,Ut),s(Ut,mt),s(an,Bs),s(an,jn),s(jn,Pe),s(an,$s),l(e,Is,k),x(gt,e,k),l(e,js,k),x(hn,e,k),l(e,zn,k),l(e,Vn,k),s(Vn,_t),l(e,Vs,k),x(Pn,e,k),l(e,Rs,k),l(e,dn,k),s(dn,dt),s(dn,Xt),s(Xt,Wt),s(dn,De),s(dn,Yt),s(Yt,ln),s(dn,He),l(e,Zt,k),l(e,Jt,k),s(Jt,Ae),l(e,ft,k),x(yt,e,k),l(e,Ks,k),x(Dn,e,k),l(e,Us,k),l(e,Rn,k),s(Rn,Xs),s(Rn,fn),s(fn,vt),s(Rn,la),l(e,Ws,k),x(yn,e,k),l(e,Oe,k),l(e,bt,k),s(bt,Qt),l(e,Ys,k),x(Kn,e,k),l(e,Un,k),x(Xn,e,k),Wn=!0},i(e){Wn||(y(d.$$.fragment,e),y(F.$$.fragment,e),y(Ln.$$.fragment,e),y(nt.$$.fragment,e),y(st.$$.fragment,e),y(lt.$$.fragment,e),y(rt.$$.fragment,e),y(qn.$$.fragment,e),y(un.$$.fragment,e),y(mn.$$.fragment,e),y(gn.$$.fragment,e),y(_n.$$.fragment,e),y(In.$$.fragment,e),y(gt.$$.fragment,e),y(hn.$$.fragment,e),y(Pn.$$.fragment,e),y(yt.$$.fragment,e),y(Dn.$$.fragment,e),y(yn.$$.fragment,e),y(Kn.$$.fragment,e),y(Xn.$$.fragment,e),Wn=!0)},o(e){v(d.$$.fragment,e),v(F.$$.fragment,e),v(Ln.$$.fragment,e),v(nt.$$.fragment,e),v(st.$$.fragment,e),v(lt.$$.fragment,e),v(rt.$$.fragment,e),v(qn.$$.fragment,e),v(un.$$.fragment,e),v(mn.$$.fragment,e),v(gn.$$.fragment,e),v(_n.$$.fragment,e),v(In.$$.fragment,e),v(gt.$$.fragment,e),v(hn.$$.fragment,e),v(Pn.$$.fragment,e),v(yt.$$.fragment,e),v(Dn.$$.fragment,e),v(yn.$$.fragment,e),v(Kn.$$.fragment,e),v(Xn.$$.fragment,e),Wn=!1},d(e){e&&t(a),E(d),e&&t(z),e&&t(T),e&&t(P),E(F,e),e&&t(S),e&&t(G),e&&t(Mn),E(Ln,e),e&&t(rn),E(nt,e),e&&t(tt),e&&t(Nt),e&&t(Mt),E(st,e),e&&t(zs),e&&t(et),e&&t(be),e&&t(on),e&&t(Lt),e&&t(nn),e&&t(Hs),E(lt,e),e&&t(Ft),e&&t(Gt),e&&t(Gn),E(rt,e),e&&t(As),e&&t(it),e&&t(Os),E(qn,e),e&&t(Ns),e&&t($n),e&&t(gs),E(un,e),e&&t(_s),e&&t(En),e&&t(ds),E(mn,e),e&&t(fs),e&&t(Tn),e&&t(ys),E(gn,e),e&&t(vs),E(_n,e),e&&t(bs),e&&t(en),e&&t(Bn),E(In,e),e&&t(Cn),e&&t(an),e&&t(Is),E(gt,e),e&&t(js),E(hn,e),e&&t(zn),e&&t(Vn),e&&t(Vs),E(Pn,e),e&&t(Rs),e&&t(dn),e&&t(Zt),e&&t(Jt),e&&t(ft),E(yt,e),e&&t(Ks),E(Dn,e),e&&t(Us),e&&t(Rn),e&&t(Ws),E(yn,e),e&&t(Oe),e&&t(bt),e&&t(Ys),E(Kn,e),e&&t(Un),E(Xn,e)}}}function Cr(A){let a,_,r,d,$;return{c(){a=o("p"),_=h("\u{1F6A8} N\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU, b\u1EA1n s\u1EBD c\u1EA7n chuy\u1EC3n t\u1EA5t c\u1EA3 m\xE3 b\u1EAFt \u0111\u1EA7u t\u1EEB \xF4 \u1EDF tr\xEAn v\xE0o m\u1ED9t h\xE0m hu\u1EA5n luy\u1EC7n chuy\xEAn d\u1EE5ng. Xem "),r=o("a"),d=h("Chapter 3"),$=h(" \u0111\u1EC3 bi\u1EBFt th\xEAm chi ti\u1EBFt."),this.h()},l(b){a=p(b,"P",{});var j=u(a);_=c(j,"\u{1F6A8} N\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU, b\u1EA1n s\u1EBD c\u1EA7n chuy\u1EC3n t\u1EA5t c\u1EA3 m\xE3 b\u1EAFt \u0111\u1EA7u t\u1EEB \xF4 \u1EDF tr\xEAn v\xE0o m\u1ED9t h\xE0m hu\u1EA5n luy\u1EC7n chuy\xEAn d\u1EE5ng. Xem "),r=p(j,"A",{href:!0});var z=u(r);d=c(z,"Chapter 3"),z.forEach(t),$=c(j," \u0111\u1EC3 bi\u1EBFt th\xEAm chi ti\u1EBFt."),j.forEach(t),this.h()},h(){O(r,"href","/course/chapter3")},m(b,j){l(b,a,j),s(a,_),s(a,r),s(r,d),s(a,$)},d(b){b&&t(a)}}}function zr(A){let a,_,r,d,$;return{c(){a=o("p"),_=h("\u270F\uFE0F "),r=o("strong"),d=h("Th\u1EED nghi\u1EC7m th\xF4i!"),$=h(" Ho\u1EB7c t\u1EA1o h\xE0m m\u1EA5t t\xF9y ch\u1EC9nh c\u1EE7a ri\xEAng b\u1EA1n ph\xF9 h\u1EE3p v\u1EDBi tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng c\u1EE7a b\u1EA1n ho\u1EB7c th\xEAm m\u1ED9t b\u01B0\u1EDBc t\xF9y ch\u1EC9nh kh\xE1c v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n.")},l(b){a=p(b,"P",{});var j=u(a);_=c(j,"\u270F\uFE0F "),r=p(j,"STRONG",{});var z=u(r);d=c(z,"Th\u1EED nghi\u1EC7m th\xF4i!"),z.forEach(t),$=c(j," Ho\u1EB7c t\u1EA1o h\xE0m m\u1EA5t t\xF9y ch\u1EC9nh c\u1EE7a ri\xEAng b\u1EA1n ph\xF9 h\u1EE3p v\u1EDBi tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng c\u1EE7a b\u1EA1n ho\u1EB7c th\xEAm m\u1ED9t b\u01B0\u1EDBc t\xF9y ch\u1EC9nh kh\xE1c v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n."),j.forEach(t)},m(b,j){l(b,a,j),s(a,_),s(a,r),s(r,d),s(a,$)},d(b){b&&t(a)}}}function Pr(A){let a,_,r,d,$;return{c(){a=o("p"),_=h("\u270F\uFE0F "),r=o("strong"),d=h("Th\u1EED nghi\u1EC7m th\xF4i!"),$=h(" Khi ch\u1EA1y c\xE1c th\u1EED nghi\u1EC7m hu\u1EA5n luy\u1EC7n d\xE0i, b\u1EA1n n\xEAn ghi l\u1EA1i c\xE1c ch\u1EC9 s\u1ED1 quan tr\u1ECDng b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c c\xF4ng c\u1EE5 nh\u01B0 TensorBoard ho\u1EB7c Weights & Biases. Th\xEAm ghi nh\u1EADt k\xFD th\xEDch h\u1EE3p v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n \u0111\u1EC3 b\u1EA1n lu\xF4n c\xF3 th\u1EC3 ki\u1EC3m tra qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n di\u1EC5n ra nh\u01B0 th\u1EBF n\xE0o.")},l(b){a=p(b,"P",{});var j=u(a);_=c(j,"\u270F\uFE0F "),r=p(j,"STRONG",{});var z=u(r);d=c(z,"Th\u1EED nghi\u1EC7m th\xF4i!"),z.forEach(t),$=c(j," Khi ch\u1EA1y c\xE1c th\u1EED nghi\u1EC7m hu\u1EA5n luy\u1EC7n d\xE0i, b\u1EA1n n\xEAn ghi l\u1EA1i c\xE1c ch\u1EC9 s\u1ED1 quan tr\u1ECDng b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c c\xF4ng c\u1EE5 nh\u01B0 TensorBoard ho\u1EB7c Weights & Biases. Th\xEAm ghi nh\u1EADt k\xFD th\xEDch h\u1EE3p v\xE0o v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n \u0111\u1EC3 b\u1EA1n lu\xF4n c\xF3 th\u1EC3 ki\u1EC3m tra qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n di\u1EC5n ra nh\u01B0 th\u1EBF n\xE0o."),j.forEach(t)},m(b,j){l(b,a,j),s(a,_),s(a,r),s(r,d),s(a,$)},d(b){b&&t(a)}}}function Dr(A){let a,_,r,d,$,b,j,z,T,H,M,L,f,P,F,S,G,V,U,N,B,I,W,X,Y,Q,cn,C,K,An,On,At,bn,Nn,Ot,is,os,fe,ps,us,ye,Cs,Mn,Ln,rn,nt,tt,Nt,ve,Mt,st,zs,et,sn,be,on,kn,ke,at,ms,$e,Ps,pn,Fn,Lt,nn,je,ht,we,Ds,tn,qe,ct,Hs,lt,Ft,Gt,xe,Gn,rt,As,it,Sn,Os,qn,Ns,$n,Ms,xn,Ls,St,Bt,Fs,ot,gs,un,_s,En,Ee,ds,mn,fs,Tn,Te,ys,gn,vs,_n,bs,en,Ce,It,Vt,ze,Rt,pt,Gs,Bn,In,Cn,an,Kt,ks,Ss,ut,Ut,mt,Bs,jn,Pe,$s,Is,gt,js,hn,zn,Vn,_t,Vs,Pn,Rs,dn,dt,Xt,Wt,De,Yt,ln,He,Zt,Jt,Ae,ft,yt,Ks,Dn,Us,Rn,Xs,fn,vt,la,Ws,yn,Oe,bt,Qt,Ys,Kn,Un,Xn,Wn,e,k,ra,Zs,ia,Js,Ne,R,oa,Qs,pa,ua,ws,ma,ga,ne,_a,da,kt,ns,Me,ts,$t,qs,fa,Le,xs,ss,Fe,jt,Ge,Yn,vn,te,es,Zn,se,ya,Se,Hn,as,ee,va,ba,hs,ka,$a,ph,wt,qt,ja,cs,Qh,Ua,nc,tc,Xa,sc,ec,uh,ls,ac,Wa,hc,cc,Ya,lc,rc,mh,xt,Et,wa,qa,ic,gh,Be,_h,Tt,Ct,xa,Ea,oc,dh,Ta,ae,fh,Ca,pc,yh,Ie,vh,za,uc,bh,Pa,mc,kh,Ve,$h,zt,Pt,Da,he,jh,ce,wh,Es,le,Za,Re,gc,Ja,_c,qh,re,dc,Qa,fc,yc,xh,Dt,Ht,Ha,Aa,vc,Eh,Ke,Th,Ue,Ch,rs,bc,nh,kc,$c,th,jc,wc,zh,Xe,Ph,We,Dh,Jn,qc,sh,xc,Ec,eh,Tc,Cc,ah,zc,Pc,Hh,Ye,Ah,Ze,Oh,ie,Dc,hh,Hc,Ac,Nh,Je,Mh,Qe,Lh,Oa,Na,Fh;r=new lr({props:{fw:A[0]}}),z=new Ka({});const Mc=[ir,rr],na=[];function Lc(n,i){return n[0]==="pt"?0:1}f=Lc(A),P=na[f]=Mc[f](A),Mn=new Nc({props:{id:"Vpjb1lu0MDk"}}),nn=new Ka({}),xn=new D({props:{code:`def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">any_keyword_in_string</span>(<span class="hljs-params">string, keywords</span>):
    <span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> keywords:
        <span class="hljs-keyword">if</span> keyword <span class="hljs-keyword">in</span> string:
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>`}}),ot=new D({props:{code:`filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`,highlighted:`filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]
example_1 = <span class="hljs-string">&quot;import numpy as np&quot;</span>
example_2 = <span class="hljs-string">&quot;import pandas as pd&quot;</span>

<span class="hljs-built_in">print</span>(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)`}}),un=new D({props:{code:"False True",highlighted:'<span class="hljs-literal">False</span> <span class="hljs-literal">True</span>'}}),mn=new D({props:{code:`from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)`,highlighted:`<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset


<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_streaming_dataset</span>(<span class="hljs-params">dataset, filters</span>):
    filtered_dict = defaultdict(<span class="hljs-built_in">list</span>)
    total = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">iter</span>(dataset)):
        total += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> any_keyword_in_string(sample[<span class="hljs-string">&quot;content&quot;</span>], filters):
            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sample.items():
                filtered_dict[k].append(v)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">len</span>(filtered_dict[<span class="hljs-string">&#x27;content&#x27;</span>])/total:<span class="hljs-number">.2</span>%}</span> of data after filtering.&quot;</span>)
    <span class="hljs-keyword">return</span> Dataset.from_dict(filtered_dict)`}}),gn=new D({props:{code:`# \xD4 n\xE0y s\u1EBD m\u1EA5t r\u1EA5t nhi\u1EC1u th\u1EDDi gian \u0111\u1EC3 th\u1EF1c thi, v\xEC v\u1EADy b\u1EA1n n\xEAn b\u1ECF qua v\xE0 chuy\u1EC3n \u0111\u1EBFn
# c\xE1i ti\u1EBFp theo!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)`,highlighted:`<span class="hljs-comment"># \xD4 n\xE0y s\u1EBD m\u1EA5t r\u1EA5t nhi\u1EC1u th\u1EDDi gian \u0111\u1EC3 th\u1EF1c thi, v\xEC v\u1EADy b\u1EA1n n\xEAn b\u1ECF qua v\xE0 chuy\u1EC3n \u0111\u1EBFn</span>
<span class="hljs-comment"># c\xE1i ti\u1EBFp theo!</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

split = <span class="hljs-string">&quot;train&quot;</span>  <span class="hljs-comment"># &quot;valid&quot;</span>
filters = [<span class="hljs-string">&quot;pandas&quot;</span>, <span class="hljs-string">&quot;sklearn&quot;</span>, <span class="hljs-string">&quot;matplotlib&quot;</span>, <span class="hljs-string">&quot;seaborn&quot;</span>]

data = load_dataset(<span class="hljs-string">f&quot;transformersbook/codeparrot-<span class="hljs-subst">{split}</span>&quot;</span>, split=split, streaming=<span class="hljs-literal">True</span>)
filtered_data = filter_streaming_dataset(data, filters)`}}),_n=new D({props:{code:"3.26% of data after filtering.",highlighted:'<span class="hljs-number">3.26</span>% of data after filtering.'}}),pt=new D({props:{code:`from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DatasetDict

ds_train = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-train&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
ds_valid = load_dataset(<span class="hljs-string">&quot;huggingface-course/codeparrot-ds-valid&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

raw_datasets = DatasetDict(
    {
        <span class="hljs-string">&quot;train&quot;</span>: ds_train,  <span class="hljs-comment"># .shuffle().select(range(50000)),</span>
        <span class="hljs-string">&quot;valid&quot;</span>: ds_valid,  <span class="hljs-comment"># .shuffle().select(range(500))</span>
    }
)

raw_datasets`}}),Bn=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">606720</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;repo_name&#x27;</span>, <span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;copies&#x27;</span>, <span class="hljs-string">&#x27;size&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>, <span class="hljs-string">&#x27;license&#x27;</span>],
        num_rows: <span class="hljs-number">3322</span>
    })
})`}}),Cn=new de({props:{$$slots:{default:[or]},$$scope:{ctx:A}}}),ut=new D({props:{code:`for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")`,highlighted:`<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key.upper()}</span>: <span class="hljs-subst">{raw_datasets[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>][key][:<span class="hljs-number">200</span>]}</span>&quot;</span>)`}}),mt=new D({props:{code:`'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:\`sklearn.utils\` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''`,highlighted:`<span class="hljs-string">&#x27;REPO_NAME: kmike/scikit-learn&#x27;</span>
<span class="hljs-string">&#x27;PATH: sklearn/utils/__init__.py&#x27;</span>
<span class="hljs-string">&#x27;COPIES: 3&#x27;</span>
<span class="hljs-string">&#x27;SIZE: 10094&#x27;</span>
<span class="hljs-string">&#x27;&#x27;&#x27;CONTENT: &quot;&quot;&quot;
The :mod:\`sklearn.utils\` module includes various utilites.
&quot;&quot;&quot;

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause&#x27;&#x27;&#x27;</span>`}}),_t=new Ka({}),dt=new Nc({props:{id:"ma1TrR7gE7I"}}),Un=new D({props:{code:`from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

context_length = <span class="hljs-number">128</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)

outputs = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">2</span>][<span class="hljs-string">&quot;content&quot;</span>],
    truncation=<span class="hljs-literal">True</span>,
    max_length=context_length,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_length=<span class="hljs-literal">True</span>,
)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input IDs length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(outputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input chunk lengths: <span class="hljs-subst">{(outputs[<span class="hljs-string">&#x27;length&#x27;</span>])}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Chunk mapping: <span class="hljs-subst">{outputs[<span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>]}</span>&quot;</span>)`}}),Wn=new D({props:{code:`Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`,highlighted:`Input IDs length: <span class="hljs-number">34</span>
Input chunk lengths: [<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">117</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">41</span>]
Chunk mapping: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),ns=new D({props:{code:`def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">element</span>):
    outputs = tokenizer(
        element[<span class="hljs-string">&quot;content&quot;</span>],
        truncation=<span class="hljs-literal">True</span>,
        max_length=context_length,
        return_overflowing_tokens=<span class="hljs-literal">True</span>,
        return_length=<span class="hljs-literal">True</span>,
    )
    input_batch = []
    <span class="hljs-keyword">for</span> length, input_ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(outputs[<span class="hljs-string">&quot;length&quot;</span>], outputs[<span class="hljs-string">&quot;input_ids&quot;</span>]):
        <span class="hljs-keyword">if</span> length == context_length:
            input_batch.append(input_ids)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;input_ids&quot;</span>: input_batch}


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(
    tokenize, batched=<span class="hljs-literal">True</span>, remove_columns=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>].column_names
)
tokenized_datasets`}}),ts=new D({props:{code:`DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">16702061</span>
    })
    valid: Dataset({
        features: [<span class="hljs-string">&#x27;input_ids&#x27;</span>],
        num_rows: <span class="hljs-number">93164</span>
    })
})`}}),jt=new de({props:{$$slots:{default:[pr]},$$scope:{ctx:A}}}),es=new Ka({});const Fc=[mr,ur],ta=[];function Gc(n,i){return n[0]==="pt"?0:1}wt=Gc(A),qt=ta[wt]=Fc[wt](A);const Sc=[_r,gr],sa=[];function Bc(n,i){return n[0]==="pt"?0:1}xt=Bc(A),Et=sa[xt]=Sc[xt](A),Be=new D({props:{code:`out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")`,highlighted:`out = data_collator([tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> out:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span> shape: <span class="hljs-subst">{out[key].shape}</span>&quot;</span>)`}});const Ic=[fr,dr],ea=[];function Vc(n,i){return n[0]==="pt"?0:1}Tt=Vc(A),Ct=ea[Tt]=Ic[Tt](A);let Z=A[0]==="tf"&&Ql();ae=new de({props:{warning:!0,$$slots:{default:[yr]},$$scope:{ctx:A}}}),Ie=new D({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),Ve=new D({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}});const Rc=[br,vr],aa=[];function Kc(n,i){return n[0]==="pt"?0:1}zt=Kc(A),Pt=aa[zt]=Rc[zt](A),he=new de({props:{$$slots:{default:[kr]},$$scope:{ctx:A}}}),ce=new de({props:{$$slots:{default:[wr]},$$scope:{ctx:A}}}),Re=new Ka({});const Uc=[xr,qr],ha=[];function Xc(n,i){return n[0]==="pt"?0:1}Dt=Xc(A),Ht=ha[Dt]=Uc[Dt](A),Ke=new D({props:{code:`txt = """\\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Ue=new D({props:{code:`# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter`,highlighted:`<span class="hljs-comment"># create some data</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># create scatter plot with x, y</span>
plt.scatter(x, y)

<span class="hljs-comment"># create scatter</span>`}}),Xe=new D({props:{code:`txt = """\\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),We=new D({props:{code:`# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for`,highlighted:`<span class="hljs-comment"># create some data</span>
x = np.random.randn(<span class="hljs-number">100</span>)
y = np.random.randn(<span class="hljs-number">100</span>)

<span class="hljs-comment"># create dataframe from x and y</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;x&#x27;</span>: x, <span class="hljs-string">&#x27;y&#x27;</span>: y})
df.insert(<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;x&#x27;</span>, x)
<span class="hljs-keyword">for</span>`}}),Ye=new D({props:{code:`txt = """\\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;\\
# dataframe with profession, income and name
df = pd.DataFrame({&#x27;profession&#x27;: x, &#x27;income&#x27;:y, &#x27;name&#x27;: z})

# calculate the mean income per profession
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Ze=new D({props:{code:`# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the`,highlighted:`<span class="hljs-comment"># dataframe with profession, income and name</span>
df = pd.DataFrame({<span class="hljs-string">&#x27;profession&#x27;</span>: x, <span class="hljs-string">&#x27;income&#x27;</span>:y, <span class="hljs-string">&#x27;name&#x27;</span>: z})

<span class="hljs-comment"># calculate the mean income per profession</span>
profession = df.groupby([<span class="hljs-string">&#x27;profession&#x27;</span>]).mean()

<span class="hljs-comment"># compute the</span>`}}),Je=new D({props:{code:`txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])`,highlighted:`txt = <span class="hljs-string">&quot;&quot;&quot;
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
&quot;&quot;&quot;</span>
<span class="hljs-built_in">print</span>(pipe(txt, num_return_sequences=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`}}),Qe=new D({props:{code:`# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf`,highlighted:`<span class="hljs-comment"># import random forest regressor from scikit-learn</span>
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor

<span class="hljs-comment"># fit random forest model with 300 estimators on X, y:</span>
rf = RandomForestRegressor(n_estimators=<span class="hljs-number">300</span>, random_state=random_state, max_depth=<span class="hljs-number">3</span>)
rf.fit(X, y)
rf`}});function Wc(n,i){return n[0]==="tf"?Tr:Er}let Gh=Wc(A),Ts=Gh(A),J=A[0]==="pt"&&nr(A);return{c(){a=o("meta"),_=m(),w(r.$$.fragment),d=m(),$=o("h1"),b=o("a"),j=o("span"),w(z.$$.fragment),T=m(),H=o("span"),M=h("THu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 t\u1EEB \u0111\u1EA7u"),L=m(),P.c(),F=m(),S=o("p"),G=h("Cho \u0111\u1EBFn th\u1EDDi \u0111i\u1EC3m hi\u1EC7n t\u1EA1i, ch\xFAng ta ch\u1EE7 y\u1EBFu s\u1EED d\u1EE5ng c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\xE0 tinh ch\u1EC9nh ch\xFAng cho c\xE1c tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng m\u1EDBi b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EA1i c\xE1c tr\u1ECDng s\u1ED1 t\u1EEB hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc. Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),V=o("a"),U=h("Ch\u01B0\u01A1ng 1"),N=h(", \u0111i\u1EC1u n\xE0y th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),B=o("em"),I=h("transfer learning"),W=h(" hay "),X=o("em"),Y=h("h\u1ECDc chuy\u1EC3n giao"),Q=h(", v\xE0 \u0111\xF3 l\xE0 m\u1ED9t chi\u1EBFn l\u01B0\u1EE3c r\u1EA5t th\xE0nh c\xF4ng \u0111\u1EC3 \xE1p d\u1EE5ng c\xE1c m\xF4 h\xECnh Transformer cho h\u1EA7u h\u1EBFt c\xE1c tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng trong th\u1EBF gi\u1EDBi th\u1EF1c n\u01A1i d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n l\xE0 th\u01B0a th\u1EDBt. Trong ch\u01B0\u01A1ng n\xE0y, ch\xFAng ta s\u1EBD th\u1EF1c hi\u1EC7n m\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn kh\xE1c v\xE0 hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ho\xE0n to\xE0n m\u1EDBi t\u1EEB \u0111\u1EA7u. \u0110\xE2y l\xE0 m\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn t\u1ED1t \u0111\u1EC3 th\u1EF1c hi\u1EC7n n\u1EBFu b\u1EA1n c\xF3 nhi\u1EC1u d\u1EEF li\u1EC7u v\xE0 n\xF3 r\u1EA5t kh\xE1c v\u1EDBi d\u1EEF li\u1EC7u hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho c\xE1c m\xF4 h\xECnh c\xF3 s\u1EB5n. Tuy nhi\xEAn, n\xF3 c\u0169ng \u0111\xF2i h\u1ECFi nhi\u1EC1u t\xE0i nguy\xEAn m\xE1y t\xEDnh h\u01A1n \u0111\xE1ng k\u1EC3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF h\u01A1n l\xE0 ch\u1EC9 \u0111\u1EC3 tinh ch\u1EC9nh m\xF4 h\xECnh hi\u1EC7n c\xF3. C\xE1c v\xED d\u1EE5 c\xF3 th\u1EC3 c\xF3 \xFD ngh\u0129a khi hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh m\u1EDBi bao g\u1ED3m c\xE1c t\u1EADp d\u1EEF li\u1EC7u bao g\u1ED3m c\xE1c n\u1ED1t nh\u1EA1c, tr\xECnh t\u1EF1 ph\xE2n t\u1EED nh\u01B0 DNA ho\u1EB7c ng\xF4n ng\u1EEF l\u1EADp tr\xECnh. C\xF4ng c\u1EE5 th\u1EE9 hai g\u1EA7n \u0111\xE2y \u0111\xE3 \u0111\u1EA1t \u0111\u01B0\u1EE3c s\u1EE9c h\xFAt nh\u1EDD c\xE1c c\xF4ng c\u1EE5 nh\u01B0 TabNine v\xE0 GitHub\u2019s Copilot, \u0111\u01B0\u1EE3c h\u1ED7 tr\u1EE3 b\u1EDFi m\xF4 h\xECnh Codex c\u1EE7a OpenAI, c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c chu\u1ED7i m\xE3 d\xE0i. T\xE1c v\u1EE5 t\u1EA1o v\u0103n b\u1EA3n n\xE0y \u0111\u01B0\u1EE3c gi\u1EA3i quy\u1EBFt t\u1ED1t nh\u1EA5t v\u1EDBi c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy ho\u1EB7c nh\xE2n qu\u1EA3 nh\u01B0 GPT-2."),cn=m(),C=o("p"),K=h("Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD x\xE2y d\u1EF1ng m\u1ED9t phi\xEAn b\u1EA3n thu nh\u1ECF c\u1EE7a m\xF4 h\xECnh t\u1EA1o m\xE3: ch\xFAng ta s\u1EBD t\u1EADp trung v\xE0o c\xE1c ho\xE0n th\xE0nh m\u1ED9t d\xF2ng thay v\xEC c\xE1c h\xE0m ho\u1EB7c l\u1EDBp \u0111\u1EA7y \u0111\u1EE7, s\u1EED d\u1EE5ng m\u1ED9t t\u1EADp h\u1EE3p con m\xE3 Python. Khi l\xE0m vi\u1EC7c v\u1EDBi d\u1EEF li\u1EC7u b\u1EB1ng Python, b\u1EA1n th\u01B0\u1EDDng xuy\xEAn ti\u1EBFp x\xFAc v\u1EDBi b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python, bao g\u1ED3m c\xE1c th\u01B0 vi\u1EC7n "),An=o("code"),On=h("matplotlib"),At=h(", "),bn=o("code"),Nn=h("seaborn"),Ot=h(", "),is=o("code"),os=h("pandas"),fe=h(" v\xE0 "),ps=o("code"),us=h("scikit-learn"),ye=h(". Khi s\u1EED d\u1EE5ng ch\xFAng, th\xF4ng th\u01B0\u1EDDng c\u1EA7n ph\u1EA3i tra c\u1EE9u c\xE1c l\u1EC7nh c\u1EE5 th\u1EC3, v\xEC v\u1EADy s\u1EBD r\u1EA5t tuy\u1EC7t n\u1EBFu ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng m\u1ED9t m\xF4 h\xECnh \u0111\u1EC3 ho\xE0n th\xE0nh c\xE1c l\u1EC7nh g\u1ECDi n\xE0y cho ch\xFAng ta."),Cs=m(),w(Mn.$$.fragment),Ln=m(),rn=o("p"),nt=h("Trong "),tt=o("a"),Nt=h("Ch\u01B0\u01A1ng 6"),ve=h(", ch\xFAng ta \u0111\xE3 t\u1EA1o m\u1ED9t tr\xECnh tokenize hi\u1EC7u qu\u1EA3 \u0111\u1EC3 x\u1EED l\xFD m\xE3 ngu\u1ED3n Python, nh\u01B0ng nh\u1EEFng g\xEC ch\xFAng ta v\u1EABn c\u1EA7n l\xE0 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u quy m\xF4 l\u1EDBn \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh. \u1EDE \u0111\xE2y, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng tokenizer cho m\u1ED9t kho l\u01B0u tr\u1EEF m\xE3 Python c\xF3 ngu\u1ED3n g\u1ED1c t\u1EEB kho l\u01B0u tr\u1EEF GitHub. Sau \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng API "),Mt=o("code"),st=h("Trainer"),zs=h(" v\xE0 \u{1F917} Accelerate \u0111\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh. Ch\xFAng ta h\xE3y \u0111i \u0111\u1EBFn \u0111\xF3!"),et=m(),sn=o("iframe"),on=m(),kn=o("p"),ke=h("\u0110\xE2y th\u1EF1c s\u1EF1 c\xE1ch m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n v\xE0 t\u1EA3i l\xEAn Hub b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng m\xE3 \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong ph\u1EA7n n\xE0y. B\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y n\xF3 "),at=o("a"),ms=h("t\u1EA1i \u0111\xE2y"),$e=h(". L\u01B0u \xFD r\u1EB1ng v\xEC c\xF3 m\u1ED9t s\u1ED1 ng\u1EABu nhi\xEAn x\u1EA3y ra trong qu\xE1 tr\xECnh t\u1EA1o v\u0103n b\u1EA3n, b\u1EA1n c\xF3 th\u1EC3 s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t k\u1EBFt qu\u1EA3 h\u01A1i kh\xE1c."),Ps=m(),pn=o("h2"),Fn=o("a"),Lt=o("span"),w(nn.$$.fragment),je=m(),ht=o("span"),we=h("Thu th\u1EADp d\u1EEF li\u1EC7u"),Ds=m(),tn=o("p"),qe=h("M\xE3 Python c\xF3 s\u1EB5n r\u1EA5t nhi\u1EC1u t\u1EEB c\xE1c kho m\xE3 nh\u01B0 GitHub, m\xE0 ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111\u1EC3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u b\u1EB1ng c\xE1ch \u0111\xE0o m\u1ECDi kho l\u01B0u tr\u1EEF Python. \u0110\xE2y l\xE0 ph\u01B0\u01A1ng ph\xE1p \u0111\u01B0\u1EE3c th\u1EF1c hi\u1EC7n trong "),ct=o("a"),Hs=h("s\xE1ch gi\xE1o khoa v\u1EC1 Transformers"),lt=h(" \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh GPT-2 l\u1EDBn. S\u1EED d\u1EE5ng k\u1EBFt xu\u1EA5t GitHub kho\u1EA3ng 180 GB ch\u1EE9a kho\u1EA3ng 20 tri\u1EC7u t\u1EC7p Python c\xF3 t\xEAn l\xE0 "),Ft=o("code"),Gt=h("codeparrot"),xe=h(", c\xE1c t\xE1c gi\u1EA3 \u0111\xE3 x\xE2y d\u1EF1ng m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u m\xE0 sau \u0111\xF3 h\u1ECD chia s\u1EBB tr\xEAn "),Gn=o("a"),rt=h("Hugging Face Hub"),As=h(" ."),it=m(),Sn=o("p"),Os=h("Tuy nhi\xEAn, vi\u1EC7c hu\u1EA5n luy\u1EC7n tr\xEAn to\xE0n b\u1ED9 ng\u1EEF li\u1EC7u n\xE0y t\u1ED1n nhi\u1EC1u th\u1EDDi gian v\xE0 t\xEDnh to\xE1n, v\xE0 ch\xFAng ta ch\u1EC9 c\u1EA7n t\u1EADp con c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u li\xEAn quan \u0111\u1EBFn ng\u0103n x\u1EBFp khoa h\u1ECDc d\u1EEF li\u1EC7u Python. V\xEC v\u1EADy, h\xE3y b\u1EAFt \u0111\u1EA7u b\u1EB1ng c\xE1ch l\u1ECDc t\u1EADp d\u1EEF li\u1EC7u "),qn=o("code"),Ns=h("codeparrot"),$n=h(" cho t\u1EA5t c\u1EA3 c\xE1c t\u1EC7p bao g\u1ED3m b\u1EA5t k\u1EF3 th\u01B0 vi\u1EC7n n\xE0o trong ng\u0103n x\u1EBFp n\xE0y. Do k\xEDch th\u01B0\u1EDBc c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta mu\u1ED1n tr\xE1nh t\u1EA3i n\xF3 xu\u1ED1ng; thay v\xE0o \u0111\xF3, ta s\u1EBD s\u1EED d\u1EE5ng t\xEDnh n\u0103ng ph\xE1t tr\u1EF1c tuy\u1EBFn \u0111\u1EC3 l\u1ECDc n\xF3 m\u1ED9t c\xE1ch nhanh ch\xF3ng. \u0110\u1EC3 gi\xFAp ch\xFAng ta l\u1ECDc c\xE1c m\u1EABu m\xE3 b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c th\u01B0 vi\u1EC7n \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3, ta s\u1EBD s\u1EED d\u1EE5ng h\xE0m sau:"),Ms=m(),w(xn.$$.fragment),Ls=m(),St=o("p"),Bt=h("H\xE3y ki\u1EC3m tra n\xF3 tr\xEAn hai v\xED d\u1EE5:"),Fs=m(),w(ot.$$.fragment),gs=m(),w(un.$$.fragment),_s=m(),En=o("p"),Ee=h("Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111i\u1EC1u n\xE0y \u0111\u1EC3 t\u1EA1o m\u1ED9t h\xE0m s\u1EBD truy\u1EC1n tr\u1EF1c tuy\u1EBFn t\u1EADp d\u1EEF li\u1EC7u v\xE0 l\u1ECDc c\xE1c ph\u1EA7n t\u1EED ta mu\u1ED1n:"),ds=m(),w(mn.$$.fragment),fs=m(),Tn=o("p"),Te=h("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 c\u1EA7n \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng n\xE0y cho t\u1EADp d\u1EEF li\u1EC7u ph\xE1t tr\u1EF1c tuy\u1EBFn:"),ys=m(),w(gn.$$.fragment),vs=m(),w(_n.$$.fragment),bs=m(),en=o("p"),Ce=h("\u0110i\u1EC1u n\xE0y \u0111\u1EC3 l\u1EA1i cho ch\xFAng ta kho\u1EA3ng 3% t\u1EADp d\u1EEF li\u1EC7u ban \u0111\u1EA7u, v\u1EABn c\xF2n kh\xE1 l\u1EDBn - t\u1EADp d\u1EEF li\u1EC7u k\u1EBFt qu\u1EA3 l\xE0 6GB v\xE0 bao g\u1ED3m 600,000 t\u1EADp l\u1EC7nh Python!"),It=m(),Vt=o("p"),ze=h("Vi\u1EC7c l\u1ECDc to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\xF3 th\u1EC3 m\u1EA5t 2-3 gi\u1EDD t\xF9y thu\u1ED9c v\xE0o m\xE1y v\xE0 b\u0103ng th\xF4ng c\u1EE7a b\u1EA1n. N\u1EBFu b\u1EA1n kh\xF4ng mu\u1ED1n t\u1EF1 m\xECnh tr\u1EA3i qua qu\xE1 tr\xECnh k\xE9o d\xE0i n\xE0y, ch\xFAng ta cung c\u1EA5p t\u1EADp d\u1EEF li\u1EC7u \u0111\xE3 l\u1ECDc tr\xEAn Hub \u0111\u1EC3 b\u1EA1n t\u1EA3i xu\u1ED1ng:"),Rt=m(),w(pt.$$.fragment),Gs=m(),w(Bn.$$.fragment),In=m(),w(Cn.$$.fragment),an=m(),Kt=o("p"),ks=h("H\xE3y xem m\u1ED9t v\xED d\u1EE5 t\u1EEB t\u1EADp d\u1EEF li\u1EC7u. Ch\xFAng ta s\u1EBD ch\u1EC9 hi\u1EC3n th\u1ECB 200 k\xFD t\u1EF1 \u0111\u1EA7u ti\xEAn c\u1EE7a m\u1ED7i tr\u01B0\u1EDDng:"),Ss=m(),w(ut.$$.fragment),Ut=m(),w(mt.$$.fragment),Bs=m(),jn=o("p"),Pe=h("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng tr\u01B0\u1EDDng "),$s=o("code"),Is=h("content"),gt=h(" ch\u1EE9a m\xE3 m\xE0 ch\xFAng ta mu\u1ED1n m\xF4 h\xECnh c\u1EE7a m\xECnh hu\u1EA5n luy\u1EC7n. B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta c\u1EA7n chu\u1EA9n b\u1ECB c\xE1c v\u0103n b\u1EA3n \u0111\u1EC3 ch\xFAng c\xF3 \u0111\u1ECBnh d\u1EA1ng ph\xF9 h\u1EE3p \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc."),js=m(),hn=o("h2"),zn=o("a"),Vn=o("span"),w(_t.$$.fragment),Vs=m(),Pn=o("span"),Rs=h("Chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"),dn=m(),w(dt.$$.fragment),Xt=m(),Wt=o("p"),De=h("B\u01B0\u1EDBc \u0111\u1EA7u ti\xEAn s\u1EBD l\xE0 tokenize d\u1EEF li\u1EC7u \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n. V\xEC m\u1EE5c ti\xEAu c\u1EE7a ch\xFAng ta ch\u1EE7 y\u1EBFu l\xE0 t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh c\xE1c l\u1EC7nh g\u1ECDi h\xE0m ng\u1EAFn, ch\xFAng ta c\xF3 th\u1EC3 gi\u1EEF k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh t\u01B0\u01A1ng \u0111\u1ED1i nh\u1ECF. \u0110i\u1EC1u n\xE0y c\xF3 l\u1EE3i \xEDch l\xE0 ch\xFAng ta c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh nhanh h\u01A1n nhi\u1EC1u v\xE0 n\xF3 c\u1EA7n \xEDt b\u1ED9 nh\u1EDB h\u01A1n \u0111\xE1ng k\u1EC3. N\u1EBFu \u0111i\u1EC1u quan tr\u1ECDng l\xE0 \u1EE9ng d\u1EE5ng c\u1EE7a b\u1EA1n ph\u1EA3i c\xF3 nhi\u1EC1u ng\u1EEF c\u1EA3nh h\u01A1n (v\xED d\u1EE5: n\u1EBFu b\u1EA1n mu\u1ED1n m\xF4 h\xECnh vi\u1EBFt c\xE1c b\xE0i ki\u1EC3m tra \u0111\u01A1n v\u1ECB d\u1EF1a tr\xEAn t\u1EC7p c\xF3 \u0111\u1ECBnh ngh\u0129a h\xE0m), h\xE3y \u0111\u1EA3m b\u1EA3o b\u1EA1n t\u0103ng con s\u1ED1 \u0111\xF3, nh\u01B0ng c\u0169ng l\u01B0u \xFD r\u1EB1ng \u0111i\u1EC1u n\xE0y \u0111i k\xE8m v\u1EDBi b\u1ED9 nh\u1EDB GPU l\u1EDBn h\u01A1n. Hi\u1EC7n t\u1EA1i, h\xE3y s\u1EEDa k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh \u1EDF 128 token, tr\xE1i ng\u01B0\u1EE3c v\u1EDBi 1,024 ho\u1EB7c 2,048 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng trong GPT-2 ho\u1EB7c GPT-3, t\u01B0\u01A1ng \u1EE9ng."),Yt=m(),ln=o("p"),He=h("H\u1EA7u h\u1EBFt c\xE1c t\xE0i li\u1EC7u ch\u1EE9a nhi\u1EC1u h\u01A1n 128 token, v\xEC v\u1EADy ch\u1EC9 c\u1EA7n c\u1EAFt b\u1EDBt \u0111\u1EA7u v\xE0o \u0111\u1EBFn \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a s\u1EBD lo\u1EA1i b\u1ECF m\u1ED9t ph\u1EA7n l\u1EDBn t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh. Thay v\xE0o \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Zt=o("code"),Jt=h("return_overflowing_tokens"),Ae=h(" \u0111\u1EC3 token to\xE0n b\u1ED9 \u0111\u1EA7u v\xE0o v\xE0 chia n\xF3 th\xE0nh nhi\u1EC1u ph\u1EA7n, nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m trong "),ft=o("a"),yt=h("Ch\u01B0\u01A1ng 6"),Ks=h(". Ch\xFAng ta c\u0169ng s\u1EBD s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Dn=o("code"),Us=h("return_length"),Rn=h(" \u0111\u1EC3 t\u1EF1 \u0111\u1ED9ng tr\u1EA3 v\u1EC1 \u0111\u1ED9 d\xE0i c\u1EE7a m\u1ED7i \u0111o\u1EA1n \u0111\u01B0\u1EE3c t\u1EA1o. Th\u01B0\u1EDDng th\xEC ph\u1EA7n cu\u1ED1i c\xF9ng s\u1EBD nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh v\xE0 ch\xFAng ta s\u1EBD lo\u1EA1i b\u1ECF nh\u1EEFng ph\u1EA7n n\xE0y \u0111\u1EC3 tr\xE1nh c\xE1c v\u1EA5n \u0111\u1EC1 v\u1EC1 ph\u1EA7n \u0111\u1EC7m; ch\xFAng ta kh\xF4ng th\u1EF1c s\u1EF1 c\u1EA7n ch\xFAng v\xEC d\xF9 sao ch\xFAng ta c\u0169ng c\xF3 nhi\u1EC1u d\u1EEF li\u1EC7u."),Xs=m(),fn=o("div"),vt=o("img"),Ws=m(),yn=o("img"),bt=m(),Qt=o("p"),Ys=h("H\xE3y xem ch\xEDnh x\xE1c c\xE1ch th\u1EE9c ho\u1EA1t \u0111\u1ED9ng c\u1EE7a \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch xem hai v\xED d\u1EE5 \u0111\u1EA7u ti\xEAn:"),Kn=m(),w(Un.$$.fragment),Xn=m(),w(Wn.$$.fragment),e=m(),k=o("p"),ra=h("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng ch\xFAng ta nh\u1EADn \u0111\u01B0\u1EE3c t\u1ED5ng c\u1ED9ng 34 ph\xE2n \u0111o\u1EA1n t\u1EEB hai v\xED d\u1EE5 \u0111\xF3. Nh\xECn v\xE0o \u0111\u1ED9 d\xE0i ph\xE2n \u0111o\u1EA1n, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c \u0111o\u1EA1n \u1EDF cu\u1ED1i c\u1EA3 hai t\xE0i li\u1EC7u c\xF3 \xEDt h\u01A1n 128 token (t\u01B0\u01A1ng \u1EE9ng l\xE0 117 v\xE0 41). Ch\xFAng ch\u1EC9 \u0111\u1EA1i di\u1EC7n cho m\u1ED9t ph\u1EA7n nh\u1ECF trong t\u1ED5ng s\u1ED1 c\xE1c kh\u1ED1i m\xE0 ch\xFAng ta c\xF3, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 v\u1EE9t ch\xFAng \u0111i m\u1ED9t c\xE1ch an to\xE0n. V\u1EDBi tr\u01B0\u1EDDng "),Zs=o("code"),ia=h("overflow_to_sample_mapping"),Js=h(", ch\xFAng ta c\u0169ng c\xF3 th\u1EC3 t\u1EA1o l\u1EA1i c\xE1c ph\u1EA7n thu\u1ED9c v\u1EC1 m\u1EABu \u0111\u1EA7u v\xE0o n\xE0o."),Ne=m(),R=o("p"),oa=h("V\u1EDBi thao t\xE1c n\xE0y, ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng m\u1ED9t t\xEDnh n\u0103ng ti\u1EC7n d\u1EE5ng c\u1EE7a h\xE0m "),Qs=o("code"),pa=h("Dataset.map()"),ua=h(" trong \u{1F917} Datasets, \u0111\xF3 l\xE0 n\xF3 kh\xF4ng y\xEAu c\u1EA7u \xE1nh x\u1EA1 1-1; nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ws=o("a"),ma=h("ph\u1EA7n 3"),ga=h(", ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o c\xE1c l\xF4 c\xF3 nhi\u1EC1u ph\u1EA7n t\u1EED h\u01A1n ho\u1EB7c \xEDt h\u01A1n l\xF4 \u0111\u1EA7u v\xE0o. \u0110i\u1EC1u n\xE0y r\u1EA5t h\u1EEFu \xEDch khi th\u1EF1c hi\u1EC7n c\xE1c ho\u1EA1t \u0111\u1ED9ng nh\u01B0 t\u0103ng d\u1EEF li\u1EC7u ho\u1EB7c l\u1ECDc d\u1EEF li\u1EC7u l\xE0m thay \u0111\u1ED5i s\u1ED1 l\u01B0\u1EE3ng ph\u1EA7n t\u1EED. Trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta, khi tokenize m\u1ED7i ph\u1EA7n t\u1EED th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh \u0111\u01B0\u1EE3c ch\u1EC9 \u0111\u1ECBnh, ch\xFAng ta t\u1EA1o nhi\u1EC1u m\u1EABu t\u1EEB m\u1ED7i t\xE0i li\u1EC7u. Ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EA3m b\u1EA3o x\xF3a c\xE1c c\u1ED9t hi\u1EC7n c\xF3, v\xEC ch\xFAng c\xF3 k\xEDch th\u01B0\u1EDBc xung \u0111\u1ED9t. N\u1EBFu ch\xFAng ta mu\u1ED1n gi\u1EEF ch\xFAng, ch\xFAng ta c\xF3 th\u1EC3 l\u1EB7p l\u1EA1i ch\xFAng m\u1ED9t c\xE1ch th\xEDch h\u1EE3p v\xE0 tr\u1EA3 l\u1EA1i ch\xFAng trong l\u1EC7nh g\u1ECDi "),ne=o("code"),_a=h("Dataset.map()"),da=h(":"),kt=m(),w(ns.$$.fragment),Me=m(),w(ts.$$.fragment),$t=m(),qs=o("p"),fa=h("Hi\u1EC7n ch\xFAng ta c\xF3 16,7 tri\u1EC7u v\xED d\u1EE5 v\u1EDBi 128 token m\u1ED7i v\xED d\u1EE5, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi t\u1ED5ng c\u1ED9ng kho\u1EA3ng 2,1 t\u1EF7 token. \u0110\u1EC3 tham kh\u1EA3o, c\xE1c m\xF4 h\xECnh GPT-3 v\xE0 Codex c\u1EE7a OpenAI \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\xEAn 300 v\xE0 100 t\u1EF7 token t\u01B0\u01A1ng \u1EE9ng, trong \u0111\xF3 c\xE1c m\xF4 h\xECnh Codex \u0111\u01B0\u1EE3c kh\u1EDFi t\u1EA1o t\u1EEB c\xE1c checkpoint GPT-3. M\u1EE5c ti\xEAu c\u1EE7a ch\xFAng ta trong ph\u1EA7n n\xE0y kh\xF4ng ph\u1EA3i l\xE0 c\u1EA1nh tranh v\u1EDBi c\xE1c m\xF4 h\xECnh n\xE0y, c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c v\u0103n b\u1EA3n d\xE0i, m\u1EA1ch l\u1EA1c, m\xE0 l\xE0 t\u1EA1o ra m\u1ED9t phi\xEAn b\u1EA3n thu nh\u1ECF cung c\u1EA5p ch\u1EE9c n\u0103ng t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh nhanh ch\xF3ng cho c\xE1c nh\xE0 khoa h\u1ECDc d\u1EEF li\u1EC7u."),Le=m(),xs=o("p"),ss=h("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 t\u1EADp d\u1EEF li\u1EC7u s\u1EB5n s\xE0ng, h\xE3y thi\u1EBFt l\u1EADp m\xF4 h\xECnh!"),Fe=m(),w(jt.$$.fragment),Ge=m(),Yn=o("h2"),vn=o("a"),te=o("span"),w(es.$$.fragment),Zn=m(),se=o("span"),ya=h("Kh\u1EDFi t\u1EA1o m\xF4 h\xECnh m\u1EDBi"),Se=m(),Hn=o("p"),as=h("B\u01B0\u1EDBc \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta l\xE0 kh\u1EDFi ch\u1EA1y m\u1EDBi m\xF4 h\xECnh GPT-2. Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t c\u1EA5u h\xECnh cho m\xF4 h\xECnh c\u1EE7a m\xECnh nh\u01B0 cho m\xF4 h\xECnh GPT-2 nh\u1ECF, v\xEC v\u1EADy ch\xFAng ta t\u1EA3i c\u1EA5u h\xECnh \u0111\u1ECBnh s\u1EB5n, \u0111\u1EA3m b\u1EA3o r\u1EB1ng k\xEDch th\u01B0\u1EDBc tokenizer kh\u1EDBp v\u1EDBi k\xEDch th\u01B0\u1EDBc t\u1EEB v\u1EF1ng c\u1EE7a m\xF4 h\xECnh v\xE0 chuy\u1EC3n "),ee=o("code"),va=h("bos"),ba=h(" v\xE0 "),hs=o("code"),ka=h("eos"),$a=h(" (b\u1EAFt \u0111\u1EA7u v\xE0 cu\u1ED1i chu\u1ED7i) token ID:"),ph=m(),qt.c(),ja=m(),cs=o("p"),Qh=h("M\xF4 h\xECnh c\u1EE7a ch\xFAng ta c\xF3 124 tri\u1EC7u th\xF4ng s\u1ED1 m\xE0 ta s\u1EBD ph\u1EA3i \u0111i\u1EC1u ch\u1EC9nh. Tr\u01B0\u1EDBc khi c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n, ch\xFAng ta c\u1EA7n thi\u1EBFt l\u1EADp m\u1ED9t b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u s\u1EBD \u0111\u1EA3m nh\u1EADn vi\u1EC7c t\u1EA1o c\xE1c l\xF4. Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng tr\xECnh c\u1EAFt gh\xE9p "),Ua=o("code"),nc=h("DataCollatorForLanguageModeling"),tc=h(", \u0111\u01B0\u1EE3c thi\u1EBFt k\u1EBF \u0111\u1EB7c bi\u1EC7t cho m\xF4 h\xECnh ng\xF4n ng\u1EEF (nh\u01B0 t\xEAn g\u1ECDi g\u1EE3i \xFD m\u1ED9t c\xE1ch tinh t\u1EBF). B\xEAn c\u1EA1nh vi\u1EC7c x\u1EBFp ch\u1ED3ng v\xE0 \u0111\u1EC7m c\xE1c l\xF4, n\xF3 c\u0169ng \u0111\u1EA3m nh\u1EADn vi\u1EC7c t\u1EA1o c\xE1c nh\xE3n c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF - trong m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3, c\xE1c \u0111\u1EA7u v\xE0o c\u0169ng \u0111\xF3ng vai tr\xF2 l\xE0 nh\xE3n (ch\u1EC9 \u0111\u01B0\u1EE3c d\u1ECBch chuy\u1EC3n b\u1EDFi m\u1ED9t ph\u1EA7n t\u1EED) v\xE0 tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u n\xE0y t\u1EA1o ch\xFAng nhanh ch\xF3ng trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n, v\xEC v\u1EADy ch\xFAng t\xF4i ta kh\xF4ng c\u1EA7n sao ch\xE9p "),Xa=o("code"),sc=h("input_ids"),ec=h("."),uh=m(),ls=o("p"),ac=h("L\u01B0u \xFD r\u1EB1ng "),Wa=o("code"),hc=h("DataCollatorForLanguageModeling"),cc=h(" h\u1ED7 tr\u1EE3 c\u1EA3 m\xF4 h\xECnh h\xF3a ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i (MLM) v\xE0 m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 (CLM). Theo m\u1EB7c \u0111\u1ECBnh, n\xF3 chu\u1EA9n b\u1ECB d\u1EEF li\u1EC7u cho MLM, nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 chuy\u1EC3n sang CLM b\u1EB1ng c\xE1ch \u0111\u1EB7t \u0111\u1ED1i s\u1ED1 "),Ya=o("code"),lc=h("mlm=False"),rc=h(":"),mh=m(),Et.c(),wa=m(),qa=o("p"),ic=h("H\xE3y xem m\u1ED9t v\xED d\u1EE5:"),gh=m(),w(Be.$$.fragment),_h=m(),Ct.c(),xa=m(),Ea=o("p"),oc=h("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c v\xED d\u1EE5 \u0111\xE3 \u0111\u01B0\u1EE3c x\u1EBFp ch\u1ED3ng l\xEAn nhau v\xE0 t\u1EA5t c\u1EA3 c\xE1c tensor c\xF3 c\xF9ng h\xECnh d\u1EA1ng."),dh=m(),Z&&Z.c(),Ta=m(),w(ae.$$.fragment),fh=m(),Ca=o("p"),pc=h("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 m\u1ECDi th\u1EE9 \u0111\u1EC3 th\u1EF1c s\u1EF1 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh c\u1EE7a m\xECnh - \u0111\xF3 kh\xF4ng ph\u1EA3i l\xE0 qu\xE1 nhi\u1EC1u c\xF4ng vi\u1EC7c! Tr\u01B0\u1EDBc khi b\u1EAFt \u0111\u1EA7u luy\u1EC7n t\u1EADp, ch\xFAng ta n\xEAn \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face. N\u1EBFu b\u1EA1n \u0111ang l\xE0m vi\u1EC7c trong notebook, b\u1EA1n c\xF3 th\u1EC3 l\xE0m nh\u01B0 v\u1EADy v\u1EDBi h\xE0m ti\u1EC7n \xEDch sau:"),yh=m(),w(Ie.$$.fragment),vh=m(),za=o("p"),uc=h("Thao t\xE1c n\xE0y s\u1EBD hi\u1EC3n th\u1ECB m\u1ED9t ti\u1EC7n \xEDch m\xE0 b\u1EA1n c\xF3 th\u1EC3 nh\u1EADp th\xF4ng tin \u0111\u0103ng nh\u1EADp Hugging Face c\u1EE7a m\xECnh."),bh=m(),Pa=o("p"),mc=h("N\u1EBFu b\u1EA1n kh\xF4ng l\xE0m vi\u1EC7c trong notebook, ch\u1EC9 c\u1EA7n nh\u1EADp d\xF2ng sau v\xE0o thi\u1EBFt b\u1ECB \u0111\u1EA7u cu\u1ED1i c\u1EE7a b\u1EA1n:"),kh=m(),w(Ve.$$.fragment),$h=m(),Pt.c(),Da=m(),w(he.$$.fragment),jh=m(),w(ce.$$.fragment),wh=m(),Es=o("h2"),le=o("a"),Za=o("span"),w(Re.$$.fragment),gc=m(),Ja=o("span"),_c=h("T\u1EA1o m\xE3 v\u1EDBi m\u1ED9t pipeline"),qh=m(),re=o("p"),dc=h("B\xE2y gi\u1EDD l\xE0 th\u1EDDi \u0111i\u1EC3m c\u1EE7a s\u1EF1 th\u1EADt: ch\xFAng ta h\xE3y xem m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n th\u1EF1c s\u1EF1 ho\u1EA1t \u0111\u1ED9ng t\u1ED1t nh\u01B0 th\u1EBF n\xE0o! Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y trong nh\u1EADt k\xFD r\u1EB1ng m\u1EA5t m\xE1t \u0111\xE3 gi\u1EA3m \u0111\u1EC1u \u0111\u1EB7n, nh\u01B0ng \u0111\u1EC3 \u0111\u01B0a m\xF4 h\xECnh v\xE0o th\u1EED nghi\u1EC7m, ch\xFAng ta h\xE3y xem n\xF3 ho\u1EA1t \u0111\u1ED9ng t\u1ED1t nh\u01B0 th\u1EBF n\xE0o tr\xEAn m\u1ED9t s\u1ED1 l\u1EDDi nh\u1EAFc. \u0110\u1EC3 l\xE0m \u0111i\u1EC1u \u0111\xF3, ch\xFAng ta s\u1EBD bao b\u1ECDc m\xF4 h\xECnh trong m\u1ED9t "),Qa=o("code"),fc=h("pipeline"),yc=h(" t\u1EA1o v\u0103n b\u1EA3n v\xE0 ch\xFAng ta s\u1EBD \u0111\u01B0a n\xF3 v\xE0o GPU cho c\xE1c th\u1EBF h\u1EC7 nhanh n\u1EBFu c\xF3 s\u1EB5n:"),xh=m(),Ht.c(),Ha=m(),Aa=o("p"),vc=h("H\xE3y b\u1EAFt \u0111\u1EA7u v\u1EDBi t\xE1c v\u1EE5 \u0111\u01A1n gi\u1EA3n l\xE0 t\u1EA1o m\u1ED9t bi\u1EC3u \u0111\u1ED3 ph\xE2n t\xE1n:"),Eh=m(),w(Ke.$$.fragment),Th=m(),w(Ue.$$.fragment),Ch=m(),rs=o("p"),bc=h("K\u1EBFt qu\u1EA3 c\xF3 v\u1EBB ch\xEDnh x\xE1c. N\xF3 c\u0169ng ho\u1EA1t \u0111\u1ED9ng v\u1EDBi "),nh=o("code"),kc=h("pandas"),$c=h("? H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o m\u1ED9t "),th=o("code"),jc=h("DataFrame"),wc=h(" t\u1EEB hai m\u1EA3ng kh\xF4ng:"),zh=m(),w(Xe.$$.fragment),Ph=m(),w(We.$$.fragment),Dh=m(),Jn=o("p"),qc=h("Th\u1EADt tuy\u1EC7t, \u0111\xF3 l\xE0 c\xE2u tr\u1EA3 l\u1EDDi ch\xEDnh x\xE1c - m\u1EB7c d\xF9 sau \u0111\xF3 n\xF3 l\u1EA1i ch\xE8n th\xEAm c\u1ED9t "),sh=o("code"),xc=h("x"),Ec=h(". V\xEC s\u1ED1 l\u01B0\u1EE3ng token \u0111\u01B0\u1EE3c t\u1EA1o c\xF3 gi\u1EDBi h\u1EA1n, v\xF2ng l\u1EB7p "),eh=o("code"),Tc=h("for"),Cc=h(" sau \u0111\xE2y s\u1EBD b\u1ECB c\u1EAFt. H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u g\xEC \u0111\xF3 ph\u1EE9c t\u1EA1p h\u01A1n m\u1ED9t ch\xFAt v\xE0 \u0111\u1EC3 m\xF4 h\xECnh gi\xFAp ch\xFAng ta s\u1EED d\u1EE5ng ho\u1EA1t \u0111\u1ED9ng "),ah=o("code"),zc=h("groupby"),Pc=h(":"),Hh=m(),w(Ye.$$.fragment),Ah=m(),w(Ze.$$.fragment),Oh=m(),ie=o("p"),Dc=h("Kh\xF4ng t\u1EC7; \u0111\xF3 l\xE0 c\xE1ch l\xE0m \u0111\xFAng. Cu\u1ED1i c\xF9ng, h\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 cho "),hh=o("code"),Hc=h("scikit-learn"),Ac=h(" v\xE0 thi\u1EBFt l\u1EADp m\xF4 h\xECnh Random Forest hay kh\xF4ng:"),Nh=m(),w(Je.$$.fragment),Mh=m(),w(Qe.$$.fragment),Lh=m(),Ts.c(),Oa=m(),J&&J.c(),Na=Jh(),this.h()},l(n){const i=hr('[data-svelte="svelte-1phssyn"]',document.head);a=p(i,"META",{name:!0,content:!0}),i.forEach(t),_=g(n),q(r.$$.fragment,n),d=g(n),$=p(n,"H1",{class:!0});var ca=u($);b=p(ca,"A",{id:!0,class:!0,href:!0});var Ma=u(b);j=p(Ma,"SPAN",{});var ch=u(j);q(z.$$.fragment,ch),ch.forEach(t),Ma.forEach(t),T=g(ca),H=p(ca,"SPAN",{});var lh=u(H);M=c(lh,"THu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 t\u1EEB \u0111\u1EA7u"),lh.forEach(t),ca.forEach(t),L=g(n),P.l(n),F=g(n),S=p(n,"P",{});var Qn=u(S);G=c(Qn,"Cho \u0111\u1EBFn th\u1EDDi \u0111i\u1EC3m hi\u1EC7n t\u1EA1i, ch\xFAng ta ch\u1EE7 y\u1EBFu s\u1EED d\u1EE5ng c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\xE0 tinh ch\u1EC9nh ch\xFAng cho c\xE1c tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng m\u1EDBi b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EA1i c\xE1c tr\u1ECDng s\u1ED1 t\u1EEB hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc. Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),V=p(Qn,"A",{href:!0});var La=u(V);U=c(La,"Ch\u01B0\u01A1ng 1"),La.forEach(t),N=c(Qn,", \u0111i\u1EC1u n\xE0y th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),B=p(Qn,"EM",{});var Fa=u(B);I=c(Fa,"transfer learning"),Fa.forEach(t),W=c(Qn," hay "),X=p(Qn,"EM",{});var rh=u(X);Y=c(rh,"h\u1ECDc chuy\u1EC3n giao"),rh.forEach(t),Q=c(Qn,", v\xE0 \u0111\xF3 l\xE0 m\u1ED9t chi\u1EBFn l\u01B0\u1EE3c r\u1EA5t th\xE0nh c\xF4ng \u0111\u1EC3 \xE1p d\u1EE5ng c\xE1c m\xF4 h\xECnh Transformer cho h\u1EA7u h\u1EBFt c\xE1c tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng trong th\u1EBF gi\u1EDBi th\u1EF1c n\u01A1i d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n l\xE0 th\u01B0a th\u1EDBt. Trong ch\u01B0\u01A1ng n\xE0y, ch\xFAng ta s\u1EBD th\u1EF1c hi\u1EC7n m\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn kh\xE1c v\xE0 hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ho\xE0n to\xE0n m\u1EDBi t\u1EEB \u0111\u1EA7u. \u0110\xE2y l\xE0 m\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn t\u1ED1t \u0111\u1EC3 th\u1EF1c hi\u1EC7n n\u1EBFu b\u1EA1n c\xF3 nhi\u1EC1u d\u1EEF li\u1EC7u v\xE0 n\xF3 r\u1EA5t kh\xE1c v\u1EDBi d\u1EEF li\u1EC7u hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho c\xE1c m\xF4 h\xECnh c\xF3 s\u1EB5n. Tuy nhi\xEAn, n\xF3 c\u0169ng \u0111\xF2i h\u1ECFi nhi\u1EC1u t\xE0i nguy\xEAn m\xE1y t\xEDnh h\u01A1n \u0111\xE1ng k\u1EC3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF h\u01A1n l\xE0 ch\u1EC9 \u0111\u1EC3 tinh ch\u1EC9nh m\xF4 h\xECnh hi\u1EC7n c\xF3. C\xE1c v\xED d\u1EE5 c\xF3 th\u1EC3 c\xF3 \xFD ngh\u0129a khi hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh m\u1EDBi bao g\u1ED3m c\xE1c t\u1EADp d\u1EEF li\u1EC7u bao g\u1ED3m c\xE1c n\u1ED1t nh\u1EA1c, tr\xECnh t\u1EF1 ph\xE2n t\u1EED nh\u01B0 DNA ho\u1EB7c ng\xF4n ng\u1EEF l\u1EADp tr\xECnh. C\xF4ng c\u1EE5 th\u1EE9 hai g\u1EA7n \u0111\xE2y \u0111\xE3 \u0111\u1EA1t \u0111\u01B0\u1EE3c s\u1EE9c h\xFAt nh\u1EDD c\xE1c c\xF4ng c\u1EE5 nh\u01B0 TabNine v\xE0 GitHub\u2019s Copilot, \u0111\u01B0\u1EE3c h\u1ED7 tr\u1EE3 b\u1EDFi m\xF4 h\xECnh Codex c\u1EE7a OpenAI, c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c chu\u1ED7i m\xE3 d\xE0i. T\xE1c v\u1EE5 t\u1EA1o v\u0103n b\u1EA3n n\xE0y \u0111\u01B0\u1EE3c gi\u1EA3i quy\u1EBFt t\u1ED1t nh\u1EA5t v\u1EDBi c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy ho\u1EB7c nh\xE2n qu\u1EA3 nh\u01B0 GPT-2."),Qn.forEach(t),cn=g(n),C=p(n,"P",{});var wn=u(C);K=c(wn,"Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD x\xE2y d\u1EF1ng m\u1ED9t phi\xEAn b\u1EA3n thu nh\u1ECF c\u1EE7a m\xF4 h\xECnh t\u1EA1o m\xE3: ch\xFAng ta s\u1EBD t\u1EADp trung v\xE0o c\xE1c ho\xE0n th\xE0nh m\u1ED9t d\xF2ng thay v\xEC c\xE1c h\xE0m ho\u1EB7c l\u1EDBp \u0111\u1EA7y \u0111\u1EE7, s\u1EED d\u1EE5ng m\u1ED9t t\u1EADp h\u1EE3p con m\xE3 Python. Khi l\xE0m vi\u1EC7c v\u1EDBi d\u1EEF li\u1EC7u b\u1EB1ng Python, b\u1EA1n th\u01B0\u1EDDng xuy\xEAn ti\u1EBFp x\xFAc v\u1EDBi b\u1ED9 khoa h\u1ECDc d\u1EEF li\u1EC7u Python, bao g\u1ED3m c\xE1c th\u01B0 vi\u1EC7n "),An=p(wn,"CODE",{});var ih=u(An);On=c(ih,"matplotlib"),ih.forEach(t),At=c(wn,", "),bn=p(wn,"CODE",{});var oh=u(bn);Nn=c(oh,"seaborn"),oh.forEach(t),Ot=c(wn,", "),is=p(wn,"CODE",{});var Ga=u(is);os=c(Ga,"pandas"),Ga.forEach(t),fe=c(wn," v\xE0 "),ps=p(wn,"CODE",{});var Yc=u(ps);us=c(Yc,"scikit-learn"),Yc.forEach(t),ye=c(wn,". Khi s\u1EED d\u1EE5ng ch\xFAng, th\xF4ng th\u01B0\u1EDDng c\u1EA7n ph\u1EA3i tra c\u1EE9u c\xE1c l\u1EC7nh c\u1EE5 th\u1EC3, v\xEC v\u1EADy s\u1EBD r\u1EA5t tuy\u1EC7t n\u1EBFu ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng m\u1ED9t m\xF4 h\xECnh \u0111\u1EC3 ho\xE0n th\xE0nh c\xE1c l\u1EC7nh g\u1ECDi n\xE0y cho ch\xFAng ta."),wn.forEach(t),Cs=g(n),q(Mn.$$.fragment,n),Ln=g(n),rn=p(n,"P",{});var Sa=u(rn);nt=c(Sa,"Trong "),tt=p(Sa,"A",{href:!0});var Zc=u(tt);Nt=c(Zc,"Ch\u01B0\u01A1ng 6"),Zc.forEach(t),ve=c(Sa,", ch\xFAng ta \u0111\xE3 t\u1EA1o m\u1ED9t tr\xECnh tokenize hi\u1EC7u qu\u1EA3 \u0111\u1EC3 x\u1EED l\xFD m\xE3 ngu\u1ED3n Python, nh\u01B0ng nh\u1EEFng g\xEC ch\xFAng ta v\u1EABn c\u1EA7n l\xE0 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u quy m\xF4 l\u1EDBn \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh. \u1EDE \u0111\xE2y, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng tokenizer cho m\u1ED9t kho l\u01B0u tr\u1EEF m\xE3 Python c\xF3 ngu\u1ED3n g\u1ED1c t\u1EEB kho l\u01B0u tr\u1EEF GitHub. Sau \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng API "),Mt=p(Sa,"CODE",{});var Jc=u(Mt);st=c(Jc,"Trainer"),Jc.forEach(t),zs=c(Sa," v\xE0 \u{1F917} Accelerate \u0111\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh. Ch\xFAng ta h\xE3y \u0111i \u0111\u1EBFn \u0111\xF3!"),Sa.forEach(t),et=g(n),sn=p(n,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),u(sn).forEach(t),on=g(n),kn=p(n,"P",{});var Sh=u(kn);ke=c(Sh,"\u0110\xE2y th\u1EF1c s\u1EF1 c\xE1ch m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n v\xE0 t\u1EA3i l\xEAn Hub b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng m\xE3 \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong ph\u1EA7n n\xE0y. B\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y n\xF3 "),at=p(Sh,"A",{href:!0,rel:!0});var Qc=u(at);ms=c(Qc,"t\u1EA1i \u0111\xE2y"),Qc.forEach(t),$e=c(Sh,". L\u01B0u \xFD r\u1EB1ng v\xEC c\xF3 m\u1ED9t s\u1ED1 ng\u1EABu nhi\xEAn x\u1EA3y ra trong qu\xE1 tr\xECnh t\u1EA1o v\u0103n b\u1EA3n, b\u1EA1n c\xF3 th\u1EC3 s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t k\u1EBFt qu\u1EA3 h\u01A1i kh\xE1c."),Sh.forEach(t),Ps=g(n),pn=p(n,"H2",{class:!0});var Bh=u(pn);Fn=p(Bh,"A",{id:!0,class:!0,href:!0});var nl=u(Fn);Lt=p(nl,"SPAN",{});var tl=u(Lt);q(nn.$$.fragment,tl),tl.forEach(t),nl.forEach(t),je=g(Bh),ht=p(Bh,"SPAN",{});var sl=u(ht);we=c(sl,"Thu th\u1EADp d\u1EEF li\u1EC7u"),sl.forEach(t),Bh.forEach(t),Ds=g(n),tn=p(n,"P",{});var oe=u(tn);qe=c(oe,"M\xE3 Python c\xF3 s\u1EB5n r\u1EA5t nhi\u1EC1u t\u1EEB c\xE1c kho m\xE3 nh\u01B0 GitHub, m\xE0 ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111\u1EC3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u b\u1EB1ng c\xE1ch \u0111\xE0o m\u1ECDi kho l\u01B0u tr\u1EEF Python. \u0110\xE2y l\xE0 ph\u01B0\u01A1ng ph\xE1p \u0111\u01B0\u1EE3c th\u1EF1c hi\u1EC7n trong "),ct=p(oe,"A",{href:!0,rel:!0});var el=u(ct);Hs=c(el,"s\xE1ch gi\xE1o khoa v\u1EC1 Transformers"),el.forEach(t),lt=c(oe," \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\u1ED9t m\xF4 h\xECnh GPT-2 l\u1EDBn. S\u1EED d\u1EE5ng k\u1EBFt xu\u1EA5t GitHub kho\u1EA3ng 180 GB ch\u1EE9a kho\u1EA3ng 20 tri\u1EC7u t\u1EC7p Python c\xF3 t\xEAn l\xE0 "),Ft=p(oe,"CODE",{});var al=u(Ft);Gt=c(al,"codeparrot"),al.forEach(t),xe=c(oe,", c\xE1c t\xE1c gi\u1EA3 \u0111\xE3 x\xE2y d\u1EF1ng m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u m\xE0 sau \u0111\xF3 h\u1ECD chia s\u1EBB tr\xEAn "),Gn=p(oe,"A",{href:!0,rel:!0});var hl=u(Gn);rt=c(hl,"Hugging Face Hub"),hl.forEach(t),As=c(oe," ."),oe.forEach(t),it=g(n),Sn=p(n,"P",{});var Ih=u(Sn);Os=c(Ih,"Tuy nhi\xEAn, vi\u1EC7c hu\u1EA5n luy\u1EC7n tr\xEAn to\xE0n b\u1ED9 ng\u1EEF li\u1EC7u n\xE0y t\u1ED1n nhi\u1EC1u th\u1EDDi gian v\xE0 t\xEDnh to\xE1n, v\xE0 ch\xFAng ta ch\u1EC9 c\u1EA7n t\u1EADp con c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u li\xEAn quan \u0111\u1EBFn ng\u0103n x\u1EBFp khoa h\u1ECDc d\u1EEF li\u1EC7u Python. V\xEC v\u1EADy, h\xE3y b\u1EAFt \u0111\u1EA7u b\u1EB1ng c\xE1ch l\u1ECDc t\u1EADp d\u1EEF li\u1EC7u "),qn=p(Ih,"CODE",{});var cl=u(qn);Ns=c(cl,"codeparrot"),cl.forEach(t),$n=c(Ih," cho t\u1EA5t c\u1EA3 c\xE1c t\u1EC7p bao g\u1ED3m b\u1EA5t k\u1EF3 th\u01B0 vi\u1EC7n n\xE0o trong ng\u0103n x\u1EBFp n\xE0y. Do k\xEDch th\u01B0\u1EDBc c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta mu\u1ED1n tr\xE1nh t\u1EA3i n\xF3 xu\u1ED1ng; thay v\xE0o \u0111\xF3, ta s\u1EBD s\u1EED d\u1EE5ng t\xEDnh n\u0103ng ph\xE1t tr\u1EF1c tuy\u1EBFn \u0111\u1EC3 l\u1ECDc n\xF3 m\u1ED9t c\xE1ch nhanh ch\xF3ng. \u0110\u1EC3 gi\xFAp ch\xFAng ta l\u1ECDc c\xE1c m\u1EABu m\xE3 b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xE1c th\u01B0 vi\u1EC7n \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3, ta s\u1EBD s\u1EED d\u1EE5ng h\xE0m sau:"),Ih.forEach(t),Ms=g(n),q(xn.$$.fragment,n),Ls=g(n),St=p(n,"P",{});var ll=u(St);Bt=c(ll,"H\xE3y ki\u1EC3m tra n\xF3 tr\xEAn hai v\xED d\u1EE5:"),ll.forEach(t),Fs=g(n),q(ot.$$.fragment,n),gs=g(n),q(un.$$.fragment,n),_s=g(n),En=p(n,"P",{});var rl=u(En);Ee=c(rl,"Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111i\u1EC1u n\xE0y \u0111\u1EC3 t\u1EA1o m\u1ED9t h\xE0m s\u1EBD truy\u1EC1n tr\u1EF1c tuy\u1EBFn t\u1EADp d\u1EEF li\u1EC7u v\xE0 l\u1ECDc c\xE1c ph\u1EA7n t\u1EED ta mu\u1ED1n:"),rl.forEach(t),ds=g(n),q(mn.$$.fragment,n),fs=g(n),Tn=p(n,"P",{});var il=u(Tn);Te=c(il,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 c\u1EA7n \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng n\xE0y cho t\u1EADp d\u1EEF li\u1EC7u ph\xE1t tr\u1EF1c tuy\u1EBFn:"),il.forEach(t),ys=g(n),q(gn.$$.fragment,n),vs=g(n),q(_n.$$.fragment,n),bs=g(n),en=p(n,"P",{});var ol=u(en);Ce=c(ol,"\u0110i\u1EC1u n\xE0y \u0111\u1EC3 l\u1EA1i cho ch\xFAng ta kho\u1EA3ng 3% t\u1EADp d\u1EEF li\u1EC7u ban \u0111\u1EA7u, v\u1EABn c\xF2n kh\xE1 l\u1EDBn - t\u1EADp d\u1EEF li\u1EC7u k\u1EBFt qu\u1EA3 l\xE0 6GB v\xE0 bao g\u1ED3m 600,000 t\u1EADp l\u1EC7nh Python!"),ol.forEach(t),It=g(n),Vt=p(n,"P",{});var pl=u(Vt);ze=c(pl,"Vi\u1EC7c l\u1ECDc to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\xF3 th\u1EC3 m\u1EA5t 2-3 gi\u1EDD t\xF9y thu\u1ED9c v\xE0o m\xE1y v\xE0 b\u0103ng th\xF4ng c\u1EE7a b\u1EA1n. N\u1EBFu b\u1EA1n kh\xF4ng mu\u1ED1n t\u1EF1 m\xECnh tr\u1EA3i qua qu\xE1 tr\xECnh k\xE9o d\xE0i n\xE0y, ch\xFAng ta cung c\u1EA5p t\u1EADp d\u1EEF li\u1EC7u \u0111\xE3 l\u1ECDc tr\xEAn Hub \u0111\u1EC3 b\u1EA1n t\u1EA3i xu\u1ED1ng:"),pl.forEach(t),Rt=g(n),q(pt.$$.fragment,n),Gs=g(n),q(Bn.$$.fragment,n),In=g(n),q(Cn.$$.fragment,n),an=g(n),Kt=p(n,"P",{});var ul=u(Kt);ks=c(ul,"H\xE3y xem m\u1ED9t v\xED d\u1EE5 t\u1EEB t\u1EADp d\u1EEF li\u1EC7u. Ch\xFAng ta s\u1EBD ch\u1EC9 hi\u1EC3n th\u1ECB 200 k\xFD t\u1EF1 \u0111\u1EA7u ti\xEAn c\u1EE7a m\u1ED7i tr\u01B0\u1EDDng:"),ul.forEach(t),Ss=g(n),q(ut.$$.fragment,n),Ut=g(n),q(mt.$$.fragment,n),Bs=g(n),jn=p(n,"P",{});var Vh=u(jn);Pe=c(Vh,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng tr\u01B0\u1EDDng "),$s=p(Vh,"CODE",{});var ml=u($s);Is=c(ml,"content"),ml.forEach(t),gt=c(Vh," ch\u1EE9a m\xE3 m\xE0 ch\xFAng ta mu\u1ED1n m\xF4 h\xECnh c\u1EE7a m\xECnh hu\u1EA5n luy\u1EC7n. B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta c\u1EA7n chu\u1EA9n b\u1ECB c\xE1c v\u0103n b\u1EA3n \u0111\u1EC3 ch\xFAng c\xF3 \u0111\u1ECBnh d\u1EA1ng ph\xF9 h\u1EE3p \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc."),Vh.forEach(t),js=g(n),hn=p(n,"H2",{class:!0});var Rh=u(hn);zn=p(Rh,"A",{id:!0,class:!0,href:!0});var gl=u(zn);Vn=p(gl,"SPAN",{});var _l=u(Vn);q(_t.$$.fragment,_l),_l.forEach(t),gl.forEach(t),Vs=g(Rh),Pn=p(Rh,"SPAN",{});var dl=u(Pn);Rs=c(dl,"Chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"),dl.forEach(t),Rh.forEach(t),dn=g(n),q(dt.$$.fragment,n),Xt=g(n),Wt=p(n,"P",{});var fl=u(Wt);De=c(fl,"B\u01B0\u1EDBc \u0111\u1EA7u ti\xEAn s\u1EBD l\xE0 tokenize d\u1EEF li\u1EC7u \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 hu\u1EA5n luy\u1EC7n. V\xEC m\u1EE5c ti\xEAu c\u1EE7a ch\xFAng ta ch\u1EE7 y\u1EBFu l\xE0 t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh c\xE1c l\u1EC7nh g\u1ECDi h\xE0m ng\u1EAFn, ch\xFAng ta c\xF3 th\u1EC3 gi\u1EEF k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh t\u01B0\u01A1ng \u0111\u1ED1i nh\u1ECF. \u0110i\u1EC1u n\xE0y c\xF3 l\u1EE3i \xEDch l\xE0 ch\xFAng ta c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh nhanh h\u01A1n nhi\u1EC1u v\xE0 n\xF3 c\u1EA7n \xEDt b\u1ED9 nh\u1EDB h\u01A1n \u0111\xE1ng k\u1EC3. N\u1EBFu \u0111i\u1EC1u quan tr\u1ECDng l\xE0 \u1EE9ng d\u1EE5ng c\u1EE7a b\u1EA1n ph\u1EA3i c\xF3 nhi\u1EC1u ng\u1EEF c\u1EA3nh h\u01A1n (v\xED d\u1EE5: n\u1EBFu b\u1EA1n mu\u1ED1n m\xF4 h\xECnh vi\u1EBFt c\xE1c b\xE0i ki\u1EC3m tra \u0111\u01A1n v\u1ECB d\u1EF1a tr\xEAn t\u1EC7p c\xF3 \u0111\u1ECBnh ngh\u0129a h\xE0m), h\xE3y \u0111\u1EA3m b\u1EA3o b\u1EA1n t\u0103ng con s\u1ED1 \u0111\xF3, nh\u01B0ng c\u0169ng l\u01B0u \xFD r\u1EB1ng \u0111i\u1EC1u n\xE0y \u0111i k\xE8m v\u1EDBi b\u1ED9 nh\u1EDB GPU l\u1EDBn h\u01A1n. Hi\u1EC7n t\u1EA1i, h\xE3y s\u1EEDa k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh \u1EDF 128 token, tr\xE1i ng\u01B0\u1EE3c v\u1EDBi 1,024 ho\u1EB7c 2,048 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng trong GPT-2 ho\u1EB7c GPT-3, t\u01B0\u01A1ng \u1EE9ng."),fl.forEach(t),Yt=g(n),ln=p(n,"P",{});var pe=u(ln);He=c(pe,"H\u1EA7u h\u1EBFt c\xE1c t\xE0i li\u1EC7u ch\u1EE9a nhi\u1EC1u h\u01A1n 128 token, v\xEC v\u1EADy ch\u1EC9 c\u1EA7n c\u1EAFt b\u1EDBt \u0111\u1EA7u v\xE0o \u0111\u1EBFn \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a s\u1EBD lo\u1EA1i b\u1ECF m\u1ED9t ph\u1EA7n l\u1EDBn t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh. Thay v\xE0o \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Zt=p(pe,"CODE",{});var yl=u(Zt);Jt=c(yl,"return_overflowing_tokens"),yl.forEach(t),Ae=c(pe," \u0111\u1EC3 token to\xE0n b\u1ED9 \u0111\u1EA7u v\xE0o v\xE0 chia n\xF3 th\xE0nh nhi\u1EC1u ph\u1EA7n, nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m trong "),ft=p(pe,"A",{href:!0});var vl=u(ft);yt=c(vl,"Ch\u01B0\u01A1ng 6"),vl.forEach(t),Ks=c(pe,". Ch\xFAng ta c\u0169ng s\u1EBD s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Dn=p(pe,"CODE",{});var bl=u(Dn);Us=c(bl,"return_length"),bl.forEach(t),Rn=c(pe," \u0111\u1EC3 t\u1EF1 \u0111\u1ED9ng tr\u1EA3 v\u1EC1 \u0111\u1ED9 d\xE0i c\u1EE7a m\u1ED7i \u0111o\u1EA1n \u0111\u01B0\u1EE3c t\u1EA1o. Th\u01B0\u1EDDng th\xEC ph\u1EA7n cu\u1ED1i c\xF9ng s\u1EBD nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh v\xE0 ch\xFAng ta s\u1EBD lo\u1EA1i b\u1ECF nh\u1EEFng ph\u1EA7n n\xE0y \u0111\u1EC3 tr\xE1nh c\xE1c v\u1EA5n \u0111\u1EC1 v\u1EC1 ph\u1EA7n \u0111\u1EC7m; ch\xFAng ta kh\xF4ng th\u1EF1c s\u1EF1 c\u1EA7n ch\xFAng v\xEC d\xF9 sao ch\xFAng ta c\u0169ng c\xF3 nhi\u1EC1u d\u1EEF li\u1EC7u."),pe.forEach(t),Xs=g(n),fn=p(n,"DIV",{class:!0});var Kh=u(fn);vt=p(Kh,"IMG",{class:!0,src:!0,alt:!0}),Ws=g(Kh),yn=p(Kh,"IMG",{class:!0,src:!0,alt:!0}),Kh.forEach(t),bt=g(n),Qt=p(n,"P",{});var kl=u(Qt);Ys=c(kl,"H\xE3y xem ch\xEDnh x\xE1c c\xE1ch th\u1EE9c ho\u1EA1t \u0111\u1ED9ng c\u1EE7a \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch xem hai v\xED d\u1EE5 \u0111\u1EA7u ti\xEAn:"),kl.forEach(t),Kn=g(n),q(Un.$$.fragment,n),Xn=g(n),q(Wn.$$.fragment,n),e=g(n),k=p(n,"P",{});var Uh=u(k);ra=c(Uh,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng ch\xFAng ta nh\u1EADn \u0111\u01B0\u1EE3c t\u1ED5ng c\u1ED9ng 34 ph\xE2n \u0111o\u1EA1n t\u1EEB hai v\xED d\u1EE5 \u0111\xF3. Nh\xECn v\xE0o \u0111\u1ED9 d\xE0i ph\xE2n \u0111o\u1EA1n, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c \u0111o\u1EA1n \u1EDF cu\u1ED1i c\u1EA3 hai t\xE0i li\u1EC7u c\xF3 \xEDt h\u01A1n 128 token (t\u01B0\u01A1ng \u1EE9ng l\xE0 117 v\xE0 41). Ch\xFAng ch\u1EC9 \u0111\u1EA1i di\u1EC7n cho m\u1ED9t ph\u1EA7n nh\u1ECF trong t\u1ED5ng s\u1ED1 c\xE1c kh\u1ED1i m\xE0 ch\xFAng ta c\xF3, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 v\u1EE9t ch\xFAng \u0111i m\u1ED9t c\xE1ch an to\xE0n. V\u1EDBi tr\u01B0\u1EDDng "),Zs=p(Uh,"CODE",{});var $l=u(Zs);ia=c($l,"overflow_to_sample_mapping"),$l.forEach(t),Js=c(Uh,", ch\xFAng ta c\u0169ng c\xF3 th\u1EC3 t\u1EA1o l\u1EA1i c\xE1c ph\u1EA7n thu\u1ED9c v\u1EC1 m\u1EABu \u0111\u1EA7u v\xE0o n\xE0o."),Uh.forEach(t),Ne=g(n),R=p(n,"P",{});var ue=u(R);oa=c(ue,"V\u1EDBi thao t\xE1c n\xE0y, ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng m\u1ED9t t\xEDnh n\u0103ng ti\u1EC7n d\u1EE5ng c\u1EE7a h\xE0m "),Qs=p(ue,"CODE",{});var jl=u(Qs);pa=c(jl,"Dataset.map()"),jl.forEach(t),ua=c(ue," trong \u{1F917} Datasets, \u0111\xF3 l\xE0 n\xF3 kh\xF4ng y\xEAu c\u1EA7u \xE1nh x\u1EA1 1-1; nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ws=p(ue,"A",{href:!0});var wl=u(ws);ma=c(wl,"ph\u1EA7n 3"),wl.forEach(t),ga=c(ue,", ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o c\xE1c l\xF4 c\xF3 nhi\u1EC1u ph\u1EA7n t\u1EED h\u01A1n ho\u1EB7c \xEDt h\u01A1n l\xF4 \u0111\u1EA7u v\xE0o. \u0110i\u1EC1u n\xE0y r\u1EA5t h\u1EEFu \xEDch khi th\u1EF1c hi\u1EC7n c\xE1c ho\u1EA1t \u0111\u1ED9ng nh\u01B0 t\u0103ng d\u1EEF li\u1EC7u ho\u1EB7c l\u1ECDc d\u1EEF li\u1EC7u l\xE0m thay \u0111\u1ED5i s\u1ED1 l\u01B0\u1EE3ng ph\u1EA7n t\u1EED. Trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta, khi tokenize m\u1ED7i ph\u1EA7n t\u1EED th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh \u0111\u01B0\u1EE3c ch\u1EC9 \u0111\u1ECBnh, ch\xFAng ta t\u1EA1o nhi\u1EC1u m\u1EABu t\u1EEB m\u1ED7i t\xE0i li\u1EC7u. Ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EA3m b\u1EA3o x\xF3a c\xE1c c\u1ED9t hi\u1EC7n c\xF3, v\xEC ch\xFAng c\xF3 k\xEDch th\u01B0\u1EDBc xung \u0111\u1ED9t. N\u1EBFu ch\xFAng ta mu\u1ED1n gi\u1EEF ch\xFAng, ch\xFAng ta c\xF3 th\u1EC3 l\u1EB7p l\u1EA1i ch\xFAng m\u1ED9t c\xE1ch th\xEDch h\u1EE3p v\xE0 tr\u1EA3 l\u1EA1i ch\xFAng trong l\u1EC7nh g\u1ECDi "),ne=p(ue,"CODE",{});var ql=u(ne);_a=c(ql,"Dataset.map()"),ql.forEach(t),da=c(ue,":"),ue.forEach(t),kt=g(n),q(ns.$$.fragment,n),Me=g(n),q(ts.$$.fragment,n),$t=g(n),qs=p(n,"P",{});var xl=u(qs);fa=c(xl,"Hi\u1EC7n ch\xFAng ta c\xF3 16,7 tri\u1EC7u v\xED d\u1EE5 v\u1EDBi 128 token m\u1ED7i v\xED d\u1EE5, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi t\u1ED5ng c\u1ED9ng kho\u1EA3ng 2,1 t\u1EF7 token. \u0110\u1EC3 tham kh\u1EA3o, c\xE1c m\xF4 h\xECnh GPT-3 v\xE0 Codex c\u1EE7a OpenAI \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\xEAn 300 v\xE0 100 t\u1EF7 token t\u01B0\u01A1ng \u1EE9ng, trong \u0111\xF3 c\xE1c m\xF4 h\xECnh Codex \u0111\u01B0\u1EE3c kh\u1EDFi t\u1EA1o t\u1EEB c\xE1c checkpoint GPT-3. M\u1EE5c ti\xEAu c\u1EE7a ch\xFAng ta trong ph\u1EA7n n\xE0y kh\xF4ng ph\u1EA3i l\xE0 c\u1EA1nh tranh v\u1EDBi c\xE1c m\xF4 h\xECnh n\xE0y, c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c v\u0103n b\u1EA3n d\xE0i, m\u1EA1ch l\u1EA1c, m\xE0 l\xE0 t\u1EA1o ra m\u1ED9t phi\xEAn b\u1EA3n thu nh\u1ECF cung c\u1EA5p ch\u1EE9c n\u0103ng t\u1EF1 \u0111\u1ED9ng ho\xE0n th\xE0nh nhanh ch\xF3ng cho c\xE1c nh\xE0 khoa h\u1ECDc d\u1EEF li\u1EC7u."),xl.forEach(t),Le=g(n),xs=p(n,"P",{});var El=u(xs);ss=c(El,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 t\u1EADp d\u1EEF li\u1EC7u s\u1EB5n s\xE0ng, h\xE3y thi\u1EBFt l\u1EADp m\xF4 h\xECnh!"),El.forEach(t),Fe=g(n),q(jt.$$.fragment,n),Ge=g(n),Yn=p(n,"H2",{class:!0});var Xh=u(Yn);vn=p(Xh,"A",{id:!0,class:!0,href:!0});var Tl=u(vn);te=p(Tl,"SPAN",{});var Cl=u(te);q(es.$$.fragment,Cl),Cl.forEach(t),Tl.forEach(t),Zn=g(Xh),se=p(Xh,"SPAN",{});var zl=u(se);ya=c(zl,"Kh\u1EDFi t\u1EA1o m\xF4 h\xECnh m\u1EDBi"),zl.forEach(t),Xh.forEach(t),Se=g(n),Hn=p(n,"P",{});var Ba=u(Hn);as=c(Ba,"B\u01B0\u1EDBc \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta l\xE0 kh\u1EDFi ch\u1EA1y m\u1EDBi m\xF4 h\xECnh GPT-2. Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t c\u1EA5u h\xECnh cho m\xF4 h\xECnh c\u1EE7a m\xECnh nh\u01B0 cho m\xF4 h\xECnh GPT-2 nh\u1ECF, v\xEC v\u1EADy ch\xFAng ta t\u1EA3i c\u1EA5u h\xECnh \u0111\u1ECBnh s\u1EB5n, \u0111\u1EA3m b\u1EA3o r\u1EB1ng k\xEDch th\u01B0\u1EDBc tokenizer kh\u1EDBp v\u1EDBi k\xEDch th\u01B0\u1EDBc t\u1EEB v\u1EF1ng c\u1EE7a m\xF4 h\xECnh v\xE0 chuy\u1EC3n "),ee=p(Ba,"CODE",{});var Pl=u(ee);va=c(Pl,"bos"),Pl.forEach(t),ba=c(Ba," v\xE0 "),hs=p(Ba,"CODE",{});var Dl=u(hs);ka=c(Dl,"eos"),Dl.forEach(t),$a=c(Ba," (b\u1EAFt \u0111\u1EA7u v\xE0 cu\u1ED1i chu\u1ED7i) token ID:"),Ba.forEach(t),ph=g(n),qt.l(n),ja=g(n),cs=p(n,"P",{});var Ia=u(cs);Qh=c(Ia,"M\xF4 h\xECnh c\u1EE7a ch\xFAng ta c\xF3 124 tri\u1EC7u th\xF4ng s\u1ED1 m\xE0 ta s\u1EBD ph\u1EA3i \u0111i\u1EC1u ch\u1EC9nh. Tr\u01B0\u1EDBc khi c\xF3 th\u1EC3 b\u1EAFt \u0111\u1EA7u hu\u1EA5n luy\u1EC7n, ch\xFAng ta c\u1EA7n thi\u1EBFt l\u1EADp m\u1ED9t b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u s\u1EBD \u0111\u1EA3m nh\u1EADn vi\u1EC7c t\u1EA1o c\xE1c l\xF4. Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng tr\xECnh c\u1EAFt gh\xE9p "),Ua=p(Ia,"CODE",{});var Hl=u(Ua);nc=c(Hl,"DataCollatorForLanguageModeling"),Hl.forEach(t),tc=c(Ia,", \u0111\u01B0\u1EE3c thi\u1EBFt k\u1EBF \u0111\u1EB7c bi\u1EC7t cho m\xF4 h\xECnh ng\xF4n ng\u1EEF (nh\u01B0 t\xEAn g\u1ECDi g\u1EE3i \xFD m\u1ED9t c\xE1ch tinh t\u1EBF). B\xEAn c\u1EA1nh vi\u1EC7c x\u1EBFp ch\u1ED3ng v\xE0 \u0111\u1EC7m c\xE1c l\xF4, n\xF3 c\u0169ng \u0111\u1EA3m nh\u1EADn vi\u1EC7c t\u1EA1o c\xE1c nh\xE3n c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF - trong m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3, c\xE1c \u0111\u1EA7u v\xE0o c\u0169ng \u0111\xF3ng vai tr\xF2 l\xE0 nh\xE3n (ch\u1EC9 \u0111\u01B0\u1EE3c d\u1ECBch chuy\u1EC3n b\u1EDFi m\u1ED9t ph\u1EA7n t\u1EED) v\xE0 tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u n\xE0y t\u1EA1o ch\xFAng nhanh ch\xF3ng trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n, v\xEC v\u1EADy ch\xFAng t\xF4i ta kh\xF4ng c\u1EA7n sao ch\xE9p "),Xa=p(Ia,"CODE",{});var Al=u(Xa);sc=c(Al,"input_ids"),Al.forEach(t),ec=c(Ia,"."),Ia.forEach(t),uh=g(n),ls=p(n,"P",{});var Va=u(ls);ac=c(Va,"L\u01B0u \xFD r\u1EB1ng "),Wa=p(Va,"CODE",{});var Ol=u(Wa);hc=c(Ol,"DataCollatorForLanguageModeling"),Ol.forEach(t),cc=c(Va," h\u1ED7 tr\u1EE3 c\u1EA3 m\xF4 h\xECnh h\xF3a ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i (MLM) v\xE0 m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 (CLM). Theo m\u1EB7c \u0111\u1ECBnh, n\xF3 chu\u1EA9n b\u1ECB d\u1EEF li\u1EC7u cho MLM, nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 chuy\u1EC3n sang CLM b\u1EB1ng c\xE1ch \u0111\u1EB7t \u0111\u1ED1i s\u1ED1 "),Ya=p(Va,"CODE",{});var Nl=u(Ya);lc=c(Nl,"mlm=False"),Nl.forEach(t),rc=c(Va,":"),Va.forEach(t),mh=g(n),Et.l(n),wa=g(n),qa=p(n,"P",{});var Ml=u(qa);ic=c(Ml,"H\xE3y xem m\u1ED9t v\xED d\u1EE5:"),Ml.forEach(t),gh=g(n),q(Be.$$.fragment,n),_h=g(n),Ct.l(n),xa=g(n),Ea=p(n,"P",{});var Ll=u(Ea);oc=c(Ll,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c v\xED d\u1EE5 \u0111\xE3 \u0111\u01B0\u1EE3c x\u1EBFp ch\u1ED3ng l\xEAn nhau v\xE0 t\u1EA5t c\u1EA3 c\xE1c tensor c\xF3 c\xF9ng h\xECnh d\u1EA1ng."),Ll.forEach(t),dh=g(n),Z&&Z.l(n),Ta=g(n),q(ae.$$.fragment,n),fh=g(n),Ca=p(n,"P",{});var Fl=u(Ca);pc=c(Fl,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 m\u1ECDi th\u1EE9 \u0111\u1EC3 th\u1EF1c s\u1EF1 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh c\u1EE7a m\xECnh - \u0111\xF3 kh\xF4ng ph\u1EA3i l\xE0 qu\xE1 nhi\u1EC1u c\xF4ng vi\u1EC7c! Tr\u01B0\u1EDBc khi b\u1EAFt \u0111\u1EA7u luy\u1EC7n t\u1EADp, ch\xFAng ta n\xEAn \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face. N\u1EBFu b\u1EA1n \u0111ang l\xE0m vi\u1EC7c trong notebook, b\u1EA1n c\xF3 th\u1EC3 l\xE0m nh\u01B0 v\u1EADy v\u1EDBi h\xE0m ti\u1EC7n \xEDch sau:"),Fl.forEach(t),yh=g(n),q(Ie.$$.fragment,n),vh=g(n),za=p(n,"P",{});var Gl=u(za);uc=c(Gl,"Thao t\xE1c n\xE0y s\u1EBD hi\u1EC3n th\u1ECB m\u1ED9t ti\u1EC7n \xEDch m\xE0 b\u1EA1n c\xF3 th\u1EC3 nh\u1EADp th\xF4ng tin \u0111\u0103ng nh\u1EADp Hugging Face c\u1EE7a m\xECnh."),Gl.forEach(t),bh=g(n),Pa=p(n,"P",{});var Sl=u(Pa);mc=c(Sl,"N\u1EBFu b\u1EA1n kh\xF4ng l\xE0m vi\u1EC7c trong notebook, ch\u1EC9 c\u1EA7n nh\u1EADp d\xF2ng sau v\xE0o thi\u1EBFt b\u1ECB \u0111\u1EA7u cu\u1ED1i c\u1EE7a b\u1EA1n:"),Sl.forEach(t),kh=g(n),q(Ve.$$.fragment,n),$h=g(n),Pt.l(n),Da=g(n),q(he.$$.fragment,n),jh=g(n),q(ce.$$.fragment,n),wh=g(n),Es=p(n,"H2",{class:!0});var Wh=u(Es);le=p(Wh,"A",{id:!0,class:!0,href:!0});var Bl=u(le);Za=p(Bl,"SPAN",{});var Il=u(Za);q(Re.$$.fragment,Il),Il.forEach(t),Bl.forEach(t),gc=g(Wh),Ja=p(Wh,"SPAN",{});var Vl=u(Ja);_c=c(Vl,"T\u1EA1o m\xE3 v\u1EDBi m\u1ED9t pipeline"),Vl.forEach(t),Wh.forEach(t),qh=g(n),re=p(n,"P",{});var Yh=u(re);dc=c(Yh,"B\xE2y gi\u1EDD l\xE0 th\u1EDDi \u0111i\u1EC3m c\u1EE7a s\u1EF1 th\u1EADt: ch\xFAng ta h\xE3y xem m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n th\u1EF1c s\u1EF1 ho\u1EA1t \u0111\u1ED9ng t\u1ED1t nh\u01B0 th\u1EBF n\xE0o! Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y trong nh\u1EADt k\xFD r\u1EB1ng m\u1EA5t m\xE1t \u0111\xE3 gi\u1EA3m \u0111\u1EC1u \u0111\u1EB7n, nh\u01B0ng \u0111\u1EC3 \u0111\u01B0a m\xF4 h\xECnh v\xE0o th\u1EED nghi\u1EC7m, ch\xFAng ta h\xE3y xem n\xF3 ho\u1EA1t \u0111\u1ED9ng t\u1ED1t nh\u01B0 th\u1EBF n\xE0o tr\xEAn m\u1ED9t s\u1ED1 l\u1EDDi nh\u1EAFc. \u0110\u1EC3 l\xE0m \u0111i\u1EC1u \u0111\xF3, ch\xFAng ta s\u1EBD bao b\u1ECDc m\xF4 h\xECnh trong m\u1ED9t "),Qa=p(Yh,"CODE",{});var Rl=u(Qa);fc=c(Rl,"pipeline"),Rl.forEach(t),yc=c(Yh," t\u1EA1o v\u0103n b\u1EA3n v\xE0 ch\xFAng ta s\u1EBD \u0111\u01B0a n\xF3 v\xE0o GPU cho c\xE1c th\u1EBF h\u1EC7 nhanh n\u1EBFu c\xF3 s\u1EB5n:"),Yh.forEach(t),xh=g(n),Ht.l(n),Ha=g(n),Aa=p(n,"P",{});var Kl=u(Aa);vc=c(Kl,"H\xE3y b\u1EAFt \u0111\u1EA7u v\u1EDBi t\xE1c v\u1EE5 \u0111\u01A1n gi\u1EA3n l\xE0 t\u1EA1o m\u1ED9t bi\u1EC3u \u0111\u1ED3 ph\xE2n t\xE1n:"),Kl.forEach(t),Eh=g(n),q(Ke.$$.fragment,n),Th=g(n),q(Ue.$$.fragment,n),Ch=g(n),rs=p(n,"P",{});var Ra=u(rs);bc=c(Ra,"K\u1EBFt qu\u1EA3 c\xF3 v\u1EBB ch\xEDnh x\xE1c. N\xF3 c\u0169ng ho\u1EA1t \u0111\u1ED9ng v\u1EDBi "),nh=p(Ra,"CODE",{});var Ul=u(nh);kc=c(Ul,"pandas"),Ul.forEach(t),$c=c(Ra,"? H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o m\u1ED9t "),th=p(Ra,"CODE",{});var Xl=u(th);jc=c(Xl,"DataFrame"),Xl.forEach(t),wc=c(Ra," t\u1EEB hai m\u1EA3ng kh\xF4ng:"),Ra.forEach(t),zh=g(n),q(Xe.$$.fragment,n),Ph=g(n),q(We.$$.fragment,n),Dh=g(n),Jn=p(n,"P",{});var me=u(Jn);qc=c(me,"Th\u1EADt tuy\u1EC7t, \u0111\xF3 l\xE0 c\xE2u tr\u1EA3 l\u1EDDi ch\xEDnh x\xE1c - m\u1EB7c d\xF9 sau \u0111\xF3 n\xF3 l\u1EA1i ch\xE8n th\xEAm c\u1ED9t "),sh=p(me,"CODE",{});var Wl=u(sh);xc=c(Wl,"x"),Wl.forEach(t),Ec=c(me,". V\xEC s\u1ED1 l\u01B0\u1EE3ng token \u0111\u01B0\u1EE3c t\u1EA1o c\xF3 gi\u1EDBi h\u1EA1n, v\xF2ng l\u1EB7p "),eh=p(me,"CODE",{});var Yl=u(eh);Tc=c(Yl,"for"),Yl.forEach(t),Cc=c(me," sau \u0111\xE2y s\u1EBD b\u1ECB c\u1EAFt. H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u g\xEC \u0111\xF3 ph\u1EE9c t\u1EA1p h\u01A1n m\u1ED9t ch\xFAt v\xE0 \u0111\u1EC3 m\xF4 h\xECnh gi\xFAp ch\xFAng ta s\u1EED d\u1EE5ng ho\u1EA1t \u0111\u1ED9ng "),ah=p(me,"CODE",{});var Zl=u(ah);zc=c(Zl,"groupby"),Zl.forEach(t),Pc=c(me,":"),me.forEach(t),Hh=g(n),q(Ye.$$.fragment,n),Ah=g(n),q(Ze.$$.fragment,n),Oh=g(n),ie=p(n,"P",{});var Zh=u(ie);Dc=c(Zh,"Kh\xF4ng t\u1EC7; \u0111\xF3 l\xE0 c\xE1ch l\xE0m \u0111\xFAng. Cu\u1ED1i c\xF9ng, h\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 cho "),hh=p(Zh,"CODE",{});var Jl=u(hh);Hc=c(Jl,"scikit-learn"),Jl.forEach(t),Ac=c(Zh," v\xE0 thi\u1EBFt l\u1EADp m\xF4 h\xECnh Random Forest hay kh\xF4ng:"),Zh.forEach(t),Nh=g(n),q(Je.$$.fragment,n),Mh=g(n),q(Qe.$$.fragment,n),Lh=g(n),Ts.l(n),Oa=g(n),J&&J.l(n),Na=Jh(),this.h()},h(){O(a,"name","hf:doc:metadata"),O(a,"content",JSON.stringify(Hr)),O(b,"id","thun-luyn-mt-m-hnh-ngn-ng-nhn-qu-t-u"),O(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(b,"href","#thun-luyn-mt-m-hnh-ngn-ng-nhn-qu-t-u"),O($,"class","relative group"),O(V,"href","/course/chapter1"),O(tt,"href","/course/chapter6"),Oc(sn.src,be="https://hf.space/gradioiframe/course-demos/codeparrot-ds/+")||O(sn,"src",be),O(sn,"frameborder","0"),O(sn,"height","300"),O(sn,"title","Gradio app"),O(sn,"class","block dark:hidden container p-0 flex-grow space-iframe"),O(sn,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),O(sn,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),O(at,"href","https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28"),O(at,"rel","nofollow"),O(Fn,"id","thu-thp-d-liu"),O(Fn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(Fn,"href","#thu-thp-d-liu"),O(pn,"class","relative group"),O(ct,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/"),O(ct,"rel","nofollow"),O(Gn,"href","https://huggingface.co/datasets/transformersbook/codeparrot"),O(Gn,"rel","nofollow"),O(zn,"id","chun-b-tp-d-liu"),O(zn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(zn,"href","#chun-b-tp-d-liu"),O(hn,"class","relative group"),O(ft,"href","/course/chapter6/4"),O(vt,"class","block dark:hidden"),Oc(vt.src,la="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg")||O(vt,"src",la),O(vt,"alt","Chunking a large texts in several pieces."),O(yn,"class","hidden dark:block"),Oc(yn.src,Oe="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg")||O(yn,"src",Oe),O(yn,"alt","Chunking a large texts in several pieces."),O(fn,"class","flex justify-center"),O(ws,"href","/course/chapter7/3"),O(vn,"id","khi-to-m-hnh-mi"),O(vn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(vn,"href","#khi-to-m-hnh-mi"),O(Yn,"class","relative group"),O(le,"id","to-m-vi-mt-pipeline"),O(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),O(le,"href","#to-m-vi-mt-pipeline"),O(Es,"class","relative group")},m(n,i){s(document.head,a),l(n,_,i),x(r,n,i),l(n,d,i),l(n,$,i),s($,b),s(b,j),x(z,j,null),s($,T),s($,H),s(H,M),l(n,L,i),na[f].m(n,i),l(n,F,i),l(n,S,i),s(S,G),s(S,V),s(V,U),s(S,N),s(S,B),s(B,I),s(S,W),s(S,X),s(X,Y),s(S,Q),l(n,cn,i),l(n,C,i),s(C,K),s(C,An),s(An,On),s(C,At),s(C,bn),s(bn,Nn),s(C,Ot),s(C,is),s(is,os),s(C,fe),s(C,ps),s(ps,us),s(C,ye),l(n,Cs,i),x(Mn,n,i),l(n,Ln,i),l(n,rn,i),s(rn,nt),s(rn,tt),s(tt,Nt),s(rn,ve),s(rn,Mt),s(Mt,st),s(rn,zs),l(n,et,i),l(n,sn,i),l(n,on,i),l(n,kn,i),s(kn,ke),s(kn,at),s(at,ms),s(kn,$e),l(n,Ps,i),l(n,pn,i),s(pn,Fn),s(Fn,Lt),x(nn,Lt,null),s(pn,je),s(pn,ht),s(ht,we),l(n,Ds,i),l(n,tn,i),s(tn,qe),s(tn,ct),s(ct,Hs),s(tn,lt),s(tn,Ft),s(Ft,Gt),s(tn,xe),s(tn,Gn),s(Gn,rt),s(tn,As),l(n,it,i),l(n,Sn,i),s(Sn,Os),s(Sn,qn),s(qn,Ns),s(Sn,$n),l(n,Ms,i),x(xn,n,i),l(n,Ls,i),l(n,St,i),s(St,Bt),l(n,Fs,i),x(ot,n,i),l(n,gs,i),x(un,n,i),l(n,_s,i),l(n,En,i),s(En,Ee),l(n,ds,i),x(mn,n,i),l(n,fs,i),l(n,Tn,i),s(Tn,Te),l(n,ys,i),x(gn,n,i),l(n,vs,i),x(_n,n,i),l(n,bs,i),l(n,en,i),s(en,Ce),l(n,It,i),l(n,Vt,i),s(Vt,ze),l(n,Rt,i),x(pt,n,i),l(n,Gs,i),x(Bn,n,i),l(n,In,i),x(Cn,n,i),l(n,an,i),l(n,Kt,i),s(Kt,ks),l(n,Ss,i),x(ut,n,i),l(n,Ut,i),x(mt,n,i),l(n,Bs,i),l(n,jn,i),s(jn,Pe),s(jn,$s),s($s,Is),s(jn,gt),l(n,js,i),l(n,hn,i),s(hn,zn),s(zn,Vn),x(_t,Vn,null),s(hn,Vs),s(hn,Pn),s(Pn,Rs),l(n,dn,i),x(dt,n,i),l(n,Xt,i),l(n,Wt,i),s(Wt,De),l(n,Yt,i),l(n,ln,i),s(ln,He),s(ln,Zt),s(Zt,Jt),s(ln,Ae),s(ln,ft),s(ft,yt),s(ln,Ks),s(ln,Dn),s(Dn,Us),s(ln,Rn),l(n,Xs,i),l(n,fn,i),s(fn,vt),s(fn,Ws),s(fn,yn),l(n,bt,i),l(n,Qt,i),s(Qt,Ys),l(n,Kn,i),x(Un,n,i),l(n,Xn,i),x(Wn,n,i),l(n,e,i),l(n,k,i),s(k,ra),s(k,Zs),s(Zs,ia),s(k,Js),l(n,Ne,i),l(n,R,i),s(R,oa),s(R,Qs),s(Qs,pa),s(R,ua),s(R,ws),s(ws,ma),s(R,ga),s(R,ne),s(ne,_a),s(R,da),l(n,kt,i),x(ns,n,i),l(n,Me,i),x(ts,n,i),l(n,$t,i),l(n,qs,i),s(qs,fa),l(n,Le,i),l(n,xs,i),s(xs,ss),l(n,Fe,i),x(jt,n,i),l(n,Ge,i),l(n,Yn,i),s(Yn,vn),s(vn,te),x(es,te,null),s(Yn,Zn),s(Yn,se),s(se,ya),l(n,Se,i),l(n,Hn,i),s(Hn,as),s(Hn,ee),s(ee,va),s(Hn,ba),s(Hn,hs),s(hs,ka),s(Hn,$a),l(n,ph,i),ta[wt].m(n,i),l(n,ja,i),l(n,cs,i),s(cs,Qh),s(cs,Ua),s(Ua,nc),s(cs,tc),s(cs,Xa),s(Xa,sc),s(cs,ec),l(n,uh,i),l(n,ls,i),s(ls,ac),s(ls,Wa),s(Wa,hc),s(ls,cc),s(ls,Ya),s(Ya,lc),s(ls,rc),l(n,mh,i),sa[xt].m(n,i),l(n,wa,i),l(n,qa,i),s(qa,ic),l(n,gh,i),x(Be,n,i),l(n,_h,i),ea[Tt].m(n,i),l(n,xa,i),l(n,Ea,i),s(Ea,oc),l(n,dh,i),Z&&Z.m(n,i),l(n,Ta,i),x(ae,n,i),l(n,fh,i),l(n,Ca,i),s(Ca,pc),l(n,yh,i),x(Ie,n,i),l(n,vh,i),l(n,za,i),s(za,uc),l(n,bh,i),l(n,Pa,i),s(Pa,mc),l(n,kh,i),x(Ve,n,i),l(n,$h,i),aa[zt].m(n,i),l(n,Da,i),x(he,n,i),l(n,jh,i),x(ce,n,i),l(n,wh,i),l(n,Es,i),s(Es,le),s(le,Za),x(Re,Za,null),s(Es,gc),s(Es,Ja),s(Ja,_c),l(n,qh,i),l(n,re,i),s(re,dc),s(re,Qa),s(Qa,fc),s(re,yc),l(n,xh,i),ha[Dt].m(n,i),l(n,Ha,i),l(n,Aa,i),s(Aa,vc),l(n,Eh,i),x(Ke,n,i),l(n,Th,i),x(Ue,n,i),l(n,Ch,i),l(n,rs,i),s(rs,bc),s(rs,nh),s(nh,kc),s(rs,$c),s(rs,th),s(th,jc),s(rs,wc),l(n,zh,i),x(Xe,n,i),l(n,Ph,i),x(We,n,i),l(n,Dh,i),l(n,Jn,i),s(Jn,qc),s(Jn,sh),s(sh,xc),s(Jn,Ec),s(Jn,eh),s(eh,Tc),s(Jn,Cc),s(Jn,ah),s(ah,zc),s(Jn,Pc),l(n,Hh,i),x(Ye,n,i),l(n,Ah,i),x(Ze,n,i),l(n,Oh,i),l(n,ie,i),s(ie,Dc),s(ie,hh),s(hh,Hc),s(ie,Ac),l(n,Nh,i),x(Je,n,i),l(n,Mh,i),x(Qe,n,i),l(n,Lh,i),Ts.m(n,i),l(n,Oa,i),J&&J.m(n,i),l(n,Na,i),Fh=!0},p(n,[i]){const ca={};i&1&&(ca.fw=n[0]),r.$set(ca);let Ma=f;f=Lc(n),f!==Ma&&(_e(),v(na[Ma],1,1,()=>{na[Ma]=null}),ge(),P=na[f],P||(P=na[f]=Mc[f](n),P.c()),y(P,1),P.m(F.parentNode,F));const ch={};i&2&&(ch.$$scope={dirty:i,ctx:n}),Cn.$set(ch);const lh={};i&2&&(lh.$$scope={dirty:i,ctx:n}),jt.$set(lh);let Qn=wt;wt=Gc(n),wt!==Qn&&(_e(),v(ta[Qn],1,1,()=>{ta[Qn]=null}),ge(),qt=ta[wt],qt||(qt=ta[wt]=Fc[wt](n),qt.c()),y(qt,1),qt.m(ja.parentNode,ja));let La=xt;xt=Bc(n),xt!==La&&(_e(),v(sa[La],1,1,()=>{sa[La]=null}),ge(),Et=sa[xt],Et||(Et=sa[xt]=Sc[xt](n),Et.c()),y(Et,1),Et.m(wa.parentNode,wa));let Fa=Tt;Tt=Vc(n),Tt!==Fa&&(_e(),v(ea[Fa],1,1,()=>{ea[Fa]=null}),ge(),Ct=ea[Tt],Ct||(Ct=ea[Tt]=Ic[Tt](n),Ct.c()),y(Ct,1),Ct.m(xa.parentNode,xa)),n[0]==="tf"?Z?i&1&&y(Z,1):(Z=Ql(),Z.c(),y(Z,1),Z.m(Ta.parentNode,Ta)):Z&&(_e(),v(Z,1,1,()=>{Z=null}),ge());const rh={};i&2&&(rh.$$scope={dirty:i,ctx:n}),ae.$set(rh);let wn=zt;zt=Kc(n),zt!==wn&&(_e(),v(aa[wn],1,1,()=>{aa[wn]=null}),ge(),Pt=aa[zt],Pt||(Pt=aa[zt]=Rc[zt](n),Pt.c()),y(Pt,1),Pt.m(Da.parentNode,Da));const ih={};i&2&&(ih.$$scope={dirty:i,ctx:n}),he.$set(ih);const oh={};i&3&&(oh.$$scope={dirty:i,ctx:n}),ce.$set(oh);let Ga=Dt;Dt=Xc(n),Dt!==Ga&&(_e(),v(ha[Ga],1,1,()=>{ha[Ga]=null}),ge(),Ht=ha[Dt],Ht||(Ht=ha[Dt]=Uc[Dt](n),Ht.c()),y(Ht,1),Ht.m(Ha.parentNode,Ha)),Gh!==(Gh=Wc(n))&&(Ts.d(1),Ts=Gh(n),Ts&&(Ts.c(),Ts.m(Oa.parentNode,Oa))),n[0]==="pt"?J?i&1&&y(J,1):(J=nr(n),J.c(),y(J,1),J.m(Na.parentNode,Na)):J&&(_e(),v(J,1,1,()=>{J=null}),ge())},i(n){Fh||(y(r.$$.fragment,n),y(z.$$.fragment,n),y(P),y(Mn.$$.fragment,n),y(nn.$$.fragment,n),y(xn.$$.fragment,n),y(ot.$$.fragment,n),y(un.$$.fragment,n),y(mn.$$.fragment,n),y(gn.$$.fragment,n),y(_n.$$.fragment,n),y(pt.$$.fragment,n),y(Bn.$$.fragment,n),y(Cn.$$.fragment,n),y(ut.$$.fragment,n),y(mt.$$.fragment,n),y(_t.$$.fragment,n),y(dt.$$.fragment,n),y(Un.$$.fragment,n),y(Wn.$$.fragment,n),y(ns.$$.fragment,n),y(ts.$$.fragment,n),y(jt.$$.fragment,n),y(es.$$.fragment,n),y(qt),y(Et),y(Be.$$.fragment,n),y(Ct),y(Z),y(ae.$$.fragment,n),y(Ie.$$.fragment,n),y(Ve.$$.fragment,n),y(Pt),y(he.$$.fragment,n),y(ce.$$.fragment,n),y(Re.$$.fragment,n),y(Ht),y(Ke.$$.fragment,n),y(Ue.$$.fragment,n),y(Xe.$$.fragment,n),y(We.$$.fragment,n),y(Ye.$$.fragment,n),y(Ze.$$.fragment,n),y(Je.$$.fragment,n),y(Qe.$$.fragment,n),y(J),Fh=!0)},o(n){v(r.$$.fragment,n),v(z.$$.fragment,n),v(P),v(Mn.$$.fragment,n),v(nn.$$.fragment,n),v(xn.$$.fragment,n),v(ot.$$.fragment,n),v(un.$$.fragment,n),v(mn.$$.fragment,n),v(gn.$$.fragment,n),v(_n.$$.fragment,n),v(pt.$$.fragment,n),v(Bn.$$.fragment,n),v(Cn.$$.fragment,n),v(ut.$$.fragment,n),v(mt.$$.fragment,n),v(_t.$$.fragment,n),v(dt.$$.fragment,n),v(Un.$$.fragment,n),v(Wn.$$.fragment,n),v(ns.$$.fragment,n),v(ts.$$.fragment,n),v(jt.$$.fragment,n),v(es.$$.fragment,n),v(qt),v(Et),v(Be.$$.fragment,n),v(Ct),v(Z),v(ae.$$.fragment,n),v(Ie.$$.fragment,n),v(Ve.$$.fragment,n),v(Pt),v(he.$$.fragment,n),v(ce.$$.fragment,n),v(Re.$$.fragment,n),v(Ht),v(Ke.$$.fragment,n),v(Ue.$$.fragment,n),v(Xe.$$.fragment,n),v(We.$$.fragment,n),v(Ye.$$.fragment,n),v(Ze.$$.fragment,n),v(Je.$$.fragment,n),v(Qe.$$.fragment,n),v(J),Fh=!1},d(n){t(a),n&&t(_),E(r,n),n&&t(d),n&&t($),E(z),n&&t(L),na[f].d(n),n&&t(F),n&&t(S),n&&t(cn),n&&t(C),n&&t(Cs),E(Mn,n),n&&t(Ln),n&&t(rn),n&&t(et),n&&t(sn),n&&t(on),n&&t(kn),n&&t(Ps),n&&t(pn),E(nn),n&&t(Ds),n&&t(tn),n&&t(it),n&&t(Sn),n&&t(Ms),E(xn,n),n&&t(Ls),n&&t(St),n&&t(Fs),E(ot,n),n&&t(gs),E(un,n),n&&t(_s),n&&t(En),n&&t(ds),E(mn,n),n&&t(fs),n&&t(Tn),n&&t(ys),E(gn,n),n&&t(vs),E(_n,n),n&&t(bs),n&&t(en),n&&t(It),n&&t(Vt),n&&t(Rt),E(pt,n),n&&t(Gs),E(Bn,n),n&&t(In),E(Cn,n),n&&t(an),n&&t(Kt),n&&t(Ss),E(ut,n),n&&t(Ut),E(mt,n),n&&t(Bs),n&&t(jn),n&&t(js),n&&t(hn),E(_t),n&&t(dn),E(dt,n),n&&t(Xt),n&&t(Wt),n&&t(Yt),n&&t(ln),n&&t(Xs),n&&t(fn),n&&t(bt),n&&t(Qt),n&&t(Kn),E(Un,n),n&&t(Xn),E(Wn,n),n&&t(e),n&&t(k),n&&t(Ne),n&&t(R),n&&t(kt),E(ns,n),n&&t(Me),E(ts,n),n&&t($t),n&&t(qs),n&&t(Le),n&&t(xs),n&&t(Fe),E(jt,n),n&&t(Ge),n&&t(Yn),E(es),n&&t(Se),n&&t(Hn),n&&t(ph),ta[wt].d(n),n&&t(ja),n&&t(cs),n&&t(uh),n&&t(ls),n&&t(mh),sa[xt].d(n),n&&t(wa),n&&t(qa),n&&t(gh),E(Be,n),n&&t(_h),ea[Tt].d(n),n&&t(xa),n&&t(Ea),n&&t(dh),Z&&Z.d(n),n&&t(Ta),E(ae,n),n&&t(fh),n&&t(Ca),n&&t(yh),E(Ie,n),n&&t(vh),n&&t(za),n&&t(bh),n&&t(Pa),n&&t(kh),E(Ve,n),n&&t($h),aa[zt].d(n),n&&t(Da),E(he,n),n&&t(jh),E(ce,n),n&&t(wh),n&&t(Es),E(Re),n&&t(qh),n&&t(re),n&&t(xh),ha[Dt].d(n),n&&t(Ha),n&&t(Aa),n&&t(Eh),E(Ke,n),n&&t(Th),E(Ue,n),n&&t(Ch),n&&t(rs),n&&t(zh),E(Xe,n),n&&t(Ph),E(We,n),n&&t(Dh),n&&t(Jn),n&&t(Hh),E(Ye,n),n&&t(Ah),E(Ze,n),n&&t(Oh),n&&t(ie),n&&t(Nh),E(Je,n),n&&t(Mh),E(Qe,n),n&&t(Lh),Ts.d(n),n&&t(Oa),J&&J.d(n),n&&t(Na)}}}const Hr={local:"thun-luyn-mt-m-hnh-ngn-ng-nhn-qu-t-u",sections:[{local:"thu-thp-d-liu",title:"Thu th\u1EADp d\u1EEF li\u1EC7u"},{local:"chun-b-tp-d-liu",title:"Chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"},{local:"khi-to-m-hnh-mi",title:"Kh\u1EDFi t\u1EA1o m\xF4 h\xECnh m\u1EDBi"},{local:"to-m-vi-mt-pipeline",title:"T\u1EA1o m\xE3 v\u1EDBi m\u1ED9t pipeline"},{local:"hun-luyn-vi-accelerate",title:"Hu\u1EA5n luy\u1EC7n v\u1EDBi \u{1F917} Accelerate"}],title:"THu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF nh\xE2n qu\u1EA3 t\u1EEB \u0111\u1EA7u"};function Ar(A,a,_){let r="pt";return cr(()=>{const d=new URLSearchParams(window.location.search);_(0,r=d.get("fw")||"pt")}),[r]}class Br extends sr{constructor(a){super();er(this,a,Ar,Dr,ar,{})}}export{Br as default,Hr as metadata};
