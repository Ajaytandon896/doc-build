import{S as bu,i as ku,s as vu,e as l,t as s,c,a as r,h as a,d as e,g as i,G as n,k as u,w,m as g,x as j,y as x,q as f,o as b,B as E,b as T,M as yu,N as tc,p as se,v as $u,n as ae}from"../../chunks/vendor-hf-doc-builder.js";import{T as Fe}from"../../chunks/Tip-hf-doc-builder.js";import{Y as nc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as M}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as fu}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as wu}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function ju(B){let o,d;return o=new fu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_tf.ipynb"}]}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function xu(B){let o,d;return o=new fu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_pt.ipynb"}]}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Eu(B){let o,d,p,v,q;return{c(){o=l("p"),d=s("\u{1F64B} N\u1EBFu c\xE1c thu\u1EADt ng\u1EEF \u201Cm\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i\u201D v\xE0 \u201Cm\xF4 h\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc\u201D nghe c\xF3 v\u1EBB xa l\u1EA1 v\u1EDBi b\u1EA1n, h\xE3y xem "),p=l("a"),v=s("Ch\u01B0\u01A1ng 1"),q=s(", n\u01A1i ch\xFAng t\xF4i gi\u1EA3i th\xEDch t\u1EA5t c\u1EA3 c\xE1c kh\xE1i ni\u1EC7m c\u1ED1t l\xF5i n\xE0y, k\xE8m theo video!"),this.h()},l($){o=c($,"P",{});var k=r(o);d=a(k,"\u{1F64B} N\u1EBFu c\xE1c thu\u1EADt ng\u1EEF \u201Cm\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i\u201D v\xE0 \u201Cm\xF4 h\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc\u201D nghe c\xF3 v\u1EBB xa l\u1EA1 v\u1EDBi b\u1EA1n, h\xE3y xem "),p=c(k,"A",{href:!0});var D=r(p);v=a(D,"Ch\u01B0\u01A1ng 1"),D.forEach(e),q=a(k,", n\u01A1i ch\xFAng t\xF4i gi\u1EA3i th\xEDch t\u1EA5t c\u1EA3 c\xE1c kh\xE1i ni\u1EC7m c\u1ED1t l\xF5i n\xE0y, k\xE8m theo video!"),k.forEach(e),this.h()},h(){T(p,"href","/course/chapter1")},m($,k){i($,o,k),n(o,d),n(o,p),n(p,v),n(o,q)},d($){$&&e(o)}}}function Tu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W;return k=new M({props:{code:`from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),H=new M({props:{code:`model(model.dummy_inputs)  # X\xE2y d\u1EF1ng m\xF4 h\xECnh
model.summary()`,highlighted:`model(model.dummy_inputs)  <span class="hljs-comment"># X\xE2y d\u1EF1ng m\xF4 h\xECnh</span>
model.summary()`}}),F=new M({props:{code:`Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________`,highlighted:`Model: <span class="hljs-string">&quot;tf_distil_bert_for_masked_lm&quot;</span>
_________________________________________________________________
Layer (<span class="hljs-built_in">type</span>)                 Output Shape              Param <span class="hljs-comment">#   </span>
=================================================================
distilbert (TFDistilBertMain multiple                  <span class="hljs-number">66362880</span>  
_________________________________________________________________
vocab_transform (Dense)      multiple                  <span class="hljs-number">590592</span>    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  <span class="hljs-number">1536</span>      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  <span class="hljs-number">23866170</span>  
=================================================================
Total params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Trainable params: <span class="hljs-number">66</span>,<span class="hljs-number">985</span>,<span class="hljs-number">530</span>
Non-trainable params: <span class="hljs-number">0</span>
_________________________________________________________________`}}),{c(){o=l("p"),d=s("H\xE3y ti\u1EBFp t\u1EE5c v\xE0 t\u1EA3i xu\u1ED1ng DistilBERT b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),p=l("code"),v=s("AutoModelForMaskedLM"),q=s(":"),$=u(),w(k.$$.fragment),D=u(),_=l("p"),C=s("Ch\xFAng ta c\xF3 th\u1EC3 xem m\xF4 h\xECnh n\xE0y c\xF3 bao nhi\xEAu tham s\u1ED1 b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),P=l("code"),K=s("summary()"),A=s(":"),N=u(),w(H.$$.fragment),R=u(),w(F.$$.fragment)},l(y){o=c(y,"P",{});var L=r(o);d=a(L,"H\xE3y ti\u1EBFp t\u1EE5c v\xE0 t\u1EA3i xu\u1ED1ng DistilBERT b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),p=c(L,"CODE",{});var I=r(p);v=a(I,"AutoModelForMaskedLM"),I.forEach(e),q=a(L,":"),L.forEach(e),$=g(y),j(k.$$.fragment,y),D=g(y),_=c(y,"P",{});var G=r(_);C=a(G,"Ch\xFAng ta c\xF3 th\u1EC3 xem m\xF4 h\xECnh n\xE0y c\xF3 bao nhi\xEAu tham s\u1ED1 b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),P=c(G,"CODE",{});var tt=r(P);K=a(tt,"summary()"),tt.forEach(e),A=a(G,":"),G.forEach(e),N=g(y),j(H.$$.fragment,y),R=g(y),j(F.$$.fragment,y)},m(y,L){i(y,o,L),n(o,d),n(o,p),n(p,v),n(o,q),i(y,$,L),x(k,y,L),i(y,D,L),i(y,_,L),n(_,C),n(_,P),n(P,K),n(_,A),i(y,N,L),x(H,y,L),i(y,R,L),x(F,y,L),W=!0},i(y){W||(f(k.$$.fragment,y),f(H.$$.fragment,y),f(F.$$.fragment,y),W=!0)},o(y){b(k.$$.fragment,y),b(H.$$.fragment,y),b(F.$$.fragment,y),W=!1},d(y){y&&e(o),y&&e($),E(k,y),y&&e(D),y&&e(_),y&&e(N),E(H,y),y&&e(R),E(F,y)}}}function qu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W;return k=new M({props:{code:`from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)`}}),H=new M({props:{code:`distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")`,highlighted:`distilbert_num_parameters = model.num_parameters() / <span class="hljs-number">1_000_000</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; DistilBERT number of parameters: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(distilbert_num_parameters)}</span>M&#x27;&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; BERT number of parameters: 110M&#x27;&quot;</span>)`}}),F=new M({props:{code:`'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; DistilBERT number of parameters: 67M&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; BERT number of parameters: 110M&#x27;</span>`}}),{c(){o=l("p"),d=s("H\xE3y ti\u1EBFp t\u1EE5c v\xE0 t\u1EA3i xu\u1ED1ng DistilBERT b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),p=l("code"),v=s("AutoModelForMaskedLM"),q=s(":"),$=u(),w(k.$$.fragment),D=u(),_=l("p"),C=s("Ch\xFAng ta c\xF3 th\u1EC3 xem m\xF4 h\xECnh n\xE0y c\xF3 bao nhi\xEAu tham s\u1ED1 b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),P=l("code"),K=s("num_parameters()"),A=s(":"),N=u(),w(H.$$.fragment),R=u(),w(F.$$.fragment)},l(y){o=c(y,"P",{});var L=r(o);d=a(L,"H\xE3y ti\u1EBFp t\u1EE5c v\xE0 t\u1EA3i xu\u1ED1ng DistilBERT b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),p=c(L,"CODE",{});var I=r(p);v=a(I,"AutoModelForMaskedLM"),I.forEach(e),q=a(L,":"),L.forEach(e),$=g(y),j(k.$$.fragment,y),D=g(y),_=c(y,"P",{});var G=r(_);C=a(G,"Ch\xFAng ta c\xF3 th\u1EC3 xem m\xF4 h\xECnh n\xE0y c\xF3 bao nhi\xEAu tham s\u1ED1 b\u1EB1ng c\xE1ch g\u1ECDi ph\u01B0\u01A1ng th\u1EE9c "),P=c(G,"CODE",{});var tt=r(P);K=a(tt,"num_parameters()"),tt.forEach(e),A=a(G,":"),G.forEach(e),N=g(y),j(H.$$.fragment,y),R=g(y),j(F.$$.fragment,y)},m(y,L){i(y,o,L),n(o,d),n(o,p),n(p,v),n(o,q),i(y,$,L),x(k,y,L),i(y,D,L),i(y,_,L),n(_,C),n(_,P),n(P,K),n(_,A),i(y,N,L),x(H,y,L),i(y,R,L),x(F,y,L),W=!0},i(y){W||(f(k.$$.fragment,y),f(H.$$.fragment,y),f(F.$$.fragment,y),W=!0)},o(y){b(k.$$.fragment,y),b(H.$$.fragment,y),b(F.$$.fragment,y),W=!1},d(y){y&&e(o),y&&e($),E(k,y),y&&e(D),y&&e(_),y&&e(N),E(H,y),y&&e(R),E(F,y)}}}function Cu(B){let o,d;return o=new M({props:{code:`import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# T\xECm v\u1ECB tr\xED [MASK] v\xE0 tr\xEDch xu\u1EA5t logit
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# PCh\u1ECDn \u1EE9ng vi\xEAn cho [MASK] v\u1EDBi logit cao nh\u1EA5t
# Ch\xFAng ta ph\u1EE7 \u0111\u1ECBnh m\u1EA3ng tr\u01B0\u1EDBc argsort \u0111\u1EC3 l\u1EA5y logits l\u1EDBn nh\u1EA5t ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1ECF nh\u1EA5t
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># T\xECm v\u1ECB tr\xED [MASK] v\xE0 tr\xEDch xu\u1EA5t logit</span>
mask_token_index = np.argwhere(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># PCh\u1ECDn \u1EE9ng vi\xEAn cho [MASK] v\u1EDBi logit cao nh\u1EA5t</span>
<span class="hljs-comment"># Ch\xFAng ta ph\u1EE7 \u0111\u1ECBnh m\u1EA3ng tr\u01B0\u1EDBc argsort \u0111\u1EC3 l\u1EA5y logits l\u1EDBn nh\u1EA5t ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1ECF nh\u1EA5t</span>
top_5_tokens = np.argsort(-mask_token_logits)[:<span class="hljs-number">5</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&quot;</span>)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Du(B){let o,d;return o=new M({props:{code:`import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# T\xECm v\u1ECB tr\xED [MASK] v\xE0 tr\xEDch xu\u1EA5t logit
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Ch\u1ECDn \u1EE9ng vi\xEAn cho [MASK] v\u1EDBi logit cao nh\u1EA5t
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")`,highlighted:`<span class="hljs-keyword">import</span> torch

inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
token_logits = model(**inputs).logits
<span class="hljs-comment"># T\xECm v\u1ECB tr\xED [MASK] v\xE0 tr\xEDch xu\u1EA5t logit</span>
mask_token_index = torch.where(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
mask_token_logits = token_logits[<span class="hljs-number">0</span>, mask_token_index, :]
<span class="hljs-comment"># Ch\u1ECDn \u1EE9ng vi\xEAn cho [MASK] v\u1EDBi logit cao nh\u1EA5t</span>
top_5_tokens = torch.topk(mask_token_logits, <span class="hljs-number">5</span>, dim=<span class="hljs-number">1</span>).indices[<span class="hljs-number">0</span>].tolist()

<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> top_5_tokens:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; <span class="hljs-subst">{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}</span>&#x27;&quot;</span>)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Mu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W,y,L,I,G,tt,V,qt,lt;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("Th\u1EED nghi\u1EC7m th\xF4i!"),q=s(" T\u1EA1o ra c\xE1c m\u1EABu ng\u1EABu nhi\u1EC1n t\u1EEB ph\u1EA7n "),$=l("code"),k=s("phi gi\xE1m s\xE1t"),D=s(" v\xE0 ki\u1EC3m \u0111\u1ECBnh xem nh\xE3n c\u1EE7a ch\xFAng l\xE0 "),_=l("code"),C=s("0"),P=s(" hay "),K=l("code"),A=s("1"),N=s(". Khi \u0111ang \u1EDF \u0111\xF3, b\u1EA1n c\u0169ng c\xF3 th\u1EC3 ki\u1EC3m tra xem c\xE1c nh\xE3n trong ph\u1EA7n "),H=l("code"),R=s("hu\u1EA5n luy\u1EC7n"),F=s(" v\xE0 "),W=l("code"),y=s("ki\u1EC3m th\u1EED"),L=s(" c\xF3 th\u1EF1c s\u1EED l\xE0 "),I=l("code"),G=s("0"),tt=s(" ho\u1EB7c "),V=l("code"),qt=s("1"),lt=s(" kh\xF4ng \u2014 \u0111\xE2y l\xE0 m\u1ED9t ph\u1EA7n ki\u1EC3m tra h\u1EEFu \xEDch m\xE0nh\u1EEFng nh\xE0 NLP n\xEAn th\u1EF1c hi\u1EC7n \u0111\u1EA7u d\u1EF1 \xE1n!.")},l(gt){o=c(gt,"P",{});var U=r(o);d=a(U,"\u270F\uFE0F "),p=c(U,"STRONG",{});var _t=r(p);v=a(_t,"Th\u1EED nghi\u1EC7m th\xF4i!"),_t.forEach(e),q=a(U," T\u1EA1o ra c\xE1c m\u1EABu ng\u1EABu nhi\u1EC1n t\u1EEB ph\u1EA7n "),$=c(U,"CODE",{});var Y=r($);k=a(Y,"phi gi\xE1m s\xE1t"),Y.forEach(e),D=a(U," v\xE0 ki\u1EC3m \u0111\u1ECBnh xem nh\xE3n c\u1EE7a ch\xFAng l\xE0 "),_=c(U,"CODE",{});var Pt=r(_);C=a(Pt,"0"),Pt.forEach(e),P=a(U," hay "),K=c(U,"CODE",{});var ct=r(K);A=a(ct,"1"),ct.forEach(e),N=a(U,". Khi \u0111ang \u1EDF \u0111\xF3, b\u1EA1n c\u0169ng c\xF3 th\u1EC3 ki\u1EC3m tra xem c\xE1c nh\xE3n trong ph\u1EA7n "),H=c(U,"CODE",{});var ot=r(H);R=a(ot,"hu\u1EA5n luy\u1EC7n"),ot.forEach(e),F=a(U," v\xE0 "),W=c(U,"CODE",{});var rt=r(W);y=a(rt,"ki\u1EC3m th\u1EED"),rt.forEach(e),L=a(U," c\xF3 th\u1EF1c s\u1EED l\xE0 "),I=c(U,"CODE",{});var Z=r(I);G=a(Z,"0"),Z.forEach(e),tt=a(U," ho\u1EB7c "),V=c(U,"CODE",{});var vt=r(V);qt=a(vt,"1"),vt.forEach(e),lt=a(U," kh\xF4ng \u2014 \u0111\xE2y l\xE0 m\u1ED9t ph\u1EA7n ki\u1EC3m tra h\u1EEFu \xEDch m\xE0nh\u1EEFng nh\xE0 NLP n\xEAn th\u1EF1c hi\u1EC7n \u0111\u1EA7u d\u1EF1 \xE1n!."),U.forEach(e)},m(gt,U){i(gt,o,U),n(o,d),n(o,p),n(p,v),n(o,q),n(o,$),n($,k),n(o,D),n(o,_),n(_,C),n(o,P),n(o,K),n(K,A),n(o,N),n(o,H),n(H,R),n(o,F),n(o,W),n(W,y),n(o,L),n(o,I),n(I,G),n(o,tt),n(o,V),n(V,qt),n(o,lt)},d(gt){gt&&e(o)}}}function zu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("Th\u1EED nghi\u1EC7m th\xF4i!"),q=s(" M\u1ED9t s\u1ED1 m\xF4 h\xECnh Transformer, nh\u01B0"),$=l("a"),k=s("BigBird"),D=s(" v\xE0 "),_=l("a"),C=s("Longformer"),P=s(",c\xF3 \u0111\u1ED9 d\xE0i ng\u1EEF c\u1EA3nh d\xE0i h\u01A1n nhi\u1EC1u so v\u1EDBi BERT v\xE0 c\xE1c m\xF4 h\xECnh Transformer \u0111\u1EDDi \u0111\u1EA7u kh\xE1c. Kh\u1EDFi t\u1EA1o tokenizer cho m\u1ED9t trong nh\u1EEFng checkpoint v\xE0 x\xE1c minh r\u1EB1ng "),K=l("code"),A=s("model_max_length"),N=s(" t\u01B0\u01A1ng \u1EE9ng v\u1EDBi nh\u1EEFng g\xEC \u0111\u01B0\u1EE3c tr\xEDch d\u1EABn tr\xEAn th\u1EBB m\xF4 h\xECnh c\u1EE7a n\xF3."),this.h()},l(H){o=c(H,"P",{});var R=r(o);d=a(R,"\u270F\uFE0F "),p=c(R,"STRONG",{});var F=r(p);v=a(F,"Th\u1EED nghi\u1EC7m th\xF4i!"),F.forEach(e),q=a(R," M\u1ED9t s\u1ED1 m\xF4 h\xECnh Transformer, nh\u01B0"),$=c(R,"A",{href:!0,rel:!0});var W=r($);k=a(W,"BigBird"),W.forEach(e),D=a(R," v\xE0 "),_=c(R,"A",{href:!0});var y=r(_);C=a(y,"Longformer"),y.forEach(e),P=a(R,",c\xF3 \u0111\u1ED9 d\xE0i ng\u1EEF c\u1EA3nh d\xE0i h\u01A1n nhi\u1EC1u so v\u1EDBi BERT v\xE0 c\xE1c m\xF4 h\xECnh Transformer \u0111\u1EDDi \u0111\u1EA7u kh\xE1c. Kh\u1EDFi t\u1EA1o tokenizer cho m\u1ED9t trong nh\u1EEFng checkpoint v\xE0 x\xE1c minh r\u1EB1ng "),K=c(R,"CODE",{});var L=r(K);A=a(L,"model_max_length"),L.forEach(e),N=a(R," t\u01B0\u01A1ng \u1EE9ng v\u1EDBi nh\u1EEFng g\xEC \u0111\u01B0\u1EE3c tr\xEDch d\u1EABn tr\xEAn th\u1EBB m\xF4 h\xECnh c\u1EE7a n\xF3."),R.forEach(e),this.h()},h(){T($,"href","https://huggingface.co/google/bigbird-roberta-base"),T($,"rel","nofollow"),T(_,"href","hf.co/allenai/longformer-base-4096")},m(H,R){i(H,o,R),n(o,d),n(o,p),n(p,v),n(o,q),n(o,$),n($,k),n(o,D),n(o,_),n(_,C),n(o,P),n(o,K),n(K,A),n(o,N)},d(H){H&&e(o)}}}function Au(B){let o,d;return{c(){o=l("p"),d=s("L\u01B0u \xFD r\u1EB1ng vi\u1EC7c s\u1EED d\u1EE5ng k\xEDch th\u01B0\u1EDBc ph\xE2n \u0111o\u1EA1n nh\u1ECF c\xF3 th\u1EC3 g\xE2y b\u1EA5t l\u1EE3i trong c\xE1c t\xECnh hu\u1ED1ng th\u1EF1c t\u1EBF, v\xEC v\u1EADy b\u1EA1n n\xEAn s\u1EED d\u1EE5ng k\xEDch th\u01B0\u1EDBc t\u01B0\u01A1ng \u1EE9ng v\u1EDBi tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng m\xE0 b\u1EA1n s\u1EBD \xE1p d\u1EE5ng m\xF4 h\xECnh c\u1EE7a m\xECnh.")},l(p){o=c(p,"P",{});var v=r(o);d=a(v,"L\u01B0u \xFD r\u1EB1ng vi\u1EC7c s\u1EED d\u1EE5ng k\xEDch th\u01B0\u1EDBc ph\xE2n \u0111o\u1EA1n nh\u1ECF c\xF3 th\u1EC3 g\xE2y b\u1EA5t l\u1EE3i trong c\xE1c t\xECnh hu\u1ED1ng th\u1EF1c t\u1EBF, v\xEC v\u1EADy b\u1EA1n n\xEAn s\u1EED d\u1EE5ng k\xEDch th\u01B0\u1EDBc t\u01B0\u01A1ng \u1EE9ng v\u1EDBi tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng m\xE0 b\u1EA1n s\u1EBD \xE1p d\u1EE5ng m\xF4 h\xECnh c\u1EE7a m\xECnh."),v.forEach(e)},m(p,v){i(p,o,v),n(o,d)},d(p){p&&e(o)}}}function Pu(B){let o,d,p,v,q,$,k,D,_,C,P;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("Th\u1EED nghi\u1EC7m th\xF4i!"),q=s(" Ch\u1EA1y \u0111o\u1EA1n m\xE3 tr\xEAn v\xE0i l\u1EA7n \u0111\u1EC3 xem vi\u1EC7c che ng\u1EABu nhi\xEAn di\u1EC5n ra ngay tr\u01B0\u1EDBc m\u1EAFt b\u1EA1n! \u0110\u1ED3ng th\u1EDDi  th\u1EED thay th\u1EBF ph\u01B0\u01A1ng th\u1EE9c "),$=l("code"),k=s("tokenizer.decode()"),D=s(" b\u1EB1ng "),_=l("code"),C=s("tokenizer.convert_ids_to_tokens()"),P=s(" \u0111\u1EC3 th\u1EA5y r\u1EB1ng \u0111\xF4i khi m\u1ED9t token t\u1EEB m\u1ED9t t\u1EEB nh\u1EA5t \u0111\u1ECBnh b\u1ECB che, ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1EEFng c\xE1i kh\xE1c.")},l(K){o=c(K,"P",{});var A=r(o);d=a(A,"\u270F\uFE0F "),p=c(A,"STRONG",{});var N=r(p);v=a(N,"Th\u1EED nghi\u1EC7m th\xF4i!"),N.forEach(e),q=a(A," Ch\u1EA1y \u0111o\u1EA1n m\xE3 tr\xEAn v\xE0i l\u1EA7n \u0111\u1EC3 xem vi\u1EC7c che ng\u1EABu nhi\xEAn di\u1EC5n ra ngay tr\u01B0\u1EDBc m\u1EAFt b\u1EA1n! \u0110\u1ED3ng th\u1EDDi  th\u1EED thay th\u1EBF ph\u01B0\u01A1ng th\u1EE9c "),$=c(A,"CODE",{});var H=r($);k=a(H,"tokenizer.decode()"),H.forEach(e),D=a(A," b\u1EB1ng "),_=c(A,"CODE",{});var R=r(_);C=a(R,"tokenizer.convert_ids_to_tokens()"),R.forEach(e),P=a(A," \u0111\u1EC3 th\u1EA5y r\u1EB1ng \u0111\xF4i khi m\u1ED9t token t\u1EEB m\u1ED9t t\u1EEB nh\u1EA5t \u0111\u1ECBnh b\u1ECB che, ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1EEFng c\xE1i kh\xE1c."),A.forEach(e)},m(K,A){i(K,o,A),n(o,d),n(o,p),n(p,v),n(o,q),n(o,$),n($,k),n(o,D),n(o,_),n(_,C),n(o,P)},d(K){K&&e(o)}}}function gu(B){let o,d,p,v,q;return{c(){o=l("p"),d=s("M\u1ED9t t\xE1c d\u1EE5ng ph\u1EE5 c\u1EE7a vi\u1EC7c che ng\u1EABu nhi\xEAn l\xE0 c\xE1c ch\u1EC9 s\u1ED1 \u0111\xE1nh gi\xE1 c\u1EE7a ch\xFAng ta s\u1EBD kh\xF4ng x\xE1c \u0111\u1ECBnh khi s\u1EED d\u1EE5ng "),p=l("code"),v=s("Trainer"),q=s(", v\xEC ch\xFAng ta s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u cho c\xE1c t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 ki\u1EC3m th\u1EE7. Ch\xFAng ta s\u1EBD th\u1EA5y \u1EDF ph\u1EA7n sau, khi ch\xFAng ta xem x\xE9t vi\u1EC7c tinh ch\u1EC9nh v\u1EDBi \u{1F917} Accelerate, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng t\xEDnh linh ho\u1EA1t c\u1EE7a v\xF2ng \u0111\xE1nh gi\xE1 t\xF9y ch\u1EC9nh nh\u01B0 th\u1EBF n\xE0o \u0111\u1EC3 \u0111\xF3ng b\u0103ng t\xEDnh ng\u1EABu nhi\xEAn.")},l($){o=c($,"P",{});var k=r(o);d=a(k,"M\u1ED9t t\xE1c d\u1EE5ng ph\u1EE5 c\u1EE7a vi\u1EC7c che ng\u1EABu nhi\xEAn l\xE0 c\xE1c ch\u1EC9 s\u1ED1 \u0111\xE1nh gi\xE1 c\u1EE7a ch\xFAng ta s\u1EBD kh\xF4ng x\xE1c \u0111\u1ECBnh khi s\u1EED d\u1EE5ng "),p=c(k,"CODE",{});var D=r(p);v=a(D,"Trainer"),D.forEach(e),q=a(k,", v\xEC ch\xFAng ta s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u cho c\xE1c t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 ki\u1EC3m th\u1EE7. Ch\xFAng ta s\u1EBD th\u1EA5y \u1EDF ph\u1EA7n sau, khi ch\xFAng ta xem x\xE9t vi\u1EC7c tinh ch\u1EC9nh v\u1EDBi \u{1F917} Accelerate, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng t\xEDnh linh ho\u1EA1t c\u1EE7a v\xF2ng \u0111\xE1nh gi\xE1 t\xF9y ch\u1EC9nh nh\u01B0 th\u1EBF n\xE0o \u0111\u1EC3 \u0111\xF3ng b\u0103ng t\xEDnh ng\u1EABu nhi\xEAn."),k.forEach(e)},m($,k){i($,o,k),n(o,d),n(o,p),n(p,v),n(o,q)},d($){$&&e(o)}}}function Su(B){let o,d;return o=new M({props:{code:`import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # T\u1EA1o ra \xE1nh x\u1EA1 gi\u1EEFa c\xE1c t\u1EEB v\xE0 ch\u1EC9 m\u1EE5c token t\u01B0\u01A1ng \u1EE9ng
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Che ng\u1EABu nhi\u1EC1n t\u1EEB
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return tf_default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers.data.data_collator <span class="hljs-keyword">import</span> tf_default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># T\u1EA1o ra \xE1nh x\u1EA1 gi\u1EEFa c\xE1c t\u1EEB v\xE0 ch\u1EC9 m\u1EE5c token t\u01B0\u01A1ng \u1EE9ng</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Che ng\u1EABu nhi\u1EC1n t\u1EEB</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> tf_default_data_collator(features)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Ku(B){let o,d;return o=new M({props:{code:`import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # T\u1EA1o ra \xE1nh x\u1EA1 gi\u1EEFa c\xE1c t\u1EEB v\xE0 ch\u1EC9 m\u1EE5c token t\u01B0\u01A1ng \u1EE9ng
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Che ng\u1EABu nhi\u1EC1n t\u1EEB
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    return default_data_collator(features)`,highlighted:`<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

wwm_probability = <span class="hljs-number">0.2</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">whole_word_masking_data_collator</span>(<span class="hljs-params">features</span>):
    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:
        word_ids = feature.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

        <span class="hljs-comment"># T\u1EA1o ra \xE1nh x\u1EA1 gi\u1EEFa c\xE1c t\u1EEB v\xE0 ch\u1EC9 m\u1EE5c token t\u01B0\u01A1ng \u1EE9ng</span>
        mapping = collections.defaultdict(<span class="hljs-built_in">list</span>)
        current_word_index = -<span class="hljs-number">1</span>
        current_word = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> idx, word_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_ids):
            <span class="hljs-keyword">if</span> word_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                <span class="hljs-keyword">if</span> word_id != current_word:
                    current_word = word_id
                    current_word_index += <span class="hljs-number">1</span>
                mapping[current_word_index].append(idx)

        <span class="hljs-comment"># Che ng\u1EABu nhi\u1EC1n t\u1EEB</span>
        mask = np.random.binomial(<span class="hljs-number">1</span>, wwm_probability, (<span class="hljs-built_in">len</span>(mapping),))
        input_ids = feature[<span class="hljs-string">&quot;input_ids&quot;</span>]
        labels = feature[<span class="hljs-string">&quot;labels&quot;</span>]
        new_labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(labels)
        <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> np.where(mask)[<span class="hljs-number">0</span>]:
            word_id = word_id.item()
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id

    <span class="hljs-keyword">return</span> default_data_collator(features)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Ou(B){let o,d,p,v,q,$,k,D,_,C,P;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("Th\u1EED nghi\u1EC7m th\xF4i!"),q=s(" Ch\u1EA1y \u0111o\u1EA1n m\xE3 tr\xEAn v\xE0i l\u1EA7n \u0111\u1EC3 xem vi\u1EC7c che ng\u1EABu nhi\xEAn di\u1EC5n ra ngay tr\u01B0\u1EDBc m\u1EAFt b\u1EA1n! \u0110\u1ED3ng th\u1EDDi  th\u1EED thay th\u1EBF ph\u01B0\u01A1ng th\u1EE9c "),$=l("code"),k=s("tokenizer.decode()"),D=s(" b\u1EB1ng "),_=l("code"),C=s("tokenizer.convert_ids_to_tokens()"),P=s(" \u0111\u1EC3 th\u1EA5y r\u1EB1ng \u0111\xF4i khi m\u1ED9t token t\u1EEB m\u1ED9t t\u1EEB nh\u1EA5t \u0111\u1ECBnh b\u1ECB che, ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1EEFng c\xE1i kh\xE1c.")},l(K){o=c(K,"P",{});var A=r(o);d=a(A,"\u270F\uFE0F "),p=c(A,"STRONG",{});var N=r(p);v=a(N,"Th\u1EED nghi\u1EC7m th\xF4i!"),N.forEach(e),q=a(A," Ch\u1EA1y \u0111o\u1EA1n m\xE3 tr\xEAn v\xE0i l\u1EA7n \u0111\u1EC3 xem vi\u1EC7c che ng\u1EABu nhi\xEAn di\u1EC5n ra ngay tr\u01B0\u1EDBc m\u1EAFt b\u1EA1n! \u0110\u1ED3ng th\u1EDDi  th\u1EED thay th\u1EBF ph\u01B0\u01A1ng th\u1EE9c "),$=c(A,"CODE",{});var H=r($);k=a(H,"tokenizer.decode()"),H.forEach(e),D=a(A," b\u1EB1ng "),_=c(A,"CODE",{});var R=r(_);C=a(R,"tokenizer.convert_ids_to_tokens()"),R.forEach(e),P=a(A," \u0111\u1EC3 th\u1EA5y r\u1EB1ng \u0111\xF4i khi m\u1ED9t token t\u1EEB m\u1ED9t t\u1EEB nh\u1EA5t \u0111\u1ECBnh b\u1ECB che, ch\u1EE9 kh\xF4ng ph\u1EA3i nh\u1EEFng c\xE1i kh\xE1c."),A.forEach(e)},m(K,A){i(K,o,A),n(o,d),n(o,p),n(p,v),n(o,q),n(o,$),n($,k),n(o,D),n(o,_),n(_,C),n(o,P)},d(K){K&&e(o)}}}function Bu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W,y,L,I,G,tt,V,qt,lt,gt,U,_t,Y,Pt,ct,ot,rt,Z,vt,Lt,Ct,St,Nt,ut,Dt,st,Ft,it,Gt,at,It,ht,yt,xt,Vt,$t,S,J,Et,nt,pt,Ht,mt,Zt,X,tn,Tt,wt,Ut,Kt,nn;return k=new M({props:{code:`from transformers import TrainingArguments

batch_size = 64
# In ra s\u1EF1 m\u1EA5t m\xE1t khi hu\u1EA5n luy\u1EC7n \u1EDF m\u1ED7i epoch
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># In ra s\u1EF1 m\u1EA5t m\xE1t khi hu\u1EA5n luy\u1EC7n \u1EDF m\u1ED7i epoch</span>
logging_steps = <span class="hljs-built_in">len</span>(downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>,
    overwrite_output_dir=<span class="hljs-literal">True</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
)`}}),pt=new M({props:{code:`from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>],
    data_collator=data_collator,
)`}}),{c(){o=l("p"),d=s("Khi \u0111\xE3 \u0111\u0103ng nh\u1EADp, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh c\xE1c tham s\u1ED1 cho "),p=l("code"),v=s("Trainer"),q=s(":"),$=u(),w(k.$$.fragment),D=u(),_=l("p"),C=s("\u1EDE \u0111\xE2y, ch\xFAng ta \u0111\xE3 \u0111i\u1EC1u ch\u1EC9nh m\u1ED9t s\u1ED1 t\xF9y ch\u1ECDn m\u1EB7c \u0111\u1ECBnh, bao g\u1ED3m "),P=l("code"),K=s("log_steps"),A=s(" \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o theo d\xF5i s\u1EF1 m\u1EA5t m\xE1t trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n theo t\u1EEBng epoch. Ch\xFAng ta c\u0169ng \u0111\xE3 s\u1EED d\u1EE5ng "),N=l("code"),H=s("fp16=True"),R=s(" \u0111\u1EC3 cho ph\xE9p hu\u1EA5n luy\u1EC7n ch\xEDnh x\xE1c h\u1ED7n h\u1EE3p, gi\xFAp t\u0103ng t\u1ED1c \u0111\u1ED9. Theo m\u1EB7c \u0111\u1ECBnh, "),F=l("code"),W=s("Trainer"),y=s(" s\u1EBD lo\u1EA1i b\u1ECF b\u1EA5t k\u1EF3 c\u1ED9t n\xE0o kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ph\u01B0\u01A1ng th\u1EE9c "),L=l("code"),I=s("forward()"),G=s(" c\u1EE7a m\xF4 h\xECnh. \u0110i\u1EC1u n\xE0y c\xF3 ngh\u0129a l\xE0 n\u1EBFu b\u1EA1n \u0111ang s\u1EED d\u1EE5ng c\xF4ng c\u1EE5 che to\xE0n b\u1ED9 t\u1EEB, b\u1EA1n c\u0169ng c\u1EA7n \u0111\u1EB7t "),tt=l("code"),V=s("remove_unused_columns=False"),qt=s(" \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng ta kh\xF4ng m\u1EA5t c\u1ED9t "),lt=l("code"),gt=s("word_ids"),U=s(" trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n."),_t=u(),Y=l("p"),Pt=s("L\u01B0u \xFD r\u1EB1ng b\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh t\xEAn c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 b\u1EA1n mu\u1ED1n \u0111\u1EA9y \u0111\u1EBFn b\u1EB1ng tham s\u1ED1 "),ct=l("code"),ot=s("hub_model_id"),rt=s(" (c\u1EE5 th\u1EC3 l\xE0 b\u1EA1n s\u1EBD ph\u1EA3i s\u1EED d\u1EE5ng tham s\u1ED1 n\xE0y \u0111\u1EC3 \u0111\u1EA9y \u0111\u1EBFn m\u1ED9t t\u1ED5 ch\u1EE9c). V\xED d\u1EE5: khi ch\xFAng ta \u0111\u1EA9y m\xF4 h\xECnh v\xE0o t\u1ED5 ch\u1EE9c "),Z=l("a"),vt=l("code"),Lt=s("huggingface-course"),Ct=s(", ch\xFAng ta \u0111\xE3 th\xEAm "),St=l("code"),Nt=s('hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),ut=s(" v\xE0o "),Dt=l("code"),st=s("TrainingArguments"),Ft=s(". Theo m\u1EB7c \u0111\u1ECBnh, kho l\u01B0u tr\u1EEF \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng s\u1EBD n\u1EB1m trong kh\xF4ng gian t\xEAn c\u1EE7a b\u1EA1n v\xE0 \u0111\u01B0\u1EE3c \u0111\u1EB7t t\xEAn theo th\u01B0 m\u1EE5c \u0111\u1EA7u ra m\xE0 b\u1EA1n \u0111\xE3 \u0111\u1EB7t, v\xEC v\u1EADy trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta, n\xF3 s\u1EBD l\xE0"),it=l("code"),Gt=s('"lewtun/distilbert-finetuned-imdb"'),at=s("."),It=u(),ht=l("p"),yt=s("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 t\u1EA5t c\u1EA3 c\xE1c th\xE0nh ph\u1EA7n \u0111\u1EC3 t\u1EA1o ra "),xt=l("code"),Vt=s("Trainer"),$t=s(". \u1EDE \u0111\xE2y ch\xFAng ta ch\u1EC9 s\u1EED d\u1EE5ng "),S=l("code"),J=s("data_collator"),Et=s(" ti\xEAu chu\u1EA9n, nh\u01B0ng b\u1EA1n c\xF3 th\u1EC3 th\u1EED to\xE0n b\u1ED9 c\xF4ng c\u1EE5 che to\xE0n b\u1ED9 t\u1EEB v\xE0 so s\xE1nh k\u1EBFt qu\u1EA3 nh\u01B0 m\u1ED9t b\xE0i t\u1EADp:"),nt=u(),w(pt.$$.fragment),Ht=u(),mt=l("p"),Zt=s("Gi\u1EDD ch\xFAng ta \u0111\xE3 s\u1EB5n s\xE0ng ch\u1EA1y "),X=l("code"),tn=s("trainer.train()"),Tt=s(" - nh\u01B0ng tr\u01B0\u1EDBc khi l\xE0m nh\u01B0 v\u1EADy, ch\xFAng ta h\xE3y xem x\xE9t ng\u1EAFn g\u1ECDn "),wt=l("em"),Ut=s("perplexity"),Kt=s(", l\xE0 m\u1ED9t ch\u1EC9 s\u1ED1 ph\u1ED5 bi\u1EBFn \u0111\u1EC3 \u0111\xE1nh gi\xE1 hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF."),this.h()},l(z){o=c(z,"P",{});var Q=r(o);d=a(Q,"Khi \u0111\xE3 \u0111\u0103ng nh\u1EADp, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh c\xE1c tham s\u1ED1 cho "),p=c(Q,"CODE",{});var Ot=r(p);v=a(Ot,"Trainer"),Ot.forEach(e),q=a(Q,":"),Q.forEach(e),$=g(z),j(k.$$.fragment,z),D=g(z),_=c(z,"P",{});var et=r(_);C=a(et,"\u1EDE \u0111\xE2y, ch\xFAng ta \u0111\xE3 \u0111i\u1EC1u ch\u1EC9nh m\u1ED9t s\u1ED1 t\xF9y ch\u1ECDn m\u1EB7c \u0111\u1ECBnh, bao g\u1ED3m "),P=c(et,"CODE",{});var he=r(P);K=a(he,"log_steps"),he.forEach(e),A=a(et," \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o theo d\xF5i s\u1EF1 m\u1EA5t m\xE1t trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n theo t\u1EEBng epoch. Ch\xFAng ta c\u0169ng \u0111\xE3 s\u1EED d\u1EE5ng "),N=c(et,"CODE",{});var un=r(N);H=a(un,"fp16=True"),un.forEach(e),R=a(et," \u0111\u1EC3 cho ph\xE9p hu\u1EA5n luy\u1EC7n ch\xEDnh x\xE1c h\u1ED7n h\u1EE3p, gi\xFAp t\u0103ng t\u1ED1c \u0111\u1ED9. Theo m\u1EB7c \u0111\u1ECBnh, "),F=c(et,"CODE",{});var dt=r(F);W=a(dt,"Trainer"),dt.forEach(e),y=a(et," s\u1EBD lo\u1EA1i b\u1ECF b\u1EA5t k\u1EF3 c\u1ED9t n\xE0o kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ph\u01B0\u01A1ng th\u1EE9c "),L=c(et,"CODE",{});var Tn=r(L);I=a(Tn,"forward()"),Tn.forEach(e),G=a(et," c\u1EE7a m\xF4 h\xECnh. \u0110i\u1EC1u n\xE0y c\xF3 ngh\u0129a l\xE0 n\u1EBFu b\u1EA1n \u0111ang s\u1EED d\u1EE5ng c\xF4ng c\u1EE5 che to\xE0n b\u1ED9 t\u1EEB, b\u1EA1n c\u0169ng c\u1EA7n \u0111\u1EB7t "),tt=c(et,"CODE",{});var Rt=r(tt);V=a(Rt,"remove_unused_columns=False"),Rt.forEach(e),qt=a(et," \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng ta kh\xF4ng m\u1EA5t c\u1ED9t "),lt=c(et,"CODE",{});var Rn=r(lt);gt=a(Rn,"word_ids"),Rn.forEach(e),U=a(et," trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n."),et.forEach(e),_t=g(z),Y=c(z,"P",{});var jt=r(Y);Pt=a(jt,"L\u01B0u \xFD r\u1EB1ng b\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh t\xEAn c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 b\u1EA1n mu\u1ED1n \u0111\u1EA9y \u0111\u1EBFn b\u1EB1ng tham s\u1ED1 "),ct=c(jt,"CODE",{});var Bt=r(ct);ot=a(Bt,"hub_model_id"),Bt.forEach(e),rt=a(jt," (c\u1EE5 th\u1EC3 l\xE0 b\u1EA1n s\u1EBD ph\u1EA3i s\u1EED d\u1EE5ng tham s\u1ED1 n\xE0y \u0111\u1EC3 \u0111\u1EA9y \u0111\u1EBFn m\u1ED9t t\u1ED5 ch\u1EE9c). V\xED d\u1EE5: khi ch\xFAng ta \u0111\u1EA9y m\xF4 h\xECnh v\xE0o t\u1ED5 ch\u1EE9c "),Z=c(jt,"A",{href:!0,rel:!0});var on=r(Z);vt=c(on,"CODE",{});var an=r(vt);Lt=a(an,"huggingface-course"),an.forEach(e),on.forEach(e),Ct=a(jt,", ch\xFAng ta \u0111\xE3 th\xEAm "),St=c(jt,"CODE",{});var qn=r(St);Nt=a(qn,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),qn.forEach(e),ut=a(jt," v\xE0o "),Dt=c(jt,"CODE",{});var Mt=r(Dt);st=a(Mt,"TrainingArguments"),Mt.forEach(e),Ft=a(jt,". Theo m\u1EB7c \u0111\u1ECBnh, kho l\u01B0u tr\u1EEF \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng s\u1EBD n\u1EB1m trong kh\xF4ng gian t\xEAn c\u1EE7a b\u1EA1n v\xE0 \u0111\u01B0\u1EE3c \u0111\u1EB7t t\xEAn theo th\u01B0 m\u1EE5c \u0111\u1EA7u ra m\xE0 b\u1EA1n \u0111\xE3 \u0111\u1EB7t, v\xEC v\u1EADy trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta, n\xF3 s\u1EBD l\xE0"),it=c(jt,"CODE",{});var gn=r(it);Gt=a(gn,'"lewtun/distilbert-finetuned-imdb"'),gn.forEach(e),at=a(jt,"."),jt.forEach(e),It=g(z),ht=c(z,"P",{});var Wt=r(ht);yt=a(Wt,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 t\u1EA5t c\u1EA3 c\xE1c th\xE0nh ph\u1EA7n \u0111\u1EC3 t\u1EA1o ra "),xt=c(Wt,"CODE",{});var Fn=r(xt);Vt=a(Fn,"Trainer"),Fn.forEach(e),$t=a(Wt,". \u1EDE \u0111\xE2y ch\xFAng ta ch\u1EC9 s\u1EED d\u1EE5ng "),S=c(Wt,"CODE",{});var Cn=r(S);J=a(Cn,"data_collator"),Cn.forEach(e),Et=a(Wt," ti\xEAu chu\u1EA9n, nh\u01B0ng b\u1EA1n c\xF3 th\u1EC3 th\u1EED to\xE0n b\u1ED9 c\xF4ng c\u1EE5 che to\xE0n b\u1ED9 t\u1EEB v\xE0 so s\xE1nh k\u1EBFt qu\u1EA3 nh\u01B0 m\u1ED9t b\xE0i t\u1EADp:"),Wt.forEach(e),nt=g(z),j(pt.$$.fragment,z),Ht=g(z),mt=c(z,"P",{});var ft=r(mt);Zt=a(ft,"Gi\u1EDD ch\xFAng ta \u0111\xE3 s\u1EB5n s\xE0ng ch\u1EA1y "),X=c(ft,"CODE",{});var bt=r(X);tn=a(bt,"trainer.train()"),bt.forEach(e),Tt=a(ft," - nh\u01B0ng tr\u01B0\u1EDBc khi l\xE0m nh\u01B0 v\u1EADy, ch\xFAng ta h\xE3y xem x\xE9t ng\u1EAFn g\u1ECDn "),wt=c(ft,"EM",{});var hn=r(wt);Ut=a(hn,"perplexity"),hn.forEach(e),Kt=a(ft,", l\xE0 m\u1ED9t ch\u1EC9 s\u1ED1 ph\u1ED5 bi\u1EBFn \u0111\u1EC3 \u0111\xE1nh gi\xE1 hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF."),ft.forEach(e),this.h()},h(){T(Z,"href","https://huggingface.co/huggingface-course"),T(Z,"rel","nofollow")},m(z,Q){i(z,o,Q),n(o,d),n(o,p),n(p,v),n(o,q),i(z,$,Q),x(k,z,Q),i(z,D,Q),i(z,_,Q),n(_,C),n(_,P),n(P,K),n(_,A),n(_,N),n(N,H),n(_,R),n(_,F),n(F,W),n(_,y),n(_,L),n(L,I),n(_,G),n(_,tt),n(tt,V),n(_,qt),n(_,lt),n(lt,gt),n(_,U),i(z,_t,Q),i(z,Y,Q),n(Y,Pt),n(Y,ct),n(ct,ot),n(Y,rt),n(Y,Z),n(Z,vt),n(vt,Lt),n(Y,Ct),n(Y,St),n(St,Nt),n(Y,ut),n(Y,Dt),n(Dt,st),n(Y,Ft),n(Y,it),n(it,Gt),n(Y,at),i(z,It,Q),i(z,ht,Q),n(ht,yt),n(ht,xt),n(xt,Vt),n(ht,$t),n(ht,S),n(S,J),n(ht,Et),i(z,nt,Q),x(pt,z,Q),i(z,Ht,Q),i(z,mt,Q),n(mt,Zt),n(mt,X),n(X,tn),n(mt,Tt),n(mt,wt),n(wt,Ut),n(mt,Kt),nn=!0},i(z){nn||(f(k.$$.fragment,z),f(pt.$$.fragment,z),nn=!0)},o(z){b(k.$$.fragment,z),b(pt.$$.fragment,z),nn=!1},d(z){z&&e(o),z&&e($),E(k,z),z&&e(D),z&&e(_),z&&e(_t),z&&e(Y),z&&e(It),z&&e(ht),z&&e(nt),E(pt,z),z&&e(Ht),z&&e(mt)}}}function Lu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W,y,L,I,G,tt,V,qt,lt,gt,U,_t,Y,Pt,ct,ot,rt,Z,vt,Lt,Ct,St,Nt,ut,Dt,st,Ft,it,Gt,at,It,ht,yt,xt,Vt,$t;return k=new M({props:{code:`tf_train_dataset = downsampled_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = downsampled_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)`,highlighted:`tf_train_dataset = downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">32</span>,
)

tf_eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">32</span>,
)`}}),st=new M({props:{code:`from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Hu\u1EA5n luy\u1EC7n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>,
    num_warmup_steps=<span class="hljs-number">1_000</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Hu\u1EA5n luy\u1EC7n trong mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-imdb&quot;</span>, tokenizer=tokenizer
)`}}),{c(){o=l("p"),d=s("Khi \u0111\xE3 \u0111\u0103ng nh\u1EADp, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u "),p=l("code"),v=s("tf.data"),q=s(" c\u1EE7a m\xECnh. Ch\xFAng t\xF4i s\u1EBD ch\u1EC9 s\u1EED d\u1EE5ng tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ti\xEAu chu\u1EA9n \u1EDF \u0111\xE2y, nh\u01B0ng b\u1EA1n c\u0169ng c\xF3 th\u1EC3 th\u1EED tr\xECnh \u0111\u1ED1i chi\u1EBFu che to\xE0n b\u1ED9 t\u1EEB v\xE0 so s\xE1nh k\u1EBFt qu\u1EA3 nh\u01B0 m\u1ED9t b\xE0i t\u1EADp:"),$=u(),w(k.$$.fragment),D=u(),_=l("p"),C=s("Ti\u1EBFp theo, ch\xFAng ta thi\u1EBFt l\u1EADp c\xE1c si\xEAu tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 bi\xEAn d\u1ECBch m\xF4 h\xECnh. Ch\xFAng ta s\u1EED d\u1EE5ng h\xE0m "),P=l("code"),K=s("create_optimizer()"),A=s(" t\u1EEB th\u01B0 vi\u1EC7n \u{1F917} Transformers, cung c\u1EA5p cho ch\xFAng ta tr\xECnh t\u1ED1i \u01B0u h\xF3a "),N=l("code"),H=s("AdamW"),R=s(" v\u1EDBi ph\xE2n r\xE3 t\u1ED1c \u0111\u1ED9 h\u1ECDc tuy\u1EBFn t\xEDnh. Ch\xFAng ta c\u0169ng s\u1EED d\u1EE5ng h\xE0m m\u1EA5t m\xE1t c\xF3 s\u1EB5n c\u1EE7a m\xF4 h\xECnh, l\xE0 m\u1EB7c \u0111\u1ECBnh khi kh\xF4ng c\xF3 t\u1ED5n th\u1EA5t n\xE0o \u0111\u01B0\u1EE3c ch\u1EC9 \u0111\u1ECBnh l\xE0m tham s\u1ED1 cho "),F=l("code"),W=s("compile()"),y=s(" v\xE0 \u0111\u1EB7t \u0111\u1ED9 ch\xEDnh x\xE1c hu\u1EA5n luy\u1EC7n th\xE0nh "),L=l("code"),I=s('"mixed_float16"'),G=s(". L\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang s\u1EED d\u1EE5ng GPU Colab ho\u1EB7c GPU kh\xE1c kh\xF4ng c\xF3 h\u1ED7 tr\u1EE3 float16 t\u0103ng t\u1ED1c, b\u1EA1n c\xF3 th\u1EC3 n\xEAn \u0111\u1ED5i d\xF2ng \u0111\xF3 th\xE0nh ch\xFA th\xEDch."),tt=u(),V=l("p"),qt=s("Ngo\xE0i ra, ch\xFAng ta thi\u1EBFt l\u1EADp m\u1ED9t "),lt=l("code"),gt=s("PushToHubCallback"),U=s(" s\u1EBD l\u01B0u m\xF4 h\xECnh v\xE0o Hub sau m\u1ED7i epoch. B\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh t\xEAn c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 b\u1EA1n mu\u1ED1n \u0111\u1EA9y \u0111\u1EBFn b\u1EB1ng tham s\u1ED1 "),_t=l("code"),Y=s("hub_model_id"),Pt=s(" (c\u1EE5 th\u1EC3 l\xE0 b\u1EA1n s\u1EBD ph\u1EA3i s\u1EED d\u1EE5ng tham s\u1ED1 n\xE0y \u0111\u1EC3 \u0111\u1EA9y \u0111\u1EBFn m\u1ED9t t\u1ED5 ch\u1EE9c). V\xED d\u1EE5: \u0111\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0o t\u1ED5 ch\u1EE9c "),ct=l("a"),ot=l("code"),rt=s("huggingface-course"),Z=s(", ch\xFAng t\xF4i \u0111\xE3 th\xEAm "),vt=l("code"),Lt=s('hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),Ct=s(". Theo m\u1EB7c \u0111\u1ECBnh, kho l\u01B0u tr\u1EEF \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng s\u1EBD n\u1EB1m trong kh\xF4ng gian t\xEAn c\u1EE7a b\u1EA1n v\xE0 \u0111\u01B0\u1EE3c \u0111\u1EB7t t\xEAn theo th\u01B0 m\u1EE5c \u0111\u1EA7u ra m\xE0 b\u1EA1n \u0111\xE3 \u0111\u1EB7t, v\xEC v\u1EADy trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng t\xF4i, n\xF3 s\u1EBD l\xE0 "),St=l("code"),Nt=s('"lewtun/distilbert-finetuned-imdb"'),ut=s("."),Dt=u(),w(st.$$.fragment),Ft=u(),it=l("p"),Gt=s("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 s\u1EB5n s\xE0ng ch\u1EA1y "),at=l("code"),It=s("model.fit()"),ht=s(" - nh\u01B0ng tr\u01B0\u1EDBc khi l\xE0m nh\u01B0 v\u1EADy, h\xE3y xem x\xE9t ng\u1EAFn g\u1ECDn "),yt=l("em"),xt=s("perplexity"),Vt=s(", l\xE0 m\u1ED9t ch\u1EC9 s\u1ED1 ph\u1ED5 bi\u1EBFn \u0111\u1EC3 \u0111\xE1nh gi\xE1 hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF."),this.h()},l(S){o=c(S,"P",{});var J=r(o);d=a(J,"Khi \u0111\xE3 \u0111\u0103ng nh\u1EADp, ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u "),p=c(J,"CODE",{});var Et=r(p);v=a(Et,"tf.data"),Et.forEach(e),q=a(J," c\u1EE7a m\xECnh. Ch\xFAng t\xF4i s\u1EBD ch\u1EC9 s\u1EED d\u1EE5ng tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ti\xEAu chu\u1EA9n \u1EDF \u0111\xE2y, nh\u01B0ng b\u1EA1n c\u0169ng c\xF3 th\u1EC3 th\u1EED tr\xECnh \u0111\u1ED1i chi\u1EBFu che to\xE0n b\u1ED9 t\u1EEB v\xE0 so s\xE1nh k\u1EBFt qu\u1EA3 nh\u01B0 m\u1ED9t b\xE0i t\u1EADp:"),J.forEach(e),$=g(S),j(k.$$.fragment,S),D=g(S),_=c(S,"P",{});var nt=r(_);C=a(nt,"Ti\u1EBFp theo, ch\xFAng ta thi\u1EBFt l\u1EADp c\xE1c si\xEAu tham s\u1ED1 hu\u1EA5n luy\u1EC7n v\xE0 bi\xEAn d\u1ECBch m\xF4 h\xECnh. Ch\xFAng ta s\u1EED d\u1EE5ng h\xE0m "),P=c(nt,"CODE",{});var pt=r(P);K=a(pt,"create_optimizer()"),pt.forEach(e),A=a(nt," t\u1EEB th\u01B0 vi\u1EC7n \u{1F917} Transformers, cung c\u1EA5p cho ch\xFAng ta tr\xECnh t\u1ED1i \u01B0u h\xF3a "),N=c(nt,"CODE",{});var Ht=r(N);H=a(Ht,"AdamW"),Ht.forEach(e),R=a(nt," v\u1EDBi ph\xE2n r\xE3 t\u1ED1c \u0111\u1ED9 h\u1ECDc tuy\u1EBFn t\xEDnh. Ch\xFAng ta c\u0169ng s\u1EED d\u1EE5ng h\xE0m m\u1EA5t m\xE1t c\xF3 s\u1EB5n c\u1EE7a m\xF4 h\xECnh, l\xE0 m\u1EB7c \u0111\u1ECBnh khi kh\xF4ng c\xF3 t\u1ED5n th\u1EA5t n\xE0o \u0111\u01B0\u1EE3c ch\u1EC9 \u0111\u1ECBnh l\xE0m tham s\u1ED1 cho "),F=c(nt,"CODE",{});var mt=r(F);W=a(mt,"compile()"),mt.forEach(e),y=a(nt," v\xE0 \u0111\u1EB7t \u0111\u1ED9 ch\xEDnh x\xE1c hu\u1EA5n luy\u1EC7n th\xE0nh "),L=c(nt,"CODE",{});var Zt=r(L);I=a(Zt,'"mixed_float16"'),Zt.forEach(e),G=a(nt,". L\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang s\u1EED d\u1EE5ng GPU Colab ho\u1EB7c GPU kh\xE1c kh\xF4ng c\xF3 h\u1ED7 tr\u1EE3 float16 t\u0103ng t\u1ED1c, b\u1EA1n c\xF3 th\u1EC3 n\xEAn \u0111\u1ED5i d\xF2ng \u0111\xF3 th\xE0nh ch\xFA th\xEDch."),nt.forEach(e),tt=g(S),V=c(S,"P",{});var X=r(V);qt=a(X,"Ngo\xE0i ra, ch\xFAng ta thi\u1EBFt l\u1EADp m\u1ED9t "),lt=c(X,"CODE",{});var tn=r(lt);gt=a(tn,"PushToHubCallback"),tn.forEach(e),U=a(X," s\u1EBD l\u01B0u m\xF4 h\xECnh v\xE0o Hub sau m\u1ED7i epoch. B\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh t\xEAn c\u1EE7a kho l\u01B0u tr\u1EEF m\xE0 b\u1EA1n mu\u1ED1n \u0111\u1EA9y \u0111\u1EBFn b\u1EB1ng tham s\u1ED1 "),_t=c(X,"CODE",{});var Tt=r(_t);Y=a(Tt,"hub_model_id"),Tt.forEach(e),Pt=a(X," (c\u1EE5 th\u1EC3 l\xE0 b\u1EA1n s\u1EBD ph\u1EA3i s\u1EED d\u1EE5ng tham s\u1ED1 n\xE0y \u0111\u1EC3 \u0111\u1EA9y \u0111\u1EBFn m\u1ED9t t\u1ED5 ch\u1EE9c). V\xED d\u1EE5: \u0111\u1EC3 \u0111\u1EA9y m\xF4 h\xECnh v\xE0o t\u1ED5 ch\u1EE9c "),ct=c(X,"A",{href:!0,rel:!0});var wt=r(ct);ot=c(wt,"CODE",{});var Ut=r(ot);rt=a(Ut,"huggingface-course"),Ut.forEach(e),wt.forEach(e),Z=a(X,", ch\xFAng t\xF4i \u0111\xE3 th\xEAm "),vt=c(X,"CODE",{});var Kt=r(vt);Lt=a(Kt,'hub_model_id="huggingface-course/distilbert-finetuned-imdb"'),Kt.forEach(e),Ct=a(X,". Theo m\u1EB7c \u0111\u1ECBnh, kho l\u01B0u tr\u1EEF \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng s\u1EBD n\u1EB1m trong kh\xF4ng gian t\xEAn c\u1EE7a b\u1EA1n v\xE0 \u0111\u01B0\u1EE3c \u0111\u1EB7t t\xEAn theo th\u01B0 m\u1EE5c \u0111\u1EA7u ra m\xE0 b\u1EA1n \u0111\xE3 \u0111\u1EB7t, v\xEC v\u1EADy trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng t\xF4i, n\xF3 s\u1EBD l\xE0 "),St=c(X,"CODE",{});var nn=r(St);Nt=a(nn,'"lewtun/distilbert-finetuned-imdb"'),nn.forEach(e),ut=a(X,"."),X.forEach(e),Dt=g(S),j(st.$$.fragment,S),Ft=g(S),it=c(S,"P",{});var z=r(it);Gt=a(z,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 s\u1EB5n s\xE0ng ch\u1EA1y "),at=c(z,"CODE",{});var Q=r(at);It=a(Q,"model.fit()"),Q.forEach(e),ht=a(z," - nh\u01B0ng tr\u01B0\u1EDBc khi l\xE0m nh\u01B0 v\u1EADy, h\xE3y xem x\xE9t ng\u1EAFn g\u1ECDn "),yt=c(z,"EM",{});var Ot=r(yt);xt=a(Ot,"perplexity"),Ot.forEach(e),Vt=a(z,", l\xE0 m\u1ED9t ch\u1EC9 s\u1ED1 ph\u1ED5 bi\u1EBFn \u0111\u1EC3 \u0111\xE1nh gi\xE1 hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF."),z.forEach(e),this.h()},h(){T(ct,"href","https://huggingface.co/huggingface-course"),T(ct,"rel","nofollow")},m(S,J){i(S,o,J),n(o,d),n(o,p),n(p,v),n(o,q),i(S,$,J),x(k,S,J),i(S,D,J),i(S,_,J),n(_,C),n(_,P),n(P,K),n(_,A),n(_,N),n(N,H),n(_,R),n(_,F),n(F,W),n(_,y),n(_,L),n(L,I),n(_,G),i(S,tt,J),i(S,V,J),n(V,qt),n(V,lt),n(lt,gt),n(V,U),n(V,_t),n(_t,Y),n(V,Pt),n(V,ct),n(ct,ot),n(ot,rt),n(V,Z),n(V,vt),n(vt,Lt),n(V,Ct),n(V,St),n(St,Nt),n(V,ut),i(S,Dt,J),x(st,S,J),i(S,Ft,J),i(S,it,J),n(it,Gt),n(it,at),n(at,It),n(it,ht),n(it,yt),n(yt,xt),n(it,Vt),$t=!0},i(S){$t||(f(k.$$.fragment,S),f(st.$$.fragment,S),$t=!0)},o(S){b(k.$$.fragment,S),b(st.$$.fragment,S),$t=!1},d(S){S&&e(o),S&&e($),E(k,S),S&&e(D),S&&e(_),S&&e(tt),S&&e(V),S&&e(Dt),E(st,S),S&&e(Ft),S&&e(it)}}}function Nu(B){let o,d,p,v,q,$,k,D;return k=new M({props:{code:`import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexity: <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){o=l("p"),d=s("Gi\u1EA3 s\u1EED b\u1ED9 ki\u1EC3m th\u1EED c\u1EE7a ch\xFAng ta bao g\u1ED3m h\u1EA7u h\u1EBFt c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p, th\xEC m\u1ED9t c\xE1ch \u0111\u1EC3 \u0111o l\u01B0\u1EDDng ch\u1EA5t l\u01B0\u1EE3ng c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF l\xE0 t\xEDnh to\xE1n x\xE1c su\u1EA5t n\xF3 g\xE1n cho t\u1EEB ti\u1EBFp theo trong t\u1EA5t c\u1EA3 c\xE1c c\xE2u c\u1EE7a b\u1ED9 ki\u1EC3m th\u1EED. Kh\u1EA3 n\u0103ng x\u1EA3y ra cao ch\u1EC9 ra r\u1EB1ng m\xF4 h\xECnh kh\xF4ng b\u1ECB \u201Cng\u1EA1c nhi\xEAn\u201D ho\u1EB7c \u201Cb\u1ED1i r\u1ED1i\u201D b\u1EDFi c\xE1c m\u1EABu kh\xF4ng nh\xECn th\u1EA5y v\xE0 cho th\u1EA5y n\xF3 \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c c\xE1c m\u1EABu ng\u1EEF ph\xE1p c\u01A1 b\u1EA3n trong ng\xF4n ng\u1EEF. C\xF3 nhi\u1EC1u \u0111\u1ECBnh ngh\u0129a to\xE1n h\u1ECDc kh\xE1c nhau v\u1EC1 perplexity, nh\u01B0ng ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng \u0111\u1ECBnh ngh\u0129a l\xE0 h\xE0m m\u0169 c\u1EE7a m\u1EA5t m\xE1t entropy ch\xE9o. Do \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n perplexity c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),p=l("code"),v=s("Trainer.evaluate()"),q=s(" \u0111\u1EC3 t\xEDnh to\xE1n m\u1EA5t m\xE1t entropy ch\xE9o tr\xEAn t\u1EADp ki\u1EC3m th\u1EED v\xE0 sau \u0111\xF3 l\u1EA5y theo c\u1EA5p s\u1ED1 nh\xE2n c\u1EE7a k\u1EBFt qu\u1EA3:"),$=u(),w(k.$$.fragment)},l(_){o=c(_,"P",{});var C=r(o);d=a(C,"Gi\u1EA3 s\u1EED b\u1ED9 ki\u1EC3m th\u1EED c\u1EE7a ch\xFAng ta bao g\u1ED3m h\u1EA7u h\u1EBFt c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p, th\xEC m\u1ED9t c\xE1ch \u0111\u1EC3 \u0111o l\u01B0\u1EDDng ch\u1EA5t l\u01B0\u1EE3ng c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF l\xE0 t\xEDnh to\xE1n x\xE1c su\u1EA5t n\xF3 g\xE1n cho t\u1EEB ti\u1EBFp theo trong t\u1EA5t c\u1EA3 c\xE1c c\xE2u c\u1EE7a b\u1ED9 ki\u1EC3m th\u1EED. Kh\u1EA3 n\u0103ng x\u1EA3y ra cao ch\u1EC9 ra r\u1EB1ng m\xF4 h\xECnh kh\xF4ng b\u1ECB \u201Cng\u1EA1c nhi\xEAn\u201D ho\u1EB7c \u201Cb\u1ED1i r\u1ED1i\u201D b\u1EDFi c\xE1c m\u1EABu kh\xF4ng nh\xECn th\u1EA5y v\xE0 cho th\u1EA5y n\xF3 \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c c\xE1c m\u1EABu ng\u1EEF ph\xE1p c\u01A1 b\u1EA3n trong ng\xF4n ng\u1EEF. C\xF3 nhi\u1EC1u \u0111\u1ECBnh ngh\u0129a to\xE1n h\u1ECDc kh\xE1c nhau v\u1EC1 perplexity, nh\u01B0ng ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng \u0111\u1ECBnh ngh\u0129a l\xE0 h\xE0m m\u0169 c\u1EE7a m\u1EA5t m\xE1t entropy ch\xE9o. Do \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n perplexity c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),p=c(C,"CODE",{});var P=r(p);v=a(P,"Trainer.evaluate()"),P.forEach(e),q=a(C," \u0111\u1EC3 t\xEDnh to\xE1n m\u1EA5t m\xE1t entropy ch\xE9o tr\xEAn t\u1EADp ki\u1EC3m th\u1EED v\xE0 sau \u0111\xF3 l\u1EA5y theo c\u1EA5p s\u1ED1 nh\xE2n c\u1EE7a k\u1EBFt qu\u1EA3:"),C.forEach(e),$=g(_),j(k.$$.fragment,_)},m(_,C){i(_,o,C),n(o,d),n(o,p),n(p,v),n(o,q),i(_,$,C),x(k,_,C),D=!0},i(_){D||(f(k.$$.fragment,_),D=!0)},o(_){b(k.$$.fragment,_),D=!1},d(_){_&&e(o),_&&e($),E(k,_)}}}function Hu(B){let o,d,p,v,q,$,k,D;return k=new M({props:{code:`import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`<span class="hljs-keyword">import</span> math

eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexity: <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){o=l("p"),d=s("Gi\u1EA3 s\u1EED b\u1ED9 ki\u1EC3m th\u1EED c\u1EE7a ch\xFAng ta bao g\u1ED3m h\u1EA7u h\u1EBFt c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p, th\xEC m\u1ED9t c\xE1ch \u0111\u1EC3 \u0111o l\u01B0\u1EDDng ch\u1EA5t l\u01B0\u1EE3ng c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF l\xE0 t\xEDnh to\xE1n x\xE1c su\u1EA5t n\xF3 g\xE1n cho t\u1EEB ti\u1EBFp theo trong t\u1EA5t c\u1EA3 c\xE1c c\xE2u c\u1EE7a b\u1ED9 ki\u1EC3m th\u1EED. Kh\u1EA3 n\u0103ng x\u1EA3y ra cao ch\u1EC9 ra r\u1EB1ng m\xF4 h\xECnh kh\xF4ng b\u1ECB \u201Cng\u1EA1c nhi\xEAn\u201D ho\u1EB7c \u201Cb\u1ED1i r\u1ED1i\u201D b\u1EDFi c\xE1c m\u1EABu kh\xF4ng nh\xECn th\u1EA5y v\xE0 cho th\u1EA5y n\xF3 \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c c\xE1c m\u1EABu ng\u1EEF ph\xE1p c\u01A1 b\u1EA3n trong ng\xF4n ng\u1EEF. C\xF3 nhi\u1EC1u \u0111\u1ECBnh ngh\u0129a to\xE1n h\u1ECDc kh\xE1c nhau v\u1EC1 perplexity, nh\u01B0ng ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng \u0111\u1ECBnh ngh\u0129a l\xE0 h\xE0m m\u0169 c\u1EE7a m\u1EA5t m\xE1t entropy ch\xE9o. Do \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n perplexity c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),p=l("code"),v=s("Trainer.evaluate()"),q=s(" \u0111\u1EC3 t\xEDnh to\xE1n m\u1EA5t m\xE1t entropy ch\xE9o tr\xEAn t\u1EADp ki\u1EC3m th\u1EED v\xE0 sau \u0111\xF3 l\u1EA5y theo c\u1EA5p s\u1ED1 nh\xE2n c\u1EE7a k\u1EBFt qu\u1EA3:"),$=u(),w(k.$$.fragment)},l(_){o=c(_,"P",{});var C=r(o);d=a(C,"Gi\u1EA3 s\u1EED b\u1ED9 ki\u1EC3m th\u1EED c\u1EE7a ch\xFAng ta bao g\u1ED3m h\u1EA7u h\u1EBFt c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p, th\xEC m\u1ED9t c\xE1ch \u0111\u1EC3 \u0111o l\u01B0\u1EDDng ch\u1EA5t l\u01B0\u1EE3ng c\u1EE7a m\xF4 h\xECnh ng\xF4n ng\u1EEF l\xE0 t\xEDnh to\xE1n x\xE1c su\u1EA5t n\xF3 g\xE1n cho t\u1EEB ti\u1EBFp theo trong t\u1EA5t c\u1EA3 c\xE1c c\xE2u c\u1EE7a b\u1ED9 ki\u1EC3m th\u1EED. Kh\u1EA3 n\u0103ng x\u1EA3y ra cao ch\u1EC9 ra r\u1EB1ng m\xF4 h\xECnh kh\xF4ng b\u1ECB \u201Cng\u1EA1c nhi\xEAn\u201D ho\u1EB7c \u201Cb\u1ED1i r\u1ED1i\u201D b\u1EDFi c\xE1c m\u1EABu kh\xF4ng nh\xECn th\u1EA5y v\xE0 cho th\u1EA5y n\xF3 \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c c\xE1c m\u1EABu ng\u1EEF ph\xE1p c\u01A1 b\u1EA3n trong ng\xF4n ng\u1EEF. C\xF3 nhi\u1EC1u \u0111\u1ECBnh ngh\u0129a to\xE1n h\u1ECDc kh\xE1c nhau v\u1EC1 perplexity, nh\u01B0ng ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng \u0111\u1ECBnh ngh\u0129a l\xE0 h\xE0m m\u0169 c\u1EE7a m\u1EA5t m\xE1t entropy ch\xE9o. Do \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 t\xEDnh to\xE1n perplexity c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),p=c(C,"CODE",{});var P=r(p);v=a(P,"Trainer.evaluate()"),P.forEach(e),q=a(C," \u0111\u1EC3 t\xEDnh to\xE1n m\u1EA5t m\xE1t entropy ch\xE9o tr\xEAn t\u1EADp ki\u1EC3m th\u1EED v\xE0 sau \u0111\xF3 l\u1EA5y theo c\u1EA5p s\u1ED1 nh\xE2n c\u1EE7a k\u1EBFt qu\u1EA3:"),C.forEach(e),$=g(_),j(k.$$.fragment,_)},m(_,C){i(_,o,C),n(o,d),n(o,p),n(p,v),n(o,q),i(_,$,C),x(k,_,C),D=!0},i(_){D||(f(k.$$.fragment,_),D=!0)},o(_){b(k.$$.fragment,_),D=!1},d(_){_&&e(o),_&&e($),E(k,_)}}}function Ru(B){let o,d;return o=new M({props:{code:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])",highlighted:"model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Fu(B){let o,d;return o=new M({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Gu(B){let o,d;return o=new M({props:{code:`eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")`,highlighted:`eval_loss = model.evaluate(tf_eval_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexity: <span class="hljs-subst">{math.exp(eval_loss):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function Iu(B){let o,d;return o=new M({props:{code:`eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")`,highlighted:`eval_results = trainer.evaluate()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Perplexity: <span class="hljs-subst">{math.exp(eval_results[<span class="hljs-string">&#x27;eval_loss&#x27;</span>]):<span class="hljs-number">.2</span>f}</span>&quot;</span>)`}}),{c(){w(o.$$.fragment)},l(p){j(o.$$.fragment,p)},m(p,v){x(o,p,v),d=!0},i(p){d||(f(o.$$.fragment,p),d=!0)},o(p){b(o.$$.fragment,p),d=!1},d(p){E(o,p)}}}function _u(B){let o,d,p,v,q,$,k,D;return k=new M({props:{code:"trainer.push_to_hub()",highlighted:"trainer.push_to_hub()"}}),{c(){o=l("p"),d=s("Sau khi qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y th\u1EBB m\xF4 h\xECnh c\xF3 th\xF4ng tin hu\u1EA5n luy\u1EC7n v\xE0o Hub (c\xE1c checkpoint \u0111\u01B0\u1EE3c l\u01B0u trong qu\xE1 tr\xECnh t\u1EF1 hu\u1EA5n luy\u1EC7n):"),p=u(),v=l("p"),q=s("Once training is finished, we can push the model card with the training information to the Hub (the checkpoints are saved during training itself):"),$=u(),w(k.$$.fragment)},l(_){o=c(_,"P",{});var C=r(o);d=a(C,"Sau khi qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n k\u1EBFt th\xFAc, ch\xFAng ta c\xF3 th\u1EC3 \u0111\u1EA9y th\u1EBB m\xF4 h\xECnh c\xF3 th\xF4ng tin hu\u1EA5n luy\u1EC7n v\xE0o Hub (c\xE1c checkpoint \u0111\u01B0\u1EE3c l\u01B0u trong qu\xE1 tr\xECnh t\u1EF1 hu\u1EA5n luy\u1EC7n):"),C.forEach(e),p=g(_),v=c(_,"P",{});var P=r(v);q=a(P,"Once training is finished, we can push the model card with the training information to the Hub (the checkpoints are saved during training itself):"),P.forEach(e),$=g(_),j(k.$$.fragment,_)},m(_,C){i(_,o,C),n(o,d),i(_,p,C),i(_,v,C),n(v,q),i(_,$,C),x(k,_,C),D=!0},i(_){D||(f(k.$$.fragment,_),D=!0)},o(_){b(k.$$.fragment,_),D=!1},d(_){_&&e(o),_&&e(p),_&&e(v),_&&e($),E(k,_)}}}function Vu(B){let o,d,p,v,q;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("\u0110\u1EBFn l\u01B0\u1EE3t b\u1EA1n!"),q=s(" Ch\u1EA1y b\u01B0\u1EDBchu\u1EA5n luy\u1EC7n tr\xEAn sau khi thay \u0111\u1ED5i tr\xECnh thu th\u1EADp d\u1EEF li\u1EC7u th\xE0nh che to\xE0n b\u1ED9 t\u1EEB. B\u1EA1n c\xF3 nh\u1EADn \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t h\u01A1n kh\xF4ng?")},l($){o=c($,"P",{});var k=r(o);d=a(k,"\u270F\uFE0F "),p=c(k,"STRONG",{});var D=r(p);v=a(D,"\u0110\u1EBFn l\u01B0\u1EE3t b\u1EA1n!"),D.forEach(e),q=a(k," Ch\u1EA1y b\u01B0\u1EDBchu\u1EA5n luy\u1EC7n tr\xEAn sau khi thay \u0111\u1ED5i tr\xECnh thu th\u1EADp d\u1EEF li\u1EC7u th\xE0nh che to\xE0n b\u1ED9 t\u1EEB. B\u1EA1n c\xF3 nh\u1EADn \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t h\u01A1n kh\xF4ng?"),k.forEach(e)},m($,k){i($,o,k),n(o,d),n(o,p),n(p,v),n(o,q)},d($){$&&e(o)}}}function du(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W,y,L,I,G,tt,V,qt,lt,gt,U,_t,Y,Pt,ct,ot,rt,Z,vt,Lt,Ct,St,Nt,ut,Dt,st,Ft,it,Gt,at,It,ht,yt,xt,Vt,$t,S,J,Et,nt,pt,Ht,mt,Zt,X,tn,Tt,wt,Ut,Kt,nn,z,Q,Ot,et,he,un,dt,Tn,Rt,Rn,jt,Bt,on,an,qn,Mt,gn,Wt,Fn,Cn,ft,bt,hn,rn,Ie,ie,en,le,kt,ge,_n,Ve,_e;return k=new Ge({}),ot=new M({props:{code:`def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # T\u1EA1o ra m\u1ED9t c\u1ED9t "masked" m\u1EDBi cho m\u1ED7i c\u1ED9t trong b\u1ED9 d\u1EEF li\u1EC7u
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">insert_random_mask</span>(<span class="hljs-params">batch</span>):
    features = [<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(batch, t)) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(*batch.values())]
    masked_inputs = data_collator(features)
    <span class="hljs-comment"># T\u1EA1o ra m\u1ED9t c\u1ED9t &quot;masked&quot; m\u1EDBi cho m\u1ED7i c\u1ED9t trong b\u1ED9 d\u1EEF li\u1EC7u</span>
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;masked_&quot;</span> + k: v.numpy() <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> masked_inputs.items()}`}}),ut=new M({props:{code:`downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)`,highlighted:`downsampled_dataset = downsampled_dataset.remove_columns([<span class="hljs-string">&quot;word_ids&quot;</span>])
eval_dataset = downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].<span class="hljs-built_in">map</span>(
    insert_random_mask,
    batched=<span class="hljs-literal">True</span>,
    remove_columns=downsampled_dataset[<span class="hljs-string">&quot;test&quot;</span>].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        <span class="hljs-string">&quot;masked_input_ids&quot;</span>: <span class="hljs-string">&quot;input_ids&quot;</span>,
        <span class="hljs-string">&quot;masked_attention_mask&quot;</span>: <span class="hljs-string">&quot;attention_mask&quot;</span>,
        <span class="hljs-string">&quot;masked_labels&quot;</span>: <span class="hljs-string">&quot;labels&quot;</span>,
    }
)`}}),ht=new M({props:{code:`from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator

batch_size = <span class="hljs-number">64</span>
train_dataloader = DataLoader(
    downsampled_dataset[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)`}}),S=new M({props:{code:"model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)",highlighted:'model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoModelForMaskedLM</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">model_checkpoint</span>)</span>'}}),X=new M({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`}}),Q=new M({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),dt=new M({props:{code:`from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">3</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),Bt=new M({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-imdb-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),an=new M({props:{code:"'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/distilbert-base-uncased-finetuned-imdb-accelerate&#x27;</span>'}}),bt=new M({props:{code:`from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)`}}),en=new M({props:{code:`from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Hu\u1EA5n luy\u1EC7n
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # \u0110\xE1nh gi\xE1
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # L\u01B0u v\xE0 t\u1EA3i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> math

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Hu\u1EA5n luy\u1EC7n</span>
    model.train()
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># \u0110\xE1nh gi\xE1</span>
    model.<span class="hljs-built_in">eval</span>()
    losses = []
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: <span class="hljs-built_in">len</span>(eval_dataset)]
    <span class="hljs-keyword">try</span>:
        perplexity = math.exp(torch.mean(losses))
    <span class="hljs-keyword">except</span> OverflowError:
        perplexity = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)

    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; Epoch <span class="hljs-subst">{epoch}</span>: Perplexity: <span class="hljs-subst">{perplexity}</span>&quot;</span>)

    <span class="hljs-comment"># L\u01B0u v\xE0 t\u1EA3i</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),kt=new M({props:{code:`Epoch 0: Perplexity: 11.397545307900472
Epoch 1: Perplexity: 10.904909330983092
Epoch 2: Perplexity: 10.729503505340409`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">0</span>: Perplexity: <span class="hljs-number">11.397545307900472</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">1</span>: Perplexity: <span class="hljs-number">10.904909330983092</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>Epoch <span class="hljs-number">2</span>: Perplexity: <span class="hljs-number">10.729503505340409</span>`}}),{c(){o=l("p"),d=s("Trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a m\xECnh, ch\xFAng ta kh\xF4ng c\u1EA7n l\xE0m g\xEC \u0111\u1EB7c bi\u1EC7t v\u1EDBi v\xF2ng hu\u1EA5n luy\u1EC7n, nh\u01B0ng m\u1ED9t s\u1ED1 tr\u01B0\u1EDDng h\u1EE3p b\u1EA1n s\u1EBD c\u1EA7n ph\u1EA3i tri\u1EC3n khai m\u1ED9t s\u1ED1 logic tu\u1EF3 ch\u1EC9nh. V\u1EDBi nh\u1EEFng \u1EE9ng d\u1EE5ng n\xE0y, b\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u{1F917} Accelerate \u2014 h\xE3y c\u0169ng xem xem!"),p=u(),v=l("h2"),q=l("a"),$=l("span"),w(k.$$.fragment),D=u(),_=l("span"),C=s("Tinh ch\u1EC9nh DistilBERT v\u1EDBi \u{1F917} Accelerate"),P=u(),K=l("p"),A=s("Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y v\u1EDBi "),N=l("code"),H=s("Trainer"),R=s(", vi\u1EC7c tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA3n \u0111i r\u1EA5t gi\u1ED1ng v\u1EDBi v\xED d\u1EE5 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n t\u1EEB "),F=l("a"),W=s("Chapter 3"),y=s(". Tr\xEAn th\u1EF1c t\u1EBF, s\u1EF1 tinh t\u1EBF duy nh\u1EA5t l\xE0 vi\u1EC7c s\u1EED d\u1EE5ng m\u1ED9t c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t v\xE0 ch\xFAng ta \u0111\xE3 \u0111\u1EC1 c\u1EADp \u0111\u1EBFn \u0111i\u1EC1u \u0111\xF3 tr\u01B0\u1EDBc \u0111\xF3 trong ph\u1EA7n n\xE0y!"),L=u(),I=l("p"),G=s("Tuy nhi\xEAn, ch\xFAng ta th\u1EA5y r\u1EB1ng "),tt=l("code"),V=s("DataCollatorForLanguageModeling"),qt=s(" c\u0169ng \xE1p d\u1EE5ng t\xEDnh n\u0103ng che ng\u1EABu nhi\xEAn v\u1EDBi m\u1ED7i l\u1EA7n \u0111\xE1nh gi\xE1, v\xEC v\u1EADy ch\xFAng ta s\u1EBD th\u1EA5y m\u1ED9t s\u1ED1 bi\u1EBFn \u0111\u1ED9ng v\u1EC1 perplexity v\u1EDBi m\u1ED7i l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n. M\u1ED9t c\xE1ch \u0111\u1EC3 lo\u1EA1i b\u1ECF t\xEDnh ng\u1EABu nhi\xEAn n\xE0y l\xE0 \xE1p d\u1EE5ng  che "),lt=l("em"),gt=s("ch\u1EC9 m\u1ED9t l\u1EA7n"),U=s(" tr\xEAn to\xE0n b\u1ED9 t\u1EADp ki\u1EC3m th\u1EED, sau \u0111\xF3 s\u1EED d\u1EE5ng tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u m\u1EB7c \u0111\u1ECBnh trong \u{1F917} Transformers \u0111\u1EC3 thu th\u1EADp c\xE1c l\xF4 trong qu\xE1 tr\xECnh \u0111\xE1nh gi\xE1. \u0110\u1EC3 xem c\xE1ch n\xE0y ho\u1EA1t \u0111\u1ED9ng nh\u01B0 th\u1EBF n\xE0o, h\xE3y tri\u1EC3n khai m\u1ED9t ch\u1EE9c n\u0103ng \u0111\u01A1n gi\u1EA3n \xE1p d\u1EE5ng che tr\xEAn m\u1ED9t l\xF4, t\u01B0\u01A1ng t\u1EF1 nh\u01B0 l\u1EA7n \u0111\u1EA7u c\u1EE7a ch\xFAng ta v\u1EDBi "),_t=l("code"),Y=s("DataCollatorForLanguageModeling"),Pt=s(":"),ct=u(),w(ot.$$.fragment),rt=u(),Z=l("p"),vt=s("Ti\u1EBFp theo, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng n\xE0y cho t\u1EADp ki\u1EC3m th\u1EED c\u1EE7a m\xECnh v\xE0 b\u1ECF c\xE1c c\u1ED9t kh\xF4ng che \u0111\u1EC3 c\xF3 th\u1EC3 thay th\u1EBF ch\xFAng b\u1EB1ng nh\u1EEFng c\u1ED9t b\u1ECB che. B\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng che to\xE0n b\u1ED9 t\u1EEB b\u1EB1ng c\xE1ch thay th\u1EBF "),Lt=l("code"),Ct=s("data_collator"),St=s(" \u1EDF tr\xEAn b\u1EB1ng c\xE1i th\xEDch h\u1EE3p, trong tr\u01B0\u1EDDng h\u1EE3p \u0111\xF3, b\u1EA1n n\xEAn x\xF3a d\xF2ng \u0111\u1EA7u ti\xEAn t\u1EA1i \u0111\xE2y:"),Nt=u(),w(ut.$$.fragment),Dt=u(),st=l("p"),Ft=s("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 thi\u1EBFt l\u1EADp b\u1ED9 l\u01B0u d\u1EEF li\u1EC7u nh\u01B0 b\xECnh th\u01B0\u1EDDng, nh\u01B0ng ta s\u1EBD s\u1EED d\u1EE5ng "),it=l("code"),Gt=s("default_data_collator"),at=s(" t\u1EEB \u{1F917} Transformers cho t\u1EADp ki\u1EC3m \u0111\u1ECBnh:"),It=u(),w(ht.$$.fragment),yt=u(),xt=l("p"),Vt=s("T\u1EEB \u0111\xE2y, ch\xFAng ta l\xE0m theo c\xE1c b\u01B0\u1EDBc ti\xEAu chu\u1EA9n v\u1EDBi \u{1F917} Accelerate. Y\xEAu c\u1EA7u \u0111\u1EA7u ti\xEAn c\u1EE7a c\xF4ng vi\u1EC7c l\xE0 t\u1EA3i m\u1ED9t phi\xEAn b\u1EA3n m\u1EDBi c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc:"),$t=u(),w(S.$$.fragment),J=u(),Et=l("p"),nt=s("Sau \u0111\xF3, ch\xFAng ta c\u1EA7n ch\u1EC9 \u0111\u1ECBnh tr\xECnh t\u1ED1i \u01B0u h\xF3a; ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng ti\xEAu chu\u1EA9n "),pt=l("code"),Ht=s("AdamW"),mt=s(":"),Zt=u(),w(X.$$.fragment),tn=u(),Tt=l("p"),wt=s("V\u1EDBi nh\u1EEFng \u0111\u1ED1i t\u01B0\u1EE3ng n\xE0y, b\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 chu\u1EA9n b\u1ECB m\u1ECDi th\u1EE9 cho qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n v\u1EDBi \u0111\u1ED1i t\u01B0\u1EE3ng "),Ut=l("code"),Kt=s("Accelerator"),nn=s(":"),z=u(),w(Q.$$.fragment),Ot=u(),et=l("p"),he=s("B\xE2y gi\u1EDD m\xF4 h\xECnh, tr\xECnh t\u1ED1i \u01B0u h\xF3a v\xE0 b\u1ED9 ghi d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta \u0111\xE3 \u0111\u01B0\u1EE3c \u0111\u1ECBnh c\u1EA5u h\xECnh, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh b\u1ED9 l\u1EADp l\u1ECBch t\u1ED1c \u0111\u1ED9 h\u1ECDc nh\u01B0 sau:"),un=u(),w(dt.$$.fragment),Tn=u(),Rt=l("p"),Rn=s("Ch\u1EC9 c\xF3 m\u1ED9t \u0111i\u1EC1u cu\u1ED1i c\xF9ng c\u1EA7n l\xE0m tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n: t\u1EA1o m\u1ED9t kho l\u01B0u tr\u1EEF m\xF4 h\xECnh tr\xEAn Hugging Face Hub! Tr\u01B0\u1EDBc ti\xEAn, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng th\u01B0 vi\u1EC7n \u{1F917} Hub \u0111\u1EC3 t\u1EA1o t\xEAn \u0111\u1EA7y \u0111\u1EE7 cho repo c\u1EE7a m\xECnh:"),jt=u(),w(Bt.$$.fragment),on=u(),w(an.$$.fragment),qn=u(),Mt=l("p"),gn=s("sau \u0111\xF3 t\u1EA1o v\xE0 sao ch\xE9p kho l\u01B0u tr\u1EEF b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),Wt=l("code"),Fn=s("Repository"),Cn=s(" t\u1EEB \u{1F917} Hub:"),ft=u(),w(bt.$$.fragment),hn=u(),rn=l("p"),Ie=s("Sau khi th\u1EF1c hi\u1EC7n xong, vi\u1EC7c vi\u1EBFt ra to\xE0n b\u1ED9 v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n v\xE0 \u0111\xE1nh gi\xE1 ch\u1EC9 l\xE0 m\u1ED9t v\u1EA5n \u0111\u1EC1 \u0111\u01A1n gi\u1EA3n:"),ie=u(),w(en.$$.fragment),le=u(),w(kt.$$.fragment),ge=u(),_n=l("p"),Ve=s("Tuy\u1EC7t v\u1EDDi, ch\xFAng t\xF4i \u0111\xE3 c\xF3 th\u1EC3 \u0111\xE1nh gi\xE1 m\u1EE9c \u0111\u1ED9 ph\u1EE9c t\u1EA1p theo t\u1EEBng epoch v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng nhi\u1EC1u l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n c\xF3 th\u1EC3 t\xE1i t\u1EA1o!"),this.h()},l(m){o=c(m,"P",{});var O=r(o);d=a(O,"Trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a m\xECnh, ch\xFAng ta kh\xF4ng c\u1EA7n l\xE0m g\xEC \u0111\u1EB7c bi\u1EC7t v\u1EDBi v\xF2ng hu\u1EA5n luy\u1EC7n, nh\u01B0ng m\u1ED9t s\u1ED1 tr\u01B0\u1EDDng h\u1EE3p b\u1EA1n s\u1EBD c\u1EA7n ph\u1EA3i tri\u1EC3n khai m\u1ED9t s\u1ED1 logic tu\u1EF3 ch\u1EC9nh. V\u1EDBi nh\u1EEFng \u1EE9ng d\u1EE5ng n\xE0y, b\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u{1F917} Accelerate \u2014 h\xE3y c\u0169ng xem xem!"),O.forEach(e),p=g(m),v=c(m,"H2",{class:!0});var de=r(v);q=c(de,"A",{id:!0,class:!0,href:!0});var Gn=r(q);$=c(Gn,"SPAN",{});var Zs=r($);j(k.$$.fragment,Zs),Zs.forEach(e),Gn.forEach(e),D=g(de),_=c(de,"SPAN",{});var ta=r(_);C=a(ta,"Tinh ch\u1EC9nh DistilBERT v\u1EDBi \u{1F917} Accelerate"),ta.forEach(e),de.forEach(e),P=g(m),K=c(m,"P",{});var dn=r(K);A=a(dn,"Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y v\u1EDBi "),N=c(dn,"CODE",{});var na=r(N);H=a(na,"Trainer"),na.forEach(e),R=a(dn,", vi\u1EC7c tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA3n \u0111i r\u1EA5t gi\u1ED1ng v\u1EDBi v\xED d\u1EE5 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n t\u1EEB "),F=c(dn,"A",{href:!0});var ea=r(F);W=a(ea,"Chapter 3"),ea.forEach(e),y=a(dn,". Tr\xEAn th\u1EF1c t\u1EBF, s\u1EF1 tinh t\u1EBF duy nh\u1EA5t l\xE0 vi\u1EC7c s\u1EED d\u1EE5ng m\u1ED9t c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t v\xE0 ch\xFAng ta \u0111\xE3 \u0111\u1EC1 c\u1EADp \u0111\u1EBFn \u0111i\u1EC1u \u0111\xF3 tr\u01B0\u1EDBc \u0111\xF3 trong ph\u1EA7n n\xE0y!"),dn.forEach(e),L=g(m),I=c(m,"P",{});var pn=r(I);G=a(pn,"Tuy nhi\xEAn, ch\xFAng ta th\u1EA5y r\u1EB1ng "),tt=c(pn,"CODE",{});var In=r(tt);V=a(In,"DataCollatorForLanguageModeling"),In.forEach(e),qt=a(pn," c\u0169ng \xE1p d\u1EE5ng t\xEDnh n\u0103ng che ng\u1EABu nhi\xEAn v\u1EDBi m\u1ED7i l\u1EA7n \u0111\xE1nh gi\xE1, v\xEC v\u1EADy ch\xFAng ta s\u1EBD th\u1EA5y m\u1ED9t s\u1ED1 bi\u1EBFn \u0111\u1ED9ng v\u1EC1 perplexity v\u1EDBi m\u1ED7i l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n. M\u1ED9t c\xE1ch \u0111\u1EC3 lo\u1EA1i b\u1ECF t\xEDnh ng\u1EABu nhi\xEAn n\xE0y l\xE0 \xE1p d\u1EE5ng  che "),lt=c(pn,"EM",{});var Ue=r(lt);gt=a(Ue,"ch\u1EC9 m\u1ED9t l\u1EA7n"),Ue.forEach(e),U=a(pn," tr\xEAn to\xE0n b\u1ED9 t\u1EADp ki\u1EC3m th\u1EED, sau \u0111\xF3 s\u1EED d\u1EE5ng tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u m\u1EB7c \u0111\u1ECBnh trong \u{1F917} Transformers \u0111\u1EC3 thu th\u1EADp c\xE1c l\xF4 trong qu\xE1 tr\xECnh \u0111\xE1nh gi\xE1. \u0110\u1EC3 xem c\xE1ch n\xE0y ho\u1EA1t \u0111\u1ED9ng nh\u01B0 th\u1EBF n\xE0o, h\xE3y tri\u1EC3n khai m\u1ED9t ch\u1EE9c n\u0103ng \u0111\u01A1n gi\u1EA3n \xE1p d\u1EE5ng che tr\xEAn m\u1ED9t l\xF4, t\u01B0\u01A1ng t\u1EF1 nh\u01B0 l\u1EA7n \u0111\u1EA7u c\u1EE7a ch\xFAng ta v\u1EDBi "),_t=c(pn,"CODE",{});var ce=r(_t);Y=a(ce,"DataCollatorForLanguageModeling"),ce.forEach(e),Pt=a(pn,":"),pn.forEach(e),ct=g(m),j(ot.$$.fragment,m),rt=g(m),Z=c(m,"P",{});var fe=r(Z);vt=a(fe,"Ti\u1EBFp theo, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng n\xE0y cho t\u1EADp ki\u1EC3m th\u1EED c\u1EE7a m\xECnh v\xE0 b\u1ECF c\xE1c c\u1ED9t kh\xF4ng che \u0111\u1EC3 c\xF3 th\u1EC3 thay th\u1EBF ch\xFAng b\u1EB1ng nh\u1EEFng c\u1ED9t b\u1ECB che. B\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng che to\xE0n b\u1ED9 t\u1EEB b\u1EB1ng c\xE1ch thay th\u1EBF "),Lt=c(fe,"CODE",{});var We=r(Lt);Ct=a(We,"data_collator"),We.forEach(e),St=a(fe," \u1EDF tr\xEAn b\u1EB1ng c\xE1i th\xEDch h\u1EE3p, trong tr\u01B0\u1EDDng h\u1EE3p \u0111\xF3, b\u1EA1n n\xEAn x\xF3a d\xF2ng \u0111\u1EA7u ti\xEAn t\u1EA1i \u0111\xE2y:"),fe.forEach(e),Nt=g(m),j(ut.$$.fragment,m),Dt=g(m),st=c(m,"P",{});var Yt=r(st);Ft=a(Yt,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 thi\u1EBFt l\u1EADp b\u1ED9 l\u01B0u d\u1EEF li\u1EC7u nh\u01B0 b\xECnh th\u01B0\u1EDDng, nh\u01B0ng ta s\u1EBD s\u1EED d\u1EE5ng "),it=c(Yt,"CODE",{});var ln=r(it);Gt=a(ln,"default_data_collator"),ln.forEach(e),at=a(Yt," t\u1EEB \u{1F917} Transformers cho t\u1EADp ki\u1EC3m \u0111\u1ECBnh:"),Yt.forEach(e),It=g(m),j(ht.$$.fragment,m),yt=g(m),xt=c(m,"P",{});var oe=r(xt);Vt=a(oe,"T\u1EEB \u0111\xE2y, ch\xFAng ta l\xE0m theo c\xE1c b\u01B0\u1EDBc ti\xEAu chu\u1EA9n v\u1EDBi \u{1F917} Accelerate. Y\xEAu c\u1EA7u \u0111\u1EA7u ti\xEAn c\u1EE7a c\xF4ng vi\u1EC7c l\xE0 t\u1EA3i m\u1ED9t phi\xEAn b\u1EA3n m\u1EDBi c\u1EE7a m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc:"),oe.forEach(e),$t=g(m),j(S.$$.fragment,m),J=g(m),Et=c(m,"P",{});var fn=r(Et);nt=a(fn,"Sau \u0111\xF3, ch\xFAng ta c\u1EA7n ch\u1EC9 \u0111\u1ECBnh tr\xECnh t\u1ED1i \u01B0u h\xF3a; ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng ti\xEAu chu\u1EA9n "),pt=c(fn,"CODE",{});var Ye=r(pt);Ht=a(Ye,"AdamW"),Ye.forEach(e),mt=a(fn,":"),fn.forEach(e),Zt=g(m),j(X.$$.fragment,m),tn=g(m),Tt=c(m,"P",{});var Dn=r(Tt);wt=a(Dn,"V\u1EDBi nh\u1EEFng \u0111\u1ED1i t\u01B0\u1EE3ng n\xE0y, b\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 chu\u1EA9n b\u1ECB m\u1ECDi th\u1EE9 cho qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n v\u1EDBi \u0111\u1ED1i t\u01B0\u1EE3ng "),Ut=c(Dn,"CODE",{});var sa=r(Ut);Kt=a(sa,"Accelerator"),sa.forEach(e),nn=a(Dn,":"),Dn.forEach(e),z=g(m),j(Q.$$.fragment,m),Ot=g(m),et=c(m,"P",{});var Je=r(et);he=a(Je,"B\xE2y gi\u1EDD m\xF4 h\xECnh, tr\xECnh t\u1ED1i \u01B0u h\xF3a v\xE0 b\u1ED9 ghi d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta \u0111\xE3 \u0111\u01B0\u1EE3c \u0111\u1ECBnh c\u1EA5u h\xECnh, ch\xFAng ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh b\u1ED9 l\u1EADp l\u1ECBch t\u1ED1c \u0111\u1ED9 h\u1ECDc nh\u01B0 sau:"),Je.forEach(e),un=g(m),j(dt.$$.fragment,m),Tn=g(m),Rt=c(m,"P",{});var bn=r(Rt);Rn=a(bn,"Ch\u1EC9 c\xF3 m\u1ED9t \u0111i\u1EC1u cu\u1ED1i c\xF9ng c\u1EA7n l\xE0m tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n: t\u1EA1o m\u1ED9t kho l\u01B0u tr\u1EEF m\xF4 h\xECnh tr\xEAn Hugging Face Hub! Tr\u01B0\u1EDBc ti\xEAn, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng th\u01B0 vi\u1EC7n \u{1F917} Hub \u0111\u1EC3 t\u1EA1o t\xEAn \u0111\u1EA7y \u0111\u1EE7 cho repo c\u1EE7a m\xECnh:"),bn.forEach(e),jt=g(m),j(Bt.$$.fragment,m),on=g(m),j(an.$$.fragment,m),qn=g(m),Mt=c(m,"P",{});var mn=r(Mt);gn=a(mn,"sau \u0111\xF3 t\u1EA1o v\xE0 sao ch\xE9p kho l\u01B0u tr\u1EEF b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),Wt=c(mn,"CODE",{});var be=r(Wt);Fn=a(be,"Repository"),be.forEach(e),Cn=a(mn," t\u1EEB \u{1F917} Hub:"),mn.forEach(e),ft=g(m),j(bt.$$.fragment,m),hn=g(m),rn=c(m,"P",{});var Vn=r(rn);Ie=a(Vn,"Sau khi th\u1EF1c hi\u1EC7n xong, vi\u1EC7c vi\u1EBFt ra to\xE0n b\u1ED9 v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n v\xE0 \u0111\xE1nh gi\xE1 ch\u1EC9 l\xE0 m\u1ED9t v\u1EA5n \u0111\u1EC1 \u0111\u01A1n gi\u1EA3n:"),Vn.forEach(e),ie=g(m),j(en.$$.fragment,m),le=g(m),j(kt.$$.fragment,m),ge=g(m),_n=c(m,"P",{});var aa=r(_n);Ve=a(aa,"Tuy\u1EC7t v\u1EDDi, ch\xFAng t\xF4i \u0111\xE3 c\xF3 th\u1EC3 \u0111\xE1nh gi\xE1 m\u1EE9c \u0111\u1ED9 ph\u1EE9c t\u1EA1p theo t\u1EEBng epoch v\xE0 \u0111\u1EA3m b\u1EA3o r\u1EB1ng nhi\u1EC1u l\u1EA7n ch\u1EA1y hu\u1EA5n luy\u1EC7n c\xF3 th\u1EC3 t\xE1i t\u1EA1o!"),aa.forEach(e),this.h()},h(){T(q,"id","tinh-chnh-distilbert-vi-accelerate"),T(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(q,"href","#tinh-chnh-distilbert-vi-accelerate"),T(v,"class","relative group"),T(F,"href","/course/chapter3")},m(m,O){i(m,o,O),n(o,d),i(m,p,O),i(m,v,O),n(v,q),n(q,$),x(k,$,null),n(v,D),n(v,_),n(_,C),i(m,P,O),i(m,K,O),n(K,A),n(K,N),n(N,H),n(K,R),n(K,F),n(F,W),n(K,y),i(m,L,O),i(m,I,O),n(I,G),n(I,tt),n(tt,V),n(I,qt),n(I,lt),n(lt,gt),n(I,U),n(I,_t),n(_t,Y),n(I,Pt),i(m,ct,O),x(ot,m,O),i(m,rt,O),i(m,Z,O),n(Z,vt),n(Z,Lt),n(Lt,Ct),n(Z,St),i(m,Nt,O),x(ut,m,O),i(m,Dt,O),i(m,st,O),n(st,Ft),n(st,it),n(it,Gt),n(st,at),i(m,It,O),x(ht,m,O),i(m,yt,O),i(m,xt,O),n(xt,Vt),i(m,$t,O),x(S,m,O),i(m,J,O),i(m,Et,O),n(Et,nt),n(Et,pt),n(pt,Ht),n(Et,mt),i(m,Zt,O),x(X,m,O),i(m,tn,O),i(m,Tt,O),n(Tt,wt),n(Tt,Ut),n(Ut,Kt),n(Tt,nn),i(m,z,O),x(Q,m,O),i(m,Ot,O),i(m,et,O),n(et,he),i(m,un,O),x(dt,m,O),i(m,Tn,O),i(m,Rt,O),n(Rt,Rn),i(m,jt,O),x(Bt,m,O),i(m,on,O),x(an,m,O),i(m,qn,O),i(m,Mt,O),n(Mt,gn),n(Mt,Wt),n(Wt,Fn),n(Mt,Cn),i(m,ft,O),x(bt,m,O),i(m,hn,O),i(m,rn,O),n(rn,Ie),i(m,ie,O),x(en,m,O),i(m,le,O),x(kt,m,O),i(m,ge,O),i(m,_n,O),n(_n,Ve),_e=!0},i(m){_e||(f(k.$$.fragment,m),f(ot.$$.fragment,m),f(ut.$$.fragment,m),f(ht.$$.fragment,m),f(S.$$.fragment,m),f(X.$$.fragment,m),f(Q.$$.fragment,m),f(dt.$$.fragment,m),f(Bt.$$.fragment,m),f(an.$$.fragment,m),f(bt.$$.fragment,m),f(en.$$.fragment,m),f(kt.$$.fragment,m),_e=!0)},o(m){b(k.$$.fragment,m),b(ot.$$.fragment,m),b(ut.$$.fragment,m),b(ht.$$.fragment,m),b(S.$$.fragment,m),b(X.$$.fragment,m),b(Q.$$.fragment,m),b(dt.$$.fragment,m),b(Bt.$$.fragment,m),b(an.$$.fragment,m),b(bt.$$.fragment,m),b(en.$$.fragment,m),b(kt.$$.fragment,m),_e=!1},d(m){m&&e(o),m&&e(p),m&&e(v),E(k),m&&e(P),m&&e(K),m&&e(L),m&&e(I),m&&e(ct),E(ot,m),m&&e(rt),m&&e(Z),m&&e(Nt),E(ut,m),m&&e(Dt),m&&e(st),m&&e(It),E(ht,m),m&&e(yt),m&&e(xt),m&&e($t),E(S,m),m&&e(J),m&&e(Et),m&&e(Zt),E(X,m),m&&e(tn),m&&e(Tt),m&&e(z),E(Q,m),m&&e(Ot),m&&e(et),m&&e(un),E(dt,m),m&&e(Tn),m&&e(Rt),m&&e(jt),E(Bt,m),m&&e(on),E(an,m),m&&e(qn),m&&e(Mt),m&&e(ft),E(bt,m),m&&e(hn),m&&e(rn),m&&e(ie),E(en,m),m&&e(le),E(kt,m),m&&e(ge),m&&e(_n)}}}function Uu(B){let o,d,p,v,q,$,k,D;return{c(){o=l("p"),d=s("\u270F\uFE0F "),p=l("strong"),v=s("Th\u1EED nghi\u1EC7m th\xF4i!"),q=s(" \u0110\u1EC3 \u0111\u1ECBnh l\u01B0\u1EE3ng l\u1EE3i \xEDch c\u1EE7a vi\u1EC7c th\xEDch \u1EE9ng chuy\xEAn m\xF4n, h\xE3y tinh ch\u1EC9nh b\u1ED9 ph\xE2n lo\u1EA1i tr\xEAn c\xE1c nh\xE3n IMDb cho c\u1EA3 c\xE1c checkpoint DistilBERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\xE0 tinh ch\u1EC9nh. N\u1EBFu b\u1EA1n c\u1EA7n b\u1ED3i d\u01B0\u1EE1ng v\u1EC1 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n, h\xE3y xem "),$=l("a"),k=s("Ch\u01B0\u01A1ng 3"),D=s("."),this.h()},l(_){o=c(_,"P",{});var C=r(o);d=a(C,"\u270F\uFE0F "),p=c(C,"STRONG",{});var P=r(p);v=a(P,"Th\u1EED nghi\u1EC7m th\xF4i!"),P.forEach(e),q=a(C," \u0110\u1EC3 \u0111\u1ECBnh l\u01B0\u1EE3ng l\u1EE3i \xEDch c\u1EE7a vi\u1EC7c th\xEDch \u1EE9ng chuy\xEAn m\xF4n, h\xE3y tinh ch\u1EC9nh b\u1ED9 ph\xE2n lo\u1EA1i tr\xEAn c\xE1c nh\xE3n IMDb cho c\u1EA3 c\xE1c checkpoint DistilBERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\xE0 tinh ch\u1EC9nh. N\u1EBFu b\u1EA1n c\u1EA7n b\u1ED3i d\u01B0\u1EE1ng v\u1EC1 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n, h\xE3y xem "),$=c(C,"A",{href:!0});var K=r($);k=a(K,"Ch\u01B0\u01A1ng 3"),K.forEach(e),D=a(C,"."),C.forEach(e),this.h()},h(){T($,"href","/course/chapter3")},m(_,C){i(_,o,C),n(o,d),n(o,p),n(p,v),n(o,q),n(o,$),n($,k),n(o,D)},d(_){_&&e(o)}}}function Wu(B){let o,d,p,v,q,$,k,D,_,C,P,K,A,N,H,R,F,W,y,L,I,G,tt,V,qt,lt,gt,U,_t,Y,Pt,ct,ot,rt,Z,vt,Lt,Ct,St,Nt,ut,Dt,st,Ft,it,Gt,at,It,ht,yt,xt,Vt,$t,S,J,Et,nt,pt,Ht,mt,Zt,X,tn,Tt,wt,Ut,Kt,nn,z,Q,Ot,et,he,un,dt,Tn,Rt,Rn,jt,Bt,on,an,qn,Mt,gn,Wt,Fn,Cn,ft,bt,hn,rn,Ie,ie,en,le,kt,ge,_n,Ve,_e,m,O,de,Gn,Zs,ta,dn,na,ea,pn,In,Ue,ce,fe,We,Yt,ln,oe,fn,Ye,Dn,sa,Je,bn,mn,be,Vn,aa,Ya,ec,ii,Un,sc,Qe,ac,hc,Ja,ic,lc,li,Xe,ci,Ze,oi,Jt,cc,Qa,oc,rc,Xa,pc,mc,Za,uc,gc,th,_c,dc,nh,fc,bc,ri,ts,pi,ns,mi,Wn,kc,eh,vc,yc,sh,$c,wc,ui,ke,gi,ve,jc,ha,xc,Ec,_i,re,ye,ah,es,Tc,hh,qc,di,ss,fi,ia,Cc,bi,Qt,Dc,ih,Mc,zc,lh,Ac,Pc,la,Sc,Kc,ch,Oc,Bc,oh,Lc,Nc,ki,as,vi,hs,yi,kn,Hc,rh,Rc,Fc,ph,Gc,Ic,mh,Vc,Uc,$i,$e,Wc,uh,Yc,Jc,wi,is,ji,ls,xi,we,Qc,gh,Xc,Zc,Ei,je,Ti,ca,to,qi,cs,Ci,xe,Di,oa,no,Mi,os,zi,rs,Ai,ra,eo,Pi,ps,Si,ms,Ki,Yn,so,_h,ao,ho,dh,io,lo,Oi,us,Bi,gs,Li,pa,co,Ni,Ee,_s,oo,fh,ro,po,mo,ds,uo,bh,go,_o,Hi,ma,fo,Ri,fs,Fi,cn,bo,kh,ko,vo,vh,yo,$o,yh,wo,jo,$h,xo,Eo,Gi,Jn,To,wh,qo,Co,jh,Do,Mo,Ii,bs,Vi,ks,Ui,Xt,zo,xh,Ao,Po,Eh,So,Ko,Th,Oo,Bo,qh,Lo,No,Ch,Ho,Ro,Wi,vs,Yi,ys,Ji,ua,Fo,Qi,$s,Xi,ws,Zi,vn,Go,Dh,Io,Vo,Mh,Uo,Wo,zh,Yo,Jo,tl,pe,Te,Ah,js,Qo,ga,Xo,Ph,Zo,nl,yn,tr,_a,nr,er,Sh,sr,ar,Kh,hr,ir,el,xs,sl,$n,lr,Oh,cr,or,Bh,rr,pr,Lh,mr,ur,al,Es,hl,Ts,il,Qn,gr,Nh,_r,dr,Hh,fr,br,ll,qe,cl,da,wn,kr,Rh,vr,yr,Fh,$r,wr,Gh,jr,xr,ol,Mn,zn,fa,ba,Er,rl,qs,pl,Cs,ml,Ce,ul,Xn,Tr,Ih,qr,Cr,ka,Dr,Mr,gl,Ds,_l,Ms,dl,Zn,zr,Vh,Ar,Pr,Uh,Sr,Kr,fl,zs,bl,va,Or,kl,As,vl,ya,Br,yl,An,Pn,$a,me,De,Wh,Ps,Lr,Yh,Nr,$l,Ss,wl,wa,Hr,jl,Sn,Kn,ja,Ks,xl,xa,Rr,El,On,Bn,Ea,Ta,Fr,Tl,Ln,Nn,qa,Os,ql,Ca,Gr,Cl,Da,Me,Dl,Ma,ue,ze,Jh,Bs,Ir,Qh,Vr,Ml,te,Ur,Xh,Wr,Yr,Zh,Jr,Qr,zl,Ls,Al,za,Xr,Pl,Ns,Sl,Hs,Kl,Aa,Zr,Ol,Rs,Bl,Ae,tp,Pa,np,ep,Ll,Pe,Nl;p=new wu({props:{fw:B[0]}}),D=new Ge({});const ap=[xu,ju],Fs=[];function hp(t,h){return t[0]==="pt"?0:1}A=hp(B),N=Fs[A]=ap[A](B),$t=new nc({props:{id:"mqElG5QJWUg"}}),J=new Fe({props:{$$slots:{default:[Eu]},$$scope:{ctx:B}}}),mt=new Ge({});const ip=[qu,Tu],Gs=[];function lp(t,h){return t[0]==="pt"?0:1}ft=lp(B),bt=Gs[ft]=ip[ft](B),en=new M({props:{code:'text = "This is a great [MASK]."',highlighted:'text = <span class="hljs-string">&quot;This is a great [MASK].&quot;</span>'}}),In=new M({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}});const cp=[Du,Cu],Is=[];function op(t,h){return t[0]==="pt"?0:1}Yt=op(B),ln=Is[Yt]=cp[Yt](B),fn=new M({props:{code:`'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great deal.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great success.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great adventure.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great idea.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; This is a great feat.&#x27;</span>`}}),Vn=new Ge({}),Xe=new M({props:{code:`from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

imdb_dataset = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)
imdb_dataset`}}),Ze=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),ts=new M({props:{code:`sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")`,highlighted:`sample = imdb_dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>))

<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> sample:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;text&#x27;</span>]}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Label: <span class="hljs-subst">{row[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>&#x27;&quot;</span>)`}}),ns=new M({props:{code:`
'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'`,highlighted:`
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, &quot;kodokoo&quot; in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\&#x27;t get me wrong; as clich\xE9d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.&lt;br /&gt;&lt;br /&gt;Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam&quot; Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.&lt;br /&gt;&lt;br /&gt;The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 0&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. &lt;br /&gt;&lt;br /&gt;My 4 and 6 year old children love this movie and have been asking again and again to watch it. &lt;br /&gt;&lt;br /&gt;I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.&lt;br /&gt;&lt;br /&gt;I do not have older children so I do not know what they would think of it. &lt;br /&gt;&lt;br /&gt;The songs are very cute. My daughter keeps singing them over and over.&lt;br /&gt;&lt;br /&gt;Hope this helps.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Label: 1&#x27;</span>`}}),ke=new Fe({props:{$$slots:{default:[Mu]},$$scope:{ctx:B}}}),es=new Ge({}),ss=new nc({props:{id:"8PmhEIXhBvI"}}),as=new M({props:{code:`def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# D\xF9ng batched=True \u0111\u1EC3 k\xEDch ho\u1EA1t \u0111a lu\u1ED3ng nhanh!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
    result = tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>])
    <span class="hljs-keyword">if</span> tokenizer.is_fast:
        result[<span class="hljs-string">&quot;word_ids&quot;</span>] = [result.word_ids(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(result[<span class="hljs-string">&quot;input_ids&quot;</span>]))]
    <span class="hljs-keyword">return</span> result


<span class="hljs-comment"># D\xF9ng batched=True \u0111\u1EC3 k\xEDch ho\u1EA1t \u0111a lu\u1ED3ng nhanh!</span>
tokenized_datasets = imdb_dataset.<span class="hljs-built_in">map</span>(
    tokenize_function, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>]
)
tokenized_datasets`}}),hs=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">25000</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">50000</span>
    })
})`}}),is=new M({props:{code:"tokenizer.model_max_length",highlighted:"tokenizer.model_max_length"}}),ls=new M({props:{code:"512",highlighted:'<span class="hljs-number">512</span>'}}),je=new Fe({props:{$$slots:{default:[zu]},$$scope:{ctx:B}}}),cs=new M({props:{code:"chunk_size = 128",highlighted:'chunk_size = <span class="hljs-number">128</span>'}}),xe=new Fe({props:{warning:!0,$$slots:{default:[Au]},$$scope:{ctx:B}}}),os=new M({props:{code:`# T\u1EA1o ra m\u1ED9t danh s\xE1ch c\xE1c danh s\xE1ch cho t\u1EEBng \u0111\u1EB7c tr\u01B0ng
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")`,highlighted:`<span class="hljs-comment"># T\u1EA1o ra m\u1ED9t danh s\xE1ch c\xE1c danh s\xE1ch cho t\u1EEBng \u0111\u1EB7c tr\u01B0ng</span>
tokenized_samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">3</span>]

<span class="hljs-keyword">for</span> idx, sample <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokenized_samples[<span class="hljs-string">&quot;input_ids&quot;</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review <span class="hljs-subst">{idx}</span> length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(sample)}</span>&#x27;&quot;</span>)`}}),rs=new M({props:{code:`'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 0 length: 200&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 1 length: 559&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Review 2 length: 192&#x27;</span>`}}),ps=new M({props:{code:`concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")`,highlighted:`concatenated_examples = {
    k: <span class="hljs-built_in">sum</span>(tokenized_samples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> tokenized_samples.keys()
}
total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Concatenated reviews length: <span class="hljs-subst">{total_length}</span>&#x27;&quot;</span>)`}}),ms=new M({props:{code:"'>>> Concatenated reviews length: 951'",highlighted:'<span class="hljs-string">&#x27;&gt;&gt;&gt; Concatenated reviews length: 951&#x27;</span>'}}),us=new M({props:{code:`chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")`,highlighted:`chunks = {
    k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
    <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
}

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Chunk length: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(chunk)}</span>&#x27;&quot;</span>)`}}),gs=new M({props:{code:`'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 128&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; Chunk length: 55&#x27;</span>`}}),fs=new M({props:{code:`def group_texts(examples):
    # N\u1ED1i t\u1EA5t c\u1EA3 c\xE1c v\u0103n b\u1EA3n
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # T\xEDnh \u0111\u1ED9 d\xE0i c\u1EE7a c\xE1c v\u0103n b\u1EA3n \u0111\u01B0\u1EE3c n\u1ED1i
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # Ch\xFAng t\xF4i b\u1ECF \u0111o\u1EA1n cu\u1ED1i c\xF9ng n\u1EBFu n\xF3 nh\u1ECF h\u01A1n chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Chia ph\u1EA7n theo max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # T\u1EA1o c\u1ED9t nh\xE3n m\u1EDBi
    result["labels"] = result["input_ids"].copy()
    return result`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-comment"># N\u1ED1i t\u1EA5t c\u1EA3 c\xE1c v\u0103n b\u1EA3n</span>
    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
    <span class="hljs-comment"># T\xEDnh \u0111\u1ED9 d\xE0i c\u1EE7a c\xE1c v\u0103n b\u1EA3n \u0111\u01B0\u1EE3c n\u1ED1i</span>
    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
    <span class="hljs-comment"># Ch\xFAng t\xF4i b\u1ECF \u0111o\u1EA1n cu\u1ED1i c\xF9ng n\u1EBFu n\xF3 nh\u1ECF h\u01A1n chunk_size</span>
    total_length = (total_length // chunk_size) * chunk_size
    <span class="hljs-comment"># Chia ph\u1EA7n theo max_len</span>
    result = {
        k: [t[i : i + chunk_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, chunk_size)]
        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
    }
    <span class="hljs-comment"># T\u1EA1o c\u1ED9t nh\xE3n m\u1EDBi</span>
    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
    <span class="hljs-keyword">return</span> result`}}),bs=new M({props:{code:`lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets`,highlighted:`lm_datasets = tokenized_datasets.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>)
lm_datasets`}}),ks=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">61289</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">59905</span>
    })
    unsupervised: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">122963</span>
    })
})`}}),vs=new M({props:{code:'tokenizer.decode(lm_datasets["train"][1]["input_ids"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),ys=new M({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),$s=new M({props:{code:'tokenizer.decode(lm_datasets["train"][1]["labels"])',highlighted:'tokenizer.decode(lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;labels&quot;</span>])'}}),ws=new M({props:{code:`".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"`,highlighted:'<span class="hljs-string">&quot;.... at.......... high. a classic line : inspector : i&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&quot;</span>'}}),js=new Ge({}),xs=new M({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`}}),Es=new M({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
<span class="hljs-keyword">for</span> sample <span class="hljs-keyword">in</span> samples:
    _ = sample.pop(<span class="hljs-string">&quot;word_ids&quot;</span>)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> data_collator(samples)[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),Ts=new M({props:{code:`'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as &quot; teachers &quot;. [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\&#x27;[MASK] satire is much closer to reality than is &quot; teachers &quot;. the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\&#x27;[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george \u5B87in stated )\u516C been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless&#x27;</span>`}}),qe=new Fe({props:{$$slots:{default:[Pu]},$$scope:{ctx:B}}});let sn=B[0]==="pt"&&gu();const rp=[Ku,Su],Vs=[];function pp(t,h){return t[0]==="pt"?0:1}Mn=pp(B),zn=Vs[Mn]=rp[Mn](B),qs=new M({props:{code:`samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\\n'>>> {tokenizer.decode(chunk)}'")`,highlighted:`samples = [lm_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
batch = whole_word_masking_data_collator(samples)

<span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> batch[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; <span class="hljs-subst">{tokenizer.decode(chunk)}</span>&#x27;&quot;</span>)`}}),Cs=new M({props:{code:`'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as &quot; teachers &quot;. my 35 years in the teaching profession lead me to believe that bromwell high\\&#x27;s satire is much closer to reality than is &quot; teachers &quot;. the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\&#x27;pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\&#x27;m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\&#x27;t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless&#x27;</span>`}}),Ce=new Fe({props:{$$slots:{default:[Ou]},$$scope:{ctx:B}}}),Ds=new M({props:{code:`train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset`,highlighted:`train_size = <span class="hljs-number">10_000</span>
test_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.1</span> * train_size)

downsampled_dataset = lm_datasets[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(
    train_size=train_size, test_size=test_size, seed=<span class="hljs-number">42</span>
)
downsampled_dataset`}}),Ms=new M({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">10000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;word_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1000</span>
    })
})`}}),zs=new M({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),As=new M({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});const mp=[Lu,Bu],Us=[];function up(t,h){return t[0]==="tf"?0:1}An=up(B),Pn=Us[An]=mp[An](B),Ps=new Ge({}),Ss=new nc({props:{id:"NURcDHhYe98"}});const gp=[Hu,Nu],Ws=[];function _p(t,h){return t[0]==="pt"?0:1}Sn=_p(B),Kn=Ws[Sn]=gp[Sn](B),Ks=new M({props:{code:"Perplexity: 21.75",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexity: <span class="hljs-number">21.75</span>'}});const dp=[Fu,Ru],Ys=[];function fp(t,h){return t[0]==="pt"?0:1}On=fp(B),Bn=Ys[On]=dp[On](B);const bp=[Iu,Gu],Js=[];function kp(t,h){return t[0]==="pt"?0:1}Ln=kp(B),Nn=Js[Ln]=bp[Ln](B),Os=new M({props:{code:"Perplexity: 11.32",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>Perplexity: <span class="hljs-number">11.32</span>'}});let zt=B[0]==="pt"&&_u();Me=new Fe({props:{$$slots:{default:[Vu]},$$scope:{ctx:B}}});let At=B[0]==="pt"&&du();return Bs=new Ge({}),Ls=new M({props:{code:`from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

mask_filler = pipeline(
    <span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;huggingface-course/distilbert-base-uncased-finetuned-imdb&quot;</span>
)`}}),Ns=new M({props:{code:`preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")`,highlighted:`preds = mask_filler(text)

<span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&gt;&gt;&gt; <span class="hljs-subst">{pred[<span class="hljs-string">&#x27;sequence&#x27;</span>]}</span>&quot;</span>)`}}),Hs=new M({props:{code:`'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movie.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great film.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great story.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great movies.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt;&gt; this is a great character.&#x27;</span>`}}),Rs=new nc({props:{id:"0Oxphw4Q9fo"}}),Pe=new Fe({props:{$$slots:{default:[Uu]},$$scope:{ctx:B}}}),{c(){o=l("meta"),d=u(),w(p.$$.fragment),v=u(),q=l("h1"),$=l("a"),k=l("span"),w(D.$$.fragment),_=u(),C=l("span"),P=s("Tinh ch\u1EC9nh m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),K=u(),N.c(),H=u(),R=l("p"),F=s("\u0110\u1ED1i v\u1EDBi nhi\u1EC1u \u1EE9ng d\u1EE5ng NLP li\xEAn quan \u0111\u1EBFn c\xE1c m\xF4 h\xECnh Transformer, b\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 c\u1EA7n l\u1EA5y m\u1ED9t m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc t\u1EEB Hugging Face Hub v\xE0 tinh ch\u1EC9nh tr\u1EF1c ti\u1EBFp tr\xEAn d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n cho t\xE1c v\u1EE5 hi\u1EC7n t\u1EA1i. V\u1EDBi \u0111i\u1EC1u ki\u1EC7n l\xE0 ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc kh\xF4ng qu\xE1 kh\xE1c bi\u1EC7t v\u1EDBi ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 tinh ch\u1EC9nh, vi\u1EC7c h\u1ECDc chuy\u1EC3n ti\u1EBFp th\u01B0\u1EDDng s\u1EBD mang l\u1EA1i k\u1EBFt qu\u1EA3 t\u1ED1t."),W=u(),y=l("p"),L=s("Tuy nhi\xEAn, c\xF3 m\u1ED9t v\xE0i tr\u01B0\u1EDDng h\u1EE3p m\xE0 tr\u01B0\u1EDBc ti\xEAn b\u1EA1n s\u1EBD mu\u1ED1n tinh ch\u1EC9nh c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF tr\xEAn d\u1EEF li\u1EC7u c\u1EE7a m\xECnh, tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n \u0111\u1EA7u t\xE1c v\u1EE5 c\u1EE5 th\u1EC3. V\xED d\u1EE5: n\u1EBFu t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n ch\u1EE9a c\xE1c h\u1EE3p \u0111\u1ED3ng ph\xE1p l\xFD ho\u1EB7c c\xE1c b\xE0i b\xE1o khoa h\u1ECDc, th\xEC m\xF4 h\xECnh thu\u1EA7n Transformer nh\u01B0 BERT th\u01B0\u1EDDng s\u1EBD coi c\xE1c t\u1EEB chuy\xEAn m\xF4n trong kho d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n l\xE0 token hi\u1EBFm v\xE0 hi\u1EC7u su\u1EA5t k\u1EBFt qu\u1EA3 c\xF3 th\u1EC3 k\xE9m h\u01A1n. B\u1EB1ng c\xE1ch tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF tr\xEAn d\u1EEF li\u1EC7u chuy\xEAn m\xF4n, b\u1EA1n c\xF3 th\u1EC3 t\u0103ng hi\u1EC7u su\u1EA5t c\u1EE7a nhi\u1EC1u t\xE1c v\u1EE5 xu\xF4i d\xF2ng, c\xF3 ngh\u0129a l\xE0 b\u1EA1n th\u01B0\u1EDDng ch\u1EC9 ph\u1EA3i th\u1EF1c hi\u1EC7n b\u01B0\u1EDBc n\xE0y m\u1ED9t l\u1EA7n!"),I=u(),G=l("p"),tt=s("Qu\xE1 tr\xECnh tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc tr\xEAn d\u1EEF li\u1EC7u trong m\u1EA3ng n\xE0y th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),V=l("em"),qt=s("domain adapt"),lt=s(" hay "),gt=l("em"),U=s("th\xEDch \u1EE9ng chuy\xEAn m\xF4n"),_t=s(". N\xF3 \u0111\u01B0\u1EE3c ph\u1ED5 bi\u1EBFn v\xE0o n\u0103m 2018 b\u1EDFi "),Y=l("a"),Pt=s("ULMFiT"),ct=s(", l\xE0 m\u1ED9t trong nh\u1EEFng ki\u1EBFn \u200B\u200Btr\xFAc m\u1EA1ng th\u1EA7n kinh \u0111\u1EA7u ti\xEAn (d\u1EF1a tr\xEAn LSTM) \u0111\u1EC3 l\xE0m cho vi\u1EC7c h\u1ECDc chuy\u1EC3n ti\u1EBFp th\u1EF1c s\u1EF1 hi\u1EC7u qu\u1EA3 cho NLP. M\u1ED9t v\xED d\u1EE5 v\u1EC1 th\xEDch \u1EE9ng chuy\xEAn m\xF4n v\u1EDBi ULMFiT \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong h\xECnh d\u01B0\u1EDBi \u0111\xE2y; trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD l\xE0m \u0111i\u1EC1u t\u01B0\u01A1ng t\u1EF1, nh\u01B0ng v\u1EDBi Transformer thay v\xEC LSTM!"),ot=u(),rt=l("div"),Z=l("img"),Lt=u(),Ct=l("img"),Nt=u(),ut=l("p"),Dt=s("\u0110\u1EBFn cu\u1ED1i ph\u1EA7n n\xE0y, b\u1EA1n s\u1EBD c\xF3 m\u1ED9t "),st=l("a"),Ft=s("m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),it=s(" tr\xEAn Hub c\xF3 th\u1EC3 t\u1EF1 \u0111\u1ED9ng ho\xE0n thi\u1EC7n c\xE2u nh\u01B0 d\u01B0\u1EDBi \u0111\xE2y:"),Gt=u(),at=l("iframe"),ht=u(),yt=l("p"),xt=s("C\xF9ng \u0111i s\xE2u v\xE0o th\xF4i!"),Vt=u(),w($t.$$.fragment),S=u(),w(J.$$.fragment),Et=u(),nt=l("h2"),pt=l("a"),Ht=l("span"),w(mt.$$.fragment),Zt=u(),X=l("span"),tn=s("Ch\u1ECDn m\u1ED9t m\xF4 h\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc cho m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),Tt=u(),wt=l("p"),Ut=s("\u0110\u1EC3 b\u1EAFt \u0111\u1EA7u, h\xE3y ch\u1ECDn m\u1ED9t m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc ph\xF9 h\u1EE3p \u0111\u1EC3 t\u1EA1o m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i. Nh\u01B0 \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong \u1EA3nh ch\u1EE5p m\xE0n h\xECnh d\u01B0\u1EDBi \u0111\xE2y, b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y danh s\xE1ch c\xE1c \u1EE9ng c\u1EED vi\xEAn b\u1EB1ng c\xE1ch \xE1p d\u1EE5ng b\u1ED9 l\u1ECDc \u201CFill-Mask\u201D tr\xEAn "),Kt=l("a"),nn=s("Hugging Face Hub"),z=s(":"),Q=u(),Ot=l("div"),et=l("img"),un=u(),dt=l("p"),Tn=s("M\u1EB7c d\xF9 d\xF2ng m\xF4 h\xECnh BERT v\xE0 RoBERTa \u0111\u01B0\u1EE3c t\u1EA3i xu\u1ED1ng nhi\u1EC1u nh\u1EA5t, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng m\xF4 h\xECnh c\xF3 t\xEAn "),Rt=l("a"),Rn=s("DistilBERT"),jt=s(`
c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n nhanh h\u01A1n nhi\u1EC1u m\xE0 \xEDt ho\u1EB7c kh\xF4ng b\u1ECB m\u1EA5t hi\u1EC7u su\u1EA5t. M\xF4 h\xECnh n\xE0y \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng m\u1ED9t k\u1EF9 thu\u1EADt \u0111\u1EB7c bi\u1EC7t c\xF3 t\xEAn l\xE0 `),Bt=l("a"),on=l("em"),an=s("knowledge distillation"),qn=s(", trong \u0111\xF3 m\u1ED9t \u201Cm\xF4 h\xECnh gi\xE1o vi\xEAn\u201D l\u1EDBn nh\u01B0 BERT \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 h\u01B0\u1EDBng d\u1EABn hu\u1EA5n luy\u1EC7n \u201Cm\xF4 h\xECnh sinh vi\xEAn\u201D c\xF3 \xEDt tham s\u1ED1 h\u01A1n nhi\u1EC1u. Ph\u1EA7n gi\u1EA3i th\xEDch chi ti\u1EBFt v\u1EC1 qu\xE1 tr\xECnh ch\u1EAFt l\u1ECDc ki\u1EBFn \u200B\u200Bth\u1EE9c s\u1EBD \u0111\u01B0a ch\xFAng ta \u0111i qu\xE1 xa trong ph\u1EA7n n\xE0y, nh\u01B0ng n\u1EBFu b\u1EA1n quan t\xE2m, b\u1EA1n c\xF3 th\u1EC3 \u0111\u1ECDc t\u1EA5t c\u1EA3 v\u1EC1 n\xF3 trong "),Mt=l("a"),gn=l("em"),Wt=s("Natural Language Processing with Transformers"),Fn=s(" (th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 s\xE1ch gi\xE1o khoa v\u1EC1 Transformer)."),Cn=u(),bt.c(),hn=u(),rn=l("p"),Ie=s("V\u1EDBi kho\u1EA3ng 67 tri\u1EC7u tham s\u1ED1, DistilBERT nh\u1ECF h\u01A1n kho\u1EA3ng hai l\u1EA7n so v\u1EDBi m\xF4 h\xECnh c\u01A1 s\u1EDF BERT, g\u1EA7n nh\u01B0 \u0111\u01B0\u1EE3c hi\u1EC3u l\xE0 t\u0103ng t\u1ED1c g\u1EA5p hai l\u1EA7n khi hu\u1EA5n luy\u1EC7n - th\u1EADt tuy\u1EC7t! B\xE2y gi\u1EDD ch\xFAng ta h\xE3y xem nh\u1EEFng lo\u1EA1i token n\xE0o m\xF4 h\xECnh n\xE0y d\u1EF1 \u0111o\xE1n l\xE0 c\xF3 nhi\u1EC1u kh\u1EA3 n\u0103ng ho\xE0n th\xE0nh m\u1ED9t m\u1EABu v\u0103n b\u1EA3n nh\u1ECF:"),ie=u(),w(en.$$.fragment),le=u(),kt=l("p"),ge=s("L\xE0 con ng\u01B0\u1EDDi, ch\xFAng ta c\xF3 th\u1EC3 t\u01B0\u1EDFng t\u01B0\u1EE3ng ra nhi\u1EC1u kh\u1EA3 n\u0103ng \u0111\u1ED1i v\u1EDBi token "),_n=l("code"),Ve=s("[MASK]"),_e=s(", v\xED d\u1EE5 \u201Cday\u201D, \u201Cride\u201D, ho\u1EB7c \u201Cpainting\u201D. \u0110\u1ED1i v\u1EDBi c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc, c\xE1c d\u1EF1 \u0111o\xE1n ph\u1EE5 thu\u1ED9c v\xE0o kho ng\u1EEF li\u1EC7u m\xF4 h\xECnh \u0111\xF3 hu\u1EA5n luy\u1EC7n, v\xEC n\xF3 h\u1ECDc c\xE1ch ch\u1ECDn c\xE1c m\u1EABu th\u1ED1ng k\xEA c\xF3 trong d\u1EEF li\u1EC7u. Gi\u1ED1ng BERT, DistilBERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc tr\xEAn b\u1ED9 d\u1EEF li\u1EC7u "),m=l("a"),O=s("English Wikipedia"),de=s(" v\xE0 "),Gn=l("a"),Zs=s("BookCorpus"),ta=s(", n\xEAn ta k\xEC v\u1ECDng c\xE1c d\u1EF1 \u0111o\xE1n cho "),dn=l("code"),na=s("[MASK]"),ea=s(" s\u1EBD ph\u1EA3n \xE1nh c\xE1c m\u1EA3ng n\xE0y. \u0110\u1EC3 d\u1EF1 \u0111o\xE1n ta c\u1EA7n tr\xECnh tokenizer c\u1EE7a DistilBERT t\u1EA1o ra c\xE1c \u0111\u1EA7u v\xE0o cho m\xF4 h\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA3i t\u1EEB Hub:"),pn=u(),w(In.$$.fragment),Ue=u(),ce=l("p"),fe=s("V\u1EDBi m\u1ED9t tokenizer v\xE0 m\u1ED9t m\xF4 h\xECnh, ta c\xF3 th\u1EC3 truy\u1EC1n c\xE1c \u0111o\u1EA1n v\u0103n v\xED d\u1EE5 t\u1EDBi m\xF4 h\xECnh, tr\xEDch xu\u1EA5t logits, v\xE0 xin ra 5 \u1EE9ng c\u1EED vi\xEAn:"),We=u(),ln.c(),oe=u(),w(fn.$$.fragment),Ye=u(),Dn=l("p"),sa=s("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y t\u1EEB k\u1EBFt qu\u1EA3 \u0111\u1EA7u ra r\u1EB1ng c\xE1c d\u1EF1 \u0111o\xE1n c\u1EE7a m\xF4 h\xECnh \u0111\u1EC1 c\u1EADp \u0111\u1EBFn c\xE1c thu\u1EADt ng\u1EEF h\xE0ng ng\xE0y, \u0111i\u1EC1u n\xE0y c\xF3 l\u1EBD kh\xF4ng c\xF3 g\xEC \u0111\xE1ng ng\u1EA1c nhi\xEAn khi d\u1EF1a tr\xEAn n\u1EC1n t\u1EA3ng c\u1EE7a Wikipedia ti\u1EBFng Anh. H\xE3y xem c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 thay \u0111\u1ED5i m\u1EA3ng n\xE0y th\xE0nh m\u1ED9t th\u1EE9 g\xEC \u0111\xF3 th\xEDch h\u1EE3p h\u01A1n m\u1ED9t ch\xFAt - c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim ph\xE2n c\u1EF1c cao!"),Je=u(),bn=l("h2"),mn=l("a"),be=l("span"),w(Vn.$$.fragment),aa=u(),Ya=l("span"),ec=s("B\u1ED9 d\u1EEF li\u1EC7u"),ii=u(),Un=l("p"),sc=s("\u0110\u1EC3 gi\u1EDBi thi\u1EC7u vi\u1EC7c th\xEDch \u1EE9ng chuy\xEAn m\xF4n, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng b\u1ED9 d\u1EEF li\u1EC7u n\u1ED5i ti\u1EBFng "),Qe=l("a"),ac=s("Large Movie Review Dataset"),hc=s("(hay vi\u1EBFt t\u1EAFt l\xE0 IMDb), l\xE0 t\u1EADp h\u1EE3p c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim th\u01B0\u1EDDng \u0111\u01B0\u1EE3c d\xF9ng \u0111\u1EC3 \u0111\xE1nh gi\xE1 c\xE1c m\xF4 h\xECnh ph\xE2n t\xEDch c\u1EA3m x\xFAc. B\u1EB1ng c\xE1ch tinh ch\u1EC9nh DistilBERT tr\xEAn kho ng\u1EEF li\u1EC7u n\xE0y, ch\xFAng ta hy v\u1ECDng m\xF4 h\xECnh ng\xF4n ng\u1EEF s\u1EBD \u0111i\u1EC1u ch\u1EC9nh v\u1ED1n t\u1EEB v\u1EF1ng c\u1EE7a n\xF3 t\u1EEB d\u1EEF li\u1EC7u th\u1EF1c t\u1EBF c\u1EE7a Wikipedia m\xE0 n\xF3 \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc \u0111\u1EC3 ph\xF9 h\u1EE3p v\u1EDBi c\xE1c y\u1EBFu t\u1ED1 ch\u1EE7 quan h\u01A1n c\u1EE7a c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim. Ch\xFAng ta c\xF3 th\u1EC3 l\u1EA5y d\u1EEF li\u1EC7u t\u1EEB Hugging Face Hub b\u1EB1ng h\xE0m "),Ja=l("code"),ic=s("load_dataset()"),lc=s(" t\u1EEB \u{1F917} Datasets:"),li=u(),w(Xe.$$.fragment),ci=u(),w(Ze.$$.fragment),oi=u(),Jt=l("p"),cc=s("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng m\u1ED7i ph\u1EA7n "),Qa=l("code"),oc=s("hu\u1EA5n luy\u1EC7n"),rc=s(" v\xE0 "),Xa=l("code"),pc=s("ki\u1EC3m th\u1EED"),mc=s(" bao g\u1ED3m 25,000 \u0111\xE1nh gi\xE1, trong khi ph\u1EA7n kh\xF4ng \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Za=l("code"),uc=s("phi gi\xE1m s\xE1t"),gc=s(" ch\u1EE9a 50,000 \u0111\xE1nh gi\xE1. Ch\xFAng ta h\xE3y xem m\u1ED9t v\xE0i m\u1EABu \u0111\u1EC3 c\xF3 \xFD t\u01B0\u1EDFng v\u1EC1 lo\u1EA1i v\u0103n b\u1EA3n m\xE0 ta \u0111ang x\u1EED l\xFD. Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EF1c hi\u1EC7n trong c\xE1c ch\u01B0\u01A1ng tr\u01B0\u1EDBc c\u1EE7a kh\xF3a h\u1ECDc, ch\xFAng ta s\u1EBD x\xE2u chu\u1ED7i c\xE1c h\xE0m "),th=l("code"),_c=s("Dataset.shuffle()"),dc=s(" v\xE0 "),nh=l("code"),fc=s("Dataset.select()"),bc=s(" \u0111\u1EC3 t\u1EA1o m\u1ED9t m\u1EABu ng\u1EABu nhi\xEAn:"),ri=u(),w(ts.$$.fragment),pi=u(),w(ns.$$.fragment),mi=u(),Wn=l("p"),kc=s("\u0110\xFAng, \u0111\xE2y ch\u1EAFc ch\u1EAFn l\xE0 nh\u1EEFng b\xE0i \u0111\xE1nh gi\xE1 phim, v\xE0 n\u1EBFu b\u1EA1n \u0111\u1EE7 l\u1EDBn, b\u1EA1n th\u1EADm ch\xED c\xF3 th\u1EC3 hi\u1EC3u nh\u1EADn x\xE9t trong b\xE0i \u0111\xE1nh gi\xE1 cu\u1ED1i c\xF9ng v\u1EC1 vi\u1EC7c s\u1EDF h\u1EEFu phi\xEAn b\u1EA3n VHS \u{1F61C}! M\u1EB7c d\xF9 ch\xFAng ta s\u1EBD kh\xF4ng c\u1EA7n nh\xE3n \u0111\u1EC3 cho m\xF4 h\xECnh ng\xF4n ng\u1EEF, nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng "),eh=l("code"),vc=s("0"),yc=s(" bi\u1EC3u th\u1ECB m\u1ED9t \u0111\xE1nh gi\xE1 ti\xEAu c\u1EF1c, trong khi "),sh=l("code"),$c=s("1"),wc=s(" t\u01B0\u01A1ng \u1EE9ng v\u1EDBi m\u1ED9t \u0111\xE1nh gi\xE1 t\xEDch c\u1EF1c."),ui=u(),w(ke.$$.fragment),gi=u(),ve=l("p"),jc=s("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t c\xE1i nh\xECn nhanh v\u1EC1 d\u1EEF li\u1EC7u, h\xE3y \u0111i s\xE2u v\xE0o vi\u1EC7c chu\u1EA9n b\u1ECB n\xF3 cho vi\u1EC7c l\u1EADp m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i. Nh\u01B0 ch\xFAng ta s\u1EBD th\u1EA5y, c\xF3 m\u1ED9t s\u1ED1 b\u01B0\u1EDBc b\u1ED5 sung m\xE0 ng\u01B0\u1EDDi ta c\u1EA7n th\u1EF1c hi\u1EC7n so v\u1EDBi c\xE1c t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i chu\u1ED7i m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ha=l("a"),xc=s("Ch\u01B0\u01A1ng 3"),Ec=s(". \u0110i th\xF4i!"),_i=u(),re=l("h2"),ye=l("a"),ah=l("span"),w(es.$$.fragment),Tc=u(),hh=l("span"),qc=s("Ti\u1EC1n x\u1EED l\xFD d\u1EEF li\u1EC7u"),di=u(),w(ss.$$.fragment),fi=u(),ia=l("p"),Cc=s("\u0110\u1ED1i v\u1EDBi c\u1EA3 m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy v\xE0 b\u1ECB \u1EA9n \u0111i, m\u1ED9t b\u01B0\u1EDBc ti\u1EC1n x\u1EED l\xFD ph\u1ED5 bi\u1EBFn l\xE0 n\u1ED1i t\u1EA5t c\u1EA3 c\xE1c m\u1EABu v\xE0 sau \u0111\xF3 chia to\xE0n b\u1ED9 ng\u1EEF li\u1EC7u th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc b\u1EB1ng nhau. \u0110i\u1EC1u n\xE0y ho\xE0n to\xE0n kh\xE1c v\u1EDBi c\xE1ch ti\u1EBFp c\u1EADn th\xF4ng th\u01B0\u1EDDng, khi ch\xFAng ta ch\u1EC9 c\u1EA7n tokenize c\xE1c m\u1EABu ri\xEAng l\u1EBB. T\u1EA1i sao l\u1EA1i n\u1ED1i m\u1ECDi th\u1EE9 l\u1EA1i v\u1EDBi nhau? L\xFD do l\xE0 c\xE1c m\u1EABu ri\xEAng l\u1EBB c\xF3 th\u1EC3 b\u1ECB c\u1EAFt ng\u1EAFn n\u1EBFu ch\xFAng qu\xE1 d\xE0i v\xE0 \u0111i\u1EC1u \u0111\xF3 s\u1EBD d\u1EABn \u0111\u1EBFn vi\u1EC7c m\u1EA5t th\xF4ng tin c\xF3 th\u1EC3 h\u1EEFu \xEDch cho t\xE1c v\u1EE5 m\xF4 h\xECnh h\xF3a ng\xF4n ng\u1EEF!"),bi=u(),Qt=l("p"),Dc=s("V\xEC v\u1EADy, \u0111\u1EC3 b\u1EAFt \u0111\u1EA7u, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta s\u1EBD tokenize kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh nh\u01B0 b\xECnh th\u01B0\u1EDDng, nh\u01B0ng "),ih=l("em"),Mc=s("kh\xF4ng"),zc=s(" \u0111\u1EB7t t\xF9y ch\u1ECDn "),lh=l("code"),Ac=s("truncation=True"),Pc=s(" trong tr\xECnh tokenize c\u1EE7a ch\xFAng ta. Ch\xFAng ta c\u0169ng s\u1EBD l\u1EA5y c\xE1c ID t\u1EEB n\u1EBFu ch\xFAng c\xF3 s\u1EB5n ((ch\xFAng s\u1EBD c\xF3 s\u1EB5n n\u1EBFu ta \u0111ang s\u1EED d\u1EE5ng c\xF4ng c\u1EE5 tokenize nhanh, nh\u01B0 \u0111\u01B0\u1EE3c m\xF4 t\u1EA3 trong "),la=l("a"),Sc=s("Ch\u01B0\u01A1ng 6"),Kc=s("), v\xEC ta s\u1EBD c\u1EA7n ch\xFAng sau n\xE0y \u0111\u1EC3 th\u1EF1c hi\u1EC7n che to\xE0n b\u1ED9 t\u1EEB. Ch\xFAng ta s\u1EBD g\xF3i n\xF3 trong m\u1ED9t h\xE0m \u0111\u01A1n gi\u1EA3n v\xE0 trong khi th\u1EF1c hi\u1EC7n, ta s\u1EBD x\xF3a c\xE1c c\u1ED9t "),ch=l("code"),Oc=s("text"),Bc=s(" v\xE0 "),oh=l("code"),Lc=s("label"),Nc=s(" v\xEC kh\xF4ng c\u1EA7n ch\xFAng n\u1EEFa:"),ki=u(),w(as.$$.fragment),vi=u(),w(hs.$$.fragment),yi=u(),kn=l("p"),Hc=s("V\xEC DistilBERT l\xE0 m\u1ED9t m\xF4 h\xECnh gi\u1ED1ng nh\u01B0 BERT, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c v\u0103n b\u1EA3n \u0111\u01B0\u1EE3c tokenize bao g\u1ED3m "),rh=l("code"),Rc=s("input_ids"),Fc=s(" v\xE0 "),ph=l("code"),Gc=s("attention_mask"),Ic=s(" ta \u0111\xE3 th\u1EA5y trong c\xE1c ch\u01B0\u01A1ng kh\xE1c, c\u0169ng nh\u01B0 "),mh=l("code"),Vc=s("word_ids"),Uc=s(" m\xE0 ta \u0111\xE3 th\xEAm v\xE0o."),$i=u(),$e=l("p"),Wc=s("G\u1EDD ch\xFAng ta \u0111\xE3 tokenize c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim c\u1EE7a m\xECnh, b\u01B0\u1EDBc ti\u1EBFp theo l\xE0 nh\xF3m t\u1EA5t c\u1EA3 ch\xFAng l\u1EA1i v\u1EDBi nhau v\xE0 chia k\u1EBFt qu\u1EA3 th\xE0nh nhi\u1EC1u ph\u1EA7n. Nh\u01B0ng nh\u1EEFng kh\u1ED1i n\xE0y ph\u1EA3i l\u1EDBn \u0111\u1EBFn m\u1EE9c n\xE0o? \u0110i\u1EC1u n\xE0y cu\u1ED1i c\xF9ng s\u1EBD \u0111\u01B0\u1EE3c x\xE1c \u0111\u1ECBnh b\u1EDFi dung l\u01B0\u1EE3ng b\u1ED9 nh\u1EDB GPU m\xE0 b\u1EA1n c\xF3 s\u1EB5n, nh\u01B0ng \u0111i\u1EC3m kh\u1EDFi \u0111\u1EA7u t\u1ED1t l\xE0 xem k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh t\u1ED1i \u0111a c\u1EE7a m\xF4 h\xECnh l\xE0 bao nhi\xEAu. \u0110i\u1EC1u n\xE0y c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c suy ra b\u1EB1ng c\xE1ch ki\u1EC3m tra thu\u1ED9c t\xEDnh "),uh=l("code"),Yc=s("model_max_length"),Jc=s(" c\u1EE7a tokenizer:"),wi=u(),w(is.$$.fragment),ji=u(),w(ls.$$.fragment),xi=u(),we=l("p"),Qc=s("Gi\xE1 tr\u1ECB n\xE0y c\xF3 ngu\u1ED3n g\u1ED1c t\u1EEB t\u1EC7p "),gh=l("em"),Xc=s("tokenizer_config.json"),Zc=s(" \u0111\u01B0\u1EE3c li\xEAn k\u1EBFt v\u1EDBi m\u1ED9t checkpoint; trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh l\xE0 512 token, gi\u1ED1ng nh\u01B0 v\u1EDBi BERT."),Ei=u(),w(je.$$.fragment),Ti=u(),ca=l("p"),to=s("V\xEC v\u1EADy, \u0111\u1EC3 ch\u1EA1y c\xE1c th\u1EED nghi\u1EC7m tr\xEAn GPU nh\u01B0 nh\u1EEFng GPU \u0111\u01B0\u1EE3c t\xECm th\u1EA5y tr\xEAn Google Colab, ch\xFAng ta s\u1EBD ch\u1ECDn th\u1EE9 g\xEC \u0111\xF3 nh\u1ECF h\u01A1n m\u1ED9t ch\xFAt c\xF3 th\u1EC3 v\u1EEBa v\u1EDBi b\u1ED9 nh\u1EDB:"),qi=u(),w(cs.$$.fragment),Ci=u(),w(xe.$$.fragment),Di=u(),oa=l("p"),no=s("B\xE2y gi\u1EDD \u0111\u1EBFn ph\u1EA7n th\xFA v\u1ECB. \u0110\u1EC3 cho bi\u1EBFt c\xE1ch n\u1ED1i ho\u1EA1t \u0111\u1ED9ng, h\xE3y l\u1EA5y m\u1ED9t v\xE0i b\xE0i \u0111\xE1nh gi\xE1 t\u1EEB b\u1ED9 hu\u1EA5n luy\u1EC7n \u0111\u01B0\u1EE3c tokenize v\xE0 in ra s\u1ED1 l\u01B0\u1EE3ng token cho m\u1ED7i b\xE0i \u0111\xE1nh gi\xE1:"),Mi=u(),w(os.$$.fragment),zi=u(),w(rs.$$.fragment),Ai=u(),ra=l("p"),eo=s("We can then concatenate all these examples with a simple dictionary comprehension, as follows:"),Pi=u(),w(ps.$$.fragment),Si=u(),w(ms.$$.fragment),Ki=u(),Yn=l("p"),so=s("Tuy\u1EC7t v\u1EDDi, t\u1ED5ng \u0111\u1ED9 d\xE0i \u0111\xE3 \u0111\u01B0\u1EE3c ki\u1EC3m tra - v\xEC v\u1EADy b\xE2y gi\u1EDD h\xE3y chia c\xE1c b\xE0i \u0111\xE1nh gi\xE1 \u0111\u01B0\u1EE3c n\u1ED1i th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc \u0111\u01B0\u1EE3c cung c\u1EA5p b\u1EDFi "),_h=l("code"),ao=s("block_size"),ho=s(". \u0110\u1EC3 l\xE0m nh\u01B0 v\u1EADy, ch\xFAng ta l\u1EB7p qua c\xE1c \u0111\u1EB7c tr\u01B0ng trong "),dh=l("code"),io=s("concatenated_examples"),lo=s(" v\xE0 s\u1EED d\u1EE5ng kh\u1EA3 n\u0103ng hi\u1EC3u danh s\xE1ch \u0111\u1EC3 t\u1EA1o c\xE1c ph\u1EA7n c\u1EE7a t\u1EEBng \u0111\u1EB7c tr\u01B0ng. K\u1EBFt qu\u1EA3 l\xE0 m\u1ED9t t\u1EEB \u0111i\u1EC3n c\xE1c kh\u1ED1i cho t\u1EEBng \u0111\u1EB7c tr\u01B0ng:"),Oi=u(),w(us.$$.fragment),Bi=u(),w(gs.$$.fragment),Li=u(),pa=l("p"),co=s("Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y trong v\xED d\u1EE5 n\xE0y, \u0111o\u1EA1n cu\u1ED1i th\u01B0\u1EDDng s\u1EBD nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc \u0111o\u1EA1n t\u1ED1i \u0111a. C\xF3 hai chi\u1EBFn l\u01B0\u1EE3c ch\xEDnh \u0111\u1EC3 gi\u1EA3i quy\u1EBFt v\u1EA5n \u0111\u1EC1 n\xE0y:"),Ni=u(),Ee=l("ul"),_s=l("li"),oo=s("B\u1ECF \u0111o\u1EA1n cu\u1ED1i c\xF9ng n\u1EBFu n\xF3 nh\u1ECF h\u01A1n "),fh=l("code"),ro=s("chunk_size"),po=s("."),mo=u(),ds=l("li"),uo=s("\u0110\u1EC7m \u0111o\u1EA1n cu\u1ED1i c\xF9ng cho \u0111\u1EBFn khi \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3 b\u1EB1ng "),bh=l("code"),go=s("chunk_size"),_o=s("."),Hi=u(),ma=l("p"),fo=s("Ch\xFAng t\xF4i s\u1EBD th\u1EF1c hi\u1EC7n c\xE1ch ti\u1EBFp c\u1EADn \u0111\u1EA7u ti\xEAn \u1EDF \u0111\xE2y, v\xEC v\u1EADy h\xE3y g\xF3i t\u1EA5t c\u1EA3 logic \u1EDF tr\xEAn trong m\u1ED9t h\xE0m duy nh\u1EA5t m\xE0 ch\xFAng t\xF4i c\xF3 th\u1EC3 \xE1p d\u1EE5ng cho t\u1EADp d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh:"),Ri=u(),w(fs.$$.fragment),Fi=u(),cn=l("p"),bo=s("L\u01B0u \xFD r\u1EB1ng trong b\u01B0\u1EDBc cu\u1ED1i c\xF9ng c\u1EE7a "),kh=l("code"),ko=s("group_texts()"),vo=s(", ch\xFAng ta t\u1EA1o m\u1ED9t c\u1ED9t m\u1EDBi "),vh=l("code"),yo=s("labels"),$o=s(" l\xE0 b\u1EA3n sao c\u1EE7a c\u1ED9t "),yh=l("code"),wo=s("input_ids"),jo=s(". Nh\u01B0 ch\xFAng ta s\u1EBD th\u1EA5y ngay sau \u0111\xE2y, \u0111\xF3 l\xE0 b\u1EDFi v\xEC trong m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i, m\u1EE5c ti\xEAu l\xE0 d\u1EF1 \u0111o\xE1n c\xE1c token \u0111\u01B0\u1EE3c che ng\u1EABu nhi\xEAn trong l\xF4 \u0111\u1EA7u v\xE0o v\xE0 b\u1EB1ng c\xE1ch t\u1EA1o c\u1ED9t "),$h=l("code"),xo=s("labels"),Eo=s(", ch\xFAng ta cung c\u1EA5p s\u1EF1 th\u1EADt c\u01A1 b\u1EA3n cho m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u1EC3 h\u1ECDc h\u1ECFi."),Gi=u(),Jn=l("p"),To=s("B\xE2y gi\u1EDD, h\xE3y \xE1p d\u1EE5ng "),wh=l("code"),qo=s("group_texts()"),Co=s(" cho c\xE1c t\u1EADp d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),jh=l("code"),Do=s("Dataset.map()"),Mo=s(" \u0111\xE1ng tin c\u1EADy:"),Ii=u(),w(bs.$$.fragment),Vi=u(),w(ks.$$.fragment),Ui=u(),Xt=l("p"),zo=s("B\u1EA1n c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng vi\u1EC7c nh\xF3m v\xE0 sau \u0111\xF3 ph\xE2n chia c\xE1c \u0111o\u1EA1n v\u0103n b\u1EA3n \u0111\xE3 t\u1EA1o ra nhi\u1EC1u m\u1EABu h\u01A1n so v\u1EDBi 25,000 m\u1EABu ban \u0111\u1EA7u c\u1EE7a ch\xFAng ta cho ph\u1EA7n t\xE1ch "),xh=l("code"),Ao=s("hu\u1EA5n luy\u1EC7n"),Po=s(" v\xE0 "),Eh=l("code"),So=s("ki\u1EC3m th\u1EED"),Ko=s(". \u0110\xF3 l\xE0 b\u1EDFi v\xEC ch\xFAng ta hi\u1EC7n c\xF3 c\xE1c m\u1EABu li\xEAn quan \u0111\u1EBFn "),Th=l("em"),Oo=s("token li\xEAn t\u1EE5c"),Bo=s(" tr\u1EA3i d\xE0i tr\xEAn nhi\u1EC1u m\u1EABu t\u1EEB kho t\xE0i li\u1EC7u g\u1ED1c. B\u1EA1n c\xF3 th\u1EC3 th\u1EA5y \u0111i\u1EC1u n\xE0y m\u1ED9t c\xE1ch r\xF5 r\xE0ng b\u1EB1ng c\xE1ch t\xECm ki\u1EBFm c\xE1c token \u0111\u1EB7c bi\u1EC7t "),qh=l("code"),Lo=s("[SEP]"),No=s(" v\xE0 "),Ch=l("code"),Ho=s("[CLS]"),Ro=s(" trong m\u1ED9t trong c\xE1c ph\u1EA7n:"),Wi=u(),w(vs.$$.fragment),Yi=u(),w(ys.$$.fragment),Ji=u(),ua=l("p"),Fo=s("Trong v\xED d\u1EE5 n\xE0y, b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y hai b\xE0i \u0111\xE1nh gi\xE1 phim tr\xF9ng nhau, m\u1ED9t b\xE0i v\u1EC1 phim c\u1EA5p ba v\xE0 b\xE0i c\xF2n l\u1EA1i v\u1EC1 t\xECnh tr\u1EA1ng v\xF4 gia c\u01B0. H\xE3y c\u0169ng xem c\xE1c nh\xE3n tr\xF4ng nh\u01B0 th\u1EBF n\xE0o cho m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i:"),Qi=u(),w($s.$$.fragment),Xi=u(),w(ws.$$.fragment),Zi=u(),vn=l("p"),Go=s("Nh\u01B0 mong \u0111\u1EE3i t\u1EEB h\xE0m "),Dh=l("code"),Io=s("group_texts()"),Vo=s(" c\u1EE7a ch\xFAng ta \u1EDF tr\xEAn, h\xE0m n\xE0y tr\xF4ng gi\u1ED1ng h\u1EC7t v\u1EDBi "),Mh=l("code"),Uo=s("input_ids"),Wo=s(" \u0111\xE3 \u0111\u01B0\u1EE3c gi\u1EA3i m\xE3 - nh\u01B0ng sau \u0111\xF3 l\xE0m th\u1EBF n\xE0o \u0111\u1EC3 m\xF4 h\xECnh c\u1EE7a ch\xFAng ta c\xF3 th\u1EC3 h\u1ECDc \u0111\u01B0\u1EE3c b\u1EA5t c\u1EE9 \u0111i\u1EC1u g\xEC? Ch\xFAng ta \u0111ang thi\u1EBFu m\u1ED9t b\u01B0\u1EDBc quan tr\u1ECDng: ch\xE8n token "),zh=l("code"),Yo=s("[MASK]"),Jo=s(" \u1EDF c\xE1c v\u1ECB tr\xED ng\u1EABu nhi\xEAn trong \u0111\u1EA7u v\xE0o! H\xE3y xem c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n \u0111i\u1EC1u n\xE0y m\u1ED9t c\xE1ch nhanh ch\xF3ng trong qu\xE1 tr\xECnh tinh ch\u1EC9nh b\u1EB1ng c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t."),tl=u(),pe=l("h2"),Te=l("a"),Ah=l("span"),w(js.$$.fragment),Qo=u(),ga=l("span"),Xo=s("Tinh ch\u1EC9nh DistilBERT v\u1EDBi API "),Ph=l("code"),Zo=s("Trainer"),nl=u(),yn=l("p"),tr=s("Tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i g\u1EA7n gi\u1ED1ng nh\u01B0 tinh ch\u1EC9nh m\xF4 h\xECnh ph\xE2n lo\u1EA1i chu\u1ED7i, gi\u1ED1ng nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m trong "),_a=l("a"),nr=s("Ch\u01B0\u01A1ng 3"),er=s(". S\u1EF1 kh\xE1c bi\u1EC7t duy nh\u1EA5t l\xE0 ch\xFAng ta c\u1EA7n m\u1ED9t tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t c\xF3 th\u1EC3 che gi\u1EA5u ng\u1EABu nhi\xEAn m\u1ED9t s\u1ED1 token trong m\u1ED7i l\xF4 v\u0103n b\u1EA3n. May m\u1EAFn thay, \u{1F917} Transformers \u0111\u01B0\u1EE3c chu\u1EA9n b\u1ECB v\u1EDBi m\u1ED9t "),Sh=l("code"),sr=s("DataCollatorForLanguageModeling"),ar=s(" d\xE0nh ri\xEAng cho t\xE1c v\u1EE5 n\xE0y. Ch\xFAng ta ch\u1EC9 c\u1EA7n chuy\u1EC3n n\xF3 v\xE0o tokenizer v\xE0 tham s\u1ED1 "),Kh=l("code"),hr=s("mlm_probability"),ir=s(" \u0111\u1EC3 ch\u1EC9 \u0111\u1ECBnh ph\u1EA7n n\xE0o trong s\u1ED1 c\xE1c token c\u1EA7n che. Ch\xFAng t\xF4i s\u1EBD ch\u1ECDn 15%, l\xE0 s\u1ED1 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho BERT v\xE0 m\u1ED9t l\u1EF1a ch\u1ECDn ph\u1ED5 bi\u1EBFn trong c\xE1c t\xE0i li\u1EC7u:"),el=u(),w(xs.$$.fragment),sl=u(),$n=l("p"),lr=s("\u0110\u1EC3 xem c\xE1ch ho\u1EA1t \u0111\u1ED9ng c\u1EE7a vi\u1EC7c che ng\u1EABu nhi\xEAn, h\xE3y cung c\u1EA5p m\u1ED9t v\xE0i m\u1EABu cho tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u. V\xEC n\xF3 mong \u0111\u1EE3i m\u1ED9t danh s\xE1ch c\xE1c "),Oh=l("code"),cr=s("dict"),or=s(", trong \u0111\xF3 m\u1ED7i "),Bh=l("code"),rr=s("dict"),pr=s(" \u0111\u1EA1i di\u1EC7n cho m\u1ED9t \u0111o\u1EA1n v\u0103n b\u1EA3n li\u1EC1n k\u1EC1, \u0111\u1EA7u ti\xEAn ch\xFAng ta l\u1EB7p t\u1EADp d\u1EEF li\u1EC7u tr\u01B0\u1EDBc khi cung c\u1EA5p l\xF4 cho b\u1ED9 \u0111\u1ED1i chi\u1EBFu. Ch\xFAng ta x\xF3a kh\xF3a "),Lh=l("code"),mr=s('"word_ids"'),ur=s(" cho tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u n\xE0y v\xEC n\xF3 kh\xF4ng c\u1EA7n ch\xFAng:"),al=u(),w(Es.$$.fragment),hl=u(),w(Ts.$$.fragment),il=u(),Qn=l("p"),gr=s("T\u1ED1t, n\xF3 \u0111\xE3 ho\u1EA1t \u0111\u1ED9ng! Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng "),Nh=l("code"),_r=s("[MASK]"),dr=s(" \u0111\xE3 \u0111\u01B0\u1EE3c ch\xE8n ng\u1EABu nhi\xEAn t\u1EA1i c\xE1c v\u1ECB tr\xED kh\xE1c nhau trong v\u0103n b\u1EA3n. \u0110\xE2y s\u1EBD l\xE0 nh\u1EEFng token m\xE0 m\xF4 h\xECnh s\u1EBD ph\u1EA3i d\u1EF1 \u0111o\xE1n trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n - v\xE0 c\xE1i hay c\u1EE7a c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u l\xE0 n\xF3 s\u1EBD ng\u1EABu nhi\xEAn ch\xE8n "),Hh=l("code"),fr=s("[MASK]"),br=s(" v\u1EDBi m\u1ECDi l\xF4!"),ll=u(),w(qe.$$.fragment),cl=u(),sn&&sn.c(),da=u(),wn=l("p"),kr=s("Khi hu\u1EA5n luy\u1EC7n c\xE1c m\xF4 h\xECnh \u0111\u1EC3 t\u1EA1o m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i, m\u1ED9t k\u1EF9 thu\u1EADt c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng l\xE0 gh\xE9p c\xE1c t\u1EEB l\u1EA1i v\u1EDBi nhau, kh\xF4ng ch\u1EC9 c\xE1c token ri\xEAng l\u1EBB. C\xE1ch ti\u1EBFp c\u1EADn n\xE0y \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Rh=l("em"),vr=s("whole word masking"),yr=s(" hay "),Fh=l("em"),$r=s("che to\xE0n b\u1ED9 t\u1EEB"),wr=s(". N\u1EBFu ch\xFAng ta mu\u1ED1n che to\xE0n b\u1ED9 t\u1EEB, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i t\u1EF1 x\xE2y d\u1EF1ng m\u1ED9t b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u. B\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ch\u1EC9 l\xE0 m\u1ED9t ch\u1EE9c n\u0103ng l\u1EA5y danh s\xE1ch c\xE1c m\u1EABu v\xE0 chuy\u1EC3n \u0111\u1ED5i ch\xFAng th\xE0nh m\u1ED9t l\xF4, v\xEC v\u1EADy h\xE3y l\xE0m \u0111i\u1EC1u n\xE0y ngay b\xE2y gi\u1EDD! Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c ID t\u1EEB \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 t\u1EA1o b\u1EA3n \u0111\u1ED3 gi\u1EEFa c\xE1c ch\u1EC9 s\u1ED1 t\u1EEB v\xE0 c\xE1c m\xE3 th\xF4ng b\xE1o t\u01B0\u01A1ng \u1EE9ng, sau \u0111\xF3 quy\u1EBFt \u0111\u1ECBnh ng\u1EABu nhi\xEAn nh\u1EEFng t\u1EEB n\xE0o c\u1EA7n che v\xE0 che c\xE1c \u0111\u1EA7u v\xE0o. L\u01B0u \xFD r\u1EB1ng t\u1EA5t c\u1EA3 c\xE1c nh\xE3n \u0111\u1EC1u l\xE0 "),Gh=l("code"),jr=s("-100"),xr=s(" ngo\u1EA1i tr\u1EEB c\xE1c nh\xE3n t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c t\u1EEB b\u1ECB che."),ol=u(),zn.c(),fa=u(),ba=l("p"),Er=s("Ti\u1EBFp theo, ta cso th\u1EC3 th\u1EED tr\xEAn m\u1ED9t v\xE0i m\u1EABu nh\u01B0 tr\xEAn:"),rl=u(),w(qs.$$.fragment),pl=u(),w(Cs.$$.fragment),ml=u(),w(Ce.$$.fragment),ul=u(),Xn=l("p"),Tr=s("Gi\u1EDD ch\xFAng ta c\xF3 hai tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u, ph\u1EA7n c\xF2n l\u1EA1i c\u1EE7a c\xE1c b\u01B0\u1EDBc tinh ch\u1EC9nh l\xE0 ti\xEAu chu\u1EA9n. Qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n c\xF3 th\u1EC3 m\u1EA5t m\u1ED9t kho\u1EA3ng th\u1EDDi gian tr\xEAn Google Colab n\u1EBFu b\u1EA1n kh\xF4ng \u0111\u1EE7 may m\u1EAFn \u0111\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c GPU P100 th\u1EA7n tho\u1EA1i \u{1F62D}, v\xEC v\u1EADy, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta s\u1EBD gi\u1EA3m k\xEDch th\u01B0\u1EDBc c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n xu\u1ED1ng c\xF2n v\xE0i ngh\xECn m\u1EABu. \u0110\u1EEBng lo l\u1EAFng, ch\xFAng ta s\u1EBD v\u1EABn nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF kh\xE1 t\u1ED1t! M\u1ED9t c\xE1ch nhanh ch\xF3ng \u0111\u1EC3 gi\u1EA3m m\u1EABu m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u trong \u{1F917} Datasets l\xE0 th\xF4ng qua h\xE0m "),Ih=l("code"),qr=s("Dataset.train_test_split()"),Cr=s(" m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ka=l("a"),Dr=s("Chapter 5"),Mr=s(":"),gl=u(),w(Ds.$$.fragment),_l=u(),w(Ms.$$.fragment),dl=u(),Zn=l("p"),zr=s("\u0110i\u1EC1u n\xE0y \u0111\xE3 t\u1EF1 \u0111\u1ED9ng t\u1EA1o c\xE1c ph\u1EA7n t\xE1ch "),Vh=l("code"),Ar=s("hu\u1EA5n luy\u1EC7n"),Pr=s(" v\xE0 "),Uh=l("code"),Sr=s("ki\u1EC3m th\u1EED"),Kr=s(" m\u1EDBi, v\u1EDBi k\xEDch th\u01B0\u1EDBc t\u1EADp hu\u1EA5n luy\u1EC7n \u0111\u01B0\u1EE3c \u0111\u1EB7t th\xE0nh 10,000 m\u1EABu v\xE0 x\xE1c th\u1EF1c \u0111\u01B0\u1EE3c \u0111\u1EB7t th\xE0nh 10% - vui l\xF2ng t\u0103ng \u0111i\u1EC1u n\xE0y n\u1EBFu b\u1EA1n c\xF3 GPU m\u1EA1nh! \u0110i\u1EC1u ti\u1EBFp theo ch\xFAng ta c\u1EA7n l\xE0m l\xE0 \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face Hub. N\u1EBFu b\u1EA1n \u0111ang ch\u1EA1y m\xE3 n\xE0y trong notebook, b\u1EA1n c\xF3 th\u1EC3 l\xE0m nh\u01B0 v\u1EADy v\u1EDBi ch\u1EE9c n\u0103ng ti\u1EC7n \xEDch sau:"),fl=u(),w(zs.$$.fragment),bl=u(),va=l("p"),Or=s("s\u1EBD hi\u1EC3n th\u1ECB m\u1ED9t ti\u1EC7n \xEDch m\xE0 b\u1EA1n c\xF3 th\u1EC3 nh\u1EADp th\xF4ng tin \u0111\u0103ng nh\u1EADp c\u1EE7a m\xECnh. Ngo\xE0i ra, b\u1EA1n c\xF3 th\u1EC3 ch\u1EA1y:"),kl=u(),w(As.$$.fragment),vl=u(),ya=l("p"),Br=s("trong thi\u1EBFt b\u1ECB \u0111\u1EA7u cu\u1ED1i y\xEAu th\xEDch c\u1EE7a b\u1EA1n v\xE0 \u0111\u0103ng nh\u1EADp \u1EDF \u0111\xF3."),yl=u(),Pn.c(),$a=u(),me=l("h3"),De=l("a"),Wh=l("span"),w(Ps.$$.fragment),Lr=u(),Yh=l("span"),Nr=s("Perplexity cho m\xF4 h\xECnh ng\xF4n ng\u1EEF"),$l=u(),w(Ss.$$.fragment),wl=u(),wa=l("p"),Hr=s("Kh\xF4ng gi\u1ED1ng nh\u01B0 c\xE1c t\xE1c v\u1EE5 kh\xE1c nh\u01B0 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n ho\u1EB7c h\u1ECFi \u0111\xE1p m\xE0 ch\xFAng ta \u0111\u01B0\u1EE3c cung c\u1EA5p m\u1ED9t kho ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n \u0111\u1EC3 hu\u1EA5n luy\u1EC7n, v\u1EDBi m\xF4 h\xECnh ng\xF4n ng\u1EEF, ta kh\xF4ng c\xF3 b\u1EA5t k\u1EF3 nh\xE3n r\xF5 r\xE0ng n\xE0o. V\u1EADy l\xE0m c\xE1ch n\xE0o \u0111\u1EC3 x\xE1c \u0111\u1ECBnh \u0111i\u1EC1u g\xEC t\u1EA1o n\xEAn m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t? Gi\u1ED1ng nh\u01B0 t\xEDnh n\u0103ng t\u1EF1 \u0111\u1ED9ng s\u1EEDa l\u1ED7i trong \u0111i\u1EC7n tho\u1EA1i c\u1EE7a b\u1EA1n, m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t l\xE0 m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF ch\u1EC9 \u0111\u1ECBnh x\xE1c su\u1EA5t cao cho c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p v\xE0 x\xE1c su\u1EA5t th\u1EA5p cho c\xE1c c\xE2u v\xF4 ngh\u0129a. \u0110\u1EC3 gi\xFAp b\u1EA1n bi\u1EBFt r\xF5 h\u01A1n v\u1EC1 h\xECnh th\u1EE9c n\xE0y, b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y to\xE0n b\u1ED9 t\u1EADp h\u1EE3p \u201Ct\u1EF1 \u0111\u1ED9ng s\u1EEDa l\u1ED7i\u201D tr\u1EF1c tuy\u1EBFn, trong \u0111\xF3 m\xF4 h\xECnh trong \u0111i\u1EC7n tho\u1EA1i \u0111\xE3 t\u1EA1o ra m\u1ED9t s\u1ED1 ho\xE0n th\xE0nh kh\xE1 h\xE0i h\u01B0\u1EDBc (v\xE0 th\u01B0\u1EDDng kh\xF4ng ph\xF9 h\u1EE3p)!"),jl=u(),Kn.c(),ja=u(),w(Ks.$$.fragment),xl=u(),xa=l("p"),Rr=s("Perplexity th\u1EA5p h\u01A1n c\xF3 ngh\u0129a l\xE0 m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t h\u01A1n v\xE0 ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y \u1EDF \u0111\xE2y r\u1EB1ng m\xF4 h\xECnh b\u1EAFt \u0111\u1EA7u c\u1EE7a ch\xFAng ta c\xF3 m\u1ED9t gi\xE1 tr\u1ECB h\u01A1i l\u1EDBn. H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 h\u1EA1 th\u1EA5p n\xF3 b\u1EB1ng c\xE1ch tinh ch\u1EC9nh kh\xF4ng! \u0110\u1EC3 l\xE0m \u0111i\u1EC1u \u0111\xF3, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta ch\u1EA1y v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n:"),El=u(),Bn.c(),Ea=u(),Ta=l("p"),Fr=s("v\xE0 sau \u0111\xF3 t\xEDnh k\u1EBFt qu\u1EA3 perplexity tr\xEAn t\u1EADp ki\u1EC3m th\u1EED:"),Tl=u(),Nn.c(),qa=u(),w(Os.$$.fragment),ql=u(),Ca=l("p"),Gr=s("T\u1ED1t \u2014 n\xF3 gi\u1EA3m perplexity, cho th\u1EA5y m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c \u0111i\u1EC1u g\xEC \u0111\xF3 v\u1EC1 m\u1EA3ng \u0111\xE1nh gi\xE1 phim!"),Cl=u(),zt&&zt.c(),Da=u(),w(Me.$$.fragment),Dl=u(),At&&At.c(),Ma=u(),ue=l("h2"),ze=l("a"),Jh=l("span"),w(Bs.$$.fragment),Ir=u(),Qh=l("span"),Vr=s("S\u1EED d\u1EE5ng m\xF4 h\xECnh tinh ch\u1EC9nh c\u1EE7a m\xECnh"),Ml=u(),te=l("p"),Ur=s("\u1EA1n c\xF3 th\u1EC3 t\u01B0\u01A1ng t\xE1c v\u1EDBi m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c tinh ch\u1EC9nh c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng ti\u1EC7n \xEDch c\u1EE7a n\xF3 tr\xEAn Hub ho\u1EB7c c\u1EE5c b\u1ED9 v\u1EDBi "),Xh=l("code"),Wr=s("pipeline"),Yr=s(" t\u1EEB \u{1F917} Transformers. H\xE3y s\u1EED d\u1EE5ng c\xE1i sau \u0111\u1EC3 t\u1EA3i xu\u1ED1ng m\xF4 h\xECnh c\u1EE7a ch\xFAng t\xF4i b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng pipeline "),Zh=l("code"),Jr=s("fill-mask"),Qr=s(":"),zl=u(),w(Ls.$$.fragment),Al=u(),za=l("p"),Xr=s("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p v\u0103n b\u1EA3n m\u1EABu \u201CThis is a great [MASK]\u201D v\xE0 xem 5 d\u1EF1 \u0111o\xE1n \u0111\u1EA7u l\xE0 g\xEC:"),Pl=u(),w(Ns.$$.fragment),Sl=u(),w(Hs.$$.fragment),Kl=u(),Aa=l("p"),Zr=s("G\u1ECDn g\xE0ng - m\xF4 h\xECnh c\u1EE7a ch\xFAng ta r\xF5 r\xE0ng \u0111\xE3 \u0111i\u1EC1u ch\u1EC9nh tr\u1ECDng s\u1ED1 c\u1EE7a n\xF3 \u0111\u1EC3 d\u1EF1 \u0111o\xE1n c\xE1c t\u1EEB li\xEAn quan nhi\u1EC1u h\u01A1n \u0111\u1EBFn phim!"),Ol=u(),w(Rs.$$.fragment),Bl=u(),Ae=l("p"),tp=s("\u0110i\u1EC1u n\xE0y k\u1EBFt th\xFAc th\u1EED nghi\u1EC7m \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta v\u1EDBi vi\u1EC7c hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF. Trong "),Pa=l("a"),np=s("ph\u1EA7n 6"),ep=s(", b\u1EA1n s\u1EBD h\u1ECDc c\xE1ch hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy nh\u01B0 GPT-2 t\u1EEB \u0111\u1EA7u; h\xE3y \u0111\u1EBFn \u0111\xF3 n\u1EBFu b\u1EA1n mu\u1ED1n xem c\xE1ch b\u1EA1n c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\xF4 h\xECnh Transformer c\u1EE7a ri\xEAng m\xECnh!"),Ll=u(),w(Pe.$$.fragment),this.h()},l(t){const h=yu('[data-svelte="svelte-1phssyn"]',document.head);o=c(h,"META",{name:!0,content:!0}),h.forEach(e),d=g(t),j(p.$$.fragment,t),v=g(t),q=c(t,"H1",{class:!0});var Qs=r(q);$=c(Qs,"A",{id:!0,class:!0,href:!0});var Sa=r($);k=c(Sa,"SPAN",{});var ti=r(k);j(D.$$.fragment,ti),ti.forEach(e),Sa.forEach(e),_=g(Qs),C=c(Qs,"SPAN",{});var Ka=r(C);P=a(Ka,"Tinh ch\u1EC9nh m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),Ka.forEach(e),Qs.forEach(e),K=g(t),N.l(t),H=g(t),R=c(t,"P",{});var Oa=r(R);F=a(Oa,"\u0110\u1ED1i v\u1EDBi nhi\u1EC1u \u1EE9ng d\u1EE5ng NLP li\xEAn quan \u0111\u1EBFn c\xE1c m\xF4 h\xECnh Transformer, b\u1EA1n c\xF3 th\u1EC3 ch\u1EC9 c\u1EA7n l\u1EA5y m\u1ED9t m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc t\u1EEB Hugging Face Hub v\xE0 tinh ch\u1EC9nh tr\u1EF1c ti\u1EBFp tr\xEAn d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n cho t\xE1c v\u1EE5 hi\u1EC7n t\u1EA1i. V\u1EDBi \u0111i\u1EC1u ki\u1EC7n l\xE0 ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc kh\xF4ng qu\xE1 kh\xE1c bi\u1EC7t v\u1EDBi ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 tinh ch\u1EC9nh, vi\u1EC7c h\u1ECDc chuy\u1EC3n ti\u1EBFp th\u01B0\u1EDDng s\u1EBD mang l\u1EA1i k\u1EBFt qu\u1EA3 t\u1ED1t."),Oa.forEach(e),W=g(t),y=c(t,"P",{});var ni=r(y);L=a(ni,"Tuy nhi\xEAn, c\xF3 m\u1ED9t v\xE0i tr\u01B0\u1EDDng h\u1EE3p m\xE0 tr\u01B0\u1EDBc ti\xEAn b\u1EA1n s\u1EBD mu\u1ED1n tinh ch\u1EC9nh c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF tr\xEAn d\u1EEF li\u1EC7u c\u1EE7a m\xECnh, tr\u01B0\u1EDBc khi hu\u1EA5n luy\u1EC7n \u0111\u1EA7u t\xE1c v\u1EE5 c\u1EE5 th\u1EC3. V\xED d\u1EE5: n\u1EBFu t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n ch\u1EE9a c\xE1c h\u1EE3p \u0111\u1ED3ng ph\xE1p l\xFD ho\u1EB7c c\xE1c b\xE0i b\xE1o khoa h\u1ECDc, th\xEC m\xF4 h\xECnh thu\u1EA7n Transformer nh\u01B0 BERT th\u01B0\u1EDDng s\u1EBD coi c\xE1c t\u1EEB chuy\xEAn m\xF4n trong kho d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n l\xE0 token hi\u1EBFm v\xE0 hi\u1EC7u su\u1EA5t k\u1EBFt qu\u1EA3 c\xF3 th\u1EC3 k\xE9m h\u01A1n. B\u1EB1ng c\xE1ch tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF tr\xEAn d\u1EEF li\u1EC7u chuy\xEAn m\xF4n, b\u1EA1n c\xF3 th\u1EC3 t\u0103ng hi\u1EC7u su\u1EA5t c\u1EE7a nhi\u1EC1u t\xE1c v\u1EE5 xu\xF4i d\xF2ng, c\xF3 ngh\u0129a l\xE0 b\u1EA1n th\u01B0\u1EDDng ch\u1EC9 ph\u1EA3i th\u1EF1c hi\u1EC7n b\u01B0\u1EDBc n\xE0y m\u1ED9t l\u1EA7n!"),ni.forEach(e),I=g(t),G=c(t,"P",{});var Hn=r(G);tt=a(Hn,"Qu\xE1 tr\xECnh tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc tr\xEAn d\u1EEF li\u1EC7u trong m\u1EA3ng n\xE0y th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),V=c(Hn,"EM",{});var ei=r(V);qt=a(ei,"domain adapt"),ei.forEach(e),lt=a(Hn," hay "),gt=c(Hn,"EM",{});var si=r(gt);U=a(si,"th\xEDch \u1EE9ng chuy\xEAn m\xF4n"),si.forEach(e),_t=a(Hn,". N\xF3 \u0111\u01B0\u1EE3c ph\u1ED5 bi\u1EBFn v\xE0o n\u0103m 2018 b\u1EDFi "),Y=c(Hn,"A",{href:!0,rel:!0});var Ba=r(Y);Pt=a(Ba,"ULMFiT"),Ba.forEach(e),ct=a(Hn,", l\xE0 m\u1ED9t trong nh\u1EEFng ki\u1EBFn \u200B\u200Btr\xFAc m\u1EA1ng th\u1EA7n kinh \u0111\u1EA7u ti\xEAn (d\u1EF1a tr\xEAn LSTM) \u0111\u1EC3 l\xE0m cho vi\u1EC7c h\u1ECDc chuy\u1EC3n ti\u1EBFp th\u1EF1c s\u1EF1 hi\u1EC7u qu\u1EA3 cho NLP. M\u1ED9t v\xED d\u1EE5 v\u1EC1 th\xEDch \u1EE9ng chuy\xEAn m\xF4n v\u1EDBi ULMFiT \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong h\xECnh d\u01B0\u1EDBi \u0111\xE2y; trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD l\xE0m \u0111i\u1EC1u t\u01B0\u01A1ng t\u1EF1, nh\u01B0ng v\u1EDBi Transformer thay v\xEC LSTM!"),Hn.forEach(e),ot=g(t),rt=c(t,"DIV",{class:!0});var Xs=r(rt);Z=c(Xs,"IMG",{class:!0,src:!0,alt:!0}),Lt=g(Xs),Ct=c(Xs,"IMG",{class:!0,src:!0,alt:!0}),Xs.forEach(e),Nt=g(t),ut=c(t,"P",{});var Se=r(ut);Dt=a(Se,"\u0110\u1EBFn cu\u1ED1i ph\u1EA7n n\xE0y, b\u1EA1n s\u1EBD c\xF3 m\u1ED9t "),st=c(Se,"A",{href:!0,rel:!0});var La=r(st);Ft=a(La,"m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),La.forEach(e),it=a(Se," tr\xEAn Hub c\xF3 th\u1EC3 t\u1EF1 \u0111\u1ED9ng ho\xE0n thi\u1EC7n c\xE2u nh\u01B0 d\u01B0\u1EDBi \u0111\xE2y:"),Se.forEach(e),Gt=g(t),at=c(t,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),r(at).forEach(e),ht=g(t),yt=c(t,"P",{});var Na=r(yt);xt=a(Na,"C\xF9ng \u0111i s\xE2u v\xE0o th\xF4i!"),Na.forEach(e),Vt=g(t),j($t.$$.fragment,t),S=g(t),j(J.$$.fragment,t),Et=g(t),nt=c(t,"H2",{class:!0});var Ke=r(nt);pt=c(Ke,"A",{id:!0,class:!0,href:!0});var ai=r(pt);Ht=c(ai,"SPAN",{});var hi=r(Ht);j(mt.$$.fragment,hi),hi.forEach(e),ai.forEach(e),Zt=g(Ke),X=c(Ke,"SPAN",{});var vp=r(X);tn=a(vp,"Ch\u1ECDn m\u1ED9t m\xF4 h\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc cho m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"),vp.forEach(e),Ke.forEach(e),Tt=g(t),wt=c(t,"P",{});var Hl=r(wt);Ut=a(Hl,"\u0110\u1EC3 b\u1EAFt \u0111\u1EA7u, h\xE3y ch\u1ECDn m\u1ED9t m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc ph\xF9 h\u1EE3p \u0111\u1EC3 t\u1EA1o m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i. Nh\u01B0 \u0111\u01B0\u1EE3c hi\u1EC3n th\u1ECB trong \u1EA3nh ch\u1EE5p m\xE0n h\xECnh d\u01B0\u1EDBi \u0111\xE2y, b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y danh s\xE1ch c\xE1c \u1EE9ng c\u1EED vi\xEAn b\u1EB1ng c\xE1ch \xE1p d\u1EE5ng b\u1ED9 l\u1ECDc \u201CFill-Mask\u201D tr\xEAn "),Kt=c(Hl,"A",{href:!0,rel:!0});var yp=r(Kt);nn=a(yp,"Hugging Face Hub"),yp.forEach(e),z=a(Hl,":"),Hl.forEach(e),Q=g(t),Ot=c(t,"DIV",{class:!0});var $p=r(Ot);et=c($p,"IMG",{src:!0,alt:!0,width:!0}),$p.forEach(e),un=g(t),dt=c(t,"P",{});var Oe=r(dt);Tn=a(Oe,"M\u1EB7c d\xF9 d\xF2ng m\xF4 h\xECnh BERT v\xE0 RoBERTa \u0111\u01B0\u1EE3c t\u1EA3i xu\u1ED1ng nhi\u1EC1u nh\u1EA5t, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng m\xF4 h\xECnh c\xF3 t\xEAn "),Rt=c(Oe,"A",{href:!0,rel:!0});var wp=r(Rt);Rn=a(wp,"DistilBERT"),wp.forEach(e),jt=a(Oe,`
c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n nhanh h\u01A1n nhi\u1EC1u m\xE0 \xEDt ho\u1EB7c kh\xF4ng b\u1ECB m\u1EA5t hi\u1EC7u su\u1EA5t. M\xF4 h\xECnh n\xE0y \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng m\u1ED9t k\u1EF9 thu\u1EADt \u0111\u1EB7c bi\u1EC7t c\xF3 t\xEAn l\xE0 `),Bt=c(Oe,"A",{href:!0,rel:!0});var jp=r(Bt);on=c(jp,"EM",{});var xp=r(on);an=a(xp,"knowledge distillation"),xp.forEach(e),jp.forEach(e),qn=a(Oe,", trong \u0111\xF3 m\u1ED9t \u201Cm\xF4 h\xECnh gi\xE1o vi\xEAn\u201D l\u1EDBn nh\u01B0 BERT \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 h\u01B0\u1EDBng d\u1EABn hu\u1EA5n luy\u1EC7n \u201Cm\xF4 h\xECnh sinh vi\xEAn\u201D c\xF3 \xEDt tham s\u1ED1 h\u01A1n nhi\u1EC1u. Ph\u1EA7n gi\u1EA3i th\xEDch chi ti\u1EBFt v\u1EC1 qu\xE1 tr\xECnh ch\u1EAFt l\u1ECDc ki\u1EBFn \u200B\u200Bth\u1EE9c s\u1EBD \u0111\u01B0a ch\xFAng ta \u0111i qu\xE1 xa trong ph\u1EA7n n\xE0y, nh\u01B0ng n\u1EBFu b\u1EA1n quan t\xE2m, b\u1EA1n c\xF3 th\u1EC3 \u0111\u1ECDc t\u1EA5t c\u1EA3 v\u1EC1 n\xF3 trong "),Mt=c(Oe,"A",{href:!0,rel:!0});var Ep=r(Mt);gn=c(Ep,"EM",{});var Tp=r(gn);Wt=a(Tp,"Natural Language Processing with Transformers"),Tp.forEach(e),Ep.forEach(e),Fn=a(Oe," (th\u01B0\u1EDDng \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 s\xE1ch gi\xE1o khoa v\u1EC1 Transformer)."),Oe.forEach(e),Cn=g(t),bt.l(t),hn=g(t),rn=c(t,"P",{});var qp=r(rn);Ie=a(qp,"V\u1EDBi kho\u1EA3ng 67 tri\u1EC7u tham s\u1ED1, DistilBERT nh\u1ECF h\u01A1n kho\u1EA3ng hai l\u1EA7n so v\u1EDBi m\xF4 h\xECnh c\u01A1 s\u1EDF BERT, g\u1EA7n nh\u01B0 \u0111\u01B0\u1EE3c hi\u1EC3u l\xE0 t\u0103ng t\u1ED1c g\u1EA5p hai l\u1EA7n khi hu\u1EA5n luy\u1EC7n - th\u1EADt tuy\u1EC7t! B\xE2y gi\u1EDD ch\xFAng ta h\xE3y xem nh\u1EEFng lo\u1EA1i token n\xE0o m\xF4 h\xECnh n\xE0y d\u1EF1 \u0111o\xE1n l\xE0 c\xF3 nhi\u1EC1u kh\u1EA3 n\u0103ng ho\xE0n th\xE0nh m\u1ED9t m\u1EABu v\u0103n b\u1EA3n nh\u1ECF:"),qp.forEach(e),ie=g(t),j(en.$$.fragment,t),le=g(t),kt=c(t,"P",{});var ne=r(kt);ge=a(ne,"L\xE0 con ng\u01B0\u1EDDi, ch\xFAng ta c\xF3 th\u1EC3 t\u01B0\u1EDFng t\u01B0\u1EE3ng ra nhi\u1EC1u kh\u1EA3 n\u0103ng \u0111\u1ED1i v\u1EDBi token "),_n=c(ne,"CODE",{});var Cp=r(_n);Ve=a(Cp,"[MASK]"),Cp.forEach(e),_e=a(ne,", v\xED d\u1EE5 \u201Cday\u201D, \u201Cride\u201D, ho\u1EB7c \u201Cpainting\u201D. \u0110\u1ED1i v\u1EDBi c\xE1c m\xF4 h\xECnh \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc, c\xE1c d\u1EF1 \u0111o\xE1n ph\u1EE5 thu\u1ED9c v\xE0o kho ng\u1EEF li\u1EC7u m\xF4 h\xECnh \u0111\xF3 hu\u1EA5n luy\u1EC7n, v\xEC n\xF3 h\u1ECDc c\xE1ch ch\u1ECDn c\xE1c m\u1EABu th\u1ED1ng k\xEA c\xF3 trong d\u1EEF li\u1EC7u. Gi\u1ED1ng BERT, DistilBERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc tr\xEAn b\u1ED9 d\u1EEF li\u1EC7u "),m=c(ne,"A",{href:!0,rel:!0});var Dp=r(m);O=a(Dp,"English Wikipedia"),Dp.forEach(e),de=a(ne," v\xE0 "),Gn=c(ne,"A",{href:!0,rel:!0});var Mp=r(Gn);Zs=a(Mp,"BookCorpus"),Mp.forEach(e),ta=a(ne,", n\xEAn ta k\xEC v\u1ECDng c\xE1c d\u1EF1 \u0111o\xE1n cho "),dn=c(ne,"CODE",{});var zp=r(dn);na=a(zp,"[MASK]"),zp.forEach(e),ea=a(ne," s\u1EBD ph\u1EA3n \xE1nh c\xE1c m\u1EA3ng n\xE0y. \u0110\u1EC3 d\u1EF1 \u0111o\xE1n ta c\u1EA7n tr\xECnh tokenizer c\u1EE7a DistilBERT t\u1EA1o ra c\xE1c \u0111\u1EA7u v\xE0o cho m\xF4 h\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA3i t\u1EEB Hub:"),ne.forEach(e),pn=g(t),j(In.$$.fragment,t),Ue=g(t),ce=c(t,"P",{});var Ap=r(ce);fe=a(Ap,"V\u1EDBi m\u1ED9t tokenizer v\xE0 m\u1ED9t m\xF4 h\xECnh, ta c\xF3 th\u1EC3 truy\u1EC1n c\xE1c \u0111o\u1EA1n v\u0103n v\xED d\u1EE5 t\u1EDBi m\xF4 h\xECnh, tr\xEDch xu\u1EA5t logits, v\xE0 xin ra 5 \u1EE9ng c\u1EED vi\xEAn:"),Ap.forEach(e),We=g(t),ln.l(t),oe=g(t),j(fn.$$.fragment,t),Ye=g(t),Dn=c(t,"P",{});var Pp=r(Dn);sa=a(Pp,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y t\u1EEB k\u1EBFt qu\u1EA3 \u0111\u1EA7u ra r\u1EB1ng c\xE1c d\u1EF1 \u0111o\xE1n c\u1EE7a m\xF4 h\xECnh \u0111\u1EC1 c\u1EADp \u0111\u1EBFn c\xE1c thu\u1EADt ng\u1EEF h\xE0ng ng\xE0y, \u0111i\u1EC1u n\xE0y c\xF3 l\u1EBD kh\xF4ng c\xF3 g\xEC \u0111\xE1ng ng\u1EA1c nhi\xEAn khi d\u1EF1a tr\xEAn n\u1EC1n t\u1EA3ng c\u1EE7a Wikipedia ti\u1EBFng Anh. H\xE3y xem c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 thay \u0111\u1ED5i m\u1EA3ng n\xE0y th\xE0nh m\u1ED9t th\u1EE9 g\xEC \u0111\xF3 th\xEDch h\u1EE3p h\u01A1n m\u1ED9t ch\xFAt - c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim ph\xE2n c\u1EF1c cao!"),Pp.forEach(e),Je=g(t),bn=c(t,"H2",{class:!0});var Rl=r(bn);mn=c(Rl,"A",{id:!0,class:!0,href:!0});var Sp=r(mn);be=c(Sp,"SPAN",{});var Kp=r(be);j(Vn.$$.fragment,Kp),Kp.forEach(e),Sp.forEach(e),aa=g(Rl),Ya=c(Rl,"SPAN",{});var Op=r(Ya);ec=a(Op,"B\u1ED9 d\u1EEF li\u1EC7u"),Op.forEach(e),Rl.forEach(e),ii=g(t),Un=c(t,"P",{});var Ha=r(Un);sc=a(Ha,"\u0110\u1EC3 gi\u1EDBi thi\u1EC7u vi\u1EC7c th\xEDch \u1EE9ng chuy\xEAn m\xF4n, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng b\u1ED9 d\u1EEF li\u1EC7u n\u1ED5i ti\u1EBFng "),Qe=c(Ha,"A",{href:!0,rel:!0});var Bp=r(Qe);ac=a(Bp,"Large Movie Review Dataset"),Bp.forEach(e),hc=a(Ha,"(hay vi\u1EBFt t\u1EAFt l\xE0 IMDb), l\xE0 t\u1EADp h\u1EE3p c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim th\u01B0\u1EDDng \u0111\u01B0\u1EE3c d\xF9ng \u0111\u1EC3 \u0111\xE1nh gi\xE1 c\xE1c m\xF4 h\xECnh ph\xE2n t\xEDch c\u1EA3m x\xFAc. B\u1EB1ng c\xE1ch tinh ch\u1EC9nh DistilBERT tr\xEAn kho ng\u1EEF li\u1EC7u n\xE0y, ch\xFAng ta hy v\u1ECDng m\xF4 h\xECnh ng\xF4n ng\u1EEF s\u1EBD \u0111i\u1EC1u ch\u1EC9nh v\u1ED1n t\u1EEB v\u1EF1ng c\u1EE7a n\xF3 t\u1EEB d\u1EEF li\u1EC7u th\u1EF1c t\u1EBF c\u1EE7a Wikipedia m\xE0 n\xF3 \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc \u0111\u1EC3 ph\xF9 h\u1EE3p v\u1EDBi c\xE1c y\u1EBFu t\u1ED1 ch\u1EE7 quan h\u01A1n c\u1EE7a c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim. Ch\xFAng ta c\xF3 th\u1EC3 l\u1EA5y d\u1EEF li\u1EC7u t\u1EEB Hugging Face Hub b\u1EB1ng h\xE0m "),Ja=c(Ha,"CODE",{});var Lp=r(Ja);ic=a(Lp,"load_dataset()"),Lp.forEach(e),lc=a(Ha," t\u1EEB \u{1F917} Datasets:"),Ha.forEach(e),li=g(t),j(Xe.$$.fragment,t),ci=g(t),j(Ze.$$.fragment,t),oi=g(t),Jt=c(t,"P",{});var jn=r(Jt);cc=a(jn,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng m\u1ED7i ph\u1EA7n "),Qa=c(jn,"CODE",{});var Np=r(Qa);oc=a(Np,"hu\u1EA5n luy\u1EC7n"),Np.forEach(e),rc=a(jn," v\xE0 "),Xa=c(jn,"CODE",{});var Hp=r(Xa);pc=a(Hp,"ki\u1EC3m th\u1EED"),Hp.forEach(e),mc=a(jn," bao g\u1ED3m 25,000 \u0111\xE1nh gi\xE1, trong khi ph\u1EA7n kh\xF4ng \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Za=c(jn,"CODE",{});var Rp=r(Za);uc=a(Rp,"phi gi\xE1m s\xE1t"),Rp.forEach(e),gc=a(jn," ch\u1EE9a 50,000 \u0111\xE1nh gi\xE1. Ch\xFAng ta h\xE3y xem m\u1ED9t v\xE0i m\u1EABu \u0111\u1EC3 c\xF3 \xFD t\u01B0\u1EDFng v\u1EC1 lo\u1EA1i v\u0103n b\u1EA3n m\xE0 ta \u0111ang x\u1EED l\xFD. Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EF1c hi\u1EC7n trong c\xE1c ch\u01B0\u01A1ng tr\u01B0\u1EDBc c\u1EE7a kh\xF3a h\u1ECDc, ch\xFAng ta s\u1EBD x\xE2u chu\u1ED7i c\xE1c h\xE0m "),th=c(jn,"CODE",{});var Fp=r(th);_c=a(Fp,"Dataset.shuffle()"),Fp.forEach(e),dc=a(jn," v\xE0 "),nh=c(jn,"CODE",{});var Gp=r(nh);fc=a(Gp,"Dataset.select()"),Gp.forEach(e),bc=a(jn," \u0111\u1EC3 t\u1EA1o m\u1ED9t m\u1EABu ng\u1EABu nhi\xEAn:"),jn.forEach(e),ri=g(t),j(ts.$$.fragment,t),pi=g(t),j(ns.$$.fragment,t),mi=g(t),Wn=c(t,"P",{});var Ra=r(Wn);kc=a(Ra,"\u0110\xFAng, \u0111\xE2y ch\u1EAFc ch\u1EAFn l\xE0 nh\u1EEFng b\xE0i \u0111\xE1nh gi\xE1 phim, v\xE0 n\u1EBFu b\u1EA1n \u0111\u1EE7 l\u1EDBn, b\u1EA1n th\u1EADm ch\xED c\xF3 th\u1EC3 hi\u1EC3u nh\u1EADn x\xE9t trong b\xE0i \u0111\xE1nh gi\xE1 cu\u1ED1i c\xF9ng v\u1EC1 vi\u1EC7c s\u1EDF h\u1EEFu phi\xEAn b\u1EA3n VHS \u{1F61C}! M\u1EB7c d\xF9 ch\xFAng ta s\u1EBD kh\xF4ng c\u1EA7n nh\xE3n \u0111\u1EC3 cho m\xF4 h\xECnh ng\xF4n ng\u1EEF, nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng "),eh=c(Ra,"CODE",{});var Ip=r(eh);vc=a(Ip,"0"),Ip.forEach(e),yc=a(Ra," bi\u1EC3u th\u1ECB m\u1ED9t \u0111\xE1nh gi\xE1 ti\xEAu c\u1EF1c, trong khi "),sh=c(Ra,"CODE",{});var Vp=r(sh);$c=a(Vp,"1"),Vp.forEach(e),wc=a(Ra," t\u01B0\u01A1ng \u1EE9ng v\u1EDBi m\u1ED9t \u0111\xE1nh gi\xE1 t\xEDch c\u1EF1c."),Ra.forEach(e),ui=g(t),j(ke.$$.fragment,t),gi=g(t),ve=c(t,"P",{});var Fl=r(ve);jc=a(Fl,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t c\xE1i nh\xECn nhanh v\u1EC1 d\u1EEF li\u1EC7u, h\xE3y \u0111i s\xE2u v\xE0o vi\u1EC7c chu\u1EA9n b\u1ECB n\xF3 cho vi\u1EC7c l\u1EADp m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i. Nh\u01B0 ch\xFAng ta s\u1EBD th\u1EA5y, c\xF3 m\u1ED9t s\u1ED1 b\u01B0\u1EDBc b\u1ED5 sung m\xE0 ng\u01B0\u1EDDi ta c\u1EA7n th\u1EF1c hi\u1EC7n so v\u1EDBi c\xE1c t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i chu\u1ED7i m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ha=c(Fl,"A",{href:!0});var Up=r(ha);xc=a(Up,"Ch\u01B0\u01A1ng 3"),Up.forEach(e),Ec=a(Fl,". \u0110i th\xF4i!"),Fl.forEach(e),_i=g(t),re=c(t,"H2",{class:!0});var Gl=r(re);ye=c(Gl,"A",{id:!0,class:!0,href:!0});var Wp=r(ye);ah=c(Wp,"SPAN",{});var Yp=r(ah);j(es.$$.fragment,Yp),Yp.forEach(e),Wp.forEach(e),Tc=g(Gl),hh=c(Gl,"SPAN",{});var Jp=r(hh);qc=a(Jp,"Ti\u1EC1n x\u1EED l\xFD d\u1EEF li\u1EC7u"),Jp.forEach(e),Gl.forEach(e),di=g(t),j(ss.$$.fragment,t),fi=g(t),ia=c(t,"P",{});var Qp=r(ia);Cc=a(Qp,"\u0110\u1ED1i v\u1EDBi c\u1EA3 m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy v\xE0 b\u1ECB \u1EA9n \u0111i, m\u1ED9t b\u01B0\u1EDBc ti\u1EC1n x\u1EED l\xFD ph\u1ED5 bi\u1EBFn l\xE0 n\u1ED1i t\u1EA5t c\u1EA3 c\xE1c m\u1EABu v\xE0 sau \u0111\xF3 chia to\xE0n b\u1ED9 ng\u1EEF li\u1EC7u th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc b\u1EB1ng nhau. \u0110i\u1EC1u n\xE0y ho\xE0n to\xE0n kh\xE1c v\u1EDBi c\xE1ch ti\u1EBFp c\u1EADn th\xF4ng th\u01B0\u1EDDng, khi ch\xFAng ta ch\u1EC9 c\u1EA7n tokenize c\xE1c m\u1EABu ri\xEAng l\u1EBB. T\u1EA1i sao l\u1EA1i n\u1ED1i m\u1ECDi th\u1EE9 l\u1EA1i v\u1EDBi nhau? L\xFD do l\xE0 c\xE1c m\u1EABu ri\xEAng l\u1EBB c\xF3 th\u1EC3 b\u1ECB c\u1EAFt ng\u1EAFn n\u1EBFu ch\xFAng qu\xE1 d\xE0i v\xE0 \u0111i\u1EC1u \u0111\xF3 s\u1EBD d\u1EABn \u0111\u1EBFn vi\u1EC7c m\u1EA5t th\xF4ng tin c\xF3 th\u1EC3 h\u1EEFu \xEDch cho t\xE1c v\u1EE5 m\xF4 h\xECnh h\xF3a ng\xF4n ng\u1EEF!"),Qp.forEach(e),bi=g(t),Qt=c(t,"P",{});var xn=r(Qt);Dc=a(xn,"V\xEC v\u1EADy, \u0111\u1EC3 b\u1EAFt \u0111\u1EA7u, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta s\u1EBD tokenize kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh nh\u01B0 b\xECnh th\u01B0\u1EDDng, nh\u01B0ng "),ih=c(xn,"EM",{});var Xp=r(ih);Mc=a(Xp,"kh\xF4ng"),Xp.forEach(e),zc=a(xn," \u0111\u1EB7t t\xF9y ch\u1ECDn "),lh=c(xn,"CODE",{});var Zp=r(lh);Ac=a(Zp,"truncation=True"),Zp.forEach(e),Pc=a(xn," trong tr\xECnh tokenize c\u1EE7a ch\xFAng ta. Ch\xFAng ta c\u0169ng s\u1EBD l\u1EA5y c\xE1c ID t\u1EEB n\u1EBFu ch\xFAng c\xF3 s\u1EB5n ((ch\xFAng s\u1EBD c\xF3 s\u1EB5n n\u1EBFu ta \u0111ang s\u1EED d\u1EE5ng c\xF4ng c\u1EE5 tokenize nhanh, nh\u01B0 \u0111\u01B0\u1EE3c m\xF4 t\u1EA3 trong "),la=c(xn,"A",{href:!0});var tm=r(la);Sc=a(tm,"Ch\u01B0\u01A1ng 6"),tm.forEach(e),Kc=a(xn,"), v\xEC ta s\u1EBD c\u1EA7n ch\xFAng sau n\xE0y \u0111\u1EC3 th\u1EF1c hi\u1EC7n che to\xE0n b\u1ED9 t\u1EEB. Ch\xFAng ta s\u1EBD g\xF3i n\xF3 trong m\u1ED9t h\xE0m \u0111\u01A1n gi\u1EA3n v\xE0 trong khi th\u1EF1c hi\u1EC7n, ta s\u1EBD x\xF3a c\xE1c c\u1ED9t "),ch=c(xn,"CODE",{});var nm=r(ch);Oc=a(nm,"text"),nm.forEach(e),Bc=a(xn," v\xE0 "),oh=c(xn,"CODE",{});var em=r(oh);Lc=a(em,"label"),em.forEach(e),Nc=a(xn," v\xEC kh\xF4ng c\u1EA7n ch\xFAng n\u1EEFa:"),xn.forEach(e),ki=g(t),j(as.$$.fragment,t),vi=g(t),j(hs.$$.fragment,t),yi=g(t),kn=c(t,"P",{});var Be=r(kn);Hc=a(Be,"V\xEC DistilBERT l\xE0 m\u1ED9t m\xF4 h\xECnh gi\u1ED1ng nh\u01B0 BERT, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xE1c v\u0103n b\u1EA3n \u0111\u01B0\u1EE3c tokenize bao g\u1ED3m "),rh=c(Be,"CODE",{});var sm=r(rh);Rc=a(sm,"input_ids"),sm.forEach(e),Fc=a(Be," v\xE0 "),ph=c(Be,"CODE",{});var am=r(ph);Gc=a(am,"attention_mask"),am.forEach(e),Ic=a(Be," ta \u0111\xE3 th\u1EA5y trong c\xE1c ch\u01B0\u01A1ng kh\xE1c, c\u0169ng nh\u01B0 "),mh=c(Be,"CODE",{});var hm=r(mh);Vc=a(hm,"word_ids"),hm.forEach(e),Uc=a(Be," m\xE0 ta \u0111\xE3 th\xEAm v\xE0o."),Be.forEach(e),$i=g(t),$e=c(t,"P",{});var Il=r($e);Wc=a(Il,"G\u1EDD ch\xFAng ta \u0111\xE3 tokenize c\xE1c b\xE0i \u0111\xE1nh gi\xE1 phim c\u1EE7a m\xECnh, b\u01B0\u1EDBc ti\u1EBFp theo l\xE0 nh\xF3m t\u1EA5t c\u1EA3 ch\xFAng l\u1EA1i v\u1EDBi nhau v\xE0 chia k\u1EBFt qu\u1EA3 th\xE0nh nhi\u1EC1u ph\u1EA7n. Nh\u01B0ng nh\u1EEFng kh\u1ED1i n\xE0y ph\u1EA3i l\u1EDBn \u0111\u1EBFn m\u1EE9c n\xE0o? \u0110i\u1EC1u n\xE0y cu\u1ED1i c\xF9ng s\u1EBD \u0111\u01B0\u1EE3c x\xE1c \u0111\u1ECBnh b\u1EDFi dung l\u01B0\u1EE3ng b\u1ED9 nh\u1EDB GPU m\xE0 b\u1EA1n c\xF3 s\u1EB5n, nh\u01B0ng \u0111i\u1EC3m kh\u1EDFi \u0111\u1EA7u t\u1ED1t l\xE0 xem k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh t\u1ED1i \u0111a c\u1EE7a m\xF4 h\xECnh l\xE0 bao nhi\xEAu. \u0110i\u1EC1u n\xE0y c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c suy ra b\u1EB1ng c\xE1ch ki\u1EC3m tra thu\u1ED9c t\xEDnh "),uh=c(Il,"CODE",{});var im=r(uh);Yc=a(im,"model_max_length"),im.forEach(e),Jc=a(Il," c\u1EE7a tokenizer:"),Il.forEach(e),wi=g(t),j(is.$$.fragment,t),ji=g(t),j(ls.$$.fragment,t),xi=g(t),we=c(t,"P",{});var Vl=r(we);Qc=a(Vl,"Gi\xE1 tr\u1ECB n\xE0y c\xF3 ngu\u1ED3n g\u1ED1c t\u1EEB t\u1EC7p "),gh=c(Vl,"EM",{});var lm=r(gh);Xc=a(lm,"tokenizer_config.json"),lm.forEach(e),Zc=a(Vl," \u0111\u01B0\u1EE3c li\xEAn k\u1EBFt v\u1EDBi m\u1ED9t checkpoint; trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng k\xEDch th\u01B0\u1EDBc ng\u1EEF c\u1EA3nh l\xE0 512 token, gi\u1ED1ng nh\u01B0 v\u1EDBi BERT."),Vl.forEach(e),Ei=g(t),j(je.$$.fragment,t),Ti=g(t),ca=c(t,"P",{});var cm=r(ca);to=a(cm,"V\xEC v\u1EADy, \u0111\u1EC3 ch\u1EA1y c\xE1c th\u1EED nghi\u1EC7m tr\xEAn GPU nh\u01B0 nh\u1EEFng GPU \u0111\u01B0\u1EE3c t\xECm th\u1EA5y tr\xEAn Google Colab, ch\xFAng ta s\u1EBD ch\u1ECDn th\u1EE9 g\xEC \u0111\xF3 nh\u1ECF h\u01A1n m\u1ED9t ch\xFAt c\xF3 th\u1EC3 v\u1EEBa v\u1EDBi b\u1ED9 nh\u1EDB:"),cm.forEach(e),qi=g(t),j(cs.$$.fragment,t),Ci=g(t),j(xe.$$.fragment,t),Di=g(t),oa=c(t,"P",{});var om=r(oa);no=a(om,"B\xE2y gi\u1EDD \u0111\u1EBFn ph\u1EA7n th\xFA v\u1ECB. \u0110\u1EC3 cho bi\u1EBFt c\xE1ch n\u1ED1i ho\u1EA1t \u0111\u1ED9ng, h\xE3y l\u1EA5y m\u1ED9t v\xE0i b\xE0i \u0111\xE1nh gi\xE1 t\u1EEB b\u1ED9 hu\u1EA5n luy\u1EC7n \u0111\u01B0\u1EE3c tokenize v\xE0 in ra s\u1ED1 l\u01B0\u1EE3ng token cho m\u1ED7i b\xE0i \u0111\xE1nh gi\xE1:"),om.forEach(e),Mi=g(t),j(os.$$.fragment,t),zi=g(t),j(rs.$$.fragment,t),Ai=g(t),ra=c(t,"P",{});var rm=r(ra);eo=a(rm,"We can then concatenate all these examples with a simple dictionary comprehension, as follows:"),rm.forEach(e),Pi=g(t),j(ps.$$.fragment,t),Si=g(t),j(ms.$$.fragment,t),Ki=g(t),Yn=c(t,"P",{});var Fa=r(Yn);so=a(Fa,"Tuy\u1EC7t v\u1EDDi, t\u1ED5ng \u0111\u1ED9 d\xE0i \u0111\xE3 \u0111\u01B0\u1EE3c ki\u1EC3m tra - v\xEC v\u1EADy b\xE2y gi\u1EDD h\xE3y chia c\xE1c b\xE0i \u0111\xE1nh gi\xE1 \u0111\u01B0\u1EE3c n\u1ED1i th\xE0nh c\xE1c ph\u1EA7n c\xF3 k\xEDch th\u01B0\u1EDBc \u0111\u01B0\u1EE3c cung c\u1EA5p b\u1EDFi "),_h=c(Fa,"CODE",{});var pm=r(_h);ao=a(pm,"block_size"),pm.forEach(e),ho=a(Fa,". \u0110\u1EC3 l\xE0m nh\u01B0 v\u1EADy, ch\xFAng ta l\u1EB7p qua c\xE1c \u0111\u1EB7c tr\u01B0ng trong "),dh=c(Fa,"CODE",{});var mm=r(dh);io=a(mm,"concatenated_examples"),mm.forEach(e),lo=a(Fa," v\xE0 s\u1EED d\u1EE5ng kh\u1EA3 n\u0103ng hi\u1EC3u danh s\xE1ch \u0111\u1EC3 t\u1EA1o c\xE1c ph\u1EA7n c\u1EE7a t\u1EEBng \u0111\u1EB7c tr\u01B0ng. K\u1EBFt qu\u1EA3 l\xE0 m\u1ED9t t\u1EEB \u0111i\u1EC3n c\xE1c kh\u1ED1i cho t\u1EEBng \u0111\u1EB7c tr\u01B0ng:"),Fa.forEach(e),Oi=g(t),j(us.$$.fragment,t),Bi=g(t),j(gs.$$.fragment,t),Li=g(t),pa=c(t,"P",{});var um=r(pa);co=a(um,"Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y trong v\xED d\u1EE5 n\xE0y, \u0111o\u1EA1n cu\u1ED1i th\u01B0\u1EDDng s\u1EBD nh\u1ECF h\u01A1n k\xEDch th\u01B0\u1EDBc \u0111o\u1EA1n t\u1ED1i \u0111a. C\xF3 hai chi\u1EBFn l\u01B0\u1EE3c ch\xEDnh \u0111\u1EC3 gi\u1EA3i quy\u1EBFt v\u1EA5n \u0111\u1EC1 n\xE0y:"),um.forEach(e),Ni=g(t),Ee=c(t,"UL",{});var Ul=r(Ee);_s=c(Ul,"LI",{});var Wl=r(_s);oo=a(Wl,"B\u1ECF \u0111o\u1EA1n cu\u1ED1i c\xF9ng n\u1EBFu n\xF3 nh\u1ECF h\u01A1n "),fh=c(Wl,"CODE",{});var gm=r(fh);ro=a(gm,"chunk_size"),gm.forEach(e),po=a(Wl,"."),Wl.forEach(e),mo=g(Ul),ds=c(Ul,"LI",{});var Yl=r(ds);uo=a(Yl,"\u0110\u1EC7m \u0111o\u1EA1n cu\u1ED1i c\xF9ng cho \u0111\u1EBFn khi \u0111\u1ED9 d\xE0i c\u1EE7a n\xF3 b\u1EB1ng "),bh=c(Yl,"CODE",{});var _m=r(bh);go=a(_m,"chunk_size"),_m.forEach(e),_o=a(Yl,"."),Yl.forEach(e),Ul.forEach(e),Hi=g(t),ma=c(t,"P",{});var dm=r(ma);fo=a(dm,"Ch\xFAng t\xF4i s\u1EBD th\u1EF1c hi\u1EC7n c\xE1ch ti\u1EBFp c\u1EADn \u0111\u1EA7u ti\xEAn \u1EDF \u0111\xE2y, v\xEC v\u1EADy h\xE3y g\xF3i t\u1EA5t c\u1EA3 logic \u1EDF tr\xEAn trong m\u1ED9t h\xE0m duy nh\u1EA5t m\xE0 ch\xFAng t\xF4i c\xF3 th\u1EC3 \xE1p d\u1EE5ng cho t\u1EADp d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh:"),dm.forEach(e),Ri=g(t),j(fs.$$.fragment,t),Fi=g(t),cn=c(t,"P",{});var ee=r(cn);bo=a(ee,"L\u01B0u \xFD r\u1EB1ng trong b\u01B0\u1EDBc cu\u1ED1i c\xF9ng c\u1EE7a "),kh=c(ee,"CODE",{});var fm=r(kh);ko=a(fm,"group_texts()"),fm.forEach(e),vo=a(ee,", ch\xFAng ta t\u1EA1o m\u1ED9t c\u1ED9t m\u1EDBi "),vh=c(ee,"CODE",{});var bm=r(vh);yo=a(bm,"labels"),bm.forEach(e),$o=a(ee," l\xE0 b\u1EA3n sao c\u1EE7a c\u1ED9t "),yh=c(ee,"CODE",{});var km=r(yh);wo=a(km,"input_ids"),km.forEach(e),jo=a(ee,". Nh\u01B0 ch\xFAng ta s\u1EBD th\u1EA5y ngay sau \u0111\xE2y, \u0111\xF3 l\xE0 b\u1EDFi v\xEC trong m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i, m\u1EE5c ti\xEAu l\xE0 d\u1EF1 \u0111o\xE1n c\xE1c token \u0111\u01B0\u1EE3c che ng\u1EABu nhi\xEAn trong l\xF4 \u0111\u1EA7u v\xE0o v\xE0 b\u1EB1ng c\xE1ch t\u1EA1o c\u1ED9t "),$h=c(ee,"CODE",{});var vm=r($h);xo=a(vm,"labels"),vm.forEach(e),Eo=a(ee,", ch\xFAng ta cung c\u1EA5p s\u1EF1 th\u1EADt c\u01A1 b\u1EA3n cho m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u1EC3 h\u1ECDc h\u1ECFi."),ee.forEach(e),Gi=g(t),Jn=c(t,"P",{});var Ga=r(Jn);To=a(Ga,"B\xE2y gi\u1EDD, h\xE3y \xE1p d\u1EE5ng "),wh=c(Ga,"CODE",{});var ym=r(wh);qo=a(ym,"group_texts()"),ym.forEach(e),Co=a(Ga," cho c\xE1c t\u1EADp d\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng h\xE0m "),jh=c(Ga,"CODE",{});var $m=r(jh);Do=a($m,"Dataset.map()"),$m.forEach(e),Mo=a(Ga," \u0111\xE1ng tin c\u1EADy:"),Ga.forEach(e),Ii=g(t),j(bs.$$.fragment,t),Vi=g(t),j(ks.$$.fragment,t),Ui=g(t),Xt=c(t,"P",{});var En=r(Xt);zo=a(En,"B\u1EA1n c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng vi\u1EC7c nh\xF3m v\xE0 sau \u0111\xF3 ph\xE2n chia c\xE1c \u0111o\u1EA1n v\u0103n b\u1EA3n \u0111\xE3 t\u1EA1o ra nhi\u1EC1u m\u1EABu h\u01A1n so v\u1EDBi 25,000 m\u1EABu ban \u0111\u1EA7u c\u1EE7a ch\xFAng ta cho ph\u1EA7n t\xE1ch "),xh=c(En,"CODE",{});var wm=r(xh);Ao=a(wm,"hu\u1EA5n luy\u1EC7n"),wm.forEach(e),Po=a(En," v\xE0 "),Eh=c(En,"CODE",{});var jm=r(Eh);So=a(jm,"ki\u1EC3m th\u1EED"),jm.forEach(e),Ko=a(En,". \u0110\xF3 l\xE0 b\u1EDFi v\xEC ch\xFAng ta hi\u1EC7n c\xF3 c\xE1c m\u1EABu li\xEAn quan \u0111\u1EBFn "),Th=c(En,"EM",{});var xm=r(Th);Oo=a(xm,"token li\xEAn t\u1EE5c"),xm.forEach(e),Bo=a(En," tr\u1EA3i d\xE0i tr\xEAn nhi\u1EC1u m\u1EABu t\u1EEB kho t\xE0i li\u1EC7u g\u1ED1c. B\u1EA1n c\xF3 th\u1EC3 th\u1EA5y \u0111i\u1EC1u n\xE0y m\u1ED9t c\xE1ch r\xF5 r\xE0ng b\u1EB1ng c\xE1ch t\xECm ki\u1EBFm c\xE1c token \u0111\u1EB7c bi\u1EC7t "),qh=c(En,"CODE",{});var Em=r(qh);Lo=a(Em,"[SEP]"),Em.forEach(e),No=a(En," v\xE0 "),Ch=c(En,"CODE",{});var Tm=r(Ch);Ho=a(Tm,"[CLS]"),Tm.forEach(e),Ro=a(En," trong m\u1ED9t trong c\xE1c ph\u1EA7n:"),En.forEach(e),Wi=g(t),j(vs.$$.fragment,t),Yi=g(t),j(ys.$$.fragment,t),Ji=g(t),ua=c(t,"P",{});var qm=r(ua);Fo=a(qm,"Trong v\xED d\u1EE5 n\xE0y, b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y hai b\xE0i \u0111\xE1nh gi\xE1 phim tr\xF9ng nhau, m\u1ED9t b\xE0i v\u1EC1 phim c\u1EA5p ba v\xE0 b\xE0i c\xF2n l\u1EA1i v\u1EC1 t\xECnh tr\u1EA1ng v\xF4 gia c\u01B0. H\xE3y c\u0169ng xem c\xE1c nh\xE3n tr\xF4ng nh\u01B0 th\u1EBF n\xE0o cho m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i:"),qm.forEach(e),Qi=g(t),j($s.$$.fragment,t),Xi=g(t),j(ws.$$.fragment,t),Zi=g(t),vn=c(t,"P",{});var Le=r(vn);Go=a(Le,"Nh\u01B0 mong \u0111\u1EE3i t\u1EEB h\xE0m "),Dh=c(Le,"CODE",{});var Cm=r(Dh);Io=a(Cm,"group_texts()"),Cm.forEach(e),Vo=a(Le," c\u1EE7a ch\xFAng ta \u1EDF tr\xEAn, h\xE0m n\xE0y tr\xF4ng gi\u1ED1ng h\u1EC7t v\u1EDBi "),Mh=c(Le,"CODE",{});var Dm=r(Mh);Uo=a(Dm,"input_ids"),Dm.forEach(e),Wo=a(Le," \u0111\xE3 \u0111\u01B0\u1EE3c gi\u1EA3i m\xE3 - nh\u01B0ng sau \u0111\xF3 l\xE0m th\u1EBF n\xE0o \u0111\u1EC3 m\xF4 h\xECnh c\u1EE7a ch\xFAng ta c\xF3 th\u1EC3 h\u1ECDc \u0111\u01B0\u1EE3c b\u1EA5t c\u1EE9 \u0111i\u1EC1u g\xEC? Ch\xFAng ta \u0111ang thi\u1EBFu m\u1ED9t b\u01B0\u1EDBc quan tr\u1ECDng: ch\xE8n token "),zh=c(Le,"CODE",{});var Mm=r(zh);Yo=a(Mm,"[MASK]"),Mm.forEach(e),Jo=a(Le," \u1EDF c\xE1c v\u1ECB tr\xED ng\u1EABu nhi\xEAn trong \u0111\u1EA7u v\xE0o! H\xE3y xem c\xE1ch ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n \u0111i\u1EC1u n\xE0y m\u1ED9t c\xE1ch nhanh ch\xF3ng trong qu\xE1 tr\xECnh tinh ch\u1EC9nh b\u1EB1ng c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t."),Le.forEach(e),tl=g(t),pe=c(t,"H2",{class:!0});var Jl=r(pe);Te=c(Jl,"A",{id:!0,class:!0,href:!0});var zm=r(Te);Ah=c(zm,"SPAN",{});var Am=r(Ah);j(js.$$.fragment,Am),Am.forEach(e),zm.forEach(e),Qo=g(Jl),ga=c(Jl,"SPAN",{});var sp=r(ga);Xo=a(sp,"Tinh ch\u1EC9nh DistilBERT v\u1EDBi API "),Ph=c(sp,"CODE",{});var Pm=r(Ph);Zo=a(Pm,"Trainer"),Pm.forEach(e),sp.forEach(e),Jl.forEach(e),nl=g(t),yn=c(t,"P",{});var Ne=r(yn);tr=a(Ne,"Tinh ch\u1EC9nh m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i g\u1EA7n gi\u1ED1ng nh\u01B0 tinh ch\u1EC9nh m\xF4 h\xECnh ph\xE2n lo\u1EA1i chu\u1ED7i, gi\u1ED1ng nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m trong "),_a=c(Ne,"A",{href:!0});var Sm=r(_a);nr=a(Sm,"Ch\u01B0\u01A1ng 3"),Sm.forEach(e),er=a(Ne,". S\u1EF1 kh\xE1c bi\u1EC7t duy nh\u1EA5t l\xE0 ch\xFAng ta c\u1EA7n m\u1ED9t tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t c\xF3 th\u1EC3 che gi\u1EA5u ng\u1EABu nhi\xEAn m\u1ED9t s\u1ED1 token trong m\u1ED7i l\xF4 v\u0103n b\u1EA3n. May m\u1EAFn thay, \u{1F917} Transformers \u0111\u01B0\u1EE3c chu\u1EA9n b\u1ECB v\u1EDBi m\u1ED9t "),Sh=c(Ne,"CODE",{});var Km=r(Sh);sr=a(Km,"DataCollatorForLanguageModeling"),Km.forEach(e),ar=a(Ne," d\xE0nh ri\xEAng cho t\xE1c v\u1EE5 n\xE0y. Ch\xFAng ta ch\u1EC9 c\u1EA7n chuy\u1EC3n n\xF3 v\xE0o tokenizer v\xE0 tham s\u1ED1 "),Kh=c(Ne,"CODE",{});var Om=r(Kh);hr=a(Om,"mlm_probability"),Om.forEach(e),ir=a(Ne," \u0111\u1EC3 ch\u1EC9 \u0111\u1ECBnh ph\u1EA7n n\xE0o trong s\u1ED1 c\xE1c token c\u1EA7n che. Ch\xFAng t\xF4i s\u1EBD ch\u1ECDn 15%, l\xE0 s\u1ED1 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho BERT v\xE0 m\u1ED9t l\u1EF1a ch\u1ECDn ph\u1ED5 bi\u1EBFn trong c\xE1c t\xE0i li\u1EC7u:"),Ne.forEach(e),el=g(t),j(xs.$$.fragment,t),sl=g(t),$n=c(t,"P",{});var He=r($n);lr=a(He,"\u0110\u1EC3 xem c\xE1ch ho\u1EA1t \u0111\u1ED9ng c\u1EE7a vi\u1EC7c che ng\u1EABu nhi\xEAn, h\xE3y cung c\u1EA5p m\u1ED9t v\xE0i m\u1EABu cho tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u. V\xEC n\xF3 mong \u0111\u1EE3i m\u1ED9t danh s\xE1ch c\xE1c "),Oh=c(He,"CODE",{});var Bm=r(Oh);cr=a(Bm,"dict"),Bm.forEach(e),or=a(He,", trong \u0111\xF3 m\u1ED7i "),Bh=c(He,"CODE",{});var Lm=r(Bh);rr=a(Lm,"dict"),Lm.forEach(e),pr=a(He," \u0111\u1EA1i di\u1EC7n cho m\u1ED9t \u0111o\u1EA1n v\u0103n b\u1EA3n li\u1EC1n k\u1EC1, \u0111\u1EA7u ti\xEAn ch\xFAng ta l\u1EB7p t\u1EADp d\u1EEF li\u1EC7u tr\u01B0\u1EDBc khi cung c\u1EA5p l\xF4 cho b\u1ED9 \u0111\u1ED1i chi\u1EBFu. Ch\xFAng ta x\xF3a kh\xF3a "),Lh=c(He,"CODE",{});var Nm=r(Lh);mr=a(Nm,'"word_ids"'),Nm.forEach(e),ur=a(He," cho tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u n\xE0y v\xEC n\xF3 kh\xF4ng c\u1EA7n ch\xFAng:"),He.forEach(e),al=g(t),j(Es.$$.fragment,t),hl=g(t),j(Ts.$$.fragment,t),il=g(t),Qn=c(t,"P",{});var Ia=r(Qn);gr=a(Ia,"T\u1ED1t, n\xF3 \u0111\xE3 ho\u1EA1t \u0111\u1ED9ng! Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng "),Nh=c(Ia,"CODE",{});var Hm=r(Nh);_r=a(Hm,"[MASK]"),Hm.forEach(e),dr=a(Ia," \u0111\xE3 \u0111\u01B0\u1EE3c ch\xE8n ng\u1EABu nhi\xEAn t\u1EA1i c\xE1c v\u1ECB tr\xED kh\xE1c nhau trong v\u0103n b\u1EA3n. \u0110\xE2y s\u1EBD l\xE0 nh\u1EEFng token m\xE0 m\xF4 h\xECnh s\u1EBD ph\u1EA3i d\u1EF1 \u0111o\xE1n trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n - v\xE0 c\xE1i hay c\u1EE7a c\xF4ng c\u1EE5 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u l\xE0 n\xF3 s\u1EBD ng\u1EABu nhi\xEAn ch\xE8n "),Hh=c(Ia,"CODE",{});var Rm=r(Hh);fr=a(Rm,"[MASK]"),Rm.forEach(e),br=a(Ia," v\u1EDBi m\u1ECDi l\xF4!"),Ia.forEach(e),ll=g(t),j(qe.$$.fragment,t),cl=g(t),sn&&sn.l(t),da=g(t),wn=c(t,"P",{});var Re=r(wn);kr=a(Re,"Khi hu\u1EA5n luy\u1EC7n c\xE1c m\xF4 h\xECnh \u0111\u1EC3 t\u1EA1o m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i, m\u1ED9t k\u1EF9 thu\u1EADt c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng l\xE0 gh\xE9p c\xE1c t\u1EEB l\u1EA1i v\u1EDBi nhau, kh\xF4ng ch\u1EC9 c\xE1c token ri\xEAng l\u1EBB. C\xE1ch ti\u1EBFp c\u1EADn n\xE0y \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Rh=c(Re,"EM",{});var Fm=r(Rh);vr=a(Fm,"whole word masking"),Fm.forEach(e),yr=a(Re," hay "),Fh=c(Re,"EM",{});var Gm=r(Fh);$r=a(Gm,"che to\xE0n b\u1ED9 t\u1EEB"),Gm.forEach(e),wr=a(Re,". N\u1EBFu ch\xFAng ta mu\u1ED1n che to\xE0n b\u1ED9 t\u1EEB, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i t\u1EF1 x\xE2y d\u1EF1ng m\u1ED9t b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u. B\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u ch\u1EC9 l\xE0 m\u1ED9t ch\u1EE9c n\u0103ng l\u1EA5y danh s\xE1ch c\xE1c m\u1EABu v\xE0 chuy\u1EC3n \u0111\u1ED5i ch\xFAng th\xE0nh m\u1ED9t l\xF4, v\xEC v\u1EADy h\xE3y l\xE0m \u0111i\u1EC1u n\xE0y ngay b\xE2y gi\u1EDD! Ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c ID t\u1EEB \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 t\u1EA1o b\u1EA3n \u0111\u1ED3 gi\u1EEFa c\xE1c ch\u1EC9 s\u1ED1 t\u1EEB v\xE0 c\xE1c m\xE3 th\xF4ng b\xE1o t\u01B0\u01A1ng \u1EE9ng, sau \u0111\xF3 quy\u1EBFt \u0111\u1ECBnh ng\u1EABu nhi\xEAn nh\u1EEFng t\u1EEB n\xE0o c\u1EA7n che v\xE0 che c\xE1c \u0111\u1EA7u v\xE0o. L\u01B0u \xFD r\u1EB1ng t\u1EA5t c\u1EA3 c\xE1c nh\xE3n \u0111\u1EC1u l\xE0 "),Gh=c(Re,"CODE",{});var Im=r(Gh);jr=a(Im,"-100"),Im.forEach(e),xr=a(Re," ngo\u1EA1i tr\u1EEB c\xE1c nh\xE3n t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c t\u1EEB b\u1ECB che."),Re.forEach(e),ol=g(t),zn.l(t),fa=g(t),ba=c(t,"P",{});var Vm=r(ba);Er=a(Vm,"Ti\u1EBFp theo, ta cso th\u1EC3 th\u1EED tr\xEAn m\u1ED9t v\xE0i m\u1EABu nh\u01B0 tr\xEAn:"),Vm.forEach(e),rl=g(t),j(qs.$$.fragment,t),pl=g(t),j(Cs.$$.fragment,t),ml=g(t),j(Ce.$$.fragment,t),ul=g(t),Xn=c(t,"P",{});var Va=r(Xn);Tr=a(Va,"Gi\u1EDD ch\xFAng ta c\xF3 hai tr\xECnh \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u, ph\u1EA7n c\xF2n l\u1EA1i c\u1EE7a c\xE1c b\u01B0\u1EDBc tinh ch\u1EC9nh l\xE0 ti\xEAu chu\u1EA9n. Qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n c\xF3 th\u1EC3 m\u1EA5t m\u1ED9t kho\u1EA3ng th\u1EDDi gian tr\xEAn Google Colab n\u1EBFu b\u1EA1n kh\xF4ng \u0111\u1EE7 may m\u1EAFn \u0111\u1EC3 \u0111\u1EA1t \u0111\u01B0\u1EE3c GPU P100 th\u1EA7n tho\u1EA1i \u{1F62D}, v\xEC v\u1EADy, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta s\u1EBD gi\u1EA3m k\xEDch th\u01B0\u1EDBc c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n xu\u1ED1ng c\xF2n v\xE0i ngh\xECn m\u1EABu. \u0110\u1EEBng lo l\u1EAFng, ch\xFAng ta s\u1EBD v\u1EABn nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF kh\xE1 t\u1ED1t! M\u1ED9t c\xE1ch nhanh ch\xF3ng \u0111\u1EC3 gi\u1EA3m m\u1EABu m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u trong \u{1F917} Datasets l\xE0 th\xF4ng qua h\xE0m "),Ih=c(Va,"CODE",{});var Um=r(Ih);qr=a(Um,"Dataset.train_test_split()"),Um.forEach(e),Cr=a(Va," m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),ka=c(Va,"A",{href:!0});var Wm=r(ka);Dr=a(Wm,"Chapter 5"),Wm.forEach(e),Mr=a(Va,":"),Va.forEach(e),gl=g(t),j(Ds.$$.fragment,t),_l=g(t),j(Ms.$$.fragment,t),dl=g(t),Zn=c(t,"P",{});var Ua=r(Zn);zr=a(Ua,"\u0110i\u1EC1u n\xE0y \u0111\xE3 t\u1EF1 \u0111\u1ED9ng t\u1EA1o c\xE1c ph\u1EA7n t\xE1ch "),Vh=c(Ua,"CODE",{});var Ym=r(Vh);Ar=a(Ym,"hu\u1EA5n luy\u1EC7n"),Ym.forEach(e),Pr=a(Ua," v\xE0 "),Uh=c(Ua,"CODE",{});var Jm=r(Uh);Sr=a(Jm,"ki\u1EC3m th\u1EED"),Jm.forEach(e),Kr=a(Ua," m\u1EDBi, v\u1EDBi k\xEDch th\u01B0\u1EDBc t\u1EADp hu\u1EA5n luy\u1EC7n \u0111\u01B0\u1EE3c \u0111\u1EB7t th\xE0nh 10,000 m\u1EABu v\xE0 x\xE1c th\u1EF1c \u0111\u01B0\u1EE3c \u0111\u1EB7t th\xE0nh 10% - vui l\xF2ng t\u0103ng \u0111i\u1EC1u n\xE0y n\u1EBFu b\u1EA1n c\xF3 GPU m\u1EA1nh! \u0110i\u1EC1u ti\u1EBFp theo ch\xFAng ta c\u1EA7n l\xE0m l\xE0 \u0111\u0103ng nh\u1EADp v\xE0o Hugging Face Hub. N\u1EBFu b\u1EA1n \u0111ang ch\u1EA1y m\xE3 n\xE0y trong notebook, b\u1EA1n c\xF3 th\u1EC3 l\xE0m nh\u01B0 v\u1EADy v\u1EDBi ch\u1EE9c n\u0103ng ti\u1EC7n \xEDch sau:"),Ua.forEach(e),fl=g(t),j(zs.$$.fragment,t),bl=g(t),va=c(t,"P",{});var Qm=r(va);Or=a(Qm,"s\u1EBD hi\u1EC3n th\u1ECB m\u1ED9t ti\u1EC7n \xEDch m\xE0 b\u1EA1n c\xF3 th\u1EC3 nh\u1EADp th\xF4ng tin \u0111\u0103ng nh\u1EADp c\u1EE7a m\xECnh. Ngo\xE0i ra, b\u1EA1n c\xF3 th\u1EC3 ch\u1EA1y:"),Qm.forEach(e),kl=g(t),j(As.$$.fragment,t),vl=g(t),ya=c(t,"P",{});var Xm=r(ya);Br=a(Xm,"trong thi\u1EBFt b\u1ECB \u0111\u1EA7u cu\u1ED1i y\xEAu th\xEDch c\u1EE7a b\u1EA1n v\xE0 \u0111\u0103ng nh\u1EADp \u1EDF \u0111\xF3."),Xm.forEach(e),yl=g(t),Pn.l(t),$a=g(t),me=c(t,"H3",{class:!0});var Ql=r(me);De=c(Ql,"A",{id:!0,class:!0,href:!0});var Zm=r(De);Wh=c(Zm,"SPAN",{});var tu=r(Wh);j(Ps.$$.fragment,tu),tu.forEach(e),Zm.forEach(e),Lr=g(Ql),Yh=c(Ql,"SPAN",{});var nu=r(Yh);Nr=a(nu,"Perplexity cho m\xF4 h\xECnh ng\xF4n ng\u1EEF"),nu.forEach(e),Ql.forEach(e),$l=g(t),j(Ss.$$.fragment,t),wl=g(t),wa=c(t,"P",{});var eu=r(wa);Hr=a(eu,"Kh\xF4ng gi\u1ED1ng nh\u01B0 c\xE1c t\xE1c v\u1EE5 kh\xE1c nh\u01B0 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n ho\u1EB7c h\u1ECFi \u0111\xE1p m\xE0 ch\xFAng ta \u0111\u01B0\u1EE3c cung c\u1EA5p m\u1ED9t kho ng\u1EEF li\u1EC7u \u0111\u01B0\u1EE3c g\u1EAFn nh\xE3n \u0111\u1EC3 hu\u1EA5n luy\u1EC7n, v\u1EDBi m\xF4 h\xECnh ng\xF4n ng\u1EEF, ta kh\xF4ng c\xF3 b\u1EA5t k\u1EF3 nh\xE3n r\xF5 r\xE0ng n\xE0o. V\u1EADy l\xE0m c\xE1ch n\xE0o \u0111\u1EC3 x\xE1c \u0111\u1ECBnh \u0111i\u1EC1u g\xEC t\u1EA1o n\xEAn m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t? Gi\u1ED1ng nh\u01B0 t\xEDnh n\u0103ng t\u1EF1 \u0111\u1ED9ng s\u1EEDa l\u1ED7i trong \u0111i\u1EC7n tho\u1EA1i c\u1EE7a b\u1EA1n, m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t l\xE0 m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF ch\u1EC9 \u0111\u1ECBnh x\xE1c su\u1EA5t cao cho c\xE1c c\xE2u \u0111\xFAng ng\u1EEF ph\xE1p v\xE0 x\xE1c su\u1EA5t th\u1EA5p cho c\xE1c c\xE2u v\xF4 ngh\u0129a. \u0110\u1EC3 gi\xFAp b\u1EA1n bi\u1EBFt r\xF5 h\u01A1n v\u1EC1 h\xECnh th\u1EE9c n\xE0y, b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y to\xE0n b\u1ED9 t\u1EADp h\u1EE3p \u201Ct\u1EF1 \u0111\u1ED9ng s\u1EEDa l\u1ED7i\u201D tr\u1EF1c tuy\u1EBFn, trong \u0111\xF3 m\xF4 h\xECnh trong \u0111i\u1EC7n tho\u1EA1i \u0111\xE3 t\u1EA1o ra m\u1ED9t s\u1ED1 ho\xE0n th\xE0nh kh\xE1 h\xE0i h\u01B0\u1EDBc (v\xE0 th\u01B0\u1EDDng kh\xF4ng ph\xF9 h\u1EE3p)!"),eu.forEach(e),jl=g(t),Kn.l(t),ja=g(t),j(Ks.$$.fragment,t),xl=g(t),xa=c(t,"P",{});var su=r(xa);Rr=a(su,"Perplexity th\u1EA5p h\u01A1n c\xF3 ngh\u0129a l\xE0 m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF t\u1ED1t h\u01A1n v\xE0 ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y \u1EDF \u0111\xE2y r\u1EB1ng m\xF4 h\xECnh b\u1EAFt \u0111\u1EA7u c\u1EE7a ch\xFAng ta c\xF3 m\u1ED9t gi\xE1 tr\u1ECB h\u01A1i l\u1EDBn. H\xE3y xem li\u1EC7u ch\xFAng ta c\xF3 th\u1EC3 h\u1EA1 th\u1EA5p n\xF3 b\u1EB1ng c\xE1ch tinh ch\u1EC9nh kh\xF4ng! \u0110\u1EC3 l\xE0m \u0111i\u1EC1u \u0111\xF3, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta ch\u1EA1y v\xF2ng l\u1EB7p hu\u1EA5n luy\u1EC7n:"),su.forEach(e),El=g(t),Bn.l(t),Ea=g(t),Ta=c(t,"P",{});var au=r(Ta);Fr=a(au,"v\xE0 sau \u0111\xF3 t\xEDnh k\u1EBFt qu\u1EA3 perplexity tr\xEAn t\u1EADp ki\u1EC3m th\u1EED:"),au.forEach(e),Tl=g(t),Nn.l(t),qa=g(t),j(Os.$$.fragment,t),ql=g(t),Ca=c(t,"P",{});var hu=r(Ca);Gr=a(hu,"T\u1ED1t \u2014 n\xF3 gi\u1EA3m perplexity, cho th\u1EA5y m\xF4 h\xECnh \u0111\xE3 h\u1ECDc \u0111\u01B0\u1EE3c \u0111i\u1EC1u g\xEC \u0111\xF3 v\u1EC1 m\u1EA3ng \u0111\xE1nh gi\xE1 phim!"),hu.forEach(e),Cl=g(t),zt&&zt.l(t),Da=g(t),j(Me.$$.fragment,t),Dl=g(t),At&&At.l(t),Ma=g(t),ue=c(t,"H2",{class:!0});var Xl=r(ue);ze=c(Xl,"A",{id:!0,class:!0,href:!0});var iu=r(ze);Jh=c(iu,"SPAN",{});var lu=r(Jh);j(Bs.$$.fragment,lu),lu.forEach(e),iu.forEach(e),Ir=g(Xl),Qh=c(Xl,"SPAN",{});var cu=r(Qh);Vr=a(cu,"S\u1EED d\u1EE5ng m\xF4 h\xECnh tinh ch\u1EC9nh c\u1EE7a m\xECnh"),cu.forEach(e),Xl.forEach(e),Ml=g(t),te=c(t,"P",{});var Wa=r(te);Ur=a(Wa,"\u1EA1n c\xF3 th\u1EC3 t\u01B0\u01A1ng t\xE1c v\u1EDBi m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c tinh ch\u1EC9nh c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng ti\u1EC7n \xEDch c\u1EE7a n\xF3 tr\xEAn Hub ho\u1EB7c c\u1EE5c b\u1ED9 v\u1EDBi "),Xh=c(Wa,"CODE",{});var ou=r(Xh);Wr=a(ou,"pipeline"),ou.forEach(e),Yr=a(Wa," t\u1EEB \u{1F917} Transformers. H\xE3y s\u1EED d\u1EE5ng c\xE1i sau \u0111\u1EC3 t\u1EA3i xu\u1ED1ng m\xF4 h\xECnh c\u1EE7a ch\xFAng t\xF4i b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng pipeline "),Zh=c(Wa,"CODE",{});var ru=r(Zh);Jr=a(ru,"fill-mask"),ru.forEach(e),Qr=a(Wa,":"),Wa.forEach(e),zl=g(t),j(Ls.$$.fragment,t),Al=g(t),za=c(t,"P",{});var pu=r(za);Xr=a(pu,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p v\u0103n b\u1EA3n m\u1EABu \u201CThis is a great [MASK]\u201D v\xE0 xem 5 d\u1EF1 \u0111o\xE1n \u0111\u1EA7u l\xE0 g\xEC:"),pu.forEach(e),Pl=g(t),j(Ns.$$.fragment,t),Sl=g(t),j(Hs.$$.fragment,t),Kl=g(t),Aa=c(t,"P",{});var mu=r(Aa);Zr=a(mu,"G\u1ECDn g\xE0ng - m\xF4 h\xECnh c\u1EE7a ch\xFAng ta r\xF5 r\xE0ng \u0111\xE3 \u0111i\u1EC1u ch\u1EC9nh tr\u1ECDng s\u1ED1 c\u1EE7a n\xF3 \u0111\u1EC3 d\u1EF1 \u0111o\xE1n c\xE1c t\u1EEB li\xEAn quan nhi\u1EC1u h\u01A1n \u0111\u1EBFn phim!"),mu.forEach(e),Ol=g(t),j(Rs.$$.fragment,t),Bl=g(t),Ae=c(t,"P",{});var Zl=r(Ae);tp=a(Zl,"\u0110i\u1EC1u n\xE0y k\u1EBFt th\xFAc th\u1EED nghi\u1EC7m \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta v\u1EDBi vi\u1EC7c hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF. Trong "),Pa=c(Zl,"A",{href:!0});var uu=r(Pa);np=a(uu,"ph\u1EA7n 6"),uu.forEach(e),ep=a(Zl,", b\u1EA1n s\u1EBD h\u1ECDc c\xE1ch hu\u1EA5n luy\u1EC7n m\u1ED9t m\xF4 h\xECnh t\u1EF1 \u0111\u1ED9ng h\u1ED3i quy nh\u01B0 GPT-2 t\u1EEB \u0111\u1EA7u; h\xE3y \u0111\u1EBFn \u0111\xF3 n\u1EBFu b\u1EA1n mu\u1ED1n xem c\xE1ch b\u1EA1n c\xF3 th\u1EC3 hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc m\xF4 h\xECnh Transformer c\u1EE7a ri\xEAng m\xECnh!"),Zl.forEach(e),Ll=g(t),j(Pe.$$.fragment,t),this.h()},h(){T(o,"name","hf:doc:metadata"),T(o,"content",JSON.stringify(Yu)),T($,"id","tinh-chnh-mt-m-hnh-ngn-ng-b-n-i"),T($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T($,"href","#tinh-chnh-mt-m-hnh-ngn-ng-b-n-i"),T(q,"class","relative group"),T(Y,"href","https://arxiv.org/abs/1801.06146"),T(Y,"rel","nofollow"),T(Z,"class","block dark:hidden"),tc(Z.src,vt="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg")||T(Z,"src",vt),T(Z,"alt","ULMFiT."),T(Ct,"class","hidden dark:block"),tc(Ct.src,St="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg")||T(Ct,"src",St),T(Ct,"alt","ULMFiT."),T(rt,"class","flex justify-center"),T(st,"href","https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D."),T(st,"rel","nofollow"),tc(at.src,It="https://hf.space/gradioiframe/course-demos/distilbert-base-uncased-finetuned-imdb/+")||T(at,"src",It),T(at,"frameborder","0"),T(at,"height","300"),T(at,"title","Gradio app"),T(at,"class","block dark:hidden container p-0 flex-grow space-iframe"),T(at,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),T(at,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),T(pt,"id","chn-mt-m-hnh-hun-luyn-trc-cho-m-hnh-ngn-ng-b-n-i"),T(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(pt,"href","#chn-mt-m-hnh-hun-luyn-trc-cho-m-hnh-ngn-ng-b-n-i"),T(nt,"class","relative group"),T(Kt,"href","https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads"),T(Kt,"rel","nofollow"),tc(et.src,he="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png")||T(et,"src",he),T(et,"alt","Hub models."),T(et,"width","80%"),T(Ot,"class","flex justify-center"),T(Rt,"href","https://huggingface.co/distilbert-base-uncased"),T(Rt,"rel","nofollow"),T(Bt,"href","https://en.wikipedia.org/wiki/Knowledge_distillation"),T(Bt,"rel","nofollow"),T(Mt,"href","https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/ch05.html"),T(Mt,"rel","nofollow"),T(m,"href","https://huggingface.co/datasets/wikipedia"),T(m,"rel","nofollow"),T(Gn,"href","https://huggingface.co/datasets/bookcorpus"),T(Gn,"rel","nofollow"),T(mn,"id","b-d-liu"),T(mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(mn,"href","#b-d-liu"),T(bn,"class","relative group"),T(Qe,"href","https://huggingface.co/datasets/imdb"),T(Qe,"rel","nofollow"),T(ha,"href","/course/chapter3"),T(ye,"id","tin-x-l-d-liu"),T(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(ye,"href","#tin-x-l-d-liu"),T(re,"class","relative group"),T(la,"href","/course/chap6/3"),T(Te,"id","tinh-chnh-distilbert-vi-api-trainer"),T(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(Te,"href","#tinh-chnh-distilbert-vi-api-trainer"),T(pe,"class","relative group"),T(_a,"href","/course/chapter3"),T(ka,"href","/course/chapter5"),T(De,"id","perplexity-cho-m-hnh-ngn-ng"),T(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(De,"href","#perplexity-cho-m-hnh-ngn-ng"),T(me,"class","relative group"),T(ze,"id","s-dng-m-hnh-tinh-chnh-ca-mnh"),T(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(ze,"href","#s-dng-m-hnh-tinh-chnh-ca-mnh"),T(ue,"class","relative group"),T(Pa,"href","/course/chapter7/section6")},m(t,h){n(document.head,o),i(t,d,h),x(p,t,h),i(t,v,h),i(t,q,h),n(q,$),n($,k),x(D,k,null),n(q,_),n(q,C),n(C,P),i(t,K,h),Fs[A].m(t,h),i(t,H,h),i(t,R,h),n(R,F),i(t,W,h),i(t,y,h),n(y,L),i(t,I,h),i(t,G,h),n(G,tt),n(G,V),n(V,qt),n(G,lt),n(G,gt),n(gt,U),n(G,_t),n(G,Y),n(Y,Pt),n(G,ct),i(t,ot,h),i(t,rt,h),n(rt,Z),n(rt,Lt),n(rt,Ct),i(t,Nt,h),i(t,ut,h),n(ut,Dt),n(ut,st),n(st,Ft),n(ut,it),i(t,Gt,h),i(t,at,h),i(t,ht,h),i(t,yt,h),n(yt,xt),i(t,Vt,h),x($t,t,h),i(t,S,h),x(J,t,h),i(t,Et,h),i(t,nt,h),n(nt,pt),n(pt,Ht),x(mt,Ht,null),n(nt,Zt),n(nt,X),n(X,tn),i(t,Tt,h),i(t,wt,h),n(wt,Ut),n(wt,Kt),n(Kt,nn),n(wt,z),i(t,Q,h),i(t,Ot,h),n(Ot,et),i(t,un,h),i(t,dt,h),n(dt,Tn),n(dt,Rt),n(Rt,Rn),n(dt,jt),n(dt,Bt),n(Bt,on),n(on,an),n(dt,qn),n(dt,Mt),n(Mt,gn),n(gn,Wt),n(dt,Fn),i(t,Cn,h),Gs[ft].m(t,h),i(t,hn,h),i(t,rn,h),n(rn,Ie),i(t,ie,h),x(en,t,h),i(t,le,h),i(t,kt,h),n(kt,ge),n(kt,_n),n(_n,Ve),n(kt,_e),n(kt,m),n(m,O),n(kt,de),n(kt,Gn),n(Gn,Zs),n(kt,ta),n(kt,dn),n(dn,na),n(kt,ea),i(t,pn,h),x(In,t,h),i(t,Ue,h),i(t,ce,h),n(ce,fe),i(t,We,h),Is[Yt].m(t,h),i(t,oe,h),x(fn,t,h),i(t,Ye,h),i(t,Dn,h),n(Dn,sa),i(t,Je,h),i(t,bn,h),n(bn,mn),n(mn,be),x(Vn,be,null),n(bn,aa),n(bn,Ya),n(Ya,ec),i(t,ii,h),i(t,Un,h),n(Un,sc),n(Un,Qe),n(Qe,ac),n(Un,hc),n(Un,Ja),n(Ja,ic),n(Un,lc),i(t,li,h),x(Xe,t,h),i(t,ci,h),x(Ze,t,h),i(t,oi,h),i(t,Jt,h),n(Jt,cc),n(Jt,Qa),n(Qa,oc),n(Jt,rc),n(Jt,Xa),n(Xa,pc),n(Jt,mc),n(Jt,Za),n(Za,uc),n(Jt,gc),n(Jt,th),n(th,_c),n(Jt,dc),n(Jt,nh),n(nh,fc),n(Jt,bc),i(t,ri,h),x(ts,t,h),i(t,pi,h),x(ns,t,h),i(t,mi,h),i(t,Wn,h),n(Wn,kc),n(Wn,eh),n(eh,vc),n(Wn,yc),n(Wn,sh),n(sh,$c),n(Wn,wc),i(t,ui,h),x(ke,t,h),i(t,gi,h),i(t,ve,h),n(ve,jc),n(ve,ha),n(ha,xc),n(ve,Ec),i(t,_i,h),i(t,re,h),n(re,ye),n(ye,ah),x(es,ah,null),n(re,Tc),n(re,hh),n(hh,qc),i(t,di,h),x(ss,t,h),i(t,fi,h),i(t,ia,h),n(ia,Cc),i(t,bi,h),i(t,Qt,h),n(Qt,Dc),n(Qt,ih),n(ih,Mc),n(Qt,zc),n(Qt,lh),n(lh,Ac),n(Qt,Pc),n(Qt,la),n(la,Sc),n(Qt,Kc),n(Qt,ch),n(ch,Oc),n(Qt,Bc),n(Qt,oh),n(oh,Lc),n(Qt,Nc),i(t,ki,h),x(as,t,h),i(t,vi,h),x(hs,t,h),i(t,yi,h),i(t,kn,h),n(kn,Hc),n(kn,rh),n(rh,Rc),n(kn,Fc),n(kn,ph),n(ph,Gc),n(kn,Ic),n(kn,mh),n(mh,Vc),n(kn,Uc),i(t,$i,h),i(t,$e,h),n($e,Wc),n($e,uh),n(uh,Yc),n($e,Jc),i(t,wi,h),x(is,t,h),i(t,ji,h),x(ls,t,h),i(t,xi,h),i(t,we,h),n(we,Qc),n(we,gh),n(gh,Xc),n(we,Zc),i(t,Ei,h),x(je,t,h),i(t,Ti,h),i(t,ca,h),n(ca,to),i(t,qi,h),x(cs,t,h),i(t,Ci,h),x(xe,t,h),i(t,Di,h),i(t,oa,h),n(oa,no),i(t,Mi,h),x(os,t,h),i(t,zi,h),x(rs,t,h),i(t,Ai,h),i(t,ra,h),n(ra,eo),i(t,Pi,h),x(ps,t,h),i(t,Si,h),x(ms,t,h),i(t,Ki,h),i(t,Yn,h),n(Yn,so),n(Yn,_h),n(_h,ao),n(Yn,ho),n(Yn,dh),n(dh,io),n(Yn,lo),i(t,Oi,h),x(us,t,h),i(t,Bi,h),x(gs,t,h),i(t,Li,h),i(t,pa,h),n(pa,co),i(t,Ni,h),i(t,Ee,h),n(Ee,_s),n(_s,oo),n(_s,fh),n(fh,ro),n(_s,po),n(Ee,mo),n(Ee,ds),n(ds,uo),n(ds,bh),n(bh,go),n(ds,_o),i(t,Hi,h),i(t,ma,h),n(ma,fo),i(t,Ri,h),x(fs,t,h),i(t,Fi,h),i(t,cn,h),n(cn,bo),n(cn,kh),n(kh,ko),n(cn,vo),n(cn,vh),n(vh,yo),n(cn,$o),n(cn,yh),n(yh,wo),n(cn,jo),n(cn,$h),n($h,xo),n(cn,Eo),i(t,Gi,h),i(t,Jn,h),n(Jn,To),n(Jn,wh),n(wh,qo),n(Jn,Co),n(Jn,jh),n(jh,Do),n(Jn,Mo),i(t,Ii,h),x(bs,t,h),i(t,Vi,h),x(ks,t,h),i(t,Ui,h),i(t,Xt,h),n(Xt,zo),n(Xt,xh),n(xh,Ao),n(Xt,Po),n(Xt,Eh),n(Eh,So),n(Xt,Ko),n(Xt,Th),n(Th,Oo),n(Xt,Bo),n(Xt,qh),n(qh,Lo),n(Xt,No),n(Xt,Ch),n(Ch,Ho),n(Xt,Ro),i(t,Wi,h),x(vs,t,h),i(t,Yi,h),x(ys,t,h),i(t,Ji,h),i(t,ua,h),n(ua,Fo),i(t,Qi,h),x($s,t,h),i(t,Xi,h),x(ws,t,h),i(t,Zi,h),i(t,vn,h),n(vn,Go),n(vn,Dh),n(Dh,Io),n(vn,Vo),n(vn,Mh),n(Mh,Uo),n(vn,Wo),n(vn,zh),n(zh,Yo),n(vn,Jo),i(t,tl,h),i(t,pe,h),n(pe,Te),n(Te,Ah),x(js,Ah,null),n(pe,Qo),n(pe,ga),n(ga,Xo),n(ga,Ph),n(Ph,Zo),i(t,nl,h),i(t,yn,h),n(yn,tr),n(yn,_a),n(_a,nr),n(yn,er),n(yn,Sh),n(Sh,sr),n(yn,ar),n(yn,Kh),n(Kh,hr),n(yn,ir),i(t,el,h),x(xs,t,h),i(t,sl,h),i(t,$n,h),n($n,lr),n($n,Oh),n(Oh,cr),n($n,or),n($n,Bh),n(Bh,rr),n($n,pr),n($n,Lh),n(Lh,mr),n($n,ur),i(t,al,h),x(Es,t,h),i(t,hl,h),x(Ts,t,h),i(t,il,h),i(t,Qn,h),n(Qn,gr),n(Qn,Nh),n(Nh,_r),n(Qn,dr),n(Qn,Hh),n(Hh,fr),n(Qn,br),i(t,ll,h),x(qe,t,h),i(t,cl,h),sn&&sn.m(t,h),i(t,da,h),i(t,wn,h),n(wn,kr),n(wn,Rh),n(Rh,vr),n(wn,yr),n(wn,Fh),n(Fh,$r),n(wn,wr),n(wn,Gh),n(Gh,jr),n(wn,xr),i(t,ol,h),Vs[Mn].m(t,h),i(t,fa,h),i(t,ba,h),n(ba,Er),i(t,rl,h),x(qs,t,h),i(t,pl,h),x(Cs,t,h),i(t,ml,h),x(Ce,t,h),i(t,ul,h),i(t,Xn,h),n(Xn,Tr),n(Xn,Ih),n(Ih,qr),n(Xn,Cr),n(Xn,ka),n(ka,Dr),n(Xn,Mr),i(t,gl,h),x(Ds,t,h),i(t,_l,h),x(Ms,t,h),i(t,dl,h),i(t,Zn,h),n(Zn,zr),n(Zn,Vh),n(Vh,Ar),n(Zn,Pr),n(Zn,Uh),n(Uh,Sr),n(Zn,Kr),i(t,fl,h),x(zs,t,h),i(t,bl,h),i(t,va,h),n(va,Or),i(t,kl,h),x(As,t,h),i(t,vl,h),i(t,ya,h),n(ya,Br),i(t,yl,h),Us[An].m(t,h),i(t,$a,h),i(t,me,h),n(me,De),n(De,Wh),x(Ps,Wh,null),n(me,Lr),n(me,Yh),n(Yh,Nr),i(t,$l,h),x(Ss,t,h),i(t,wl,h),i(t,wa,h),n(wa,Hr),i(t,jl,h),Ws[Sn].m(t,h),i(t,ja,h),x(Ks,t,h),i(t,xl,h),i(t,xa,h),n(xa,Rr),i(t,El,h),Ys[On].m(t,h),i(t,Ea,h),i(t,Ta,h),n(Ta,Fr),i(t,Tl,h),Js[Ln].m(t,h),i(t,qa,h),x(Os,t,h),i(t,ql,h),i(t,Ca,h),n(Ca,Gr),i(t,Cl,h),zt&&zt.m(t,h),i(t,Da,h),x(Me,t,h),i(t,Dl,h),At&&At.m(t,h),i(t,Ma,h),i(t,ue,h),n(ue,ze),n(ze,Jh),x(Bs,Jh,null),n(ue,Ir),n(ue,Qh),n(Qh,Vr),i(t,Ml,h),i(t,te,h),n(te,Ur),n(te,Xh),n(Xh,Wr),n(te,Yr),n(te,Zh),n(Zh,Jr),n(te,Qr),i(t,zl,h),x(Ls,t,h),i(t,Al,h),i(t,za,h),n(za,Xr),i(t,Pl,h),x(Ns,t,h),i(t,Sl,h),x(Hs,t,h),i(t,Kl,h),i(t,Aa,h),n(Aa,Zr),i(t,Ol,h),x(Rs,t,h),i(t,Bl,h),i(t,Ae,h),n(Ae,tp),n(Ae,Pa),n(Pa,np),n(Ae,ep),i(t,Ll,h),x(Pe,t,h),Nl=!0},p(t,[h]){const Qs={};h&1&&(Qs.fw=t[0]),p.$set(Qs);let Sa=A;A=hp(t),A!==Sa&&(ae(),b(Fs[Sa],1,1,()=>{Fs[Sa]=null}),se(),N=Fs[A],N||(N=Fs[A]=ap[A](t),N.c()),f(N,1),N.m(H.parentNode,H));const ti={};h&2&&(ti.$$scope={dirty:h,ctx:t}),J.$set(ti);let Ka=ft;ft=lp(t),ft!==Ka&&(ae(),b(Gs[Ka],1,1,()=>{Gs[Ka]=null}),se(),bt=Gs[ft],bt||(bt=Gs[ft]=ip[ft](t),bt.c()),f(bt,1),bt.m(hn.parentNode,hn));let Oa=Yt;Yt=op(t),Yt!==Oa&&(ae(),b(Is[Oa],1,1,()=>{Is[Oa]=null}),se(),ln=Is[Yt],ln||(ln=Is[Yt]=cp[Yt](t),ln.c()),f(ln,1),ln.m(oe.parentNode,oe));const ni={};h&2&&(ni.$$scope={dirty:h,ctx:t}),ke.$set(ni);const Hn={};h&2&&(Hn.$$scope={dirty:h,ctx:t}),je.$set(Hn);const ei={};h&2&&(ei.$$scope={dirty:h,ctx:t}),xe.$set(ei);const si={};h&2&&(si.$$scope={dirty:h,ctx:t}),qe.$set(si),t[0]==="pt"?sn||(sn=gu(),sn.c(),sn.m(da.parentNode,da)):sn&&(sn.d(1),sn=null);let Ba=Mn;Mn=pp(t),Mn!==Ba&&(ae(),b(Vs[Ba],1,1,()=>{Vs[Ba]=null}),se(),zn=Vs[Mn],zn||(zn=Vs[Mn]=rp[Mn](t),zn.c()),f(zn,1),zn.m(fa.parentNode,fa));const Xs={};h&2&&(Xs.$$scope={dirty:h,ctx:t}),Ce.$set(Xs);let Se=An;An=up(t),An!==Se&&(ae(),b(Us[Se],1,1,()=>{Us[Se]=null}),se(),Pn=Us[An],Pn||(Pn=Us[An]=mp[An](t),Pn.c()),f(Pn,1),Pn.m($a.parentNode,$a));let La=Sn;Sn=_p(t),Sn!==La&&(ae(),b(Ws[La],1,1,()=>{Ws[La]=null}),se(),Kn=Ws[Sn],Kn||(Kn=Ws[Sn]=gp[Sn](t),Kn.c()),f(Kn,1),Kn.m(ja.parentNode,ja));let Na=On;On=fp(t),On!==Na&&(ae(),b(Ys[Na],1,1,()=>{Ys[Na]=null}),se(),Bn=Ys[On],Bn||(Bn=Ys[On]=dp[On](t),Bn.c()),f(Bn,1),Bn.m(Ea.parentNode,Ea));let Ke=Ln;Ln=kp(t),Ln!==Ke&&(ae(),b(Js[Ke],1,1,()=>{Js[Ke]=null}),se(),Nn=Js[Ln],Nn||(Nn=Js[Ln]=bp[Ln](t),Nn.c()),f(Nn,1),Nn.m(qa.parentNode,qa)),t[0]==="pt"?zt?h&1&&f(zt,1):(zt=_u(),zt.c(),f(zt,1),zt.m(Da.parentNode,Da)):zt&&(ae(),b(zt,1,1,()=>{zt=null}),se());const ai={};h&2&&(ai.$$scope={dirty:h,ctx:t}),Me.$set(ai),t[0]==="pt"?At?h&1&&f(At,1):(At=du(),At.c(),f(At,1),At.m(Ma.parentNode,Ma)):At&&(ae(),b(At,1,1,()=>{At=null}),se());const hi={};h&2&&(hi.$$scope={dirty:h,ctx:t}),Pe.$set(hi)},i(t){Nl||(f(p.$$.fragment,t),f(D.$$.fragment,t),f(N),f($t.$$.fragment,t),f(J.$$.fragment,t),f(mt.$$.fragment,t),f(bt),f(en.$$.fragment,t),f(In.$$.fragment,t),f(ln),f(fn.$$.fragment,t),f(Vn.$$.fragment,t),f(Xe.$$.fragment,t),f(Ze.$$.fragment,t),f(ts.$$.fragment,t),f(ns.$$.fragment,t),f(ke.$$.fragment,t),f(es.$$.fragment,t),f(ss.$$.fragment,t),f(as.$$.fragment,t),f(hs.$$.fragment,t),f(is.$$.fragment,t),f(ls.$$.fragment,t),f(je.$$.fragment,t),f(cs.$$.fragment,t),f(xe.$$.fragment,t),f(os.$$.fragment,t),f(rs.$$.fragment,t),f(ps.$$.fragment,t),f(ms.$$.fragment,t),f(us.$$.fragment,t),f(gs.$$.fragment,t),f(fs.$$.fragment,t),f(bs.$$.fragment,t),f(ks.$$.fragment,t),f(vs.$$.fragment,t),f(ys.$$.fragment,t),f($s.$$.fragment,t),f(ws.$$.fragment,t),f(js.$$.fragment,t),f(xs.$$.fragment,t),f(Es.$$.fragment,t),f(Ts.$$.fragment,t),f(qe.$$.fragment,t),f(zn),f(qs.$$.fragment,t),f(Cs.$$.fragment,t),f(Ce.$$.fragment,t),f(Ds.$$.fragment,t),f(Ms.$$.fragment,t),f(zs.$$.fragment,t),f(As.$$.fragment,t),f(Pn),f(Ps.$$.fragment,t),f(Ss.$$.fragment,t),f(Kn),f(Ks.$$.fragment,t),f(Bn),f(Nn),f(Os.$$.fragment,t),f(zt),f(Me.$$.fragment,t),f(At),f(Bs.$$.fragment,t),f(Ls.$$.fragment,t),f(Ns.$$.fragment,t),f(Hs.$$.fragment,t),f(Rs.$$.fragment,t),f(Pe.$$.fragment,t),Nl=!0)},o(t){b(p.$$.fragment,t),b(D.$$.fragment,t),b(N),b($t.$$.fragment,t),b(J.$$.fragment,t),b(mt.$$.fragment,t),b(bt),b(en.$$.fragment,t),b(In.$$.fragment,t),b(ln),b(fn.$$.fragment,t),b(Vn.$$.fragment,t),b(Xe.$$.fragment,t),b(Ze.$$.fragment,t),b(ts.$$.fragment,t),b(ns.$$.fragment,t),b(ke.$$.fragment,t),b(es.$$.fragment,t),b(ss.$$.fragment,t),b(as.$$.fragment,t),b(hs.$$.fragment,t),b(is.$$.fragment,t),b(ls.$$.fragment,t),b(je.$$.fragment,t),b(cs.$$.fragment,t),b(xe.$$.fragment,t),b(os.$$.fragment,t),b(rs.$$.fragment,t),b(ps.$$.fragment,t),b(ms.$$.fragment,t),b(us.$$.fragment,t),b(gs.$$.fragment,t),b(fs.$$.fragment,t),b(bs.$$.fragment,t),b(ks.$$.fragment,t),b(vs.$$.fragment,t),b(ys.$$.fragment,t),b($s.$$.fragment,t),b(ws.$$.fragment,t),b(js.$$.fragment,t),b(xs.$$.fragment,t),b(Es.$$.fragment,t),b(Ts.$$.fragment,t),b(qe.$$.fragment,t),b(zn),b(qs.$$.fragment,t),b(Cs.$$.fragment,t),b(Ce.$$.fragment,t),b(Ds.$$.fragment,t),b(Ms.$$.fragment,t),b(zs.$$.fragment,t),b(As.$$.fragment,t),b(Pn),b(Ps.$$.fragment,t),b(Ss.$$.fragment,t),b(Kn),b(Ks.$$.fragment,t),b(Bn),b(Nn),b(Os.$$.fragment,t),b(zt),b(Me.$$.fragment,t),b(At),b(Bs.$$.fragment,t),b(Ls.$$.fragment,t),b(Ns.$$.fragment,t),b(Hs.$$.fragment,t),b(Rs.$$.fragment,t),b(Pe.$$.fragment,t),Nl=!1},d(t){e(o),t&&e(d),E(p,t),t&&e(v),t&&e(q),E(D),t&&e(K),Fs[A].d(t),t&&e(H),t&&e(R),t&&e(W),t&&e(y),t&&e(I),t&&e(G),t&&e(ot),t&&e(rt),t&&e(Nt),t&&e(ut),t&&e(Gt),t&&e(at),t&&e(ht),t&&e(yt),t&&e(Vt),E($t,t),t&&e(S),E(J,t),t&&e(Et),t&&e(nt),E(mt),t&&e(Tt),t&&e(wt),t&&e(Q),t&&e(Ot),t&&e(un),t&&e(dt),t&&e(Cn),Gs[ft].d(t),t&&e(hn),t&&e(rn),t&&e(ie),E(en,t),t&&e(le),t&&e(kt),t&&e(pn),E(In,t),t&&e(Ue),t&&e(ce),t&&e(We),Is[Yt].d(t),t&&e(oe),E(fn,t),t&&e(Ye),t&&e(Dn),t&&e(Je),t&&e(bn),E(Vn),t&&e(ii),t&&e(Un),t&&e(li),E(Xe,t),t&&e(ci),E(Ze,t),t&&e(oi),t&&e(Jt),t&&e(ri),E(ts,t),t&&e(pi),E(ns,t),t&&e(mi),t&&e(Wn),t&&e(ui),E(ke,t),t&&e(gi),t&&e(ve),t&&e(_i),t&&e(re),E(es),t&&e(di),E(ss,t),t&&e(fi),t&&e(ia),t&&e(bi),t&&e(Qt),t&&e(ki),E(as,t),t&&e(vi),E(hs,t),t&&e(yi),t&&e(kn),t&&e($i),t&&e($e),t&&e(wi),E(is,t),t&&e(ji),E(ls,t),t&&e(xi),t&&e(we),t&&e(Ei),E(je,t),t&&e(Ti),t&&e(ca),t&&e(qi),E(cs,t),t&&e(Ci),E(xe,t),t&&e(Di),t&&e(oa),t&&e(Mi),E(os,t),t&&e(zi),E(rs,t),t&&e(Ai),t&&e(ra),t&&e(Pi),E(ps,t),t&&e(Si),E(ms,t),t&&e(Ki),t&&e(Yn),t&&e(Oi),E(us,t),t&&e(Bi),E(gs,t),t&&e(Li),t&&e(pa),t&&e(Ni),t&&e(Ee),t&&e(Hi),t&&e(ma),t&&e(Ri),E(fs,t),t&&e(Fi),t&&e(cn),t&&e(Gi),t&&e(Jn),t&&e(Ii),E(bs,t),t&&e(Vi),E(ks,t),t&&e(Ui),t&&e(Xt),t&&e(Wi),E(vs,t),t&&e(Yi),E(ys,t),t&&e(Ji),t&&e(ua),t&&e(Qi),E($s,t),t&&e(Xi),E(ws,t),t&&e(Zi),t&&e(vn),t&&e(tl),t&&e(pe),E(js),t&&e(nl),t&&e(yn),t&&e(el),E(xs,t),t&&e(sl),t&&e($n),t&&e(al),E(Es,t),t&&e(hl),E(Ts,t),t&&e(il),t&&e(Qn),t&&e(ll),E(qe,t),t&&e(cl),sn&&sn.d(t),t&&e(da),t&&e(wn),t&&e(ol),Vs[Mn].d(t),t&&e(fa),t&&e(ba),t&&e(rl),E(qs,t),t&&e(pl),E(Cs,t),t&&e(ml),E(Ce,t),t&&e(ul),t&&e(Xn),t&&e(gl),E(Ds,t),t&&e(_l),E(Ms,t),t&&e(dl),t&&e(Zn),t&&e(fl),E(zs,t),t&&e(bl),t&&e(va),t&&e(kl),E(As,t),t&&e(vl),t&&e(ya),t&&e(yl),Us[An].d(t),t&&e($a),t&&e(me),E(Ps),t&&e($l),E(Ss,t),t&&e(wl),t&&e(wa),t&&e(jl),Ws[Sn].d(t),t&&e(ja),E(Ks,t),t&&e(xl),t&&e(xa),t&&e(El),Ys[On].d(t),t&&e(Ea),t&&e(Ta),t&&e(Tl),Js[Ln].d(t),t&&e(qa),E(Os,t),t&&e(ql),t&&e(Ca),t&&e(Cl),zt&&zt.d(t),t&&e(Da),E(Me,t),t&&e(Dl),At&&At.d(t),t&&e(Ma),t&&e(ue),E(Bs),t&&e(Ml),t&&e(te),t&&e(zl),E(Ls,t),t&&e(Al),t&&e(za),t&&e(Pl),E(Ns,t),t&&e(Sl),E(Hs,t),t&&e(Kl),t&&e(Aa),t&&e(Ol),E(Rs,t),t&&e(Bl),t&&e(Ae),t&&e(Ll),E(Pe,t)}}}const Yu={local:"tinh-chnh-mt-m-hnh-ngn-ng-b-n-i",sections:[{local:"chn-mt-m-hnh-hun-luyn-trc-cho-m-hnh-ngn-ng-b-n-i",title:"Ch\u1ECDn m\u1ED9t m\xF4 h\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc cho m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"},{local:"b-d-liu",title:"B\u1ED9 d\u1EEF li\u1EC7u"},{local:"tin-x-l-d-liu",title:"Ti\u1EC1n x\u1EED l\xFD d\u1EEF li\u1EC7u"},{local:"tinh-chnh-distilbert-vi-api-trainer",sections:[{local:"perplexity-cho-m-hnh-ngn-ng",title:"Perplexity cho m\xF4 h\xECnh ng\xF4n ng\u1EEF"}],title:"Tinh ch\u1EC9nh DistilBERT v\u1EDBi API `Trainer`"},{local:"tinh-chnh-distilbert-vi-accelerate",title:"Tinh ch\u1EC9nh DistilBERT v\u1EDBi \u{1F917} Accelerate"},{local:"s-dng-m-hnh-tinh-chnh-ca-mnh",title:"S\u1EED d\u1EE5ng m\xF4 h\xECnh tinh ch\u1EC9nh c\u1EE7a m\xECnh"}],title:"Tinh ch\u1EC9nh m\u1ED9t m\xF4 h\xECnh ng\xF4n ng\u1EEF b\u1ECB \u1EA9n \u0111i"};function Ju(B,o,d){let p="pt";return $u(()=>{const v=new URLSearchParams(window.location.search);d(0,p=v.get("fw")||"pt")}),[p]}class ag extends bu{constructor(o){super();ku(this,o,Ju,Wu,vu,{})}}export{ag as default,Yu as metadata};
