import{S as ic,i as rc,s as oc,e as h,k as u,w as k,t as r,U as Yl,M as lc,c as m,d as e,m as f,x as v,a as p,h as o,V as Zl,b as S,N as tc,G as a,g as l,y as $,o as d,p as B,q as g,B as y,v as cc,n as U}from"../../chunks/vendor-hf-doc-builder.js";import{T as si}from"../../chunks/Tip-hf-doc-builder.js";import{Y as nc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ai}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as j}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as ec}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as hc}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function mc(x){let s,c;return s=new ec({props:{chapter:6,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"}]}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function pc(x){let s,c;return s=new ec({props:{chapter:6,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"}]}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function uc(x){let s,c;return s=new nc({props:{id:"b3u8RzBCX9Y"}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function fc(x){let s,c;return s=new nc({props:{id:"_wxyB3j3mk4"}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function dc(x){let s,c;return s=new j({props:{code:`from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function gc(x){let s,c;return s=new j({props:{code:`from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function _c(x){let s,c;return s=new j({props:{code:"(1, 66) (1, 66)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">66</span>) (<span class="hljs-number">1</span>, <span class="hljs-number">66</span>)'}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function bc(x){let s,c;return s=new j({props:{code:"torch.Size([1, 66]) torch.Size([1, 66])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>]) torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>])'}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function kc(x){let s,c;return s=new j({props:{code:`import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh
mask = [i != 1 for i in sequence_ids]
# Hi\u1EC3n th\u1ECB token [CLS]
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Hi\u1EC3n th\u1ECB token [CLS]</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = tf.constant(mask)[<span class="hljs-literal">None</span>]

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function vc(x){let s,c;return s=new j({props:{code:`import torch

sequence_ids = inputs.sequence_ids()
# Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh
mask = [i != 1 for i in sequence_ids]
# Hi\u1EC3n th\u1ECB token [CLS]
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000`,highlighted:`<span class="hljs-keyword">import</span> torch

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Hi\u1EC3n th\u1ECB token [CLS]</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = torch.tensor(mask)[<span class="hljs-literal">None</span>]

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function $c(x){let s,c;return s=new j({props:{code:`start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function yc(x){let s,c;return s=new j({props:{code:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function xc(x){let s,c,n,_,b,w,E,T,z,P,L,D,N,A;return N=new j({props:{code:`import numpy as np

scores = np.triu(scores)`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

scores = np.triu(scores)`}}),{c(){s=h("p"),c=r("Sau \u0111\xF3, ch\xFAng t\xF4i s\u1EBD che c\xE1c gi\xE1 tr\u1ECB trong \u0111\xF3 "),n=h("code"),_=r("start_index > end_index"),b=r(" b\u1EB1ng c\xE1ch \u0111\u1EB7t ch\xFAng th\xE0nh "),w=h("code"),E=r("0"),T=r(" (c\xE1c x\xE1c su\u1EA5t kh\xE1c \u0111\u1EC1u l\xE0 s\u1ED1 d\u01B0\u01A1ng). H\xE0m "),z=h("code"),P=r("np.triu()"),L=r(" tr\u1EA3 v\u1EC1 ph\u1EA7n tam gi\xE1c ph\xEDa tr\xEAn c\u1EE7a tensor 2D \u0111\u01B0\u1EE3c truy\u1EC1n d\u01B0\u1EDBi d\u1EA1ng tham s\u1ED1, v\xEC v\u1EADy n\xF3 s\u1EBD th\u1EF1c hi\u1EC7n vi\u1EC7c che \u0111\xF3 cho ch\xFAng ta:"),D=u(),k(N.$$.fragment)},l(q){s=m(q,"P",{});var C=p(s);c=o(C,"Sau \u0111\xF3, ch\xFAng t\xF4i s\u1EBD che c\xE1c gi\xE1 tr\u1ECB trong \u0111\xF3 "),n=m(C,"CODE",{});var xt=p(n);_=o(xt,"start_index > end_index"),xt.forEach(e),b=o(C," b\u1EB1ng c\xE1ch \u0111\u1EB7t ch\xFAng th\xE0nh "),w=m(C,"CODE",{});var J=p(w);E=o(J,"0"),J.forEach(e),T=o(C," (c\xE1c x\xE1c su\u1EA5t kh\xE1c \u0111\u1EC1u l\xE0 s\u1ED1 d\u01B0\u01A1ng). H\xE0m "),z=m(C,"CODE",{});var wt=p(z);P=o(wt,"np.triu()"),wt.forEach(e),L=o(C," tr\u1EA3 v\u1EC1 ph\u1EA7n tam gi\xE1c ph\xEDa tr\xEAn c\u1EE7a tensor 2D \u0111\u01B0\u1EE3c truy\u1EC1n d\u01B0\u1EDBi d\u1EA1ng tham s\u1ED1, v\xEC v\u1EADy n\xF3 s\u1EBD th\u1EF1c hi\u1EC7n vi\u1EC7c che \u0111\xF3 cho ch\xFAng ta:"),C.forEach(e),D=f(q),v(N.$$.fragment,q)},m(q,C){l(q,s,C),a(s,c),a(s,n),a(n,_),a(s,b),a(s,w),a(w,E),a(s,T),a(s,z),a(z,P),a(s,L),l(q,D,C),$(N,q,C),A=!0},i(q){A||(g(N.$$.fragment,q),A=!0)},o(q){d(N.$$.fragment,q),A=!1},d(q){q&&e(s),q&&e(D),y(N,q)}}}function wc(x){let s,c,n,_,b,w,E,T,z,P,L,D,N,A;return N=new j({props:{code:"scores = torch.triu(scores)",highlighted:"scores = torch.triu(scores)"}}),{c(){s=h("p"),c=r("Sau \u0111\xF3, ch\xFAng t\xF4i s\u1EBD che c\xE1c gi\xE1 tr\u1ECB trong \u0111\xF3 "),n=h("code"),_=r("start_index > end_index"),b=r(" b\u1EB1ng c\xE1ch \u0111\u1EB7t ch\xFAng th\xE0nh "),w=h("code"),E=r("0"),T=r(" (c\xE1c x\xE1c su\u1EA5t kh\xE1c \u0111\u1EC1u l\xE0 s\u1ED1 d\u01B0\u01A1ng). H\xE0m "),z=h("code"),P=r("torch.triu()"),L=r(" tr\u1EA3 v\u1EC1 ph\u1EA7n tam gi\xE1c ph\xEDa tr\xEAn c\u1EE7a tensor 2D \u0111\u01B0\u1EE3c truy\u1EC1n d\u01B0\u1EDBi d\u1EA1ng tham s\u1ED1, v\xEC v\u1EADy n\xF3 s\u1EBD th\u1EF1c hi\u1EC7n vi\u1EC7c che \u0111\xF3 cho ch\xFAng ta:"),D=u(),k(N.$$.fragment)},l(q){s=m(q,"P",{});var C=p(s);c=o(C,"Sau \u0111\xF3, ch\xFAng t\xF4i s\u1EBD che c\xE1c gi\xE1 tr\u1ECB trong \u0111\xF3 "),n=m(C,"CODE",{});var xt=p(n);_=o(xt,"start_index > end_index"),xt.forEach(e),b=o(C," b\u1EB1ng c\xE1ch \u0111\u1EB7t ch\xFAng th\xE0nh "),w=m(C,"CODE",{});var J=p(w);E=o(J,"0"),J.forEach(e),T=o(C," (c\xE1c x\xE1c su\u1EA5t kh\xE1c \u0111\u1EC1u l\xE0 s\u1ED1 d\u01B0\u01A1ng). H\xE0m "),z=m(C,"CODE",{});var wt=p(z);P=o(wt,"torch.triu()"),wt.forEach(e),L=o(C," tr\u1EA3 v\u1EC1 ph\u1EA7n tam gi\xE1c ph\xEDa tr\xEAn c\u1EE7a tensor 2D \u0111\u01B0\u1EE3c truy\u1EC1n d\u01B0\u1EDBi d\u1EA1ng tham s\u1ED1, v\xEC v\u1EADy n\xF3 s\u1EBD th\u1EF1c hi\u1EC7n vi\u1EC7c che \u0111\xF3 cho ch\xFAng ta:"),C.forEach(e),D=f(q),v(N.$$.fragment,q)},m(q,C){l(q,s,C),a(s,c),a(s,n),a(n,_),a(s,b),a(s,w),a(w,E),a(s,T),a(s,z),a(z,P),a(s,L),l(q,D,C),$(N,q,C),A=!0},i(q){A||(g(N.$$.fragment,q),A=!0)},o(q){d(N.$$.fragment,q),A=!1},d(q){q&&e(s),q&&e(D),y(N,q)}}}function jc(x){let s,c,n,_,b;return{c(){s=h("p"),c=r("\u270F\uFE0F "),n=h("strong"),_=r("Th\u1EED nghi\u1EC7m th\xF4i!"),b=r(" T\xEDnh ch\u1EC9 m\u1EE5c b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc cho n\u0103m c\u1EA5u tr\u1EA3 l\u1EDDi \u0111\u1EA7u ti\u1EC7n.")},l(w){s=m(w,"P",{});var E=p(s);c=o(E,"\u270F\uFE0F "),n=m(E,"STRONG",{});var T=p(n);_=o(T,"Th\u1EED nghi\u1EC7m th\xF4i!"),T.forEach(e),b=o(E," T\xEDnh ch\u1EC9 m\u1EE5c b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc cho n\u0103m c\u1EA5u tr\u1EA3 l\u1EDDi \u0111\u1EA7u ti\u1EC7n."),E.forEach(e)},m(w,E){l(w,s,E),a(s,c),a(s,n),a(n,_),a(s,b)},d(w){w&&e(s)}}}function qc(x){let s,c,n,_,b,w,E,T;return{c(){s=h("p"),c=r("\u270F\uFE0F "),n=h("strong"),_=r("Th\u1EED nghi\u1EC7m th\xF4i!"),b=r(" S\u1EED d\u1EE5ng \u0111i\u1EC3m t\u1ED1t nh\u1EA5t m\xE0 b\u1EA1n \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 hi\u1EC3n th\u1ECB n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 kh\u1EA3 n\u0103ng nh\u1EA5t. \u0110\u1EC3 ki\u1EC3m tra k\u1EBFt qu\u1EA3 c\u1EE7a b\u1EA1n, h\xE3y quay l\u1EA1i \u0111\u01B0\u1EDDng d\u1EABn \u0111\u1EA7u ti\xEAn v\xE0 truy\u1EC1n v\xE0o "),w=h("code"),E=r("top_k=5"),T=r(" khi g\u1ECDi n\xF3.")},l(z){s=m(z,"P",{});var P=p(s);c=o(P,"\u270F\uFE0F "),n=m(P,"STRONG",{});var L=p(n);_=o(L,"Th\u1EED nghi\u1EC7m th\xF4i!"),L.forEach(e),b=o(P," S\u1EED d\u1EE5ng \u0111i\u1EC3m t\u1ED1t nh\u1EA5t m\xE0 b\u1EA1n \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 hi\u1EC3n th\u1ECB n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 kh\u1EA3 n\u0103ng nh\u1EA5t. \u0110\u1EC3 ki\u1EC3m tra k\u1EBFt qu\u1EA3 c\u1EE7a b\u1EA1n, h\xE3y quay l\u1EA1i \u0111\u01B0\u1EDDng d\u1EABn \u0111\u1EA7u ti\xEAn v\xE0 truy\u1EC1n v\xE0o "),w=m(P,"CODE",{});var D=p(w);E=o(D,"top_k=5"),D.forEach(e),T=o(P," khi g\u1ECDi n\xF3."),P.forEach(e)},m(z,P){l(z,s,P),a(s,c),a(s,n),a(n,_),a(s,b),a(s,w),a(w,E),a(s,T)},d(z){z&&e(s)}}}function Ec(x){let s,c,n,_;return s=new j({props:{code:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),n=new j({props:{code:"(2, 384)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){k(s.$$.fragment),c=u(),k(n.$$.fragment)},l(b){v(s.$$.fragment,b),c=f(b),v(n.$$.fragment,b)},m(b,w){$(s,b,w),l(b,c,w),$(n,b,w),_=!0},i(b){_||(g(s.$$.fragment,b),g(n.$$.fragment,b),_=!0)},o(b){d(s.$$.fragment,b),d(n.$$.fragment,b),_=!1},d(b){y(s,b),b&&e(c),y(n,b)}}}function Cc(x){let s,c,n,_;return s=new j({props:{code:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),n=new j({props:{code:"torch.Size([2, 384])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){k(s.$$.fragment),c=u(),k(n.$$.fragment)},l(b){v(s.$$.fragment,b),c=f(b),v(n.$$.fragment,b)},m(b,w){$(s,b,w),l(b,c,w),$(n,b,w),_=!0},i(b){_||(g(s.$$.fragment,b),g(n.$$.fragment,b),_=!0)},o(b){d(s.$$.fragment,b),d(n.$$.fragment,b),_=!1},d(b){y(s,b),b&&e(c),y(n,b)}}}function Pc(x){let s,c;return s=new j({props:{code:"(2, 384) (2, 384)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>) (<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Tc(x){let s,c;return s=new j({props:{code:"torch.Size([2, 384]) torch.Size([2, 384])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>]) torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Sc(x){let s,c;return s=new j({props:{code:`sequence_ids = inputs.sequence_ids()
# Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh
mask = [i != 1 for i in sequence_ids]
# Hi\u1EC3n th\u1ECB token [CLS]
mask[0] = False
# Che t\u1EA5t c\u1EA3 token [PAD]
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Hi\u1EC3n th\u1ECB token [CLS]</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 token [PAD]</span>
mask = tf.math.logical_or(tf.constant(mask)[<span class="hljs-literal">None</span>], inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>)

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function zc(x){let s,c;return s=new j({props:{code:`sequence_ids = inputs.sequence_ids()
# Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh
mask = [i != 1 for i in sequence_ids]
# Hi\u1EC3n th\u1ECB token [CLS]
mask[0] = False
# Che t\u1EA5t c\u1EA3 token [PAD]
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 m\u1ECDi th\u1EE9 tr\u1EEB token c\u1EE7a ng\u1EEF c\u1EA3nh</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Hi\u1EC3n th\u1ECB token [CLS]</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Che t\u1EA5t c\u1EA3 token [PAD]</span>
mask = torch.logical_or(torch.tensor(mask)[<span class="hljs-literal">None</span>], (inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>))

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Nc(x){let s,c;return s=new j({props:{code:`start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>).numpy()`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Ac(x){let s,c;return s=new j({props:{code:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Dc(x){let s,c;return s=new j({props:{code:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">1</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">1</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Lc(x){let s,c;return s=new j({props:{code:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">1</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">1</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){k(s.$$.fragment)},l(n){v(s.$$.fragment,n)},m(n,_){$(s,n,_),c=!0},i(n){c||(g(s.$$.fragment,n),c=!0)},o(n){d(s.$$.fragment,n),c=!1},d(n){y(s,n)}}}function Oc(x){let s,c,n,_,b;return{c(){s=h("p"),c=r("\u270F\uFE0F "),n=h("strong"),_=r("Th\u1EED nghi\u1EC7m th\xF4i!"),b=r(" H\xE3y \u0111i\u1EC1u ch\u1EC9nh \u0111o\u1EA1n m\xE3 tr\xEAn \u0111\u1EC3 tr\u1EA3 v\u1EC1 \u0111i\u1EC3m v\xE0 kho\u1EA3ng cho n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 nhi\u1EC1u kh\u1EA3 n\u0103ng nh\u1EA5t (t\u1ED5ng c\u1ED9ng, kh\xF4ng ph\u1EA3i cho m\u1ED7i \u0111o\u1EA1n).")},l(w){s=m(w,"P",{});var E=p(s);c=o(E,"\u270F\uFE0F "),n=m(E,"STRONG",{});var T=p(n);_=o(T,"Th\u1EED nghi\u1EC7m th\xF4i!"),T.forEach(e),b=o(E," H\xE3y \u0111i\u1EC1u ch\u1EC9nh \u0111o\u1EA1n m\xE3 tr\xEAn \u0111\u1EC3 tr\u1EA3 v\u1EC1 \u0111i\u1EC3m v\xE0 kho\u1EA3ng cho n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 nhi\u1EC1u kh\u1EA3 n\u0103ng nh\u1EA5t (t\u1ED5ng c\u1ED9ng, kh\xF4ng ph\u1EA3i cho m\u1ED7i \u0111o\u1EA1n)."),E.forEach(e)},m(w,E){l(w,s,E),a(s,c),a(s,n),a(n,_),a(s,b)},d(w){w&&e(s)}}}function Fc(x){let s,c,n,_,b,w,E,T;return{c(){s=h("p"),c=r("\u270F\uFE0F "),n=h("strong"),_=r("Th\u1EED nghi\u1EC7m th\xF4i!"),b=r(" S\u1EED d\u1EE5ng \u0111i\u1EC3m t\u1ED1t nh\u1EA5t b\u1EA1n \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 hi\u1EC3n th\u1ECB n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 kh\u1EA3 n\u0103ng x\u1EA3y ra nh\u1EA5t (cho to\xE0n b\u1ED9 ng\u1EEF c\u1EA3nh, kh\xF4ng ph\u1EA3i t\u1EEBng \u0111o\u1EA1n). \u0110\u1EC3 ki\u1EC3m tra k\u1EBFt qu\u1EA3 c\u1EE7a b\u1EA1n, h\xE3y quay l\u1EA1i pipeline \u0111\u1EA7u ti\xEAn v\xE0 truy\u1EC1n v\xE0o "),w=h("code"),E=r("top_k=5"),T=r(" khi g\u1ECDi n\xF3.")},l(z){s=m(z,"P",{});var P=p(s);c=o(P,"\u270F\uFE0F "),n=m(P,"STRONG",{});var L=p(n);_=o(L,"Th\u1EED nghi\u1EC7m th\xF4i!"),L.forEach(e),b=o(P," S\u1EED d\u1EE5ng \u0111i\u1EC3m t\u1ED1t nh\u1EA5t b\u1EA1n \u0111\xE3 t\xEDnh to\xE1n tr\u01B0\u1EDBc \u0111\xF3 \u0111\u1EC3 hi\u1EC3n th\u1ECB n\u0103m c\xE2u tr\u1EA3 l\u1EDDi c\xF3 kh\u1EA3 n\u0103ng x\u1EA3y ra nh\u1EA5t (cho to\xE0n b\u1ED9 ng\u1EEF c\u1EA3nh, kh\xF4ng ph\u1EA3i t\u1EEBng \u0111o\u1EA1n). \u0110\u1EC3 ki\u1EC3m tra k\u1EBFt qu\u1EA3 c\u1EE7a b\u1EA1n, h\xE3y quay l\u1EA1i pipeline \u0111\u1EA7u ti\xEAn v\xE0 truy\u1EC1n v\xE0o "),w=m(P,"CODE",{});var D=p(w);E=o(D,"top_k=5"),D.forEach(e),T=o(P," khi g\u1ECDi n\xF3."),P.forEach(e)},m(z,P){l(z,s,P),a(s,c),a(s,n),a(n,_),a(s,b),a(s,w),a(w,E),a(s,T)},d(z){z&&e(s)}}}function Mc(x){let s,c,n,_,b,w,E,T,z,P,L,D,N,A,q,C,xt,J,wt,ii,qs,Q,K,Gn,jt,Pt,Oe,Qt,ri,Bn,oi,Fe,li,Es,gt,ci,Un,hi,mi,Me,pi,ui,Cs,Kt,Ps,Wt,Ts,Jn,fi,Ss,Rt,zs,Xt,Ns,Qn,di,As,qt,Tt,He,Yt,gi,Ie,_i,Ds,H,bi,Ve,ki,vi,Zt,Ge,$i,yi,Kn,xi,wi,Ls,W,R,Wn,Rn,ji,Os,Et,tn,ko,qi,nn,vo,Fs,Xn,Ei,Ms,en,Hs,X,Y,Yn,I,Ci,Be,Pi,Ti,Ue,Si,zi,Je,Ni,Ai,Is,St,Di,Qe,Li,Oi,Vs,Z,tt,Zn,te,Fi,Gs,nt,et,ne,F,Mi,Ke,Hi,Ii,We,Vi,Gi,Re,Bi,Ui,Xe,Ji,Qi,Bs,O,Ki,Ye,Wi,Ri,Ze,Xi,Yi,ts,Zi,tr,ns,nr,er,Us,sc='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span></span>',Js,_t,sr,Qs,ac='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span>',Ks,es,ar,ir,Ws,ee,rr,Rs,sn,Xs,st,at,se,M,or,ss,lr,cr,as,hr,mr,is,pr,ur,rs,fr,dr,Ys,an,Zs,ae,gr,ta,rn,na,zt,ea,bt,_r,os,br,kr,ls,vr,$r,sa,on,aa,ie,yr,ia,ln,ra,cn,oa,re,xr,la,Nt,ca,Ct,At,cs,hn,wr,hs,jr,ha,Dt,qr,ms,Er,Cr,ma,mn,pa,pn,ua,Lt,Pr,ps,Tr,Sr,fa,un,da,fn,ga,oe,zr,_a,kt,Nr,us,Ar,Dr,fs,Lr,Or,ba,dn,ka,gn,va,Ot,Fr,ds,Mr,Hr,$a,le,Ir,ya,_n,xa,bn,wa,Ft,Vr,gs,Gr,Br,ja,kn,qa,vn,Ea,ce,Ur,Ca,$n,Pa,he,Jr,Ta,yn,Sa,me,Qr,za,V,Kr,_s,Wr,Rr,bs,Xr,Yr,ks,Zr,to,Na,xn,Aa,G,no,vs,eo,so,$s,ao,io,ys,ro,oo,Da,it,rt,pe,ue,lo,La,wn,Oa,ot,lt,fe,de,co,Fa,ct,ht,ge,_e,ho,Ma,mt,pt,be,ke,mo,Ha,ut,ft,ve,jn,Ia,$e,po,Va,Mt,Ga,qn,xs,uo,fo,Ba,En,Ua,Cn,Ja,ye,go,Qa,Ht,Ka,xe,_o,Wa;n=new hc({props:{fw:x[0]}}),T=new ai({});const $o=[pc,mc],Pn=[];function yo(t,i){return t[0]==="pt"?0:1}N=yo(x),A=Pn[N]=$o[N](x);const xo=[fc,uc],Tn=[];function wo(t,i){return t[0]==="pt"?0:1}Q=wo(x),K=Tn[Q]=xo[Q](x),Qt=new ai({}),Kt=new j({props:{code:`from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back \u{1F917} Transformers?"
question_answerer(question=question, context=context)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>)
context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question = <span class="hljs-string">&quot;Which deep learning libraries back \u{1F917} Transformers?&quot;</span>
question_answerer(question=question, context=context)`}}),Wt=new j({props:{code:`{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),Rt=new j({props:{code:`long_context = """
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)`,highlighted:`long_context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question_answerer(question=question, context=long_context)`}}),Xt=new j({props:{code:`{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),Yt=new ai({});const jo=[gc,dc],Sn=[];function qo(t,i){return t[0]==="pt"?0:1}W=qo(x),R=Sn[W]=jo[W](x),en=new j({props:{code:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)`,highlighted:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Eo=[bc,_c],zn=[];function Co(t,i){return t[0]==="pt"?0:1}X=Co(x),Y=zn[X]=Eo[X](x);const Po=[vc,kc],Nn=[];function To(t,i){return t[0]==="pt"?0:1}Z=To(x),tt=Nn[Z]=Po[Z](x);const So=[yc,$c],An=[];function zo(t,i){return t[0]==="pt"?0:1}nt=zo(x),et=An[nt]=So[nt](x),sn=new j({props:{code:"scores = start_probabilities[:, None] * end_probabilities[None, :]",highlighted:'scores = start_probabilities[:, <span class="hljs-literal">None</span>] * end_probabilities[<span class="hljs-literal">None</span>, :]'}});const No=[wc,xc],Dn=[];function Ao(t,i){return t[0]==="pt"?0:1}st=Ao(x),at=Dn[st]=No[st](x),an=new j({props:{code:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])`,highlighted:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[<span class="hljs-number">1</span>]
end_index = max_index % scores.shape[<span class="hljs-number">1</span>]
<span class="hljs-built_in">print</span>(scores[start_index, end_index])`}}),rn=new j({props:{code:"0.97773",highlighted:'<span class="hljs-number">0.97773</span>'}}),zt=new si({props:{$$slots:{default:[jc]},$$scope:{ctx:x}}}),on=new j({props:{code:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`,highlighted:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="hljs-literal">True</span>)
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`}}),ln=new j({props:{code:`result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)`,highlighted:`result = {
    <span class="hljs-string">&quot;answer&quot;</span>: answer,
    <span class="hljs-string">&quot;start&quot;</span>: start_char,
    <span class="hljs-string">&quot;end&quot;</span>: end_char,
    <span class="hljs-string">&quot;score&quot;</span>: scores[start_index, end_index],
}
<span class="hljs-built_in">print</span>(result)`}}),cn=new j({props:{code:`{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>}`}}),Nt=new si({props:{$$slots:{default:[qc]},$$scope:{ctx:x}}}),hn=new ai({}),mn=new j({props:{code:`inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))`,highlighted:`inputs = tokenizer(question, long_context)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),pn=new j({props:{code:"461",highlighted:'<span class="hljs-number">461</span>'}}),un=new j({props:{code:`inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))`,highlighted:`inputs = tokenizer(question, long_context, max_length=<span class="hljs-number">384</span>, truncation=<span class="hljs-string">&quot;only_second&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),fn=new j({props:{code:`"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
&quot;&quot;&quot;</span>`}}),dn=new j({props:{code:`sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))`,highlighted:`sentence = <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>
inputs = tokenizer(
    sentence, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),gn=new j({props:{code:`'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'`,highlighted:`<span class="hljs-string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] is not too long [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] too long but we [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] but we are going [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] are going to split [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] it anyway. [SEP]&#x27;</span>`}}),_n=new j({props:{code:"print(inputs.keys())",highlighted:'<span class="hljs-built_in">print</span>(inputs.keys())'}}),bn=new j({props:{code:"dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>])'}}),kn=new j({props:{code:'print(inputs["overflow_to_sample_mapping"])',highlighted:'<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])'}}),vn=new j({props:{code:"[0, 0, 0, 0, 0, 0, 0]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),$n=new j({props:{code:`sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])`,highlighted:`sentences = [
    <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,
    <span class="hljs-string">&quot;This sentence is shorter but will still get split.&quot;</span>,
]
inputs = tokenizer(
    sentences, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])`}}),yn=new j({props:{code:"[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),xn=new j({props:{code:`inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)`,highlighted:`inputs = tokenizer(
    question,
    long_context,
    stride=<span class="hljs-number">128</span>,
    max_length=<span class="hljs-number">384</span>,
    padding=<span class="hljs-string">&quot;longest&quot;</span>,
    truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_offsets_mapping=<span class="hljs-literal">True</span>,
)`}});const Do=[Cc,Ec],Ln=[];function Lo(t,i){return t[0]==="pt"?0:1}it=Lo(x),rt=Ln[it]=Do[it](x),wn=new j({props:{code:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)`,highlighted:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Oo=[Tc,Pc],On=[];function Fo(t,i){return t[0]==="pt"?0:1}ot=Fo(x),lt=On[ot]=Oo[ot](x);const Mo=[zc,Sc],Fn=[];function Ho(t,i){return t[0]==="pt"?0:1}ct=Ho(x),ht=Fn[ct]=Mo[ct](x);const Io=[Ac,Nc],Mn=[];function Vo(t,i){return t[0]==="pt"?0:1}mt=Vo(x),pt=Mn[mt]=Io[mt](x);const Go=[Lc,Dc],Hn=[];function Bo(t,i){return t[0]==="pt"?0:1}return ut=Bo(x),ft=Hn[ut]=Go[ut](x),jn=new j({props:{code:"[(0, 18, 0.33867), (173, 184, 0.97149)]",highlighted:'[(<span class="hljs-number">0</span>, <span class="hljs-number">18</span>, <span class="hljs-number">0.33867</span>), (<span class="hljs-number">173</span>, <span class="hljs-number">184</span>, <span class="hljs-number">0.97149</span>)]'}}),Mt=new si({props:{$$slots:{default:[Oc]},$$scope:{ctx:x}}}),En=new j({props:{code:`for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)`,highlighted:`<span class="hljs-keyword">for</span> candidate, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {<span class="hljs-string">&quot;answer&quot;</span>: answer, <span class="hljs-string">&quot;start&quot;</span>: start_char, <span class="hljs-string">&quot;end&quot;</span>: end_char, <span class="hljs-string">&quot;score&quot;</span>: score}
    <span class="hljs-built_in">print</span>(result)`}}),Cn=new j({props:{code:`{'answer': '\\n\u{1F917} Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;\\n\u{1F917} Transformers: State of the Art NLP&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">37</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.33867</span>}
{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>}`}}),Ht=new si({props:{$$slots:{default:[Fc]},$$scope:{ctx:x}}}),{c(){s=h("meta"),c=u(),k(n.$$.fragment),_=u(),b=h("h1"),w=h("a"),E=h("span"),k(T.$$.fragment),z=u(),P=h("span"),L=r("Fast tokenizers in the QA pipeline"),D=u(),A.c(),q=u(),C=h("p"),xt=r("Gi\u1EDD ch\xFAng ta s\u1EBD \u0111i s\xE2u v\xE0o pipeline "),J=h("code"),wt=r("question-answering"),ii=r(" v\xE0 xem c\xE1ch t\u1EADn d\u1EE5ng c\xE1c offset \u0111\u1EC3 l\u1EA5y c\xE2u tr\u1EA3 l\u1EDDi cho c\xE1c c\xE2u h\u1ECFi d\u1EF1a theo t\u1EEB ng\u1EEF c\u1EA3nh, gi\u1ED1ng nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m v\u1EDBi c\xE1c th\u1EF1c th\u1EC3 \u0111\u01B0\u1EE3c nh\xF3m trong ph\u1EA7n tr\u01B0\u1EDBc. Sau \u0111\xF3, ch\xFAng ta s\u1EBD xem l\xE0m th\u1EBF n\xE0o c\xF3 th\u1EC3 \u0111\u1ED1i ph\xF3 v\u1EDBi nh\u1EEFng ng\u1EEF c\u1EA3nh r\u1EA5t d\xE0i m\xE0 cu\u1ED1i c\xF9ng l\u1EA1i b\u1ECB c\u1EAFt b\u1EDBt. B\u1EA1n c\xF3 th\u1EC3 b\u1ECF qua ph\u1EA7n n\xE0y n\u1EBFu kh\xF4ng quan t\xE2m \u0111\u1EBFn t\xE1c v\u1EE5 h\u1ECFi \u0111\xE1p."),qs=u(),K.c(),Gn=u(),jt=h("h2"),Pt=h("a"),Oe=h("span"),k(Qt.$$.fragment),ri=u(),Bn=h("span"),oi=r("S\u1EED d\u1EE5ng pipeline "),Fe=h("code"),li=r("question-answering"),Es=u(),gt=h("p"),ci=r("Nh\u01B0 \u0111\xE3 th\u1EA5y trong "),Un=h("a"),hi=r("Ch\u01B0\u01A1ng 1"),mi=r(", ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng pipeline "),Me=h("code"),pi=r("question-answering"),ui=r(" nh\u01B0 sau \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c c\xE2u tr\u1EA3 l\u1EDDi cho c\xE2u h\u1ECFi:"),Cs=u(),k(Kt.$$.fragment),Ps=u(),k(Wt.$$.fragment),Ts=u(),Jn=h("p"),fi=r("Kh\xF4ng nh\u01B0 c\xE1c pipeline kh\xE1c kh\xF4ng th\u1EC3 c\u1EAFt g\u1ECDn v\xE0 chia v\u0103n b\u1EA3n d\xE0i h\u01A1n \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a cho ph\xE9p c\u1EE7a m\xF4 h\xECnh (d\u1EABn \u0111\u1EBFn b\u1ECF l\u1EE1 nh\u1EEFng th\xF4ng tin \u1EDF ph\u1EA7n cu\u1ED1i v\u0103n b\u1EA3n), pipeline n\xE0y c\xF3 th\u1EC3 x\u1EED l\xFD t\u1ED1t v\u1EDBi nh\u1EEFng ng\u1EEF c\u1EA3nh d\xE0i v\xE0 s\u1EBD tr\u1EA3 v\u1EC1 c\xE2u tr\u1EA3 l\u1EDDi k\u1EC3 c\u1EA3 khi n\xF3 n\u1EB1m \u1EDF cu\u1ED1i v\u0103n b\u1EA3n:"),Ss=u(),k(Rt.$$.fragment),zs=u(),k(Xt.$$.fragment),Ns=u(),Qn=h("p"),di=r("H\xE3y c\xF9ng nhau xem n\xF3 l\xE0m th\u1EBF n\xE0o!"),As=u(),qt=h("h2"),Tt=h("a"),He=h("span"),k(Yt.$$.fragment),gi=u(),Ie=h("span"),_i=r("S\u1EED d\u1EE5ng m\xF4 h\xECnh cho t\xE1c v\u1EE5 h\u1ECFi \u0111\xE1p"),Ds=u(),H=h("p"),bi=r("Nh\u01B0 nh\u1EEFng pipeline kh\xE1c, ta s\u1EBD b\u1EAFt \u0111\u1EA7u v\u1EDBi vi\u1EC7c tokenize \u0111\u1EA7u v\xE0o v\xE0 sau \u0111\xF3 truy\u1EC1n ch\xFAng v\xE0o trong m\xF4 h\xECnh. M\u1EB7c \u0111\u1ECBnh checkpoint \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho pipeline "),Ve=h("code"),ki=r("question-answering"),vi=r(" l\xE0 "),Zt=h("a"),Ge=h("code"),$i=r("distilbert-base-cased-distilled-squad"),yi=r(" ( \u201Csquad\u201D trong t\xEAn b\u1EAFt ngu\u1ED3n t\u1EEB b\u1ED9 d\u1EEF li\u1EC7u m\xE0 m\xF4 h\xECnh s\u1EED d\u1EE5ng \u0111\u1EC3 tinh ch\u1EC9nh; ta s\u1EBD n\xF3i s\xE2u h\u01A1n v\u1EC1 b\u1ED9 d\u1EEF li\u1EC7u SQuAD n\xE0y \u1EDF  "),Kn=h("a"),xi=r("Ch\u01B0\u01A1ng 7"),wi=r("):"),Ls=u(),R.c(),Wn=u(),Rn=h("p"),ji=r("L\u01B0u \xFD r\u1EB1ng ch\xFAng ta tokenize c\xE2u h\u1ECFi v\xE0 ng\u1EEF c\u1EA3nh nh\u01B0 m\u1ED9t c\u1EB7p, v\u1EDBi c\xE2u h\u1ECFi \u0111\u1EE9ng tr\u01B0\u1EDBc."),Os=u(),Et=h("div"),tn=h("img"),qi=u(),nn=h("img"),Fs=u(),Xn=h("p"),Ei=r("C\xE1c m\xF4 h\xECnh h\u1ECFi \u0111\xE1p ho\u1EA1t \u0111\u1ED9ng h\u01A1i kh\xE1c so v\u1EDBi c\xE1c m\xF4 h\xECnh m\xE0 ta \u0111\xE3 th\u1EA5y cho \u0111\u1EBFn nay. S\u1EED d\u1EE5ng h\xECnh tr\xEAn l\xE0m v\xED d\u1EE5, m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n \u0111\u1EC3 d\u1EF1 \u0111o\xE1n ch\u1EC9 m\u1EE5c c\u1EE7a token b\u1EAFt \u0111\u1EA7u c\xE2u tr\u1EA3 l\u1EDDi (\u1EDF \u0111\xE2y l\xE0 21) v\xE0 ch\u1EC9 m\u1EE5c c\u1EE7a token n\u01A1i c\xE2u tr\u1EA3 l\u1EDDi k\u1EBFt th\xFAc (\u1EDF \u0111\xE2y l\xE0 24). \u0110\xE2y l\xE0 l\xFD do t\u1EA1i sao c\xE1c m\xF4 h\xECnh \u0111\xF3 kh\xF4ng tr\u1EA3 v\u1EC1 m\u1ED9t tensor logit m\xE0 l\xE0 hai: m\u1ED9t cho c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi token b\u1EAFt \u0111\u1EA7u c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi v\xE0 m\u1ED9t cho c\xE1c c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi token k\u1EBFt th\xFAc c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi. V\xEC trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y, ch\xFAng ta ch\u1EC9 c\xF3 m\u1ED9t \u0111\u1EA7u v\xE0o ch\u1EE9a 66 token, ta nh\u1EADn \u0111\u01B0\u1EE3c:"),Ms=u(),k(en.$$.fragment),Hs=u(),Y.c(),Yn=u(),I=h("p"),Ci=r("\u0110\u1EC3 chuy\u1EC3n \u0111\u1ED5i c\xE1c logit \u0111\xF3 th\xE0nh x\xE1c su\u1EA5t, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng m\u1ED9t h\xE0m softmax - nh\u01B0ng tr\u01B0\u1EDBc \u0111\xF3, ch\xFAng ta c\u1EA7n \u0111\u1EA3m b\u1EA3o r\u1EB1ng ch\xFAng ta che d\u1EA5u c\xE1c ch\u1EC9 m\u1EE5c kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ng\u1EEF c\u1EA3nh. \u0110\u1EA7u v\xE0o c\u1EE7a ch\xFAng t\xF4i l\xE0 "),Be=h("code"),Pi=r("[CLS] question [SEP] context [SEP]"),Ti=r(", v\xEC v\u1EADy ch\xFAng ta c\u1EA7n che d\u1EA5u c\xE1c token c\u1EE7a c\xE2u h\u1ECFi c\u0169ng nh\u01B0 token "),Ue=h("code"),Si=r("[SEP]"),zi=r(". Tuy nhi\xEAn, ch\xFAng ta s\u1EBD gi\u1EEF token "),Je=h("code"),Ni=r("[CLS]"),Ai=r(" v\xEC m\u1ED9t s\u1ED1 m\xF4 h\xECnh s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 ch\u1EC9 ra r\u1EB1ng c\xE2u tr\u1EA3 l\u1EDDi kh\xF4ng n\u1EB1m trong ng\u1EEF c\u1EA3nh."),Is=u(),St=h("p"),Di=r("V\xEC ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng softmax sau \u0111\xF3, ch\xFAng ta ch\u1EC9 c\u1EA7n thay th\u1EBF c\xE1c logit mu\u1ED1n che b\u1EB1ng m\u1ED9t s\u1ED1 \xE2m l\u1EDBn. \u1EDE \u0111\xE2y, ch\xFAng ta s\u1EED d\u1EE5ng "),Qe=h("code"),Li=r("-10000"),Oi=r(":"),Vs=u(),tt.c(),Zn=u(),te=h("p"),Fi=r("Gi\u1EDD ch\xFAng ta \u0111\xE3 che c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c v\u1ECB tr\xED m\xE0 ch\xFAng ta kh\xF4ng mu\u1ED1n d\u1EF1 \u0111o\xE1n, ch\xFAng ta c\xF3 th\u1EC3 \xE1p d\u1EE5ng softmax:"),Gs=u(),et.c(),ne=u(),F=h("p"),Mi=r("\u1EDE giai \u0111o\u1EA1n n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 l\u1EA5y argmax x\xE1c su\u1EA5t b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc - nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 k\u1EBFt th\xFAc v\u1EDBi ch\u1EC9 m\u1EE5c b\u1EAFt \u0111\u1EA7u l\u1EDBn h\u01A1n k\u1EBFt th\xFAc, v\xEC v\u1EADy ch\xFAng ta c\u1EA7n th\u1EF1c hi\u1EC7n th\xEAm m\u1ED9t s\u1ED1 bi\u1EC7n ph\xE1p ph\xF2ng ng\u1EEBa. Ch\xFAng ta s\u1EBD t\xEDnh to\xE1n x\xE1c su\u1EA5t c\u1EE7a t\u1EEBng "),Ke=h("code"),Hi=r("start_index"),Ii=r(" v\xE0 "),We=h("code"),Vi=r("end_index"),Gi=r(" c\xF3 th\u1EC3 trong \u0111\xF3 "),Re=h("code"),Bi=r("start_index <= end_index"),Ui=r(", sau \u0111\xF3 l\u1EA5y "),Xe=h("code"),Ji=r("(start_index, end_index)"),Qi=r(" v\u1EDBi x\xE1c su\u1EA5t cao nh\u1EA5t."),Bs=u(),O=h("p"),Ki=r("Gi\u1EA3 s\u1EED c\xE1c s\u1EF1 ki\u1EC7n \u201CC\xE2u tr\u1EA3 l\u1EDDi b\u1EAFt \u0111\u1EA7u \u1EDF "),Ye=h("code"),Wi=r("start_index"),Ri=r("\u201D v\xE0 \u201CC\xE2u tr\u1EA3 l\u1EDDi k\u1EBFt th\xFAc \u1EDF "),Ze=h("code"),Xi=r("end_index"),Yi=r("\u201D l\xE0 \u0111\u1ED9c l\u1EADp, x\xE1c su\u1EA5t \u0111\u1EC3 c\xE2u tr\u1EA3 l\u1EDDi b\u1EAFt \u0111\u1EA7u t\u1EA1i "),ts=h("code"),Zi=r("start_index"),tr=r(" v\xE0 k\u1EBFt th\xFAc t\u1EA1i "),ns=h("code"),nr=r("end_index"),er=r(` l\xE0:
`),Us=new Yl,Js=u(),_t=h("p"),sr=r("V\xEC v\u1EADy, \u0111\u1EC3 t\xEDnh t\u1EA5t c\u1EA3 c\xE1c \u0111i\u1EC3m, ch\xFAng ta ch\u1EC9 c\u1EA7n t\xEDnh t\xEDch "),Qs=new Yl,Ks=r(" v\u1EDBi "),es=h("code"),ar=r("start_index <= end_index"),ir=r("."),Ws=u(),ee=h("p"),rr=r("\u0110\u1EA7u ti\xEAn, h\xE3y t\xEDnh to\xE1n t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u ra c\xF3 th\u1EC3 c\xF3:"),Rs=u(),k(sn.$$.fragment),Xs=u(),at.c(),se=u(),M=h("p"),or=r("B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n l\u1EA5y ch\u1EC9 m\u1EE5c t\u1ED1i \u0111a. V\xEC PyTorch s\u1EBD tr\u1EA3 v\u1EC1 ch\u1EC9 m\u1EE5c trong tensor ph\u1EB3ng, ch\xFAng ta c\u1EA7n s\u1EED d\u1EE5ng ph\xE9p chia l\xE0m tr\xF2n xu\u1ED1ng "),ss=h("code"),lr=r("//"),cr=r(" v\xE0 l\u1EA5y d\u01B0 "),as=h("code"),hr=r("%"),mr=r(" \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c "),is=h("code"),pr=r("start_index"),ur=r(" v\xE0 "),rs=h("code"),fr=r("end_index"),dr=r(":"),Ys=u(),k(an.$$.fragment),Zs=u(),ae=h("p"),gr=r("Ch\xFAng ta ch\u01B0a xong \u0111\xE2u, nh\u01B0ng \xEDt nh\u1EA5t ch\xFAng ta \u0111\xE3 c\xF3 \u0111i\u1EC3m ch\xEDnh x\xE1c cho c\xE2u tr\u1EA3 l\u1EDDi (b\u1EA1n c\xF3 th\u1EC3 ki\u1EC3m tra \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch so s\xE1nh n\xF3 v\u1EDBi k\u1EBFt qu\u1EA3 \u0111\u1EA7u ti\xEAn trong ph\u1EA7n tr\u01B0\u1EDBc):"),ta=u(),k(rn.$$.fragment),na=u(),k(zt.$$.fragment),ea=u(),bt=h("p"),_r=r("Ta c\xF3 "),os=h("code"),br=r("start_index"),kr=r(" v\xE0 "),ls=h("code"),vr=r("end_index"),$r=r(" c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi theo token n\xEAn ta ch\u1EC9 c\u1EA7n chuy\u1EC3n \u0111\u1ED5i c\xE1c ch\u1EC9 m\u1EE5c k\xED t\u1EF1 trong ng\u1EEF c\u1EA3nh. \u0110\u1EA5y l\xE0 n\u01A1i offset s\u1EBD c\u1EF1c k\xEC h\u1EEFu \xEDch. Ta c\xF3 th\u1EC3 l\u1EA5y v\xE0 s\u1EED d\u1EE5ng ch\xFAng nh\u01B0 c\xE1ch ta l\xE0m trong t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i token:"),sa=u(),k(on.$$.fragment),aa=u(),ie=h("p"),yr=r("B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1ECBnh d\u1EA1ng m\u1ECDi th\u1EE9 \u0111\u1EC3 c\xF3 \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3:"),ia=u(),k(ln.$$.fragment),ra=u(),k(cn.$$.fragment),oa=u(),re=h("p"),xr=r("Tuy\u1EC7t qu\xE1! K\u1EBFt qu\u1EA3 \u0111\xF3 gi\u1ED1ng nh\u01B0 trong v\xED d\u1EE5 \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta!"),la=u(),k(Nt.$$.fragment),ca=u(),Ct=h("h2"),At=h("a"),cs=h("span"),k(hn.$$.fragment),wr=u(),hs=h("span"),jr=r("X\u1EED l\xFD c\xE1c ng\u1EEF c\u1EA3nh d\xE0i"),ha=u(),Dt=h("p"),qr=r("N\u1EBFu ch\xFAng ta c\u1ED1 g\u1EAFng tokenize c\xE1c c\xE2u h\u1ECFi v\xE0 ng\u1EEF c\u1EA3nh d\xE0i ta t\u1EEBng l\u1EA5y l\xE0m v\xED d\u1EE5 tr\u01B0\u1EDBc \u0111\xF3, ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c s\u1ED1 token nhi\u1EC1u h\u01A1n \u0111\u1ED9 d\xE0i t\u1ED1i da s\u1EED d\u1EE5ng trong pipeline "),ms=h("code"),Er=r("question-answering"),Cr=r(" (\u0111\xF3 l\xE0 384):"),ma=u(),k(mn.$$.fragment),pa=u(),k(pn.$$.fragment),ua=u(),Lt=h("p"),Pr=r("V\xEC v\u1EADy, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i c\u1EAFt b\u1EDBt \u0111\u1EA7u v\xE0o c\u1EE7a m\xECnh \u1EDF \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a \u0111\xF3. C\xF3 m\u1ED9t s\u1ED1 c\xE1ch ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y, nh\u01B0ng ch\xFAng ta kh\xF4ng mu\u1ED1n c\u1EAFt ng\u1EAFn c\xE2u h\u1ECFi, ch\u1EC9 c\u1EAFt b\u1ECF ng\u1EEF c\u1EA3nh. V\xEC ng\u1EEF c\u1EA3nh l\xE0 c\xE2u th\u1EE9 hai, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng chi\u1EBFn l\u01B0\u1EE3c c\u1EAFt ng\u1EAFn "),ps=h("code"),Tr=r('"only_second"'),Sr=r(". V\u1EA5n \u0111\u1EC1 n\u1EA3y sinh sau \u0111\xF3 l\xE0 c\xE2u tr\u1EA3 l\u1EDDi cho c\xE2u h\u1ECFi c\xF3 th\u1EC3 kh\xF4ng n\u1EB1m trong ng\u1EEF c\u1EA3nh \u0111\xE3 b\u1ECB c\u1EAFt ng\u1EAFn. V\xED d\u1EE5: \u1EDF \u0111\xE2y, ch\xFAng ta \u0111\xE3 ch\u1ECDn m\u1ED9t c\xE2u h\u1ECFi trong \u0111\xF3 c\xE2u tr\u1EA3 l\u1EDDi n\u1EB1m \u1EDF cu\u1ED1i ng\u1EEF c\u1EA3nh v\xE0 khi c\u1EAFt ng\u1EAFn c\xE2u tr\u1EA3 l\u1EDDi \u0111\xF3 th\xEC c\xE2u tr\u1EA3 l\u1EDDi kh\xF4ng c\xF2n:"),fa=u(),k(un.$$.fragment),da=u(),k(fn.$$.fragment),ga=u(),oe=h("p"),zr=r("\u0110i\u1EC1u n\xE0y c\xF3 ngh\u0129a l\xE0 m\xF4 h\xECnh s\u1EBD g\u1EB7p kh\xF3 kh\u0103n trong vi\u1EC7c ch\u1ECDn ra c\xE2u tr\u1EA3 l\u1EDDi ch\xEDnh x\xE1c. \u0110\u1EC3 kh\u1EAFc ph\u1EE5c \u0111i\u1EC1u n\xE0y, pipeline h\u1ECFi \u0111\xE1p cho ph\xE9p ch\xFAng ta chia ng\u1EEF c\u1EA3nh th\xE0nh c\xE1c ph\u1EA7n nh\u1ECF h\u01A1n, ch\u1EC9 \u0111\u1ECBnh \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a. \u0110\u1EC3 \u0111\u1EA3m b\u1EA3o r\u1EB1ng ch\xFAng ta kh\xF4ng chia b\u1ED1i c\u1EA3nh ch\xEDnh x\xE1c \u1EDF v\u1ECB tr\xED sai \u0111\u1EC3 c\xF3 th\u1EC3 t\xECm ra c\xE2u tr\u1EA3 l\u1EDDi, n\xF3 c\u0169ng bao g\u1ED3m m\u1ED9t s\u1ED1 ph\u1EA7n tr\xF9ng l\u1EB7p gi\u1EEFa c\xE1c ph\u1EA7n."),_a=u(),kt=h("p"),Nr=r("Ch\xFAng ta c\xF3 th\u1EC3 y\xEAu c\u1EA7u tokenizer (nhanh ho\u1EB7c ch\u1EADm) th\u1EF1c hi\u1EC7n vi\u1EC7c n\xE0y b\u1EB1ng c\xE1ch th\xEAm "),us=h("code"),Ar=r("return_overflowing_tokens=True"),Dr=r(" v\xE0 ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh s\u1EF1 giao thoa m\xE0 ta mu\u1ED1n qua than s\u1ED1 "),fs=h("code"),Lr=r("stride"),Or=r(". \u0110\xE2y l\xE0 m\u1ED9t v\xED d\u1EE5, s\u1EED d\u1EE5ng m\u1ED9t c\xE2u nh\u1ECF h\u01A1n:"),ba=u(),k(dn.$$.fragment),ka=u(),k(gn.$$.fragment),va=u(),Ot=h("p"),Fr=r("C\xF3 th\u1EC3 th\u1EA5y, c\xE2u \u0111\xE3 b\u1ECB chia th\xE0nh c\xE1c \u0111o\u1EA1n sao cho m\u1ED7i ph\u1EA7n trong "),ds=h("code"),Mr=r('inputs["input_ids"]'),Hr=r(" c\xF3 nhi\u1EC1u nh\u1EA5t 6 token (ta s\u1EBD c\u1EA7n th\xEAm \u0111\u1EC7m \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc) v\xE0 s\u1EBD c\xF3 s\u1EED giao thoa c\u1EE7a 2 token gi\u1EEFa c\xE1c ph\u1EA7n."),$a=u(),le=h("p"),Ir=r("H\xE3y c\xF9ng nh\xECn k\u0129 h\u01A1n v\xE0o k\u1EBFt qu\u1EA3 tokenize:"),ya=u(),k(_n.$$.fragment),xa=u(),k(bn.$$.fragment),wa=u(),Ft=h("p"),Vr=r("Nh\u01B0 d\u1EF1 \u0111o\xE1n, ta nh\u1EADn \u0111\u01B0\u1EE3c ID \u0111\u1EA7u v\xE0o v\xE0 attention mask.\u1EDE \u0111\xE2y, "),gs=h("code"),Gr=r("overflow_to_sample_mapping"),Br=r(" l\xE0 m\u1ED9t ph\xE9p \xE1nh x\u1EA1 cho ta bi\u1EBFt c\xE2u n\xE0o trong k\u1EBFt qu\u1EA3 li\xEAn quan \u2014 ta c\xF3 7 k\u1EBFt qu\u1EA3 d\u1EC1u t\u1EEB c\xE2u m\xE0 ta truy\u1EC1n v\xE0o tokenizer:"),ja=u(),k(kn.$$.fragment),qa=u(),k(vn.$$.fragment),Ea=u(),ce=h("p"),Ur=r("\u0110i\u1EC1u n\xE0y h\u1EEFu \xEDch h\u01A1n khi ta tokenize nhi\u1EC1u c\xE2u c\xF9ng nhau, V\xED d\u1EE5:"),Ca=u(),k($n.$$.fragment),Pa=u(),he=h("p"),Jr=r("tr\u1EA3 cho ta:"),Ta=u(),k(yn.$$.fragment),Sa=u(),me=h("p"),Qr=r("ngh\u0129a l\xE0 c\xE2u \u0111\u1EA7u ti\xEAn \u0111\u01B0\u1EE3c chia th\xE0nh 7 \u0111o\u1EA1n nh\u01B0 ph\u1EA7n ph\xEDa tr\u01B0\u1EDBc, v\xE0 4 \u0111o\u1EA1n ti\u1EBFp theo \u0111\u1EBFn t\u1EEB c\xE2u th\u1EE9 hai."),za=u(),V=h("p"),Kr=r("B\xE2y gi\u1EDD ch\xFAng ta h\xE3y c\xF9ng quay tr\u1EDF l\u1EA1i ng\u1EEF c\u1EA3nh d\xE0i. Theo m\u1EB7c \u0111\u1ECBnh, pipeline `"),_s=h("code"),Wr=r("question-answering"),Rr=r(" s\u1EED d\u1EE5ng \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a l\xE0 384, nh\u01B0 \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3 v\xE0 kho\u1EA3ng c\xE1ch 128, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1ch m\xF4 h\xECnh \u0111\u01B0\u1EE3c tinh ch\u1EC9nh (b\u1EA1n c\xF3 th\u1EC3 \u0111i\u1EC1u ch\u1EC9nh c\xE1c tham s\u1ED1 \u0111\xF3 b\u1EB1ng c\xE1ch truy\u1EC1n "),bs=h("code"),Xr=r("max_seq_len"),Yr=r(" v\xE0 "),ks=h("code"),Zr=r("stride"),to=r(" khi g\u1ECDi pipeline). Do \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c tham s\u1ED1 \u0111\xF3 khi tokenize. Ch\xFAng ta c\u0169ng s\u1EBD th\xEAm ph\u1EA7n \u0111\u1EC7m (\u0111\u1EC3 c\xF3 c\xE1c m\u1EABu c\xF3 c\xF9ng chi\u1EC1u d\xE0i, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c tensor) c\u0169ng nh\u01B0 y\xEAu c\u1EA7u c\xE1c offset:"),Na=u(),k(xn.$$.fragment),Aa=u(),G=h("p"),no=r("C\xE1c "),vs=h("code"),eo=r("inputs"),so=r(" s\u1EBD ch\u1EE9a c\xE1c ID \u0111\u1EA7u v\xE0o v\xE0 c\xE1c attention mask m\xE0 m\xF4 h\xECnh k\xEC v\u1ECDng, c\u0169ng nh\u01B0 offset v\xE0 "),$s=h("code"),ao=r("overflow_to_sample_mapping"),io=r(" ta v\u1EEBa trao \u0111\u1ED5i \u1EDF tr\xEAn. V\xEC hai tham s\u1ED1 \u0111\xF3 kh\xF4ng ph\u1EA3i l\xE0 tham s\u1ED1 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng b\u1EDFi m\xF4 h\xECnh, ch\xFAng ta s\u1EBD \u0111\u01B0a ch\xFAng ra kh\u1ECFi "),ys=h("code"),ro=r("inputs"),oo=r(" (v\xE0 kh\xF4ng l\u01B0u tr\u1EEF \xE1nh x\u1EA1, v\xEC n\xF3 kh\xF4ng h\u1EEFu \xEDch \u1EDF \u0111\xE2y) tr\u01B0\u1EDBc khi chuy\u1EC3n \u0111\u1ED5i n\xF3 th\xE0nh tensor:"),Da=u(),rt.c(),pe=u(),ue=h("p"),lo=r("B\u1ED1i c\u1EA3nh d\xE0i c\u1EE7a ch\xFAng ta \u0111\u01B0\u1EE3c chia l\xE0m hai, \u0111\u1ED3ng ngh\u0129a sau khi n\xF3 \u0111i qua m\xF4 h\xECnh, ch\xFAng ta s\u1EBD c\xF3 hai b\u1ED9 logit b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc:"),La=u(),k(wn.$$.fragment),Oa=u(),lt.c(),fe=u(),de=h("p"),co=r("Gi\u1ED1ng nh\u01B0 tr\u01B0\u1EDBc \u0111\xE2y, \u0111\u1EA7u ti\xEAn ch\xFAng ta che c\xE1c token kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ng\u1EEF c\u1EA3nh tr\u01B0\u1EDBc khi s\u1EED d\u1EE5ng softmax. Ch\xFAng ta c\u0169ng che t\u1EA5t c\u1EA3 c\xE1c token \u0111\u1EC7m (\u0111\u01B0\u1EE3c g\u1EAFn m\xE1c b\u1EDFi attention mask):"),Fa=u(),ht.c(),ge=u(),_e=h("p"),ho=r("Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng softmax \u0111\u1EC3 chuy\u1EC3n \u0111\u1ED5i c\xE1c logit c\u1EE7a ch\xFAng ta th\xE0nh x\xE1c su\u1EA5t:"),Ma=u(),pt.c(),be=u(),ke=h("p"),mo=r("B\u01B0\u1EDBc ti\u1EBFp theo t\u01B0\u01A1ng t\u1EF1 nh\u01B0 nh\u1EEFng g\xEC ch\xFAng ta \u0111\xE3 l\xE0m cho b\u1ED1i c\u1EA3nh nh\u1ECF, nh\u01B0ng ch\xFAng ta l\u1EB7p l\u1EA1i n\xF3 cho m\u1ED7i ph\u1EA7n trong hai ph\u1EA7n c\u1EE7a m\xECnh. Ch\xFAng ta t\xEDnh \u0111i\u1EC3m cho t\u1EA5t c\u1EA3 c\xE1c kho\u1EA3ng c\xE2u tr\u1EA3 l\u1EDDi c\xF3 th\u1EC3 c\xF3, sau \u0111\xF3 l\u1EA5y ph\u1EA7n c\xF3 \u0111i\u1EC3m t\u1ED1t nh\u1EA5t:"),Ha=u(),ft.c(),ve=u(),k(jn.$$.fragment),Ia=u(),$e=h("p"),po=r("Hai \u1EE9ng c\u1EED vi\xEAn \u0111\xF3 t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c c\xE2u tr\u1EA3 l\u1EDDi t\u1ED1t nh\u1EA5t m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 t\xECm th\u1EA5y trong m\u1ED7i \u0111o\u1EA1n. M\xF4 h\xECnh ch\u1EAFc ch\u1EAFn h\u01A1n r\u1EB1ng c\xE2u tr\u1EA3 l\u1EDDi \u0111\xFAng n\u1EB1m \u1EDF ph\u1EA7n th\u1EE9 hai (\u0111\xF3 l\xE0 m\u1ED9t d\u1EA5u hi\u1EC7u t\u1ED1t!). B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n \xE1nh x\u1EA1 kho\u1EA3ng hai token \u0111\xF3 v\u1EDBi kho\u1EA3ng c\xE1c k\xFD t\u1EF1 trong ng\u1EEF c\u1EA3nh (ch\xFAng ta ch\u1EC9 c\u1EA7n l\u1EADp \xE1nh x\u1EA1 c\xE1i th\u1EE9 hai \u0111\u1EC3 c\xF3 c\xE2u tr\u1EA3 l\u1EDDi, nh\u01B0ng th\u1EADt th\xFA v\u1ECB khi xem m\xF4 h\xECnh \u0111\xE3 ch\u1ECDn nh\u1EEFng g\xEC trong \u0111o\u1EA1n \u0111\u1EA7u ti\xEAn)."),Va=u(),k(Mt.$$.fragment),Ga=u(),qn=h("p"),xs=h("code"),uo=r("offsets"),fo=r(" m\xE0 ch\xFAng ta \u0111\xE3 n\u1EAFm \u0111\u01B0\u1EE3c tr\u01B0\u1EDBc \u0111\xF3 th\u1EF1c s\u1EF1 l\xE0 m\u1ED9t danh s\xE1ch c\xE1c offset, v\u1EDBi m\u1ED9t danh s\xE1ch tr\xEAn m\u1ED7i \u0111o\u1EA1n v\u0103n b\u1EA3n:"),Ba=u(),k(En.$$.fragment),Ua=u(),k(Cn.$$.fragment),Ja=u(),ye=h("p"),go=r("N\u1EBFu ch\xFAng ta b\u1ECF qua k\u1EBFt qu\u1EA3 \u0111\u1EA7u ti\xEAn, ch\xFAng ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u01B0\u01A1ng t\u1EF1 nh\u01B0 pipeline cho ng\u1EEF c\u1EA3nh d\xE0i n\xE0y - yayy!"),Qa=u(),k(Ht.$$.fragment),Ka=u(),xe=h("p"),_o=r("\u0110i\u1EC1u n\xE0y k\u1EBFt th\xFAc ph\u1EA7n \u0111i s\xE2u v\xE0o c\xE1c kh\u1EA3 n\u0103ng c\u1EE7a tokenizer. Ch\xFAng ta s\u1EBD \u0111\u01B0a t\u1EA5t c\u1EA3 nh\u1EEFng \u0111i\u1EC1u n\xE0y v\xE0o th\u1EF1c t\u1EBF m\u1ED9t l\u1EA7n n\u1EEFa trong ch\u01B0\u01A1ng ti\u1EBFp theo, khi ch\xFAng t\xF4i h\u01B0\u1EDBng d\u1EABn b\u1EA1n c\xE1ch tinh ch\u1EC9nh m\u1ED9t m\xF4 h\xECnh v\u1EC1 m\u1ED9t lo\u1EA1t c\xE1c t\xE1c v\u1EE5 NLP ph\u1ED5 bi\u1EBFn."),this.h()},l(t){const i=lc('[data-svelte="svelte-1phssyn"]',document.head);s=m(i,"META",{name:!0,content:!0}),i.forEach(e),c=f(t),v(n.$$.fragment,t),_=f(t),b=m(t,"H1",{class:!0});var In=p(b);w=m(In,"A",{id:!0,class:!0,href:!0});var we=p(w);E=m(we,"SPAN",{});var je=p(E);v(T.$$.fragment,je),je.forEach(e),we.forEach(e),z=f(In),P=m(In,"SPAN",{});var qe=p(P);L=o(qe,"Fast tokenizers in the QA pipeline"),qe.forEach(e),In.forEach(e),D=f(t),A.l(t),q=f(t),C=m(t,"P",{});var It=p(C);xt=o(It,"Gi\u1EDD ch\xFAng ta s\u1EBD \u0111i s\xE2u v\xE0o pipeline "),J=m(It,"CODE",{});var Ee=p(J);wt=o(Ee,"question-answering"),Ee.forEach(e),ii=o(It," v\xE0 xem c\xE1ch t\u1EADn d\u1EE5ng c\xE1c offset \u0111\u1EC3 l\u1EA5y c\xE2u tr\u1EA3 l\u1EDDi cho c\xE1c c\xE2u h\u1ECFi d\u1EF1a theo t\u1EEB ng\u1EEF c\u1EA3nh, gi\u1ED1ng nh\u01B0 ch\xFAng ta \u0111\xE3 l\xE0m v\u1EDBi c\xE1c th\u1EF1c th\u1EC3 \u0111\u01B0\u1EE3c nh\xF3m trong ph\u1EA7n tr\u01B0\u1EDBc. Sau \u0111\xF3, ch\xFAng ta s\u1EBD xem l\xE0m th\u1EBF n\xE0o c\xF3 th\u1EC3 \u0111\u1ED1i ph\xF3 v\u1EDBi nh\u1EEFng ng\u1EEF c\u1EA3nh r\u1EA5t d\xE0i m\xE0 cu\u1ED1i c\xF9ng l\u1EA1i b\u1ECB c\u1EAFt b\u1EDBt. B\u1EA1n c\xF3 th\u1EC3 b\u1ECF qua ph\u1EA7n n\xE0y n\u1EBFu kh\xF4ng quan t\xE2m \u0111\u1EBFn t\xE1c v\u1EE5 h\u1ECFi \u0111\xE1p."),It.forEach(e),qs=f(t),K.l(t),Gn=f(t),jt=m(t,"H2",{class:!0});var Vt=p(jt);Pt=m(Vt,"A",{id:!0,class:!0,href:!0});var Ce=p(Pt);Oe=m(Ce,"SPAN",{});var ws=p(Oe);v(Qt.$$.fragment,ws),ws.forEach(e),Ce.forEach(e),ri=f(Vt),Bn=m(Vt,"SPAN",{});var Pe=p(Bn);oi=o(Pe,"S\u1EED d\u1EE5ng pipeline "),Fe=m(Pe,"CODE",{});var Te=p(Fe);li=o(Te,"question-answering"),Te.forEach(e),Pe.forEach(e),Vt.forEach(e),Es=f(t),gt=m(t,"P",{});var vt=p(gt);ci=o(vt,"Nh\u01B0 \u0111\xE3 th\u1EA5y trong "),Un=m(vt,"A",{href:!0});var Se=p(Un);hi=o(Se,"Ch\u01B0\u01A1ng 1"),Se.forEach(e),mi=o(vt,", ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng pipeline "),Me=m(vt,"CODE",{});var ze=p(Me);pi=o(ze,"question-answering"),ze.forEach(e),ui=o(vt," nh\u01B0 sau \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c c\xE2u tr\u1EA3 l\u1EDDi cho c\xE2u h\u1ECFi:"),vt.forEach(e),Cs=f(t),v(Kt.$$.fragment,t),Ps=f(t),v(Wt.$$.fragment,t),Ts=f(t),Jn=m(t,"P",{});var Ne=p(Jn);fi=o(Ne,"Kh\xF4ng nh\u01B0 c\xE1c pipeline kh\xE1c kh\xF4ng th\u1EC3 c\u1EAFt g\u1ECDn v\xE0 chia v\u0103n b\u1EA3n d\xE0i h\u01A1n \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a cho ph\xE9p c\u1EE7a m\xF4 h\xECnh (d\u1EABn \u0111\u1EBFn b\u1ECF l\u1EE1 nh\u1EEFng th\xF4ng tin \u1EDF ph\u1EA7n cu\u1ED1i v\u0103n b\u1EA3n), pipeline n\xE0y c\xF3 th\u1EC3 x\u1EED l\xFD t\u1ED1t v\u1EDBi nh\u1EEFng ng\u1EEF c\u1EA3nh d\xE0i v\xE0 s\u1EBD tr\u1EA3 v\u1EC1 c\xE2u tr\u1EA3 l\u1EDDi k\u1EC3 c\u1EA3 khi n\xF3 n\u1EB1m \u1EDF cu\u1ED1i v\u0103n b\u1EA3n:"),Ne.forEach(e),Ss=f(t),v(Rt.$$.fragment,t),zs=f(t),v(Xt.$$.fragment,t),Ns=f(t),Qn=m(t,"P",{});var js=p(Qn);di=o(js,"H\xE3y c\xF9ng nhau xem n\xF3 l\xE0m th\u1EBF n\xE0o!"),js.forEach(e),As=f(t),qt=m(t,"H2",{class:!0});var Vn=p(qt);Tt=m(Vn,"A",{id:!0,class:!0,href:!0});var Uo=p(Tt);He=m(Uo,"SPAN",{});var Jo=p(He);v(Yt.$$.fragment,Jo),Jo.forEach(e),Uo.forEach(e),gi=f(Vn),Ie=m(Vn,"SPAN",{});var Qo=p(Ie);_i=o(Qo,"S\u1EED d\u1EE5ng m\xF4 h\xECnh cho t\xE1c v\u1EE5 h\u1ECFi \u0111\xE1p"),Qo.forEach(e),Vn.forEach(e),Ds=f(t),H=m(t,"P",{});var Gt=p(H);bi=o(Gt,"Nh\u01B0 nh\u1EEFng pipeline kh\xE1c, ta s\u1EBD b\u1EAFt \u0111\u1EA7u v\u1EDBi vi\u1EC7c tokenize \u0111\u1EA7u v\xE0o v\xE0 sau \u0111\xF3 truy\u1EC1n ch\xFAng v\xE0o trong m\xF4 h\xECnh. M\u1EB7c \u0111\u1ECBnh checkpoint \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng cho pipeline "),Ve=m(Gt,"CODE",{});var Ko=p(Ve);ki=o(Ko,"question-answering"),Ko.forEach(e),vi=o(Gt," l\xE0 "),Zt=m(Gt,"A",{href:!0,rel:!0});var Wo=p(Zt);Ge=m(Wo,"CODE",{});var Ro=p(Ge);$i=o(Ro,"distilbert-base-cased-distilled-squad"),Ro.forEach(e),Wo.forEach(e),yi=o(Gt," ( \u201Csquad\u201D trong t\xEAn b\u1EAFt ngu\u1ED3n t\u1EEB b\u1ED9 d\u1EEF li\u1EC7u m\xE0 m\xF4 h\xECnh s\u1EED d\u1EE5ng \u0111\u1EC3 tinh ch\u1EC9nh; ta s\u1EBD n\xF3i s\xE2u h\u01A1n v\u1EC1 b\u1ED9 d\u1EEF li\u1EC7u SQuAD n\xE0y \u1EDF  "),Kn=m(Gt,"A",{href:!0});var Xo=p(Kn);xi=o(Xo,"Ch\u01B0\u01A1ng 7"),Xo.forEach(e),wi=o(Gt,"):"),Gt.forEach(e),Ls=f(t),R.l(t),Wn=f(t),Rn=m(t,"P",{});var Yo=p(Rn);ji=o(Yo,"L\u01B0u \xFD r\u1EB1ng ch\xFAng ta tokenize c\xE2u h\u1ECFi v\xE0 ng\u1EEF c\u1EA3nh nh\u01B0 m\u1ED9t c\u1EB7p, v\u1EDBi c\xE2u h\u1ECFi \u0111\u1EE9ng tr\u01B0\u1EDBc."),Yo.forEach(e),Os=f(t),Et=m(t,"DIV",{class:!0});var Ra=p(Et);tn=m(Ra,"IMG",{class:!0,src:!0,alt:!0}),qi=f(Ra),nn=m(Ra,"IMG",{class:!0,src:!0,alt:!0}),Ra.forEach(e),Fs=f(t),Xn=m(t,"P",{});var Zo=p(Xn);Ei=o(Zo,"C\xE1c m\xF4 h\xECnh h\u1ECFi \u0111\xE1p ho\u1EA1t \u0111\u1ED9ng h\u01A1i kh\xE1c so v\u1EDBi c\xE1c m\xF4 h\xECnh m\xE0 ta \u0111\xE3 th\u1EA5y cho \u0111\u1EBFn nay. S\u1EED d\u1EE5ng h\xECnh tr\xEAn l\xE0m v\xED d\u1EE5, m\xF4 h\xECnh \u0111\xE3 \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n \u0111\u1EC3 d\u1EF1 \u0111o\xE1n ch\u1EC9 m\u1EE5c c\u1EE7a token b\u1EAFt \u0111\u1EA7u c\xE2u tr\u1EA3 l\u1EDDi (\u1EDF \u0111\xE2y l\xE0 21) v\xE0 ch\u1EC9 m\u1EE5c c\u1EE7a token n\u01A1i c\xE2u tr\u1EA3 l\u1EDDi k\u1EBFt th\xFAc (\u1EDF \u0111\xE2y l\xE0 24). \u0110\xE2y l\xE0 l\xFD do t\u1EA1i sao c\xE1c m\xF4 h\xECnh \u0111\xF3 kh\xF4ng tr\u1EA3 v\u1EC1 m\u1ED9t tensor logit m\xE0 l\xE0 hai: m\u1ED9t cho c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi token b\u1EAFt \u0111\u1EA7u c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi v\xE0 m\u1ED9t cho c\xE1c c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi token k\u1EBFt th\xFAc c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi. V\xEC trong tr\u01B0\u1EDDng h\u1EE3p n\xE0y, ch\xFAng ta ch\u1EC9 c\xF3 m\u1ED9t \u0111\u1EA7u v\xE0o ch\u1EE9a 66 token, ta nh\u1EADn \u0111\u01B0\u1EE3c:"),Zo.forEach(e),Ms=f(t),v(en.$$.fragment,t),Hs=f(t),Y.l(t),Yn=f(t),I=m(t,"P",{});var Bt=p(I);Ci=o(Bt,"\u0110\u1EC3 chuy\u1EC3n \u0111\u1ED5i c\xE1c logit \u0111\xF3 th\xE0nh x\xE1c su\u1EA5t, ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng m\u1ED9t h\xE0m softmax - nh\u01B0ng tr\u01B0\u1EDBc \u0111\xF3, ch\xFAng ta c\u1EA7n \u0111\u1EA3m b\u1EA3o r\u1EB1ng ch\xFAng ta che d\u1EA5u c\xE1c ch\u1EC9 m\u1EE5c kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ng\u1EEF c\u1EA3nh. \u0110\u1EA7u v\xE0o c\u1EE7a ch\xFAng t\xF4i l\xE0 "),Be=m(Bt,"CODE",{});var tl=p(Be);Pi=o(tl,"[CLS] question [SEP] context [SEP]"),tl.forEach(e),Ti=o(Bt,", v\xEC v\u1EADy ch\xFAng ta c\u1EA7n che d\u1EA5u c\xE1c token c\u1EE7a c\xE2u h\u1ECFi c\u0169ng nh\u01B0 token "),Ue=m(Bt,"CODE",{});var nl=p(Ue);Si=o(nl,"[SEP]"),nl.forEach(e),zi=o(Bt,". Tuy nhi\xEAn, ch\xFAng ta s\u1EBD gi\u1EEF token "),Je=m(Bt,"CODE",{});var el=p(Je);Ni=o(el,"[CLS]"),el.forEach(e),Ai=o(Bt," v\xEC m\u1ED9t s\u1ED1 m\xF4 h\xECnh s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 ch\u1EC9 ra r\u1EB1ng c\xE2u tr\u1EA3 l\u1EDDi kh\xF4ng n\u1EB1m trong ng\u1EEF c\u1EA3nh."),Bt.forEach(e),Is=f(t),St=m(t,"P",{});var Xa=p(St);Di=o(Xa,"V\xEC ch\xFAng ta s\u1EBD \xE1p d\u1EE5ng softmax sau \u0111\xF3, ch\xFAng ta ch\u1EC9 c\u1EA7n thay th\u1EBF c\xE1c logit mu\u1ED1n che b\u1EB1ng m\u1ED9t s\u1ED1 \xE2m l\u1EDBn. \u1EDE \u0111\xE2y, ch\xFAng ta s\u1EED d\u1EE5ng "),Qe=m(Xa,"CODE",{});var sl=p(Qe);Li=o(sl,"-10000"),sl.forEach(e),Oi=o(Xa,":"),Xa.forEach(e),Vs=f(t),tt.l(t),Zn=f(t),te=m(t,"P",{});var al=p(te);Fi=o(al,"Gi\u1EDD ch\xFAng ta \u0111\xE3 che c\xE1c logit t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c v\u1ECB tr\xED m\xE0 ch\xFAng ta kh\xF4ng mu\u1ED1n d\u1EF1 \u0111o\xE1n, ch\xFAng ta c\xF3 th\u1EC3 \xE1p d\u1EE5ng softmax:"),al.forEach(e),Gs=f(t),et.l(t),ne=f(t),F=m(t,"P",{});var $t=p(F);Mi=o($t,"\u1EDE giai \u0111o\u1EA1n n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 l\u1EA5y argmax x\xE1c su\u1EA5t b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc - nh\u01B0ng ch\xFAng ta c\xF3 th\u1EC3 k\u1EBFt th\xFAc v\u1EDBi ch\u1EC9 m\u1EE5c b\u1EAFt \u0111\u1EA7u l\u1EDBn h\u01A1n k\u1EBFt th\xFAc, v\xEC v\u1EADy ch\xFAng ta c\u1EA7n th\u1EF1c hi\u1EC7n th\xEAm m\u1ED9t s\u1ED1 bi\u1EC7n ph\xE1p ph\xF2ng ng\u1EEBa. Ch\xFAng ta s\u1EBD t\xEDnh to\xE1n x\xE1c su\u1EA5t c\u1EE7a t\u1EEBng "),Ke=m($t,"CODE",{});var il=p(Ke);Hi=o(il,"start_index"),il.forEach(e),Ii=o($t," v\xE0 "),We=m($t,"CODE",{});var rl=p(We);Vi=o(rl,"end_index"),rl.forEach(e),Gi=o($t," c\xF3 th\u1EC3 trong \u0111\xF3 "),Re=m($t,"CODE",{});var ol=p(Re);Bi=o(ol,"start_index <= end_index"),ol.forEach(e),Ui=o($t,", sau \u0111\xF3 l\u1EA5y "),Xe=m($t,"CODE",{});var ll=p(Xe);Ji=o(ll,"(start_index, end_index)"),ll.forEach(e),Qi=o($t," v\u1EDBi x\xE1c su\u1EA5t cao nh\u1EA5t."),$t.forEach(e),Bs=f(t),O=m(t,"P",{});var dt=p(O);Ki=o(dt,"Gi\u1EA3 s\u1EED c\xE1c s\u1EF1 ki\u1EC7n \u201CC\xE2u tr\u1EA3 l\u1EDDi b\u1EAFt \u0111\u1EA7u \u1EDF "),Ye=m(dt,"CODE",{});var cl=p(Ye);Wi=o(cl,"start_index"),cl.forEach(e),Ri=o(dt,"\u201D v\xE0 \u201CC\xE2u tr\u1EA3 l\u1EDDi k\u1EBFt th\xFAc \u1EDF "),Ze=m(dt,"CODE",{});var hl=p(Ze);Xi=o(hl,"end_index"),hl.forEach(e),Yi=o(dt,"\u201D l\xE0 \u0111\u1ED9c l\u1EADp, x\xE1c su\u1EA5t \u0111\u1EC3 c\xE2u tr\u1EA3 l\u1EDDi b\u1EAFt \u0111\u1EA7u t\u1EA1i "),ts=m(dt,"CODE",{});var ml=p(ts);Zi=o(ml,"start_index"),ml.forEach(e),tr=o(dt," v\xE0 k\u1EBFt th\xFAc t\u1EA1i "),ns=m(dt,"CODE",{});var pl=p(ns);nr=o(pl,"end_index"),pl.forEach(e),er=o(dt,` l\xE0:
`),Us=Zl(dt),dt.forEach(e),Js=f(t),_t=m(t,"P",{});var Ae=p(_t);sr=o(Ae,"V\xEC v\u1EADy, \u0111\u1EC3 t\xEDnh t\u1EA5t c\u1EA3 c\xE1c \u0111i\u1EC3m, ch\xFAng ta ch\u1EC9 c\u1EA7n t\xEDnh t\xEDch "),Qs=Zl(Ae),Ks=o(Ae," v\u1EDBi "),es=m(Ae,"CODE",{});var ul=p(es);ar=o(ul,"start_index <= end_index"),ul.forEach(e),ir=o(Ae,"."),Ae.forEach(e),Ws=f(t),ee=m(t,"P",{});var fl=p(ee);rr=o(fl,"\u0110\u1EA7u ti\xEAn, h\xE3y t\xEDnh to\xE1n t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u ra c\xF3 th\u1EC3 c\xF3:"),fl.forEach(e),Rs=f(t),v(sn.$$.fragment,t),Xs=f(t),at.l(t),se=f(t),M=m(t,"P",{});var yt=p(M);or=o(yt,"B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n l\u1EA5y ch\u1EC9 m\u1EE5c t\u1ED1i \u0111a. V\xEC PyTorch s\u1EBD tr\u1EA3 v\u1EC1 ch\u1EC9 m\u1EE5c trong tensor ph\u1EB3ng, ch\xFAng ta c\u1EA7n s\u1EED d\u1EE5ng ph\xE9p chia l\xE0m tr\xF2n xu\u1ED1ng "),ss=m(yt,"CODE",{});var dl=p(ss);lr=o(dl,"//"),dl.forEach(e),cr=o(yt," v\xE0 l\u1EA5y d\u01B0 "),as=m(yt,"CODE",{});var gl=p(as);hr=o(gl,"%"),gl.forEach(e),mr=o(yt," \u0111\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c "),is=m(yt,"CODE",{});var _l=p(is);pr=o(_l,"start_index"),_l.forEach(e),ur=o(yt," v\xE0 "),rs=m(yt,"CODE",{});var bl=p(rs);fr=o(bl,"end_index"),bl.forEach(e),dr=o(yt,":"),yt.forEach(e),Ys=f(t),v(an.$$.fragment,t),Zs=f(t),ae=m(t,"P",{});var kl=p(ae);gr=o(kl,"Ch\xFAng ta ch\u01B0a xong \u0111\xE2u, nh\u01B0ng \xEDt nh\u1EA5t ch\xFAng ta \u0111\xE3 c\xF3 \u0111i\u1EC3m ch\xEDnh x\xE1c cho c\xE2u tr\u1EA3 l\u1EDDi (b\u1EA1n c\xF3 th\u1EC3 ki\u1EC3m tra \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch so s\xE1nh n\xF3 v\u1EDBi k\u1EBFt qu\u1EA3 \u0111\u1EA7u ti\xEAn trong ph\u1EA7n tr\u01B0\u1EDBc):"),kl.forEach(e),ta=f(t),v(rn.$$.fragment,t),na=f(t),v(zt.$$.fragment,t),ea=f(t),bt=m(t,"P",{});var De=p(bt);_r=o(De,"Ta c\xF3 "),os=m(De,"CODE",{});var vl=p(os);br=o(vl,"start_index"),vl.forEach(e),kr=o(De," v\xE0 "),ls=m(De,"CODE",{});var $l=p(ls);vr=o($l,"end_index"),$l.forEach(e),$r=o(De," c\u1EE7a c\xE2u tr\u1EA3 l\u1EDDi theo token n\xEAn ta ch\u1EC9 c\u1EA7n chuy\u1EC3n \u0111\u1ED5i c\xE1c ch\u1EC9 m\u1EE5c k\xED t\u1EF1 trong ng\u1EEF c\u1EA3nh. \u0110\u1EA5y l\xE0 n\u01A1i offset s\u1EBD c\u1EF1c k\xEC h\u1EEFu \xEDch. Ta c\xF3 th\u1EC3 l\u1EA5y v\xE0 s\u1EED d\u1EE5ng ch\xFAng nh\u01B0 c\xE1ch ta l\xE0m trong t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i token:"),De.forEach(e),sa=f(t),v(on.$$.fragment,t),aa=f(t),ie=m(t,"P",{});var yl=p(ie);yr=o(yl,"B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1ECBnh d\u1EA1ng m\u1ECDi th\u1EE9 \u0111\u1EC3 c\xF3 \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3:"),yl.forEach(e),ia=f(t),v(ln.$$.fragment,t),ra=f(t),v(cn.$$.fragment,t),oa=f(t),re=m(t,"P",{});var xl=p(re);xr=o(xl,"Tuy\u1EC7t qu\xE1! K\u1EBFt qu\u1EA3 \u0111\xF3 gi\u1ED1ng nh\u01B0 trong v\xED d\u1EE5 \u0111\u1EA7u ti\xEAn c\u1EE7a ch\xFAng ta!"),xl.forEach(e),la=f(t),v(Nt.$$.fragment,t),ca=f(t),Ct=m(t,"H2",{class:!0});var Ya=p(Ct);At=m(Ya,"A",{id:!0,class:!0,href:!0});var wl=p(At);cs=m(wl,"SPAN",{});var jl=p(cs);v(hn.$$.fragment,jl),jl.forEach(e),wl.forEach(e),wr=f(Ya),hs=m(Ya,"SPAN",{});var ql=p(hs);jr=o(ql,"X\u1EED l\xFD c\xE1c ng\u1EEF c\u1EA3nh d\xE0i"),ql.forEach(e),Ya.forEach(e),ha=f(t),Dt=m(t,"P",{});var Za=p(Dt);qr=o(Za,"N\u1EBFu ch\xFAng ta c\u1ED1 g\u1EAFng tokenize c\xE1c c\xE2u h\u1ECFi v\xE0 ng\u1EEF c\u1EA3nh d\xE0i ta t\u1EEBng l\u1EA5y l\xE0m v\xED d\u1EE5 tr\u01B0\u1EDBc \u0111\xF3, ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c s\u1ED1 token nhi\u1EC1u h\u01A1n \u0111\u1ED9 d\xE0i t\u1ED1i da s\u1EED d\u1EE5ng trong pipeline "),ms=m(Za,"CODE",{});var El=p(ms);Er=o(El,"question-answering"),El.forEach(e),Cr=o(Za," (\u0111\xF3 l\xE0 384):"),Za.forEach(e),ma=f(t),v(mn.$$.fragment,t),pa=f(t),v(pn.$$.fragment,t),ua=f(t),Lt=m(t,"P",{});var ti=p(Lt);Pr=o(ti,"V\xEC v\u1EADy, ch\xFAng ta s\u1EBD c\u1EA7n ph\u1EA3i c\u1EAFt b\u1EDBt \u0111\u1EA7u v\xE0o c\u1EE7a m\xECnh \u1EDF \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a \u0111\xF3. C\xF3 m\u1ED9t s\u1ED1 c\xE1ch ta c\xF3 th\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y, nh\u01B0ng ch\xFAng ta kh\xF4ng mu\u1ED1n c\u1EAFt ng\u1EAFn c\xE2u h\u1ECFi, ch\u1EC9 c\u1EAFt b\u1ECF ng\u1EEF c\u1EA3nh. V\xEC ng\u1EEF c\u1EA3nh l\xE0 c\xE2u th\u1EE9 hai, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng chi\u1EBFn l\u01B0\u1EE3c c\u1EAFt ng\u1EAFn "),ps=m(ti,"CODE",{});var Cl=p(ps);Tr=o(Cl,'"only_second"'),Cl.forEach(e),Sr=o(ti,". V\u1EA5n \u0111\u1EC1 n\u1EA3y sinh sau \u0111\xF3 l\xE0 c\xE2u tr\u1EA3 l\u1EDDi cho c\xE2u h\u1ECFi c\xF3 th\u1EC3 kh\xF4ng n\u1EB1m trong ng\u1EEF c\u1EA3nh \u0111\xE3 b\u1ECB c\u1EAFt ng\u1EAFn. V\xED d\u1EE5: \u1EDF \u0111\xE2y, ch\xFAng ta \u0111\xE3 ch\u1ECDn m\u1ED9t c\xE2u h\u1ECFi trong \u0111\xF3 c\xE2u tr\u1EA3 l\u1EDDi n\u1EB1m \u1EDF cu\u1ED1i ng\u1EEF c\u1EA3nh v\xE0 khi c\u1EAFt ng\u1EAFn c\xE2u tr\u1EA3 l\u1EDDi \u0111\xF3 th\xEC c\xE2u tr\u1EA3 l\u1EDDi kh\xF4ng c\xF2n:"),ti.forEach(e),fa=f(t),v(un.$$.fragment,t),da=f(t),v(fn.$$.fragment,t),ga=f(t),oe=m(t,"P",{});var Pl=p(oe);zr=o(Pl,"\u0110i\u1EC1u n\xE0y c\xF3 ngh\u0129a l\xE0 m\xF4 h\xECnh s\u1EBD g\u1EB7p kh\xF3 kh\u0103n trong vi\u1EC7c ch\u1ECDn ra c\xE2u tr\u1EA3 l\u1EDDi ch\xEDnh x\xE1c. \u0110\u1EC3 kh\u1EAFc ph\u1EE5c \u0111i\u1EC1u n\xE0y, pipeline h\u1ECFi \u0111\xE1p cho ph\xE9p ch\xFAng ta chia ng\u1EEF c\u1EA3nh th\xE0nh c\xE1c ph\u1EA7n nh\u1ECF h\u01A1n, ch\u1EC9 \u0111\u1ECBnh \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a. \u0110\u1EC3 \u0111\u1EA3m b\u1EA3o r\u1EB1ng ch\xFAng ta kh\xF4ng chia b\u1ED1i c\u1EA3nh ch\xEDnh x\xE1c \u1EDF v\u1ECB tr\xED sai \u0111\u1EC3 c\xF3 th\u1EC3 t\xECm ra c\xE2u tr\u1EA3 l\u1EDDi, n\xF3 c\u0169ng bao g\u1ED3m m\u1ED9t s\u1ED1 ph\u1EA7n tr\xF9ng l\u1EB7p gi\u1EEFa c\xE1c ph\u1EA7n."),Pl.forEach(e),_a=f(t),kt=m(t,"P",{});var Le=p(kt);Nr=o(Le,"Ch\xFAng ta c\xF3 th\u1EC3 y\xEAu c\u1EA7u tokenizer (nhanh ho\u1EB7c ch\u1EADm) th\u1EF1c hi\u1EC7n vi\u1EC7c n\xE0y b\u1EB1ng c\xE1ch th\xEAm "),us=m(Le,"CODE",{});var Tl=p(us);Ar=o(Tl,"return_overflowing_tokens=True"),Tl.forEach(e),Dr=o(Le," v\xE0 ta c\xF3 th\u1EC3 ch\u1EC9 \u0111\u1ECBnh s\u1EF1 giao thoa m\xE0 ta mu\u1ED1n qua than s\u1ED1 "),fs=m(Le,"CODE",{});var Sl=p(fs);Lr=o(Sl,"stride"),Sl.forEach(e),Or=o(Le,". \u0110\xE2y l\xE0 m\u1ED9t v\xED d\u1EE5, s\u1EED d\u1EE5ng m\u1ED9t c\xE2u nh\u1ECF h\u01A1n:"),Le.forEach(e),ba=f(t),v(dn.$$.fragment,t),ka=f(t),v(gn.$$.fragment,t),va=f(t),Ot=m(t,"P",{});var ni=p(Ot);Fr=o(ni,"C\xF3 th\u1EC3 th\u1EA5y, c\xE2u \u0111\xE3 b\u1ECB chia th\xE0nh c\xE1c \u0111o\u1EA1n sao cho m\u1ED7i ph\u1EA7n trong "),ds=m(ni,"CODE",{});var zl=p(ds);Mr=o(zl,'inputs["input_ids"]'),zl.forEach(e),Hr=o(ni," c\xF3 nhi\u1EC1u nh\u1EA5t 6 token (ta s\u1EBD c\u1EA7n th\xEAm \u0111\u1EC7m \u0111\u1EC3 \u0111\u1EA3m b\u1EA3o ch\xFAng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc) v\xE0 s\u1EBD c\xF3 s\u1EED giao thoa c\u1EE7a 2 token gi\u1EEFa c\xE1c ph\u1EA7n."),ni.forEach(e),$a=f(t),le=m(t,"P",{});var Nl=p(le);Ir=o(Nl,"H\xE3y c\xF9ng nh\xECn k\u0129 h\u01A1n v\xE0o k\u1EBFt qu\u1EA3 tokenize:"),Nl.forEach(e),ya=f(t),v(_n.$$.fragment,t),xa=f(t),v(bn.$$.fragment,t),wa=f(t),Ft=m(t,"P",{});var ei=p(Ft);Vr=o(ei,"Nh\u01B0 d\u1EF1 \u0111o\xE1n, ta nh\u1EADn \u0111\u01B0\u1EE3c ID \u0111\u1EA7u v\xE0o v\xE0 attention mask.\u1EDE \u0111\xE2y, "),gs=m(ei,"CODE",{});var Al=p(gs);Gr=o(Al,"overflow_to_sample_mapping"),Al.forEach(e),Br=o(ei," l\xE0 m\u1ED9t ph\xE9p \xE1nh x\u1EA1 cho ta bi\u1EBFt c\xE2u n\xE0o trong k\u1EBFt qu\u1EA3 li\xEAn quan \u2014 ta c\xF3 7 k\u1EBFt qu\u1EA3 d\u1EC1u t\u1EEB c\xE2u m\xE0 ta truy\u1EC1n v\xE0o tokenizer:"),ei.forEach(e),ja=f(t),v(kn.$$.fragment,t),qa=f(t),v(vn.$$.fragment,t),Ea=f(t),ce=m(t,"P",{});var Dl=p(ce);Ur=o(Dl,"\u0110i\u1EC1u n\xE0y h\u1EEFu \xEDch h\u01A1n khi ta tokenize nhi\u1EC1u c\xE2u c\xF9ng nhau, V\xED d\u1EE5:"),Dl.forEach(e),Ca=f(t),v($n.$$.fragment,t),Pa=f(t),he=m(t,"P",{});var Ll=p(he);Jr=o(Ll,"tr\u1EA3 cho ta:"),Ll.forEach(e),Ta=f(t),v(yn.$$.fragment,t),Sa=f(t),me=m(t,"P",{});var Ol=p(me);Qr=o(Ol,"ngh\u0129a l\xE0 c\xE2u \u0111\u1EA7u ti\xEAn \u0111\u01B0\u1EE3c chia th\xE0nh 7 \u0111o\u1EA1n nh\u01B0 ph\u1EA7n ph\xEDa tr\u01B0\u1EDBc, v\xE0 4 \u0111o\u1EA1n ti\u1EBFp theo \u0111\u1EBFn t\u1EEB c\xE2u th\u1EE9 hai."),Ol.forEach(e),za=f(t),V=m(t,"P",{});var Ut=p(V);Kr=o(Ut,"B\xE2y gi\u1EDD ch\xFAng ta h\xE3y c\xF9ng quay tr\u1EDF l\u1EA1i ng\u1EEF c\u1EA3nh d\xE0i. Theo m\u1EB7c \u0111\u1ECBnh, pipeline `"),_s=m(Ut,"CODE",{});var Fl=p(_s);Wr=o(Fl,"question-answering"),Fl.forEach(e),Rr=o(Ut," s\u1EED d\u1EE5ng \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a l\xE0 384, nh\u01B0 \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3 v\xE0 kho\u1EA3ng c\xE1ch 128, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1ch m\xF4 h\xECnh \u0111\u01B0\u1EE3c tinh ch\u1EC9nh (b\u1EA1n c\xF3 th\u1EC3 \u0111i\u1EC1u ch\u1EC9nh c\xE1c tham s\u1ED1 \u0111\xF3 b\u1EB1ng c\xE1ch truy\u1EC1n "),bs=m(Ut,"CODE",{});var Ml=p(bs);Xr=o(Ml,"max_seq_len"),Ml.forEach(e),Yr=o(Ut," v\xE0 "),ks=m(Ut,"CODE",{});var Hl=p(ks);Zr=o(Hl,"stride"),Hl.forEach(e),to=o(Ut," khi g\u1ECDi pipeline). Do \u0111\xF3, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c tham s\u1ED1 \u0111\xF3 khi tokenize. Ch\xFAng ta c\u0169ng s\u1EBD th\xEAm ph\u1EA7n \u0111\u1EC7m (\u0111\u1EC3 c\xF3 c\xE1c m\u1EABu c\xF3 c\xF9ng chi\u1EC1u d\xE0i, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 t\u1EA1o ra c\xE1c tensor) c\u0169ng nh\u01B0 y\xEAu c\u1EA7u c\xE1c offset:"),Ut.forEach(e),Na=f(t),v(xn.$$.fragment,t),Aa=f(t),G=m(t,"P",{});var Jt=p(G);no=o(Jt,"C\xE1c "),vs=m(Jt,"CODE",{});var Il=p(vs);eo=o(Il,"inputs"),Il.forEach(e),so=o(Jt," s\u1EBD ch\u1EE9a c\xE1c ID \u0111\u1EA7u v\xE0o v\xE0 c\xE1c attention mask m\xE0 m\xF4 h\xECnh k\xEC v\u1ECDng, c\u0169ng nh\u01B0 offset v\xE0 "),$s=m(Jt,"CODE",{});var Vl=p($s);ao=o(Vl,"overflow_to_sample_mapping"),Vl.forEach(e),io=o(Jt," ta v\u1EEBa trao \u0111\u1ED5i \u1EDF tr\xEAn. V\xEC hai tham s\u1ED1 \u0111\xF3 kh\xF4ng ph\u1EA3i l\xE0 tham s\u1ED1 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng b\u1EDFi m\xF4 h\xECnh, ch\xFAng ta s\u1EBD \u0111\u01B0a ch\xFAng ra kh\u1ECFi "),ys=m(Jt,"CODE",{});var Gl=p(ys);ro=o(Gl,"inputs"),Gl.forEach(e),oo=o(Jt," (v\xE0 kh\xF4ng l\u01B0u tr\u1EEF \xE1nh x\u1EA1, v\xEC n\xF3 kh\xF4ng h\u1EEFu \xEDch \u1EDF \u0111\xE2y) tr\u01B0\u1EDBc khi chuy\u1EC3n \u0111\u1ED5i n\xF3 th\xE0nh tensor:"),Jt.forEach(e),Da=f(t),rt.l(t),pe=f(t),ue=m(t,"P",{});var Bl=p(ue);lo=o(Bl,"B\u1ED1i c\u1EA3nh d\xE0i c\u1EE7a ch\xFAng ta \u0111\u01B0\u1EE3c chia l\xE0m hai, \u0111\u1ED3ng ngh\u0129a sau khi n\xF3 \u0111i qua m\xF4 h\xECnh, ch\xFAng ta s\u1EBD c\xF3 hai b\u1ED9 logit b\u1EAFt \u0111\u1EA7u v\xE0 k\u1EBFt th\xFAc:"),Bl.forEach(e),La=f(t),v(wn.$$.fragment,t),Oa=f(t),lt.l(t),fe=f(t),de=m(t,"P",{});var Ul=p(de);co=o(Ul,"Gi\u1ED1ng nh\u01B0 tr\u01B0\u1EDBc \u0111\xE2y, \u0111\u1EA7u ti\xEAn ch\xFAng ta che c\xE1c token kh\xF4ng ph\u1EA3i l\xE0 m\u1ED9t ph\u1EA7n c\u1EE7a ng\u1EEF c\u1EA3nh tr\u01B0\u1EDBc khi s\u1EED d\u1EE5ng softmax. Ch\xFAng ta c\u0169ng che t\u1EA5t c\u1EA3 c\xE1c token \u0111\u1EC7m (\u0111\u01B0\u1EE3c g\u1EAFn m\xE1c b\u1EDFi attention mask):"),Ul.forEach(e),Fa=f(t),ht.l(t),ge=f(t),_e=m(t,"P",{});var Jl=p(_e);ho=o(Jl,"Sau \u0111\xF3, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng softmax \u0111\u1EC3 chuy\u1EC3n \u0111\u1ED5i c\xE1c logit c\u1EE7a ch\xFAng ta th\xE0nh x\xE1c su\u1EA5t:"),Jl.forEach(e),Ma=f(t),pt.l(t),be=f(t),ke=m(t,"P",{});var Ql=p(ke);mo=o(Ql,"B\u01B0\u1EDBc ti\u1EBFp theo t\u01B0\u01A1ng t\u1EF1 nh\u01B0 nh\u1EEFng g\xEC ch\xFAng ta \u0111\xE3 l\xE0m cho b\u1ED1i c\u1EA3nh nh\u1ECF, nh\u01B0ng ch\xFAng ta l\u1EB7p l\u1EA1i n\xF3 cho m\u1ED7i ph\u1EA7n trong hai ph\u1EA7n c\u1EE7a m\xECnh. Ch\xFAng ta t\xEDnh \u0111i\u1EC3m cho t\u1EA5t c\u1EA3 c\xE1c kho\u1EA3ng c\xE2u tr\u1EA3 l\u1EDDi c\xF3 th\u1EC3 c\xF3, sau \u0111\xF3 l\u1EA5y ph\u1EA7n c\xF3 \u0111i\u1EC3m t\u1ED1t nh\u1EA5t:"),Ql.forEach(e),Ha=f(t),ft.l(t),ve=f(t),v(jn.$$.fragment,t),Ia=f(t),$e=m(t,"P",{});var Kl=p($e);po=o(Kl,"Hai \u1EE9ng c\u1EED vi\xEAn \u0111\xF3 t\u01B0\u01A1ng \u1EE9ng v\u1EDBi c\xE1c c\xE2u tr\u1EA3 l\u1EDDi t\u1ED1t nh\u1EA5t m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 t\xECm th\u1EA5y trong m\u1ED7i \u0111o\u1EA1n. M\xF4 h\xECnh ch\u1EAFc ch\u1EAFn h\u01A1n r\u1EB1ng c\xE2u tr\u1EA3 l\u1EDDi \u0111\xFAng n\u1EB1m \u1EDF ph\u1EA7n th\u1EE9 hai (\u0111\xF3 l\xE0 m\u1ED9t d\u1EA5u hi\u1EC7u t\u1ED1t!). B\xE2y gi\u1EDD ch\xFAng ta ch\u1EC9 c\u1EA7n \xE1nh x\u1EA1 kho\u1EA3ng hai token \u0111\xF3 v\u1EDBi kho\u1EA3ng c\xE1c k\xFD t\u1EF1 trong ng\u1EEF c\u1EA3nh (ch\xFAng ta ch\u1EC9 c\u1EA7n l\u1EADp \xE1nh x\u1EA1 c\xE1i th\u1EE9 hai \u0111\u1EC3 c\xF3 c\xE2u tr\u1EA3 l\u1EDDi, nh\u01B0ng th\u1EADt th\xFA v\u1ECB khi xem m\xF4 h\xECnh \u0111\xE3 ch\u1ECDn nh\u1EEFng g\xEC trong \u0111o\u1EA1n \u0111\u1EA7u ti\xEAn)."),Kl.forEach(e),Va=f(t),v(Mt.$$.fragment,t),Ga=f(t),qn=m(t,"P",{});var bo=p(qn);xs=m(bo,"CODE",{});var Wl=p(xs);uo=o(Wl,"offsets"),Wl.forEach(e),fo=o(bo," m\xE0 ch\xFAng ta \u0111\xE3 n\u1EAFm \u0111\u01B0\u1EE3c tr\u01B0\u1EDBc \u0111\xF3 th\u1EF1c s\u1EF1 l\xE0 m\u1ED9t danh s\xE1ch c\xE1c offset, v\u1EDBi m\u1ED9t danh s\xE1ch tr\xEAn m\u1ED7i \u0111o\u1EA1n v\u0103n b\u1EA3n:"),bo.forEach(e),Ba=f(t),v(En.$$.fragment,t),Ua=f(t),v(Cn.$$.fragment,t),Ja=f(t),ye=m(t,"P",{});var Rl=p(ye);go=o(Rl,"N\u1EBFu ch\xFAng ta b\u1ECF qua k\u1EBFt qu\u1EA3 \u0111\u1EA7u ti\xEAn, ch\xFAng ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u01B0\u01A1ng t\u1EF1 nh\u01B0 pipeline cho ng\u1EEF c\u1EA3nh d\xE0i n\xE0y - yayy!"),Rl.forEach(e),Qa=f(t),v(Ht.$$.fragment,t),Ka=f(t),xe=m(t,"P",{});var Xl=p(xe);_o=o(Xl,"\u0110i\u1EC1u n\xE0y k\u1EBFt th\xFAc ph\u1EA7n \u0111i s\xE2u v\xE0o c\xE1c kh\u1EA3 n\u0103ng c\u1EE7a tokenizer. Ch\xFAng ta s\u1EBD \u0111\u01B0a t\u1EA5t c\u1EA3 nh\u1EEFng \u0111i\u1EC1u n\xE0y v\xE0o th\u1EF1c t\u1EBF m\u1ED9t l\u1EA7n n\u1EEFa trong ch\u01B0\u01A1ng ti\u1EBFp theo, khi ch\xFAng t\xF4i h\u01B0\u1EDBng d\u1EABn b\u1EA1n c\xE1ch tinh ch\u1EC9nh m\u1ED9t m\xF4 h\xECnh v\u1EC1 m\u1ED9t lo\u1EA1t c\xE1c t\xE1c v\u1EE5 NLP ph\u1ED5 bi\u1EBFn."),Xl.forEach(e),this.h()},h(){S(s,"name","hf:doc:metadata"),S(s,"content",JSON.stringify(Hc)),S(w,"id","fast-tokenizers-in-the-qa-pipeline"),S(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(w,"href","#fast-tokenizers-in-the-qa-pipeline"),S(b,"class","relative group"),S(Pt,"id","s-dng-pipeline-questionanswering"),S(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(Pt,"href","#s-dng-pipeline-questionanswering"),S(jt,"class","relative group"),S(Un,"href","/course/chapter1"),S(Tt,"id","s-dng-m-hnh-cho-tc-v-hi-p"),S(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(Tt,"href","#s-dng-m-hnh-cho-tc-v-hi-p"),S(qt,"class","relative group"),S(Zt,"href","https://huggingface.co/distilbert-base-cased-distilled-squad"),S(Zt,"rel","nofollow"),S(Kn,"href","/course/chapter7/7"),S(tn,"class","block dark:hidden"),tc(tn.src,ko="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg")||S(tn,"src",ko),S(tn,"alt","An example of tokenization of question and context"),S(nn,"class","hidden dark:block"),tc(nn.src,vo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg")||S(nn,"src",vo),S(nn,"alt","An example of tokenization of question and context"),S(Et,"class","flex justify-center"),Us.a=null,Qs.a=Ks,S(At,"id","x-l-cc-ng-cnh-di"),S(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(At,"href","#x-l-cc-ng-cnh-di"),S(Ct,"class","relative group")},m(t,i){a(document.head,s),l(t,c,i),$(n,t,i),l(t,_,i),l(t,b,i),a(b,w),a(w,E),$(T,E,null),a(b,z),a(b,P),a(P,L),l(t,D,i),Pn[N].m(t,i),l(t,q,i),l(t,C,i),a(C,xt),a(C,J),a(J,wt),a(C,ii),l(t,qs,i),Tn[Q].m(t,i),l(t,Gn,i),l(t,jt,i),a(jt,Pt),a(Pt,Oe),$(Qt,Oe,null),a(jt,ri),a(jt,Bn),a(Bn,oi),a(Bn,Fe),a(Fe,li),l(t,Es,i),l(t,gt,i),a(gt,ci),a(gt,Un),a(Un,hi),a(gt,mi),a(gt,Me),a(Me,pi),a(gt,ui),l(t,Cs,i),$(Kt,t,i),l(t,Ps,i),$(Wt,t,i),l(t,Ts,i),l(t,Jn,i),a(Jn,fi),l(t,Ss,i),$(Rt,t,i),l(t,zs,i),$(Xt,t,i),l(t,Ns,i),l(t,Qn,i),a(Qn,di),l(t,As,i),l(t,qt,i),a(qt,Tt),a(Tt,He),$(Yt,He,null),a(qt,gi),a(qt,Ie),a(Ie,_i),l(t,Ds,i),l(t,H,i),a(H,bi),a(H,Ve),a(Ve,ki),a(H,vi),a(H,Zt),a(Zt,Ge),a(Ge,$i),a(H,yi),a(H,Kn),a(Kn,xi),a(H,wi),l(t,Ls,i),Sn[W].m(t,i),l(t,Wn,i),l(t,Rn,i),a(Rn,ji),l(t,Os,i),l(t,Et,i),a(Et,tn),a(Et,qi),a(Et,nn),l(t,Fs,i),l(t,Xn,i),a(Xn,Ei),l(t,Ms,i),$(en,t,i),l(t,Hs,i),zn[X].m(t,i),l(t,Yn,i),l(t,I,i),a(I,Ci),a(I,Be),a(Be,Pi),a(I,Ti),a(I,Ue),a(Ue,Si),a(I,zi),a(I,Je),a(Je,Ni),a(I,Ai),l(t,Is,i),l(t,St,i),a(St,Di),a(St,Qe),a(Qe,Li),a(St,Oi),l(t,Vs,i),Nn[Z].m(t,i),l(t,Zn,i),l(t,te,i),a(te,Fi),l(t,Gs,i),An[nt].m(t,i),l(t,ne,i),l(t,F,i),a(F,Mi),a(F,Ke),a(Ke,Hi),a(F,Ii),a(F,We),a(We,Vi),a(F,Gi),a(F,Re),a(Re,Bi),a(F,Ui),a(F,Xe),a(Xe,Ji),a(F,Qi),l(t,Bs,i),l(t,O,i),a(O,Ki),a(O,Ye),a(Ye,Wi),a(O,Ri),a(O,Ze),a(Ze,Xi),a(O,Yi),a(O,ts),a(ts,Zi),a(O,tr),a(O,ns),a(ns,nr),a(O,er),Us.m(sc,O),l(t,Js,i),l(t,_t,i),a(_t,sr),Qs.m(ac,_t),a(_t,Ks),a(_t,es),a(es,ar),a(_t,ir),l(t,Ws,i),l(t,ee,i),a(ee,rr),l(t,Rs,i),$(sn,t,i),l(t,Xs,i),Dn[st].m(t,i),l(t,se,i),l(t,M,i),a(M,or),a(M,ss),a(ss,lr),a(M,cr),a(M,as),a(as,hr),a(M,mr),a(M,is),a(is,pr),a(M,ur),a(M,rs),a(rs,fr),a(M,dr),l(t,Ys,i),$(an,t,i),l(t,Zs,i),l(t,ae,i),a(ae,gr),l(t,ta,i),$(rn,t,i),l(t,na,i),$(zt,t,i),l(t,ea,i),l(t,bt,i),a(bt,_r),a(bt,os),a(os,br),a(bt,kr),a(bt,ls),a(ls,vr),a(bt,$r),l(t,sa,i),$(on,t,i),l(t,aa,i),l(t,ie,i),a(ie,yr),l(t,ia,i),$(ln,t,i),l(t,ra,i),$(cn,t,i),l(t,oa,i),l(t,re,i),a(re,xr),l(t,la,i),$(Nt,t,i),l(t,ca,i),l(t,Ct,i),a(Ct,At),a(At,cs),$(hn,cs,null),a(Ct,wr),a(Ct,hs),a(hs,jr),l(t,ha,i),l(t,Dt,i),a(Dt,qr),a(Dt,ms),a(ms,Er),a(Dt,Cr),l(t,ma,i),$(mn,t,i),l(t,pa,i),$(pn,t,i),l(t,ua,i),l(t,Lt,i),a(Lt,Pr),a(Lt,ps),a(ps,Tr),a(Lt,Sr),l(t,fa,i),$(un,t,i),l(t,da,i),$(fn,t,i),l(t,ga,i),l(t,oe,i),a(oe,zr),l(t,_a,i),l(t,kt,i),a(kt,Nr),a(kt,us),a(us,Ar),a(kt,Dr),a(kt,fs),a(fs,Lr),a(kt,Or),l(t,ba,i),$(dn,t,i),l(t,ka,i),$(gn,t,i),l(t,va,i),l(t,Ot,i),a(Ot,Fr),a(Ot,ds),a(ds,Mr),a(Ot,Hr),l(t,$a,i),l(t,le,i),a(le,Ir),l(t,ya,i),$(_n,t,i),l(t,xa,i),$(bn,t,i),l(t,wa,i),l(t,Ft,i),a(Ft,Vr),a(Ft,gs),a(gs,Gr),a(Ft,Br),l(t,ja,i),$(kn,t,i),l(t,qa,i),$(vn,t,i),l(t,Ea,i),l(t,ce,i),a(ce,Ur),l(t,Ca,i),$($n,t,i),l(t,Pa,i),l(t,he,i),a(he,Jr),l(t,Ta,i),$(yn,t,i),l(t,Sa,i),l(t,me,i),a(me,Qr),l(t,za,i),l(t,V,i),a(V,Kr),a(V,_s),a(_s,Wr),a(V,Rr),a(V,bs),a(bs,Xr),a(V,Yr),a(V,ks),a(ks,Zr),a(V,to),l(t,Na,i),$(xn,t,i),l(t,Aa,i),l(t,G,i),a(G,no),a(G,vs),a(vs,eo),a(G,so),a(G,$s),a($s,ao),a(G,io),a(G,ys),a(ys,ro),a(G,oo),l(t,Da,i),Ln[it].m(t,i),l(t,pe,i),l(t,ue,i),a(ue,lo),l(t,La,i),$(wn,t,i),l(t,Oa,i),On[ot].m(t,i),l(t,fe,i),l(t,de,i),a(de,co),l(t,Fa,i),Fn[ct].m(t,i),l(t,ge,i),l(t,_e,i),a(_e,ho),l(t,Ma,i),Mn[mt].m(t,i),l(t,be,i),l(t,ke,i),a(ke,mo),l(t,Ha,i),Hn[ut].m(t,i),l(t,ve,i),$(jn,t,i),l(t,Ia,i),l(t,$e,i),a($e,po),l(t,Va,i),$(Mt,t,i),l(t,Ga,i),l(t,qn,i),a(qn,xs),a(xs,uo),a(qn,fo),l(t,Ba,i),$(En,t,i),l(t,Ua,i),$(Cn,t,i),l(t,Ja,i),l(t,ye,i),a(ye,go),l(t,Qa,i),$(Ht,t,i),l(t,Ka,i),l(t,xe,i),a(xe,_o),Wa=!0},p(t,[i]){const In={};i&1&&(In.fw=t[0]),n.$set(In);let we=N;N=yo(t),N!==we&&(U(),d(Pn[we],1,1,()=>{Pn[we]=null}),B(),A=Pn[N],A||(A=Pn[N]=$o[N](t),A.c()),g(A,1),A.m(q.parentNode,q));let je=Q;Q=wo(t),Q!==je&&(U(),d(Tn[je],1,1,()=>{Tn[je]=null}),B(),K=Tn[Q],K||(K=Tn[Q]=xo[Q](t),K.c()),g(K,1),K.m(Gn.parentNode,Gn));let qe=W;W=qo(t),W!==qe&&(U(),d(Sn[qe],1,1,()=>{Sn[qe]=null}),B(),R=Sn[W],R||(R=Sn[W]=jo[W](t),R.c()),g(R,1),R.m(Wn.parentNode,Wn));let It=X;X=Co(t),X!==It&&(U(),d(zn[It],1,1,()=>{zn[It]=null}),B(),Y=zn[X],Y||(Y=zn[X]=Eo[X](t),Y.c()),g(Y,1),Y.m(Yn.parentNode,Yn));let Ee=Z;Z=To(t),Z!==Ee&&(U(),d(Nn[Ee],1,1,()=>{Nn[Ee]=null}),B(),tt=Nn[Z],tt||(tt=Nn[Z]=Po[Z](t),tt.c()),g(tt,1),tt.m(Zn.parentNode,Zn));let Vt=nt;nt=zo(t),nt!==Vt&&(U(),d(An[Vt],1,1,()=>{An[Vt]=null}),B(),et=An[nt],et||(et=An[nt]=So[nt](t),et.c()),g(et,1),et.m(ne.parentNode,ne));let Ce=st;st=Ao(t),st!==Ce&&(U(),d(Dn[Ce],1,1,()=>{Dn[Ce]=null}),B(),at=Dn[st],at||(at=Dn[st]=No[st](t),at.c()),g(at,1),at.m(se.parentNode,se));const ws={};i&2&&(ws.$$scope={dirty:i,ctx:t}),zt.$set(ws);const Pe={};i&2&&(Pe.$$scope={dirty:i,ctx:t}),Nt.$set(Pe);let Te=it;it=Lo(t),it!==Te&&(U(),d(Ln[Te],1,1,()=>{Ln[Te]=null}),B(),rt=Ln[it],rt||(rt=Ln[it]=Do[it](t),rt.c()),g(rt,1),rt.m(pe.parentNode,pe));let vt=ot;ot=Fo(t),ot!==vt&&(U(),d(On[vt],1,1,()=>{On[vt]=null}),B(),lt=On[ot],lt||(lt=On[ot]=Oo[ot](t),lt.c()),g(lt,1),lt.m(fe.parentNode,fe));let Se=ct;ct=Ho(t),ct!==Se&&(U(),d(Fn[Se],1,1,()=>{Fn[Se]=null}),B(),ht=Fn[ct],ht||(ht=Fn[ct]=Mo[ct](t),ht.c()),g(ht,1),ht.m(ge.parentNode,ge));let ze=mt;mt=Vo(t),mt!==ze&&(U(),d(Mn[ze],1,1,()=>{Mn[ze]=null}),B(),pt=Mn[mt],pt||(pt=Mn[mt]=Io[mt](t),pt.c()),g(pt,1),pt.m(be.parentNode,be));let Ne=ut;ut=Bo(t),ut!==Ne&&(U(),d(Hn[Ne],1,1,()=>{Hn[Ne]=null}),B(),ft=Hn[ut],ft||(ft=Hn[ut]=Go[ut](t),ft.c()),g(ft,1),ft.m(ve.parentNode,ve));const js={};i&2&&(js.$$scope={dirty:i,ctx:t}),Mt.$set(js);const Vn={};i&2&&(Vn.$$scope={dirty:i,ctx:t}),Ht.$set(Vn)},i(t){Wa||(g(n.$$.fragment,t),g(T.$$.fragment,t),g(A),g(K),g(Qt.$$.fragment,t),g(Kt.$$.fragment,t),g(Wt.$$.fragment,t),g(Rt.$$.fragment,t),g(Xt.$$.fragment,t),g(Yt.$$.fragment,t),g(R),g(en.$$.fragment,t),g(Y),g(tt),g(et),g(sn.$$.fragment,t),g(at),g(an.$$.fragment,t),g(rn.$$.fragment,t),g(zt.$$.fragment,t),g(on.$$.fragment,t),g(ln.$$.fragment,t),g(cn.$$.fragment,t),g(Nt.$$.fragment,t),g(hn.$$.fragment,t),g(mn.$$.fragment,t),g(pn.$$.fragment,t),g(un.$$.fragment,t),g(fn.$$.fragment,t),g(dn.$$.fragment,t),g(gn.$$.fragment,t),g(_n.$$.fragment,t),g(bn.$$.fragment,t),g(kn.$$.fragment,t),g(vn.$$.fragment,t),g($n.$$.fragment,t),g(yn.$$.fragment,t),g(xn.$$.fragment,t),g(rt),g(wn.$$.fragment,t),g(lt),g(ht),g(pt),g(ft),g(jn.$$.fragment,t),g(Mt.$$.fragment,t),g(En.$$.fragment,t),g(Cn.$$.fragment,t),g(Ht.$$.fragment,t),Wa=!0)},o(t){d(n.$$.fragment,t),d(T.$$.fragment,t),d(A),d(K),d(Qt.$$.fragment,t),d(Kt.$$.fragment,t),d(Wt.$$.fragment,t),d(Rt.$$.fragment,t),d(Xt.$$.fragment,t),d(Yt.$$.fragment,t),d(R),d(en.$$.fragment,t),d(Y),d(tt),d(et),d(sn.$$.fragment,t),d(at),d(an.$$.fragment,t),d(rn.$$.fragment,t),d(zt.$$.fragment,t),d(on.$$.fragment,t),d(ln.$$.fragment,t),d(cn.$$.fragment,t),d(Nt.$$.fragment,t),d(hn.$$.fragment,t),d(mn.$$.fragment,t),d(pn.$$.fragment,t),d(un.$$.fragment,t),d(fn.$$.fragment,t),d(dn.$$.fragment,t),d(gn.$$.fragment,t),d(_n.$$.fragment,t),d(bn.$$.fragment,t),d(kn.$$.fragment,t),d(vn.$$.fragment,t),d($n.$$.fragment,t),d(yn.$$.fragment,t),d(xn.$$.fragment,t),d(rt),d(wn.$$.fragment,t),d(lt),d(ht),d(pt),d(ft),d(jn.$$.fragment,t),d(Mt.$$.fragment,t),d(En.$$.fragment,t),d(Cn.$$.fragment,t),d(Ht.$$.fragment,t),Wa=!1},d(t){e(s),t&&e(c),y(n,t),t&&e(_),t&&e(b),y(T),t&&e(D),Pn[N].d(t),t&&e(q),t&&e(C),t&&e(qs),Tn[Q].d(t),t&&e(Gn),t&&e(jt),y(Qt),t&&e(Es),t&&e(gt),t&&e(Cs),y(Kt,t),t&&e(Ps),y(Wt,t),t&&e(Ts),t&&e(Jn),t&&e(Ss),y(Rt,t),t&&e(zs),y(Xt,t),t&&e(Ns),t&&e(Qn),t&&e(As),t&&e(qt),y(Yt),t&&e(Ds),t&&e(H),t&&e(Ls),Sn[W].d(t),t&&e(Wn),t&&e(Rn),t&&e(Os),t&&e(Et),t&&e(Fs),t&&e(Xn),t&&e(Ms),y(en,t),t&&e(Hs),zn[X].d(t),t&&e(Yn),t&&e(I),t&&e(Is),t&&e(St),t&&e(Vs),Nn[Z].d(t),t&&e(Zn),t&&e(te),t&&e(Gs),An[nt].d(t),t&&e(ne),t&&e(F),t&&e(Bs),t&&e(O),t&&e(Js),t&&e(_t),t&&e(Ws),t&&e(ee),t&&e(Rs),y(sn,t),t&&e(Xs),Dn[st].d(t),t&&e(se),t&&e(M),t&&e(Ys),y(an,t),t&&e(Zs),t&&e(ae),t&&e(ta),y(rn,t),t&&e(na),y(zt,t),t&&e(ea),t&&e(bt),t&&e(sa),y(on,t),t&&e(aa),t&&e(ie),t&&e(ia),y(ln,t),t&&e(ra),y(cn,t),t&&e(oa),t&&e(re),t&&e(la),y(Nt,t),t&&e(ca),t&&e(Ct),y(hn),t&&e(ha),t&&e(Dt),t&&e(ma),y(mn,t),t&&e(pa),y(pn,t),t&&e(ua),t&&e(Lt),t&&e(fa),y(un,t),t&&e(da),y(fn,t),t&&e(ga),t&&e(oe),t&&e(_a),t&&e(kt),t&&e(ba),y(dn,t),t&&e(ka),y(gn,t),t&&e(va),t&&e(Ot),t&&e($a),t&&e(le),t&&e(ya),y(_n,t),t&&e(xa),y(bn,t),t&&e(wa),t&&e(Ft),t&&e(ja),y(kn,t),t&&e(qa),y(vn,t),t&&e(Ea),t&&e(ce),t&&e(Ca),y($n,t),t&&e(Pa),t&&e(he),t&&e(Ta),y(yn,t),t&&e(Sa),t&&e(me),t&&e(za),t&&e(V),t&&e(Na),y(xn,t),t&&e(Aa),t&&e(G),t&&e(Da),Ln[it].d(t),t&&e(pe),t&&e(ue),t&&e(La),y(wn,t),t&&e(Oa),On[ot].d(t),t&&e(fe),t&&e(de),t&&e(Fa),Fn[ct].d(t),t&&e(ge),t&&e(_e),t&&e(Ma),Mn[mt].d(t),t&&e(be),t&&e(ke),t&&e(Ha),Hn[ut].d(t),t&&e(ve),y(jn,t),t&&e(Ia),t&&e($e),t&&e(Va),y(Mt,t),t&&e(Ga),t&&e(qn),t&&e(Ba),y(En,t),t&&e(Ua),y(Cn,t),t&&e(Ja),t&&e(ye),t&&e(Qa),y(Ht,t),t&&e(Ka),t&&e(xe)}}}const Hc={local:"fast-tokenizers-in-the-qa-pipeline",sections:[{local:"s-dng-pipeline-questionanswering",title:"S\u1EED d\u1EE5ng pipeline `question-answering`"},{local:"s-dng-m-hnh-cho-tc-v-hi-p",title:"S\u1EED d\u1EE5ng m\xF4 h\xECnh cho t\xE1c v\u1EE5 h\u1ECFi \u0111\xE1p"},{local:"x-l-cc-ng-cnh-di",title:"X\u1EED l\xFD c\xE1c ng\u1EEF c\u1EA3nh d\xE0i"}],title:"Fast tokenizers in the QA pipeline"};function Ic(x,s,c){let n="pt";return cc(()=>{const _=new URLSearchParams(window.location.search);c(0,n=_.get("fw")||"pt")}),[n]}class Rc extends ic{constructor(s){super();rc(this,s,Ic,Mc,oc,{})}}export{Rc as default,Hc as metadata};
