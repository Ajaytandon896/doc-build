import{S as ll,i as nl,s as il,e as o,k as c,w as He,t as a,M as ul,c as l,d as r,m as d,a as n,x as xe,h as s,b as i,N as ol,G as e,g as p,y as ke,L as pl,q as De,o as Fe,B as Ge,v as cl}from"../../chunks/vendor-hf-doc-builder.js";import{Y as dl}from"../../chunks/Youtube-hf-doc-builder.js";import{I as gr}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ml}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function fl(Gs){let A,Dt,$,F,Ce,K,Er,Oe,_r,Ft,Q,Gt,N,G,je,V,br,Be,wr,Ct,W,Ot,m,qr,X,Pr,Lr,C,yr,Re,Ar,$r,O,Nr,Ue,zr,Tr,j,Ir,Ye,Mr,Sr,B,Hr,Je,xr,kr,Z,Ke,Dr,Fr,jt,z,R,Qe,ee,Gr,Ve,Cr,Bt,Le,Or,Rt,T,te,Cs,jr,re,Os,Ut,P,f,Br,We,Rr,Ur,Xe,Yr,Jr,ae,Ze,Kr,Qr,et,Vr,Wr,tt,Xr,Zr,ea,I,ta,rt,ra,aa,at,sa,oa,la,M,na,st,ia,ua,ot,pa,ca,Yt,ye,da,Jt,L,lt,ma,fa,g,ha,se,va,ga,oe,nt,Ea,_a,le,it,ba,wa,qa,S,Pa,ne,La,ya,ie,Aa,$a,Kt,U,Na,ue,za,Ta,Qt,H,Y,ut,pe,Ia,pt,Ma,Vt,Ae,Sa,Wt,E,ct,Ha,xa,ce,ka,Da,dt,Fa,Ga,Xt,x,mt,Ca,Oa,ft,ja,Ba,Zt,k,ht,Ra,Ua,vt,Ya,Ja,er,_,gt,Ka,Qa,Et,Va,Wa,de,_t,Xa,Za,tr,me,bt,es,ts,rr,D,wt,rs,as,qt,ss,os,ar,b,Pt,ls,ns,Lt,is,us,fe,ps,cs,sr,w,yt,ds,ms,At,fs,hs,he,$t,vs,gs,or,q,Nt,Es,_s,zt,bs,ws,ve,Tt,qs,Ps,lr,$e,Ls,nr,y,ge,ys,It,As,$s,Ns,Ee,zs,Mt,Ts,Is,Ms,St,Ss,ir;return K=new gr({}),Q=new ml({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),V=new gr({}),W=new dl({props:{id:"00GKzGyWFEs"}}),ee=new gr({}),pe=new gr({}),{c(){A=o("meta"),Dt=c(),$=o("h1"),F=o("a"),Ce=o("span"),He(K.$$.fragment),Er=c(),Oe=o("span"),_r=a("Introduction"),Ft=c(),He(Q.$$.fragment),Gt=c(),N=o("h2"),G=o("a"),je=o("span"),He(V.$$.fragment),br=c(),Be=o("span"),wr=a("Bienvenue au cours \u{1F917} !"),Ct=c(),He(W.$$.fragment),Ot=c(),m=o("p"),qr=a("Ce cours vous apprendra \xE0 utiliser les biblioth\xE8ques de NLP de l\u2019\xE9cosyst\xE8me "),X=o("a"),Pr=a("Hugging Face"),Lr=a(" : "),C=o("a"),yr=a("\u{1F917} "),Re=o("em"),Ar=a("Transformers"),$r=a(", "),O=o("a"),Nr=a("\u{1F917} "),Ue=o("em"),zr=a("Datasets"),Tr=a(", "),j=o("a"),Ir=a("\u{1F917} "),Ye=o("em"),Mr=a("Tokenizers"),Sr=a(" et "),B=o("a"),Hr=a("\u{1F917} "),Je=o("em"),xr=a("Accelerate"),kr=a(", ainsi que le "),Z=o("a"),Ke=o("em"),Dr=a("Hub"),Fr=a(". C\u2019est totalement gratuit et sans publicit\xE9."),jt=c(),z=o("h2"),R=o("a"),Qe=o("span"),He(ee.$$.fragment),Gr=c(),Ve=o("span"),Cr=a("\xC0 quoi s'attendre ?"),Bt=c(),Le=o("p"),Or=a("Voici un bref aper\xE7u du cours :"),Rt=c(),T=o("div"),te=o("img"),jr=c(),re=o("img"),Ut=c(),P=o("ul"),f=o("li"),Br=a("Les chapitres 1 \xE0 4 pr\xE9sentent les principaux concepts de la biblioth\xE8que \u{1F917} "),We=o("em"),Rr=a("Transformers"),Ur=a(". \xC0 la fin de ces chapitres, vous serez familier avec le fonctionnement des "),Xe=o("em"),Yr=a("transformers"),Jr=a(" et vous saurez comment utiliser un mod\xE8le pr\xE9sent sur le "),ae=o("a"),Ze=o("em"),Kr=a("Hub"),Qr=a(", le "),et=o("em"),Vr=a("finetuner"),Wr=a(" sur un jeu de donn\xE9es, et partager vos r\xE9sultats sur le "),tt=o("em"),Xr=a("Hub"),Zr=a(" !"),ea=c(),I=o("li"),ta=a("Les chapitres 5 \xE0 8 pr\xE9sentent les bases des librairies \u{1F917} "),rt=o("em"),ra=a("Datasets"),aa=a(" et \u{1F917} "),at=o("em"),sa=a("Tokenizers"),oa=a(" ainsi qu\u2019une d\xE9couverte des probl\xE8mes classiques de NLP. \xC0 la fin de ce chapitre, vous serez capable de r\xE9soudre les probl\xE8mes de NLP les plus communs par vous-m\xEAme."),la=c(),M=o("li"),na=a("Les chapitres 9 \xE0 12 proposent d\u2019aller plus loin et d\u2019explorer comment les "),st=o("em"),ia=a("transformers"),ua=a(" peuvent \xEAtre utilis\xE9s pour r\xE9soudre des probl\xE8mes de traitement de la parole et de vision par ordinateur. En suivant ces chapitres, vous apprendrez \xE0 construire et \xE0 partager vos mod\xE8les via des d\xE9monstrateurs, et vous serez capable d\u2019optimiser ces mod\xE8les pour des environnements de production. Enfin, vous serez pr\xEAt \xE0 appliquer \u{1F917} "),ot=o("em"),pa=a("Transformers"),ca=a(" \xE0 (presque) n\u2019importe quel probl\xE8me d\u2019apprentissage automatique !"),Yt=c(),ye=o("p"),da=a("Ce cours :"),Jt=c(),L=o("ul"),lt=o("li"),ma=a("requiert un bon niveau en Python,"),fa=c(),g=o("li"),ha=a("se comprend mieux si vous avez d\xE9j\xE0 suivi un cours d\u2019introduction \xE0 l\u2019apprentissage profond comme "),se=o("a"),va=a("fast.ai\u2019s"),ga=a(", "),oe=o("a"),nt=o("em"),Ea=a("Practical Deep Learning for Coders"),_a=a(" ou un des cours d\xE9velopp\xE9s par "),le=o("a"),it=o("em"),ba=a("DeepLearning.AI"),wa=a(","),qa=c(),S=o("li"),Pa=a("n\u2019attend pas une connaissance appronfondie de "),ne=o("a"),La=a("PyTorch"),ya=a(" ou de "),ie=o("a"),Aa=a("TensorFlow"),$a=a(", bien qu\u2019\xEAtre familiaris\xE9 avec l\u2019un d\u2019entre eux peut aider."),Kt=c(),U=o("p"),Na=a("Apr\xE8s avoir termin\xE9 ce cours, nous vous recommandons de suivre la "),ue=o("a"),za=a("Sp\xE9cialisation en NLP"),Ta=a(" dispens\xE9e par DeepLearning.AI, qui couvre une grande partie des mod\xE8les traditionnels de NLP comme le Bay\xE9sien na\xEFf et les LSTMs qui sont importants \xE0 conna\xEEtre !"),Qt=c(),H=o("h2"),Y=o("a"),ut=o("span"),He(pe.$$.fragment),Ia=c(),pt=o("span"),Ma=a("Qui sommes-nous ?"),Vt=c(),Ae=o("p"),Sa=a("\xC0 propos des auteurs de ce cours :"),Wt=c(),E=o("p"),ct=o("strong"),Ha=a("Abubakar Abid"),xa=a(" a obtenu son doctorat en apprentissage automatique appliqu\xE9 \xE0 Stanford. Pendant son doctorat, il a fond\xE9 "),ce=o("a"),ka=a("Gradio"),Da=a(", une biblioth\xE8que Python "),dt=o("em"),Fa=a("open source"),Ga=a(" qui a \xE9t\xE9 utilis\xE9e pour construire plus de 600 000 d\xE9mos d\u2019apprentissage automatique. Gradio a \xE9t\xE9 rachet\xE9e par Hugging Face, o\xF9 Abubakar occupe d\xE9sormais le poste de responsable de l\u2019\xE9quipe d\u2019apprentissage automatique."),Xt=c(),x=o("p"),mt=o("strong"),Ca=a("Matthew Carrigan"),Oa=a(" est ing\xE9nieur en apprentissage machine chez Hugging Face. Il vit \xE0 Dublin en Irlande. Il a travaill\xE9 auparavant comme ing\xE9nieur en apprentissage machine chez Parse.ly et avant cela comme chercheur postdoctoral au Trinity College Dublin. Il ne croit pas que nous arrivions \xE0 l\u2019"),ft=o("em"),ja=a("AGI"),Ba=a(" en mettant \xE0 l\u2019\xE9chelle les architectures existantes mais a tout de m\xEAme beaucoup d\u2019espoir dans l\u2019immortalit\xE9 des robots."),Zt=c(),k=o("p"),ht=o("strong"),Ra=a("Lysandre Debut"),Ua=a(" est ing\xE9nieur en apprentissage machine chez Hugging Face et a travaill\xE9 sur la biblioth\xE8que \u{1F917} "),vt=o("em"),Ya=a("Transformers"),Ja=a(" depuis les premi\xE8res phases de d\xE9veloppement. Son but est de rendre le NLP accessible \xE0 tous en d\xE9veloppant des outils disposant d\u2019une API tr\xE8s simple."),er=c(),_=o("p"),gt=o("strong"),Ka=a("Sylvain Gugger"),Qa=a(" est ing\xE9nieur de recherche chez Hugging Face et un des principaux responsables de la biblioth\xE8que \u{1F917} "),Et=o("em"),Va=a("Transformers"),Wa=a(". Avant cela, il \xE9tait chercheur en apprentissage machine chez fast.ai et a \xE9crit le livre "),de=o("a"),_t=o("em"),Xa=a("Deep Learning for Coders with fastai and PyTorch"),Za=a(" avec Jeremy Howard. Son but est de rendre l\u2019apprentissage profond plus accessible en d\xE9veloppant et en am\xE9liorant des techniques permettant aux mod\xE8les d\u2019apprendre rapidement sur des ressources limit\xE9es."),tr=c(),me=o("p"),bt=o("strong"),es=a("Dawood Khan"),ts=a(" est un ing\xE9nieur en apprentissage automatique chez Hugging Face. Il vient de New York et est dipl\xF4m\xE9 en informatique de l\u2019Universit\xE9 de New York. Apr\xE8s avoir travaill\xE9 comme ing\xE9nieur iOS pendant quelques ann\xE9es, Dawood a quitt\xE9 son poste pour cr\xE9er Gradio avec ses cofondateurs. Gradio a finalement \xE9t\xE9 acquis par Hugging Face."),rr=c(),D=o("p"),wt=o("strong"),rs=a("Merve Noyan"),as=a(" est d\xE9veloppeuse "),qt=o("em"),ss=a("advocate"),os=a(" chez Hugging Face et travaille \xE0 la cr\xE9ation d\u2019outils et de contenus visant \xE0 d\xE9mocratiser l\u2019apprentissage machine pour tous."),ar=c(),b=o("p"),Pt=o("strong"),ls=a("Lucile Saulnier"),ns=a(" est ing\xE9nieure en apprentissage machine chez Hugging Face et travaille au d\xE9veloppement et \xE0 l\u2019impl\xE9mentation de nombreux outils "),Lt=o("em"),is=a("open source"),us=a(". Elle est \xE9galement activement impliqu\xE9e dans de nombreux projets de recherche dans le domaine du NLP comme l\u2019entra\xEEnement collaboratif de mod\xE8les et le projet "),fe=o("a"),ps=a("BigScience"),cs=a("."),sr=c(),w=o("p"),yt=o("strong"),ds=a("Lewis Tunstall"),ms=a(" est ing\xE9nieur en apprentissage machine chez Hugging Face et d\xE9vou\xE9 au d\xE9veloppement d\u2019outils "),At=o("em"),fs=a("open source"),hs=a(" avec la volont\xE9 de les rendre accessibles \xE0 une communaut\xE9 plus large. Il est \xE9galement co-auteur du livre "),he=o("a"),$t=o("em"),vs=a("Natural Language Processing with Transformers"),gs=a("."),or=c(),q=o("p"),Nt=o("strong"),Es=a("Leandro von Werra"),_s=a(" est ing\xE9nieur en apprentissage machine dans l\u2019\xE9quipe "),zt=o("em"),bs=a("open source"),ws=a(" d\u2019Hugging Face et \xE9galement co-auteur du livre "),ve=o("a"),Tt=o("em"),qs=a("Natural Language Processing with Transformers"),Ps=a(". Il a plusieurs ann\xE9es d\u2019exp\xE9rience dans l\u2019industrie o\xF9 il a pu d\xE9ployer des projets de NLP en production et travailler sur toutes les \xE9tapes clefs du d\xE9ploiement."),lr=c(),$e=o("p"),Ls=a("\xCAtes-vous pr\xEAt \xE0 commencer ? Dans ce chapitre, vous apprendrez :"),nr=c(),y=o("ul"),ge=o("li"),ys=a("\xE0 utiliser la fonction "),It=o("code"),As=a("pipeline()"),$s=a(" pour r\xE9soudre des probl\xE8mes de NLP comme la g\xE9n\xE9ration de texte et la classification,"),Ns=c(),Ee=o("li"),zs=a("l\u2019architecture d\u2019un "),Mt=o("em"),Ts=a("transformer"),Is=a(","),Ms=c(),St=o("li"),Ss=a("comment faire la distinction entre les diff\xE9rentes architectures d\u2019encodeur, de d\xE9codeur et d\u2019encodeur-d\xE9codeur ainsi que leurs diff\xE9rents cas d\u2019usage."),this.h()},l(t){const u=ul('[data-svelte="svelte-1phssyn"]',document.head);A=l(u,"META",{name:!0,content:!0}),u.forEach(r),Dt=d(t),$=l(t,"H1",{class:!0});var ur=n($);F=l(ur,"A",{id:!0,class:!0,href:!0});var js=n(F);Ce=l(js,"SPAN",{});var Bs=n(Ce);xe(K.$$.fragment,Bs),Bs.forEach(r),js.forEach(r),Er=d(ur),Oe=l(ur,"SPAN",{});var Rs=n(Oe);_r=s(Rs,"Introduction"),Rs.forEach(r),ur.forEach(r),Ft=d(t),xe(Q.$$.fragment,t),Gt=d(t),N=l(t,"H2",{class:!0});var pr=n(N);G=l(pr,"A",{id:!0,class:!0,href:!0});var Us=n(G);je=l(Us,"SPAN",{});var Ys=n(je);xe(V.$$.fragment,Ys),Ys.forEach(r),Us.forEach(r),br=d(pr),Be=l(pr,"SPAN",{});var Js=n(Be);wr=s(Js,"Bienvenue au cours \u{1F917} !"),Js.forEach(r),pr.forEach(r),Ct=d(t),xe(W.$$.fragment,t),Ot=d(t),m=l(t,"P",{});var h=n(m);qr=s(h,"Ce cours vous apprendra \xE0 utiliser les biblioth\xE8ques de NLP de l\u2019\xE9cosyst\xE8me "),X=l(h,"A",{href:!0,rel:!0});var Ks=n(X);Pr=s(Ks,"Hugging Face"),Ks.forEach(r),Lr=s(h," : "),C=l(h,"A",{href:!0,rel:!0});var Hs=n(C);yr=s(Hs,"\u{1F917} "),Re=l(Hs,"EM",{});var Qs=n(Re);Ar=s(Qs,"Transformers"),Qs.forEach(r),Hs.forEach(r),$r=s(h,", "),O=l(h,"A",{href:!0,rel:!0});var xs=n(O);Nr=s(xs,"\u{1F917} "),Ue=l(xs,"EM",{});var Vs=n(Ue);zr=s(Vs,"Datasets"),Vs.forEach(r),xs.forEach(r),Tr=s(h,", "),j=l(h,"A",{href:!0,rel:!0});var ks=n(j);Ir=s(ks,"\u{1F917} "),Ye=l(ks,"EM",{});var Ws=n(Ye);Mr=s(Ws,"Tokenizers"),Ws.forEach(r),ks.forEach(r),Sr=s(h," et "),B=l(h,"A",{href:!0,rel:!0});var Ds=n(B);Hr=s(Ds,"\u{1F917} "),Je=l(Ds,"EM",{});var Xs=n(Je);xr=s(Xs,"Accelerate"),Xs.forEach(r),Ds.forEach(r),kr=s(h,", ainsi que le "),Z=l(h,"A",{href:!0,rel:!0});var Zs=n(Z);Ke=l(Zs,"EM",{});var eo=n(Ke);Dr=s(eo,"Hub"),eo.forEach(r),Zs.forEach(r),Fr=s(h,". C\u2019est totalement gratuit et sans publicit\xE9."),h.forEach(r),jt=d(t),z=l(t,"H2",{class:!0});var cr=n(z);R=l(cr,"A",{id:!0,class:!0,href:!0});var to=n(R);Qe=l(to,"SPAN",{});var ro=n(Qe);xe(ee.$$.fragment,ro),ro.forEach(r),to.forEach(r),Gr=d(cr),Ve=l(cr,"SPAN",{});var ao=n(Ve);Cr=s(ao,"\xC0 quoi s'attendre ?"),ao.forEach(r),cr.forEach(r),Bt=d(t),Le=l(t,"P",{});var so=n(Le);Or=s(so,"Voici un bref aper\xE7u du cours :"),so.forEach(r),Rt=d(t),T=l(t,"DIV",{class:!0});var dr=n(T);te=l(dr,"IMG",{class:!0,src:!0,alt:!0}),jr=d(dr),re=l(dr,"IMG",{class:!0,src:!0,alt:!0}),dr.forEach(r),Ut=d(t),P=l(t,"UL",{});var Ne=n(P);f=l(Ne,"LI",{});var v=n(f);Br=s(v,"Les chapitres 1 \xE0 4 pr\xE9sentent les principaux concepts de la biblioth\xE8que \u{1F917} "),We=l(v,"EM",{});var oo=n(We);Rr=s(oo,"Transformers"),oo.forEach(r),Ur=s(v,". \xC0 la fin de ces chapitres, vous serez familier avec le fonctionnement des "),Xe=l(v,"EM",{});var lo=n(Xe);Yr=s(lo,"transformers"),lo.forEach(r),Jr=s(v," et vous saurez comment utiliser un mod\xE8le pr\xE9sent sur le "),ae=l(v,"A",{href:!0,rel:!0});var no=n(ae);Ze=l(no,"EM",{});var io=n(Ze);Kr=s(io,"Hub"),io.forEach(r),no.forEach(r),Qr=s(v,", le "),et=l(v,"EM",{});var uo=n(et);Vr=s(uo,"finetuner"),uo.forEach(r),Wr=s(v," sur un jeu de donn\xE9es, et partager vos r\xE9sultats sur le "),tt=l(v,"EM",{});var po=n(tt);Xr=s(po,"Hub"),po.forEach(r),Zr=s(v," !"),v.forEach(r),ea=d(Ne),I=l(Ne,"LI",{});var ze=n(I);ta=s(ze,"Les chapitres 5 \xE0 8 pr\xE9sentent les bases des librairies \u{1F917} "),rt=l(ze,"EM",{});var co=n(rt);ra=s(co,"Datasets"),co.forEach(r),aa=s(ze," et \u{1F917} "),at=l(ze,"EM",{});var mo=n(at);sa=s(mo,"Tokenizers"),mo.forEach(r),oa=s(ze," ainsi qu\u2019une d\xE9couverte des probl\xE8mes classiques de NLP. \xC0 la fin de ce chapitre, vous serez capable de r\xE9soudre les probl\xE8mes de NLP les plus communs par vous-m\xEAme."),ze.forEach(r),la=d(Ne),M=l(Ne,"LI",{});var Te=n(M);na=s(Te,"Les chapitres 9 \xE0 12 proposent d\u2019aller plus loin et d\u2019explorer comment les "),st=l(Te,"EM",{});var fo=n(st);ia=s(fo,"transformers"),fo.forEach(r),ua=s(Te," peuvent \xEAtre utilis\xE9s pour r\xE9soudre des probl\xE8mes de traitement de la parole et de vision par ordinateur. En suivant ces chapitres, vous apprendrez \xE0 construire et \xE0 partager vos mod\xE8les via des d\xE9monstrateurs, et vous serez capable d\u2019optimiser ces mod\xE8les pour des environnements de production. Enfin, vous serez pr\xEAt \xE0 appliquer \u{1F917} "),ot=l(Te,"EM",{});var ho=n(ot);pa=s(ho,"Transformers"),ho.forEach(r),ca=s(Te," \xE0 (presque) n\u2019importe quel probl\xE8me d\u2019apprentissage automatique !"),Te.forEach(r),Ne.forEach(r),Yt=d(t),ye=l(t,"P",{});var vo=n(ye);da=s(vo,"Ce cours :"),vo.forEach(r),Jt=d(t),L=l(t,"UL",{});var Ie=n(L);lt=l(Ie,"LI",{});var go=n(lt);ma=s(go,"requiert un bon niveau en Python,"),go.forEach(r),fa=d(Ie),g=l(Ie,"LI",{});var J=n(g);ha=s(J,"se comprend mieux si vous avez d\xE9j\xE0 suivi un cours d\u2019introduction \xE0 l\u2019apprentissage profond comme "),se=l(J,"A",{href:!0,rel:!0});var Eo=n(se);va=s(Eo,"fast.ai\u2019s"),Eo.forEach(r),ga=s(J,", "),oe=l(J,"A",{href:!0,rel:!0});var _o=n(oe);nt=l(_o,"EM",{});var bo=n(nt);Ea=s(bo,"Practical Deep Learning for Coders"),bo.forEach(r),_o.forEach(r),_a=s(J," ou un des cours d\xE9velopp\xE9s par "),le=l(J,"A",{href:!0,rel:!0});var wo=n(le);it=l(wo,"EM",{});var qo=n(it);ba=s(qo,"DeepLearning.AI"),qo.forEach(r),wo.forEach(r),wa=s(J,","),J.forEach(r),qa=d(Ie),S=l(Ie,"LI",{});var Me=n(S);Pa=s(Me,"n\u2019attend pas une connaissance appronfondie de "),ne=l(Me,"A",{href:!0,rel:!0});var Po=n(ne);La=s(Po,"PyTorch"),Po.forEach(r),ya=s(Me," ou de "),ie=l(Me,"A",{href:!0,rel:!0});var Lo=n(ie);Aa=s(Lo,"TensorFlow"),Lo.forEach(r),$a=s(Me,", bien qu\u2019\xEAtre familiaris\xE9 avec l\u2019un d\u2019entre eux peut aider."),Me.forEach(r),Ie.forEach(r),Kt=d(t),U=l(t,"P",{});var mr=n(U);Na=s(mr,"Apr\xE8s avoir termin\xE9 ce cours, nous vous recommandons de suivre la "),ue=l(mr,"A",{href:!0,rel:!0});var yo=n(ue);za=s(yo,"Sp\xE9cialisation en NLP"),yo.forEach(r),Ta=s(mr," dispens\xE9e par DeepLearning.AI, qui couvre une grande partie des mod\xE8les traditionnels de NLP comme le Bay\xE9sien na\xEFf et les LSTMs qui sont importants \xE0 conna\xEEtre !"),mr.forEach(r),Qt=d(t),H=l(t,"H2",{class:!0});var fr=n(H);Y=l(fr,"A",{id:!0,class:!0,href:!0});var Ao=n(Y);ut=l(Ao,"SPAN",{});var $o=n(ut);xe(pe.$$.fragment,$o),$o.forEach(r),Ao.forEach(r),Ia=d(fr),pt=l(fr,"SPAN",{});var No=n(pt);Ma=s(No,"Qui sommes-nous ?"),No.forEach(r),fr.forEach(r),Vt=d(t),Ae=l(t,"P",{});var zo=n(Ae);Sa=s(zo,"\xC0 propos des auteurs de ce cours :"),zo.forEach(r),Wt=d(t),E=l(t,"P",{});var _e=n(E);ct=l(_e,"STRONG",{});var To=n(ct);Ha=s(To,"Abubakar Abid"),To.forEach(r),xa=s(_e," a obtenu son doctorat en apprentissage automatique appliqu\xE9 \xE0 Stanford. Pendant son doctorat, il a fond\xE9 "),ce=l(_e,"A",{href:!0,rel:!0});var Io=n(ce);ka=s(Io,"Gradio"),Io.forEach(r),Da=s(_e,", une biblioth\xE8que Python "),dt=l(_e,"EM",{});var Mo=n(dt);Fa=s(Mo,"open source"),Mo.forEach(r),Ga=s(_e," qui a \xE9t\xE9 utilis\xE9e pour construire plus de 600 000 d\xE9mos d\u2019apprentissage automatique. Gradio a \xE9t\xE9 rachet\xE9e par Hugging Face, o\xF9 Abubakar occupe d\xE9sormais le poste de responsable de l\u2019\xE9quipe d\u2019apprentissage automatique."),_e.forEach(r),Xt=d(t),x=l(t,"P",{});var Ht=n(x);mt=l(Ht,"STRONG",{});var So=n(mt);Ca=s(So,"Matthew Carrigan"),So.forEach(r),Oa=s(Ht," est ing\xE9nieur en apprentissage machine chez Hugging Face. Il vit \xE0 Dublin en Irlande. Il a travaill\xE9 auparavant comme ing\xE9nieur en apprentissage machine chez Parse.ly et avant cela comme chercheur postdoctoral au Trinity College Dublin. Il ne croit pas que nous arrivions \xE0 l\u2019"),ft=l(Ht,"EM",{});var Ho=n(ft);ja=s(Ho,"AGI"),Ho.forEach(r),Ba=s(Ht," en mettant \xE0 l\u2019\xE9chelle les architectures existantes mais a tout de m\xEAme beaucoup d\u2019espoir dans l\u2019immortalit\xE9 des robots."),Ht.forEach(r),Zt=d(t),k=l(t,"P",{});var xt=n(k);ht=l(xt,"STRONG",{});var xo=n(ht);Ra=s(xo,"Lysandre Debut"),xo.forEach(r),Ua=s(xt," est ing\xE9nieur en apprentissage machine chez Hugging Face et a travaill\xE9 sur la biblioth\xE8que \u{1F917} "),vt=l(xt,"EM",{});var ko=n(vt);Ya=s(ko,"Transformers"),ko.forEach(r),Ja=s(xt," depuis les premi\xE8res phases de d\xE9veloppement. Son but est de rendre le NLP accessible \xE0 tous en d\xE9veloppant des outils disposant d\u2019une API tr\xE8s simple."),xt.forEach(r),er=d(t),_=l(t,"P",{});var be=n(_);gt=l(be,"STRONG",{});var Do=n(gt);Ka=s(Do,"Sylvain Gugger"),Do.forEach(r),Qa=s(be," est ing\xE9nieur de recherche chez Hugging Face et un des principaux responsables de la biblioth\xE8que \u{1F917} "),Et=l(be,"EM",{});var Fo=n(Et);Va=s(Fo,"Transformers"),Fo.forEach(r),Wa=s(be,". Avant cela, il \xE9tait chercheur en apprentissage machine chez fast.ai et a \xE9crit le livre "),de=l(be,"A",{href:!0,rel:!0});var Go=n(de);_t=l(Go,"EM",{});var Co=n(_t);Xa=s(Co,"Deep Learning for Coders with fastai and PyTorch"),Co.forEach(r),Go.forEach(r),Za=s(be," avec Jeremy Howard. Son but est de rendre l\u2019apprentissage profond plus accessible en d\xE9veloppant et en am\xE9liorant des techniques permettant aux mod\xE8les d\u2019apprendre rapidement sur des ressources limit\xE9es."),be.forEach(r),tr=d(t),me=l(t,"P",{});var Fs=n(me);bt=l(Fs,"STRONG",{});var Oo=n(bt);es=s(Oo,"Dawood Khan"),Oo.forEach(r),ts=s(Fs," est un ing\xE9nieur en apprentissage automatique chez Hugging Face. Il vient de New York et est dipl\xF4m\xE9 en informatique de l\u2019Universit\xE9 de New York. Apr\xE8s avoir travaill\xE9 comme ing\xE9nieur iOS pendant quelques ann\xE9es, Dawood a quitt\xE9 son poste pour cr\xE9er Gradio avec ses cofondateurs. Gradio a finalement \xE9t\xE9 acquis par Hugging Face."),Fs.forEach(r),rr=d(t),D=l(t,"P",{});var kt=n(D);wt=l(kt,"STRONG",{});var jo=n(wt);rs=s(jo,"Merve Noyan"),jo.forEach(r),as=s(kt," est d\xE9veloppeuse "),qt=l(kt,"EM",{});var Bo=n(qt);ss=s(Bo,"advocate"),Bo.forEach(r),os=s(kt," chez Hugging Face et travaille \xE0 la cr\xE9ation d\u2019outils et de contenus visant \xE0 d\xE9mocratiser l\u2019apprentissage machine pour tous."),kt.forEach(r),ar=d(t),b=l(t,"P",{});var we=n(b);Pt=l(we,"STRONG",{});var Ro=n(Pt);ls=s(Ro,"Lucile Saulnier"),Ro.forEach(r),ns=s(we," est ing\xE9nieure en apprentissage machine chez Hugging Face et travaille au d\xE9veloppement et \xE0 l\u2019impl\xE9mentation de nombreux outils "),Lt=l(we,"EM",{});var Uo=n(Lt);is=s(Uo,"open source"),Uo.forEach(r),us=s(we,". Elle est \xE9galement activement impliqu\xE9e dans de nombreux projets de recherche dans le domaine du NLP comme l\u2019entra\xEEnement collaboratif de mod\xE8les et le projet "),fe=l(we,"A",{href:!0,rel:!0});var Yo=n(fe);ps=s(Yo,"BigScience"),Yo.forEach(r),cs=s(we,"."),we.forEach(r),sr=d(t),w=l(t,"P",{});var qe=n(w);yt=l(qe,"STRONG",{});var Jo=n(yt);ds=s(Jo,"Lewis Tunstall"),Jo.forEach(r),ms=s(qe," est ing\xE9nieur en apprentissage machine chez Hugging Face et d\xE9vou\xE9 au d\xE9veloppement d\u2019outils "),At=l(qe,"EM",{});var Ko=n(At);fs=s(Ko,"open source"),Ko.forEach(r),hs=s(qe," avec la volont\xE9 de les rendre accessibles \xE0 une communaut\xE9 plus large. Il est \xE9galement co-auteur du livre "),he=l(qe,"A",{href:!0,rel:!0});var Qo=n(he);$t=l(Qo,"EM",{});var Vo=n($t);vs=s(Vo,"Natural Language Processing with Transformers"),Vo.forEach(r),Qo.forEach(r),gs=s(qe,"."),qe.forEach(r),or=d(t),q=l(t,"P",{});var Pe=n(q);Nt=l(Pe,"STRONG",{});var Wo=n(Nt);Es=s(Wo,"Leandro von Werra"),Wo.forEach(r),_s=s(Pe," est ing\xE9nieur en apprentissage machine dans l\u2019\xE9quipe "),zt=l(Pe,"EM",{});var Xo=n(zt);bs=s(Xo,"open source"),Xo.forEach(r),ws=s(Pe," d\u2019Hugging Face et \xE9galement co-auteur du livre "),ve=l(Pe,"A",{href:!0,rel:!0});var Zo=n(ve);Tt=l(Zo,"EM",{});var el=n(Tt);qs=s(el,"Natural Language Processing with Transformers"),el.forEach(r),Zo.forEach(r),Ps=s(Pe,". Il a plusieurs ann\xE9es d\u2019exp\xE9rience dans l\u2019industrie o\xF9 il a pu d\xE9ployer des projets de NLP en production et travailler sur toutes les \xE9tapes clefs du d\xE9ploiement."),Pe.forEach(r),lr=d(t),$e=l(t,"P",{});var tl=n($e);Ls=s(tl,"\xCAtes-vous pr\xEAt \xE0 commencer ? Dans ce chapitre, vous apprendrez :"),tl.forEach(r),nr=d(t),y=l(t,"UL",{});var Se=n(y);ge=l(Se,"LI",{});var hr=n(ge);ys=s(hr,"\xE0 utiliser la fonction "),It=l(hr,"CODE",{});var rl=n(It);As=s(rl,"pipeline()"),rl.forEach(r),$s=s(hr," pour r\xE9soudre des probl\xE8mes de NLP comme la g\xE9n\xE9ration de texte et la classification,"),hr.forEach(r),Ns=d(Se),Ee=l(Se,"LI",{});var vr=n(Ee);zs=s(vr,"l\u2019architecture d\u2019un "),Mt=l(vr,"EM",{});var al=n(Mt);Ts=s(al,"transformer"),al.forEach(r),Is=s(vr,","),vr.forEach(r),Ms=d(Se),St=l(Se,"LI",{});var sl=n(St);Ss=s(sl,"comment faire la distinction entre les diff\xE9rentes architectures d\u2019encodeur, de d\xE9codeur et d\u2019encodeur-d\xE9codeur ainsi que leurs diff\xE9rents cas d\u2019usage."),sl.forEach(r),Se.forEach(r),this.h()},h(){i(A,"name","hf:doc:metadata"),i(A,"content",JSON.stringify(hl)),i(F,"id","introduction"),i(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(F,"href","#introduction"),i($,"class","relative group"),i(G,"id","bienvenue-au-cours"),i(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(G,"href","#bienvenue-au-cours"),i(N,"class","relative group"),i(X,"href","https://huggingface.co/"),i(X,"rel","nofollow"),i(C,"href","https://github.com/huggingface/transformers"),i(C,"rel","nofollow"),i(O,"href","https://github.com/huggingface/datasets"),i(O,"rel","nofollow"),i(j,"href","https://github.com/huggingface/tokenizers"),i(j,"rel","nofollow"),i(B,"href","https://github.com/huggingface/accelerate"),i(B,"rel","nofollow"),i(Z,"href","https://huggingface.co/models"),i(Z,"rel","nofollow"),i(R,"id","quoi-sattendre"),i(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(R,"href","#quoi-sattendre"),i(z,"class","relative group"),i(te,"class","block dark:hidden"),ol(te.src,Cs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||i(te,"src",Cs),i(te,"alt","Bref aper\xE7u du contenu du cours."),i(re,"class","hidden dark:block"),ol(re.src,Os="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||i(re,"src",Os),i(re,"alt","Bref aper\xE7u des diff\xE9rents chapitres du cours."),i(T,"class","flex justify-center"),i(ae,"href","https://huggingface.co/models"),i(ae,"rel","nofollow"),i(se,"href","https://www.fast.ai/"),i(se,"rel","nofollow"),i(oe,"href","https://course.fast.ai/"),i(oe,"rel","nofollow"),i(le,"href","https://www.deeplearning.ai/"),i(le,"rel","nofollow"),i(ne,"href","https://pytorch.org/"),i(ne,"rel","nofollow"),i(ie,"href","https://www.tensorflow.org/"),i(ie,"rel","nofollow"),i(ue,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),i(ue,"rel","nofollow"),i(Y,"id","qui-sommesnous"),i(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Y,"href","#qui-sommesnous"),i(H,"class","relative group"),i(ce,"href","https://github.com/gradio-app/gradio"),i(ce,"rel","nofollow"),i(de,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),i(de,"rel","nofollow"),i(fe,"href","https://bigscience.huggingface.co/"),i(fe,"rel","nofollow"),i(he,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),i(he,"rel","nofollow"),i(ve,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),i(ve,"rel","nofollow")},m(t,u){e(document.head,A),p(t,Dt,u),p(t,$,u),e($,F),e(F,Ce),ke(K,Ce,null),e($,Er),e($,Oe),e(Oe,_r),p(t,Ft,u),ke(Q,t,u),p(t,Gt,u),p(t,N,u),e(N,G),e(G,je),ke(V,je,null),e(N,br),e(N,Be),e(Be,wr),p(t,Ct,u),ke(W,t,u),p(t,Ot,u),p(t,m,u),e(m,qr),e(m,X),e(X,Pr),e(m,Lr),e(m,C),e(C,yr),e(C,Re),e(Re,Ar),e(m,$r),e(m,O),e(O,Nr),e(O,Ue),e(Ue,zr),e(m,Tr),e(m,j),e(j,Ir),e(j,Ye),e(Ye,Mr),e(m,Sr),e(m,B),e(B,Hr),e(B,Je),e(Je,xr),e(m,kr),e(m,Z),e(Z,Ke),e(Ke,Dr),e(m,Fr),p(t,jt,u),p(t,z,u),e(z,R),e(R,Qe),ke(ee,Qe,null),e(z,Gr),e(z,Ve),e(Ve,Cr),p(t,Bt,u),p(t,Le,u),e(Le,Or),p(t,Rt,u),p(t,T,u),e(T,te),e(T,jr),e(T,re),p(t,Ut,u),p(t,P,u),e(P,f),e(f,Br),e(f,We),e(We,Rr),e(f,Ur),e(f,Xe),e(Xe,Yr),e(f,Jr),e(f,ae),e(ae,Ze),e(Ze,Kr),e(f,Qr),e(f,et),e(et,Vr),e(f,Wr),e(f,tt),e(tt,Xr),e(f,Zr),e(P,ea),e(P,I),e(I,ta),e(I,rt),e(rt,ra),e(I,aa),e(I,at),e(at,sa),e(I,oa),e(P,la),e(P,M),e(M,na),e(M,st),e(st,ia),e(M,ua),e(M,ot),e(ot,pa),e(M,ca),p(t,Yt,u),p(t,ye,u),e(ye,da),p(t,Jt,u),p(t,L,u),e(L,lt),e(lt,ma),e(L,fa),e(L,g),e(g,ha),e(g,se),e(se,va),e(g,ga),e(g,oe),e(oe,nt),e(nt,Ea),e(g,_a),e(g,le),e(le,it),e(it,ba),e(g,wa),e(L,qa),e(L,S),e(S,Pa),e(S,ne),e(ne,La),e(S,ya),e(S,ie),e(ie,Aa),e(S,$a),p(t,Kt,u),p(t,U,u),e(U,Na),e(U,ue),e(ue,za),e(U,Ta),p(t,Qt,u),p(t,H,u),e(H,Y),e(Y,ut),ke(pe,ut,null),e(H,Ia),e(H,pt),e(pt,Ma),p(t,Vt,u),p(t,Ae,u),e(Ae,Sa),p(t,Wt,u),p(t,E,u),e(E,ct),e(ct,Ha),e(E,xa),e(E,ce),e(ce,ka),e(E,Da),e(E,dt),e(dt,Fa),e(E,Ga),p(t,Xt,u),p(t,x,u),e(x,mt),e(mt,Ca),e(x,Oa),e(x,ft),e(ft,ja),e(x,Ba),p(t,Zt,u),p(t,k,u),e(k,ht),e(ht,Ra),e(k,Ua),e(k,vt),e(vt,Ya),e(k,Ja),p(t,er,u),p(t,_,u),e(_,gt),e(gt,Ka),e(_,Qa),e(_,Et),e(Et,Va),e(_,Wa),e(_,de),e(de,_t),e(_t,Xa),e(_,Za),p(t,tr,u),p(t,me,u),e(me,bt),e(bt,es),e(me,ts),p(t,rr,u),p(t,D,u),e(D,wt),e(wt,rs),e(D,as),e(D,qt),e(qt,ss),e(D,os),p(t,ar,u),p(t,b,u),e(b,Pt),e(Pt,ls),e(b,ns),e(b,Lt),e(Lt,is),e(b,us),e(b,fe),e(fe,ps),e(b,cs),p(t,sr,u),p(t,w,u),e(w,yt),e(yt,ds),e(w,ms),e(w,At),e(At,fs),e(w,hs),e(w,he),e(he,$t),e($t,vs),e(w,gs),p(t,or,u),p(t,q,u),e(q,Nt),e(Nt,Es),e(q,_s),e(q,zt),e(zt,bs),e(q,ws),e(q,ve),e(ve,Tt),e(Tt,qs),e(q,Ps),p(t,lr,u),p(t,$e,u),e($e,Ls),p(t,nr,u),p(t,y,u),e(y,ge),e(ge,ys),e(ge,It),e(It,As),e(ge,$s),e(y,Ns),e(y,Ee),e(Ee,zs),e(Ee,Mt),e(Mt,Ts),e(Ee,Is),e(y,Ms),e(y,St),e(St,Ss),ir=!0},p:pl,i(t){ir||(De(K.$$.fragment,t),De(Q.$$.fragment,t),De(V.$$.fragment,t),De(W.$$.fragment,t),De(ee.$$.fragment,t),De(pe.$$.fragment,t),ir=!0)},o(t){Fe(K.$$.fragment,t),Fe(Q.$$.fragment,t),Fe(V.$$.fragment,t),Fe(W.$$.fragment,t),Fe(ee.$$.fragment,t),Fe(pe.$$.fragment,t),ir=!1},d(t){r(A),t&&r(Dt),t&&r($),Ge(K),t&&r(Ft),Ge(Q,t),t&&r(Gt),t&&r(N),Ge(V),t&&r(Ct),Ge(W,t),t&&r(Ot),t&&r(m),t&&r(jt),t&&r(z),Ge(ee),t&&r(Bt),t&&r(Le),t&&r(Rt),t&&r(T),t&&r(Ut),t&&r(P),t&&r(Yt),t&&r(ye),t&&r(Jt),t&&r(L),t&&r(Kt),t&&r(U),t&&r(Qt),t&&r(H),Ge(pe),t&&r(Vt),t&&r(Ae),t&&r(Wt),t&&r(E),t&&r(Xt),t&&r(x),t&&r(Zt),t&&r(k),t&&r(er),t&&r(_),t&&r(tr),t&&r(me),t&&r(rr),t&&r(D),t&&r(ar),t&&r(b),t&&r(sr),t&&r(w),t&&r(or),t&&r(q),t&&r(lr),t&&r($e),t&&r(nr),t&&r(y)}}}const hl={local:"introduction",sections:[{local:"bienvenue-au-cours",title:"Bienvenue au cours \u{1F917} !"},{local:"quoi-sattendre",title:"\xC0 quoi s'attendre ?"},{local:"qui-sommesnous",title:"Qui sommes-nous ?"}],title:"Introduction"};function vl(Gs){return cl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wl extends ll{constructor(A){super();nl(this,A,vl,fl,il,{})}}export{wl as default,hl as metadata};
