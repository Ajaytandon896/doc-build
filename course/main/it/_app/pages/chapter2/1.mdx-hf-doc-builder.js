import{S as ai,i as ti,s as ni,e as a,k as h,w as xe,t as l,M as si,c as t,d as o,m as _,a as n,x as Fe,h as r,b as I,G as i,g as c,y as De,q as Ge,o as Qe,B as Re,v as ci}from"../../chunks/vendor-hf-doc-builder.js";import{T as ui}from"../../chunks/Tip-hf-doc-builder.js";import{I as mi}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as di}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function pi(K){let m,v,p,f;return{c(){m=l("\u26A0\uFE0F Per poter usufruire di tutte le funzioni disponibili con il Model Hub e i \u{1F917} Transformers, si consiglia di "),v=a("a"),p=l("creare un account"),f=l("."),this.h()},l(u){m=r(u,"\u26A0\uFE0F Per poter usufruire di tutte le funzioni disponibili con il Model Hub e i \u{1F917} Transformers, si consiglia di "),v=t(u,"A",{href:!0});var d=n(v);p=r(d,"creare un account"),d.forEach(o),f=r(u,"."),this.h()},h(){I(v,"href","https://huggingface.co/join")},m(u,d){c(u,m,d),c(u,v,d),i(v,p),c(u,f,d)},d(u){u&&o(m),u&&o(v),u&&o(f)}}}function fi(K){let m,v,p,f,u,d,re,F,ae,V,P,W,z,te,L,ne,se,D,ce,ue,X,k,me,Y,b,A,G,de,pe,fe,E,Q,ve,he,R,_e,ze,H,be,Ee,$e,C,B,ge,Ie,Z,S,qe,ee,$,Pe,U,Te,we,y,Le,ke,ie,g,Ae,j,Ce,Se,J,ye,Me,oe,q,le;return d=new mi({}),P=new di({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),q=new ui({props:{$$slots:{default:[pi]},$$scope:{ctx:K}}}),{c(){m=a("meta"),v=h(),p=a("h1"),f=a("a"),u=a("span"),xe(d.$$.fragment),re=h(),F=a("span"),ae=l("Introduzione"),V=h(),xe(P.$$.fragment),W=h(),z=a("p"),te=l("Come si \xE8 visto nel "),L=a("a"),ne=l("Capitolo 1"),se=l(`,  I modelli Transformers sono solitamente molto grandi.
Con milioni o decine di `),D=a("em"),ce=l("miliardi"),ue=l(` di parametri, l\u2019addestramento e la distribuzione di questi modelli \xE8 un\u2019impresa complicata.
Inoltre, con i nuovi modelli che vengono rilasciati quasi ogni giorno e ognuno dei quali ha una propria implementazione, provarli tutti non \xE8 un lavoro facile.`),X=h(),k=a("p"),me=l("La libreria \u{1F917} Transformers \xE8 stata creata per risolvere questo problema. Il suo obiettivo \xE8 fornire un\u2019unica API attraverso la quale caricare, addestrare e salvare qualsiasi modello Transformer. Le caratteristiche principali della libreria sono:"),Y=h(),b=a("ul"),A=a("li"),G=a("strong"),de=l("Facilit\xE0 d\u2019uso"),pe=l(": \xC8 possibile scaricare, caricare ed utilizzare un modello NLP all\u2019avanguardia per fare inferenza con appena due righe di codice."),fe=h(),E=a("li"),Q=a("strong"),ve=l("Flessibilit\xE0"),he=l(": Al loro interno, tutti i modelli sono semplici classi PyTorch "),R=a("code"),_e=l("nn.Module"),ze=l(" o TensorFlow "),H=a("code"),be=l("tf.keras.Model"),Ee=l(" e possono essere gestiti come qualsiasi altro modello nei rispettivi framework di apprendimento automatico (ML)."),$e=h(),C=a("li"),B=a("strong"),ge=l("Semplicit\xE0"),Ie=l(": La libreria non contiene quasi nessuna astrazione. Il concetto di \u201CAll in one file\u201D \xE8 fondamentale: il forward pass di un modello \xE8 interamente definito in un singolo file, in modo che il codice stesso sia comprensibile e violabile."),Z=h(),S=a("p"),qe=l("Quest\u2019ultima caratteristica rende \u{1F917} Transformers molto diversi da altre librerie ML. I modelli non sono costruiti su moduli condivisi tra i file, ma ogni modello ha i propri layers. Oltre a rendere i modelli pi\xF9 accessibili e comprensibili, questo permette di sperimentare facilmente su un modello senza influenzare gli altri."),ee=h(),$=a("p"),Pe=l("Questo capitolo inizier\xE0 con un esempio in cui usiamo un modello e un tokenizer insieme per replicare la funzione "),U=a("code"),Te=l("pipeline()"),we=l(" introdotta nel "),y=a("a"),Le=l("Capitolo 1"),ke=l(". Successivamente, parleremo dell\u2019API del modello: ci immergeremo nelle classi del modello e della configurazione e mostreremo come caricare un modello e come esso elabora gli input numerici per produrre previsioni."),ie=h(),g=a("p"),Ae=l("Successivamente vedremo l\u2019API del tokenizer, che \xE8 l\u2019altro componente principale della funzione "),j=a("code"),Ce=l("pipeline()"),Se=l(". I tokenizer si occupano della prima e dell\u2019ultima fase di elaborazione, gestendo la conversione da testo a input numerici per la rete neurale e la conversione di nuovo in testo quando \xE8 necessario. Infine, mostreremo come gestire l\u2019invio di pi\xF9 frasi a un modello in un batch preparato, per poi concludere il tutto con un\u2019analisi pi\xF9 approfondita della funzione di alto livello "),J=a("code"),ye=l("tokenizer()"),Me=l("."),oe=h(),xe(q.$$.fragment),this.h()},l(e){const s=si('[data-svelte="svelte-1phssyn"]',document.head);m=t(s,"META",{name:!0,content:!0}),s.forEach(o),v=_(e),p=t(e,"H1",{class:!0});var T=n(p);f=t(T,"A",{id:!0,class:!0,href:!0});var He=n(f);u=t(He,"SPAN",{});var Be=n(u);Fe(d.$$.fragment,Be),Be.forEach(o),He.forEach(o),re=_(T),F=t(T,"SPAN",{});var Ue=n(F);ae=r(Ue,"Introduzione"),Ue.forEach(o),T.forEach(o),V=_(e),Fe(P.$$.fragment,e),W=_(e),z=t(e,"P",{});var M=n(z);te=r(M,"Come si \xE8 visto nel "),L=t(M,"A",{href:!0});var je=n(L);ne=r(je,"Capitolo 1"),je.forEach(o),se=r(M,`,  I modelli Transformers sono solitamente molto grandi.
Con milioni o decine di `),D=t(M,"EM",{});var Je=n(D);ce=r(Je,"miliardi"),Je.forEach(o),ue=r(M,` di parametri, l\u2019addestramento e la distribuzione di questi modelli \xE8 un\u2019impresa complicata.
Inoltre, con i nuovi modelli che vengono rilasciati quasi ogni giorno e ognuno dei quali ha una propria implementazione, provarli tutti non \xE8 un lavoro facile.`),M.forEach(o),X=_(e),k=t(e,"P",{});var Ke=n(k);me=r(Ke,"La libreria \u{1F917} Transformers \xE8 stata creata per risolvere questo problema. Il suo obiettivo \xE8 fornire un\u2019unica API attraverso la quale caricare, addestrare e salvare qualsiasi modello Transformer. Le caratteristiche principali della libreria sono:"),Ke.forEach(o),Y=_(e),b=t(e,"UL",{});var O=n(b);A=t(O,"LI",{});var Oe=n(A);G=t(Oe,"STRONG",{});var Ve=n(G);de=r(Ve,"Facilit\xE0 d\u2019uso"),Ve.forEach(o),pe=r(Oe,": \xC8 possibile scaricare, caricare ed utilizzare un modello NLP all\u2019avanguardia per fare inferenza con appena due righe di codice."),Oe.forEach(o),fe=_(O),E=t(O,"LI",{});var w=n(E);Q=t(w,"STRONG",{});var We=n(Q);ve=r(We,"Flessibilit\xE0"),We.forEach(o),he=r(w,": Al loro interno, tutti i modelli sono semplici classi PyTorch "),R=t(w,"CODE",{});var Xe=n(R);_e=r(Xe,"nn.Module"),Xe.forEach(o),ze=r(w," o TensorFlow "),H=t(w,"CODE",{});var Ye=n(H);be=r(Ye,"tf.keras.Model"),Ye.forEach(o),Ee=r(w," e possono essere gestiti come qualsiasi altro modello nei rispettivi framework di apprendimento automatico (ML)."),w.forEach(o),$e=_(O),C=t(O,"LI",{});var Ne=n(C);B=t(Ne,"STRONG",{});var Ze=n(B);ge=r(Ze,"Semplicit\xE0"),Ze.forEach(o),Ie=r(Ne,": La libreria non contiene quasi nessuna astrazione. Il concetto di \u201CAll in one file\u201D \xE8 fondamentale: il forward pass di un modello \xE8 interamente definito in un singolo file, in modo che il codice stesso sia comprensibile e violabile."),Ne.forEach(o),O.forEach(o),Z=_(e),S=t(e,"P",{});var ei=n(S);qe=r(ei,"Quest\u2019ultima caratteristica rende \u{1F917} Transformers molto diversi da altre librerie ML. I modelli non sono costruiti su moduli condivisi tra i file, ma ogni modello ha i propri layers. Oltre a rendere i modelli pi\xF9 accessibili e comprensibili, questo permette di sperimentare facilmente su un modello senza influenzare gli altri."),ei.forEach(o),ee=_(e),$=t(e,"P",{});var N=n($);Pe=r(N,"Questo capitolo inizier\xE0 con un esempio in cui usiamo un modello e un tokenizer insieme per replicare la funzione "),U=t(N,"CODE",{});var ii=n(U);Te=r(ii,"pipeline()"),ii.forEach(o),we=r(N," introdotta nel "),y=t(N,"A",{href:!0});var oi=n(y);Le=r(oi,"Capitolo 1"),oi.forEach(o),ke=r(N,". Successivamente, parleremo dell\u2019API del modello: ci immergeremo nelle classi del modello e della configurazione e mostreremo come caricare un modello e come esso elabora gli input numerici per produrre previsioni."),N.forEach(o),ie=_(e),g=t(e,"P",{});var x=n(g);Ae=r(x,"Successivamente vedremo l\u2019API del tokenizer, che \xE8 l\u2019altro componente principale della funzione "),j=t(x,"CODE",{});var li=n(j);Ce=r(li,"pipeline()"),li.forEach(o),Se=r(x,". I tokenizer si occupano della prima e dell\u2019ultima fase di elaborazione, gestendo la conversione da testo a input numerici per la rete neurale e la conversione di nuovo in testo quando \xE8 necessario. Infine, mostreremo come gestire l\u2019invio di pi\xF9 frasi a un modello in un batch preparato, per poi concludere il tutto con un\u2019analisi pi\xF9 approfondita della funzione di alto livello "),J=t(x,"CODE",{});var ri=n(J);ye=r(ri,"tokenizer()"),ri.forEach(o),Me=r(x,"."),x.forEach(o),oe=_(e),Fe(q.$$.fragment,e),this.h()},h(){I(m,"name","hf:doc:metadata"),I(m,"content",JSON.stringify(vi)),I(f,"id","introduzione"),I(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),I(f,"href","#introduzione"),I(p,"class","relative group"),I(L,"href","/course/chapter1"),I(y,"href","/course/chapter1")},m(e,s){i(document.head,m),c(e,v,s),c(e,p,s),i(p,f),i(f,u),De(d,u,null),i(p,re),i(p,F),i(F,ae),c(e,V,s),De(P,e,s),c(e,W,s),c(e,z,s),i(z,te),i(z,L),i(L,ne),i(z,se),i(z,D),i(D,ce),i(z,ue),c(e,X,s),c(e,k,s),i(k,me),c(e,Y,s),c(e,b,s),i(b,A),i(A,G),i(G,de),i(A,pe),i(b,fe),i(b,E),i(E,Q),i(Q,ve),i(E,he),i(E,R),i(R,_e),i(E,ze),i(E,H),i(H,be),i(E,Ee),i(b,$e),i(b,C),i(C,B),i(B,ge),i(C,Ie),c(e,Z,s),c(e,S,s),i(S,qe),c(e,ee,s),c(e,$,s),i($,Pe),i($,U),i(U,Te),i($,we),i($,y),i(y,Le),i($,ke),c(e,ie,s),c(e,g,s),i(g,Ae),i(g,j),i(j,Ce),i(g,Se),i(g,J),i(J,ye),i(g,Me),c(e,oe,s),De(q,e,s),le=!0},p(e,[s]){const T={};s&2&&(T.$$scope={dirty:s,ctx:e}),q.$set(T)},i(e){le||(Ge(d.$$.fragment,e),Ge(P.$$.fragment,e),Ge(q.$$.fragment,e),le=!0)},o(e){Qe(d.$$.fragment,e),Qe(P.$$.fragment,e),Qe(q.$$.fragment,e),le=!1},d(e){o(m),e&&o(v),e&&o(p),Re(d),e&&o(V),Re(P,e),e&&o(W),e&&o(z),e&&o(X),e&&o(k),e&&o(Y),e&&o(b),e&&o(Z),e&&o(S),e&&o(ee),e&&o($),e&&o(ie),e&&o(g),e&&o(oe),Re(q,e)}}}const vi={local:"introduzione",title:"Introduzione"};function hi(K){return ci(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gi extends ai{constructor(m){super();ti(this,m,hi,fi,ni,{})}}export{gi as default,vi as metadata};
