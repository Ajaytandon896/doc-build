import{S as Pl,i as jl,s as Il,e as s,k as c,w as h,t as l,M as yl,c as r,d as o,m as u,x as k,a as n,h as p,b as m,N as Oo,G as i,g as a,y as b,o as v,p as Al,q as z,B as $,v as Dl,n as Tl}from"../../chunks/vendor-hf-doc-builder.js";import{T as xl}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Ni}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Q}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as D}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as ql}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Cl}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Sl(T){let d,_;return d=new ql({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){h(d.$$.fragment)},l(f){k(d.$$.fragment,f)},m(f,w){b(d,f,w),_=!0},i(f){_||(z(d.$$.fragment,f),_=!0)},o(f){v(d.$$.fragment,f),_=!1},d(f){$(d,f)}}}function Nl(T){let d,_;return d=new ql({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){h(d.$$.fragment)},l(f){k(d.$$.fragment,f)},m(f,w){b(d,f,w),_=!0},i(f){_||(z(d.$$.fragment,f),_=!0)},o(f){v(d.$$.fragment,f),_=!1},d(f){$(d,f)}}}function Ll(T){let d,_,f,w,q,g,P,j;return{c(){d=s("p"),_=l("In modo simile a "),f=s("code"),w=l("TFAutoModel"),q=l(", la classe "),g=s("code"),P=l("AutoTokenizer"),j=l(" prender\xE0 la classe tokenizer appropriata nella libreria in base al nome del checkpoint e pu\xF2 essere usata direttamente con qualsiasi checkpoint:")},l(I){d=r(I,"P",{});var E=n(d);_=p(E,"In modo simile a "),f=r(E,"CODE",{});var B=n(f);w=p(B,"TFAutoModel"),B.forEach(o),q=p(E,", la classe "),g=r(E,"CODE",{});var N=n(g);P=p(N,"AutoTokenizer"),N.forEach(o),j=p(E," prender\xE0 la classe tokenizer appropriata nella libreria in base al nome del checkpoint e pu\xF2 essere usata direttamente con qualsiasi checkpoint:"),E.forEach(o)},m(I,E){a(I,d,E),i(d,_),i(d,f),i(f,w),i(d,q),i(d,g),i(g,P),i(d,j)},d(I){I&&o(d)}}}function Ul(T){let d,_,f,w,q,g,P,j;return{c(){d=s("p"),_=l("In modo simile a "),f=s("code"),w=l("AutoModel"),q=l(", la classe "),g=s("code"),P=l("AutoTokenizer"),j=l(" prender\xE0 la classe tokenizer appropriata nella libreria in base al nome del checkpoint e pu\xF2 essere usata direttamente con qualsiasi checkpoint:")},l(I){d=r(I,"P",{});var E=n(d);_=p(E,"In modo simile a "),f=r(E,"CODE",{});var B=n(f);w=p(B,"AutoModel"),B.forEach(o),q=p(E,", la classe "),g=r(E,"CODE",{});var N=n(g);P=p(N,"AutoTokenizer"),N.forEach(o),j=p(E," prender\xE0 la classe tokenizer appropriata nella libreria in base al nome del checkpoint e pu\xF2 essere usata direttamente con qualsiasi checkpoint:"),E.forEach(o)},m(I,E){a(I,d,E),i(d,_),i(d,f),i(f,w),i(d,q),i(d,g),i(g,P),i(d,j)},d(I){I&&o(d)}}}function Ml(T){let d,_,f,w,q;return{c(){d=s("p"),_=l("\u270F\uFE0F "),f=s("strong"),w=l("Provaci anche tu!"),q=l(" Replica gli ultimi due passaggi (tokenizzazione e conversione in ID di input) sulle frasi di input utilizzate nella sezione 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Verificate di ottenere gli stessi ID di input che abbiamo ottenuto in precedenza!")},l(g){d=r(g,"P",{});var P=n(d);_=p(P,"\u270F\uFE0F "),f=r(P,"STRONG",{});var j=n(f);w=p(j,"Provaci anche tu!"),j.forEach(o),q=p(P," Replica gli ultimi due passaggi (tokenizzazione e conversione in ID di input) sulle frasi di input utilizzate nella sezione 2 (\u201CI\u2019ve been waiting for a HuggingFace course my whole life.\u201D e \u201CI hate this so much!\u201D). Verificate di ottenere gli stessi ID di input che abbiamo ottenuto in precedenza!"),P.forEach(o)},m(g,P){a(g,d,P),i(d,_),i(d,f),i(f,w),i(d,q)},d(g){g&&o(d)}}}function Ol(T){let d,_,f,w,q,g,P,j,I,E,B,N,C,S,lo,qe,Li,po,ja,Ui,co,Ia,Mi,Pe,Oi,uo,ya,Hi,mo,Aa,Qi,G,te,Ho,je,Da,Qo,Ta,Bi,Ie,Gi,ae,xa,Bo,Ca,Sa,Ri,R,ye,Or,Na,Ae,Hr,Ji,se,La,Go,Ua,Ma,Vi,De,Fi,Te,Ki,fo,Oa,Yi,vo,Ha,Wi,zo,Qa,Xi,xe,Ba,Ro,Ga,Zi,re,Ra,Jo,Ja,Va,et,J,ne,Vo,Ce,Fa,Fo,Ka,ot,Se,it,ho,Ya,tt,le,Ko,Wa,Xa,Yo,Za,at,ko,es,st,V,Ne,Qr,os,Le,Br,rt,bo,is,nt,$o,ts,lt,pe,as,Wo,ss,rs,pt,F,ce,Xo,Ue,ns,Zo,ls,ct,Me,ut,_o,ps,mt,go,cs,dt,Eo,us,ft,K,Oe,Gr,ms,He,Rr,vt,wo,ds,zt,qo,fs,ht,Y,ue,ei,Qe,vs,oi,zs,kt,Po,hs,bt,L,ii,ks,bs,ti,$s,_s,ai,gs,$t,jo,Es,_t,W,me,si,Be,ws,ri,qs,gt,y,Ps,ni,js,Is,li,ys,As,pi,Ds,Ts,ci,xs,Cs,Et,de,Ss,ui,Ns,Ls,wt,Ge,qt,Io,Re,Pt,yo,Us,jt,Je,It,Ve,yt,Ao,Ms,At,Fe,Dt,A,Os,mi,Hs,Qs,Do,Bs,Gs,di,Rs,Js,fi,Vs,Fs,Tt,X,fe,vi,Ke,Ks,zi,Ys,xt,Ye,Ct,ve,Ws,hi,Xs,Zs,St,ze,er,ki,or,ir,Nt,U,tr,bi,ar,sr,$i,rr,nr,Lt,To,lr,Ut,Z,he,_i,We,pr,gi,cr,Mt,ke,ur,Ei,mr,dr,Ot,Xe,Ht,xo,fr,Qt,Ze,Bt,x,vr,wi,zr,hr,qi,kr,br,Pi,$r,_r,Gt,ee,be,ji,eo,gr,Ii,Er,Rt,$e,wr,yi,qr,Pr,Jt,oo,Vt,io,Ft,Co,jr,Kt,_e,Yt,oe,ge,Ai,to,Ir,Di,yr,Wt,M,Ar,Ti,Dr,Tr,xi,xr,Cr,Xt,ao,Zt,so,ea,Ee,Sr,Ci,Nr,Lr,oa,So,Ur,ia;f=new Cl({props:{fw:T[0]}}),j=new Q({});const Jr=[Nl,Sl],ro=[];function Vr(e,t){return e[0]==="pt"?0:1}C=Vr(T),S=ro[C]=Jr[C](T),qe=new Ni({props:{id:"VFp38yj8h3A"}}),Pe=new D({props:{code:"Jim Henson was a puppeteer",highlighted:'<span class="hljs-comment">Jim Henson was a puppeteer</span>'}}),je=new Q({}),Ie=new Ni({props:{id:"nhJxYji1aho"}}),De=new D({props:{code:`tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson was a puppeteer&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),Te=new D({props:{code:"['Jim', 'Henson', 'was', 'a', 'puppeteer']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;puppeteer&#x27;</span>]'}}),Ce=new Q({}),Se=new Ni({props:{id:"ssLq_EK2jLE"}}),Ue=new Q({}),Me=new Ni({props:{id:"zHvTiHr506c"}}),Qe=new Q({}),Be=new Q({}),Ge=new D({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function Fr(e,t){return e[0]==="pt"?Ul:Ll}let ta=Fr(T),ie=ta(T);return Re=new D({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Je=new D({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),Ve=new D({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Fe=new D({props:{code:'tokenizer.save_pretrained("directory_on_my_computer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}}),Ke=new Q({}),Ye=new Ni({props:{id:"Yffk5aydLzg"}}),We=new Q({}),Xe=new D({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),Ze=new D({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),eo=new Q({}),oo=new D({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),io=new D({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),_e=new xl({props:{$$slots:{default:[Ml]},$$scope:{ctx:T}}}),to=new Q({}),ao=new D({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),so=new D({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){d=s("meta"),_=c(),h(f.$$.fragment),w=c(),q=s("h1"),g=s("a"),P=s("span"),h(j.$$.fragment),I=c(),E=s("span"),B=l("Tokenizers"),N=c(),S.c(),lo=c(),h(qe.$$.fragment),Li=c(),po=s("p"),ja=l("I tokenizer sono uno dei componenti fondamentali della pipeline NLP. Servono a uno scopo: tradurre il testo in dati che possono essere elaborati dal modello. I modelli possono elaborare solo numeri, quindi i tokenizer devono convertire i nostri input testuali in dati numerici. In questa sezione analizzeremo cosa succede esattamente nella pipeline di tokenizzazione."),Ui=c(),co=s("p"),Ia=l("Nelle attivit\xE0 di NLP, i dati che vengono generalmente processati sono testi non elaborati, grezzi. Ecco un esempio di testo grezzo:"),Mi=c(),h(Pe.$$.fragment),Oi=c(),uo=s("p"),ya=l("Tuttavia, i modelli possono elaborare solo numeri, quindi dobbiamo trovare un modo per convertire il testo non elaborato in numeri. Questo \xE8 ci\xF2 che fanno i tokenizer, e ci sono molti modi per farlo. L\u2019obiettivo \xE8 trovare la rappresentazione pi\xF9 significativa, cio\xE8 quella che ha pi\xF9 senso per il modello, e, se possibile, la rappresentazione pi\xF9 piccola."),Hi=c(),mo=s("p"),Aa=l("Vediamo alcuni esempi di algoritmi di tokenizzazione e cerchiamo di rispondere ad alcune domande sulla tokenizzazione."),Qi=c(),G=s("h2"),te=s("a"),Ho=s("span"),h(je.$$.fragment),Da=c(),Qo=s("span"),Ta=l("Tokenizer basati sulle parole"),Bi=c(),h(Ie.$$.fragment),Gi=c(),ae=s("p"),xa=l("Il primo tipo di tokenizzatore che viene in mente \xE8 quello "),Bo=s("em"),Ca=l("basato sulle parole"),Sa=l(". In genere \xE8 molto facile da configurare e utilizzare con poche regole e spesso produce risultati decenti. Ad esempio, nell\u2019immagine qui sotto, l\u2019obiettivo \xE8 dividere il testo non elaborato in parole e trovare una rappresentazione numerica per ciascuna di esse:"),Ri=c(),R=s("div"),ye=s("img"),Na=c(),Ae=s("img"),Ji=c(),se=s("p"),La=l("Esistono diversi modi per dividere il testo. Ad esempio, si possono usare gli spazi bianchi per suddividere il testo in parole, applicando la funzione "),Go=s("code"),Ua=l("split()"),Ma=l(" di Python:"),Vi=c(),h(De.$$.fragment),Fi=c(),h(Te.$$.fragment),Ki=c(),fo=s("p"),Oa=l("Esistono anche varianti di tokenizzatori di parole che prevedono regole aggiuntive per la punteggiatura. Con questo tipo di tokenizer, possiamo ritrovarci con \u201Cvocabolari\u201D piuttosto grandi, dove un vocabolario \xE8 definito dal numero totale di token indipendenti che abbiamo nel nostro corpus."),Yi=c(),vo=s("p"),Ha=l("A ogni parola viene assegnato un ID, a partire da 0 fino alla dimensione del vocabolario. Il modello utilizza questi ID per identificare ogni parola."),Wi=c(),zo=s("p"),Qa=l("Se vogliamo coprire completamente una lingua con un tokenizzatore basato sulle parole, dovremo avere un identificatore per ogni parola della lingua, il che generer\xE0 un\u2019enorme quantit\xE0 di token. Per esempio, nella lingua inglese ci sono pi\xF9 di 500.000 parole, quindi per costruire una mappa da ogni parola a un ID di input dovremmo tenere traccia di cos\xEC tanti ID. Inoltre, parole come \u201Ccane\u201D sono rappresentate in modo diverso da parole come \u201Ccani\u201D, e il modello inizialmente non avr\xE0 modo di sapere che \u201Ccane\u201D e \u201Ccani\u201D sono simili: identificher\xE0 le due parole come non correlate. Lo stesso vale per altre parole simili, come \u201Ccorrere\u201D e \u201Ccorrendo\u201D, che il modello non vedr\xE0 inizialmente come simili."),Xi=c(),xe=s("p"),Ba=l("Infine, abbiamo bisogno di un token personalizzato per rappresentare le parole che non fanno parte del nostro vocabolario. Questo \xE8 noto come token \u201Cunknown\u201D, spesso rappresentato come \u201D[UNK]\u201D o \u201D"),Ro=s("unk"),Ga=l("\u201D. Se il tokenizer produce molti token di questo tipo \xE8 generalmente un brutto segno, perch\xE9 non \xE8 riuscito a trovare una rappresentazione sensata della parola e si stanno perdendo informazioni. L\u2019obiettivo della creazione del vocabolario \xE8 quello di fare in modo che il tokenizzatore inserisca il minor numero possibile di parole nel token sconosciuto."),Zi=c(),re=s("p"),Ra=l("Un modo per ridurre la quantit\xE0 di token sconosciuti \xE8 quello di andare un livello pi\xF9 in profondit\xE0, usando un tokenizer "),Jo=s("em"),Ja=l("character-based"),Va=l("."),et=c(),J=s("h2"),ne=s("a"),Vo=s("span"),h(Ce.$$.fragment),Fa=c(),Fo=s("span"),Ka=l("Character-based"),ot=c(),h(Se.$$.fragment),it=c(),ho=s("p"),Ya=l("I tokenizer basati sui caratteri dividono il testo in caratteri, anzich\xE9 in parole. Ci\xF2 comporta due vantaggi principali:"),tt=c(),le=s("ul"),Ko=s("li"),Wa=l("Il vocabolario \xE8 molto pi\xF9 ridotto."),Xa=c(),Yo=s("li"),Za=l("I token fuori vocabolario (sconosciuti) sono molto meno numerosi, poich\xE9 ogni parola pu\xF2 essere costruita a partire dai caratteri."),at=c(),ko=s("p"),es=l("Ma anche in questo caso sorgono alcune questioni relative agli spazi e alla punteggiatura:"),st=c(),V=s("div"),Ne=s("img"),os=c(),Le=s("img"),rt=c(),bo=s("p"),is=l("Anche questo approccio non \xE8 perfetto. Poich\xE9 la rappresentazione \xE8 ora basata su caratteri anzich\xE9 su parole, si potrebbe sostenere che, intuitivamente, \xE8 meno significativa: ogni carattere non significa molto da solo, mentre \xE8 cos\xEC per le parole. Tuttavia, anche in questo caso il significato varia a seconda della lingua; in cinese, ad esempio, ogni carattere porta con s\xE9 pi\xF9 informazioni di un carattere in una lingua latina."),nt=c(),$o=s("p"),ts=l("Un\u2019altra cosa da considerare \xE8 che ci ritroveremo con una quantit\xE0 molto elevata di token da elaborare da parte del nostro modello: mentre una parola sarebbe un singolo token con un tokenizzatore basato sulle parole, pu\xF2 facilmente trasformarsi in 10 o pi\xF9 token quando viene convertita in caratteri."),lt=c(),pe=s("p"),as=l("Per ottenere il meglio dei due mondi, possiamo utilizzare una terza tecnica che combina i due approcci: la "),Wo=s("em"),ss=l("tokenizzazione delle sottoparole"),rs=l("."),pt=c(),F=s("h2"),ce=s("a"),Xo=s("span"),h(Ue.$$.fragment),ns=c(),Zo=s("span"),ls=l("Tokenizzazione delle sottoparole"),ct=c(),h(Me.$$.fragment),ut=c(),_o=s("p"),ps=l("Gli algoritmi di tokenizzazione delle sottoparole si basano sul principio che le parole di uso frequente non devono essere suddivise in sottoparole pi\xF9 piccole, ma le parole rare devono essere scomposte in sottoparole significative."),mt=c(),go=s("p"),cs=l("Ad esempio, \u201Cfastidiosamente\u201D potrebbe essere considerata una parola rara e potrebbe essere scomposta in \u201Cfastidioso\u201D e \u201Cmente\u201D. \xC8 probabile che queste due parole compaiano pi\xF9 frequentemente come sottoparole a s\xE9 stanti, mentre il significato di \u201Cfastidiosamente\u201D viene mantenuto dal significato composito di \u201Cfastidioso\u201D e \u201Cmente\u201D."),dt=c(),Eo=s("p"),us=l("Ecco un esempio che mostra come un algoritmo di tokenizzazione delle sottoparole tokenizzerebbe la sequenza \u201CLet\u2019s do tokenization!\u201C:"),ft=c(),K=s("div"),Oe=s("img"),ms=c(),He=s("img"),vt=c(),wo=s("p"),ds=l("Queste sottoparole finiscono per fornire un significato semantico: per esempio, nell\u2019esempio precedente \u201Ctokenization\u201D \xE8 stato diviso in \u201Ctoken\u201D e \u201Cization\u201D, due token che hanno un significato semantico pur essendo efficienti dal punto di vista dello spazio (sono necessari solo due token per rappresentare una parola lunga). Questo ci permette di avere una copertura relativamente buona con vocabolari piccoli e quasi nessun token sconosciuto."),zt=c(),qo=s("p"),fs=l("Questo approccio \xE8 particolarmente utile nelle lingue agglutinanti come il turco, dove \xE8 possibile formare parole complesse (quasi) arbitrariamente lunghe mettendo insieme sottoparole."),ht=c(),Y=s("h3"),ue=s("a"),ei=s("span"),h(Qe.$$.fragment),vs=c(),oi=s("span"),zs=l("E non solo!"),kt=c(),Po=s("p"),hs=l("Non sorprende che esistano molte altre tecniche. Per citarne alcune:"),bt=c(),L=s("ul"),ii=s("li"),ks=l("Byte-level BPE, utilizzato in GPT-2"),bs=c(),ti=s("li"),$s=l("WordPiece, utilizzato in BERT"),_s=c(),ai=s("li"),gs=l("SentencePiece o Unigram, utilizzato in diversi modelli multilingua."),$t=c(),jo=s("p"),Es=l("A questo punto dovresti avere una conoscenza sufficiente di come funzionano i tokenizer per iniziare a usare l\u2019API."),_t=c(),W=s("h2"),me=s("a"),si=s("span"),h(Be.$$.fragment),ws=c(),ri=s("span"),qs=l("Caricamento e salvataggio"),gt=c(),y=s("p"),Ps=l("Caricare e salvare i tokenizer \xE8 semplice come per i modelli. In realt\xE0, si basa sugli stessi due metodi: "),ni=s("code"),js=l("from_pretrained()"),Is=l(" e "),li=s("code"),ys=l("save_pretrained()"),As=l(". Questi metodi caricano o salvano l\u2019algoritmo usato dal tokenizer (un po\u2019 come l\u2019"),pi=s("em"),Ds=l("architettura"),Ts=l(" del modello) ed il suo vocabolario (un po\u2019 come i "),ci=s("em"),xs=l("pesi"),Cs=l(" del modello)."),Et=c(),de=s("p"),Ss=l("Il caricamento del tokenizer di BERT, addestrato con lo stesso checkpoint di BERT, avviene nello stesso modo in cui si carica il modello, con la differenza che si usa la classe "),ui=s("code"),Ns=l("BertTokenizer"),Ls=l(":"),wt=c(),h(Ge.$$.fragment),qt=c(),ie.c(),Io=c(),h(Re.$$.fragment),Pt=c(),yo=s("p"),Us=l("Ora possiamo usare il tokenizer come mostrato nella sezione precedente:"),jt=c(),h(Je.$$.fragment),It=c(),h(Ve.$$.fragment),yt=c(),Ao=s("p"),Ms=l("Salvare un tokenizer \xE8 identico a salvare un modello:"),At=c(),h(Fe.$$.fragment),Dt=c(),A=s("p"),Os=l("Parleremo meglio dei "),mi=s("code"),Hs=l("token_type_ids"),Qs=l(" nel "),Do=s("a"),Bs=l("Capitolo 3"),Gs=l(" e spiegheremo la chiave "),di=s("code"),Rs=l("attention_mask"),Js=l(" un po\u2019 pi\xF9 avanti. Per prima cosa, vediamo come vengono generati gli "),fi=s("code"),Vs=l("input_ids"),Fs=l(". Per farlo, dobbiamo esaminare i metodi intermedi del tokenizer."),Tt=c(),X=s("h2"),fe=s("a"),vi=s("span"),h(Ke.$$.fragment),Ks=c(),zi=s("span"),Ys=l("Codifica"),xt=c(),h(Ye.$$.fragment),Ct=c(),ve=s("p"),Ws=l("La traduzione del testo in numeri \xE8 nota come "),hi=s("em"),Xs=l("codifica"),Zs=l(". La codifica avviene in due fasi: la tokenizzazione, seguita dalla conversione in input ID."),St=c(),ze=s("p"),er=l("Come abbiamo visto, il primo passo consiste nel dividere il testo in parole (o parti di parole, simboli di punteggiatura, ecc.), solitamente chiamate "),ki=s("em"),or=l("token"),ir=l(". Ci sono diverse regole che possono governare questo processo, ed \xE8 per questo che dobbiamo istanziare il tokenizer usando il nome del modello, per assicurarci di usare le stesse regole che sono state usate quando il modello \xE8 stato preaddestrato."),Nt=c(),U=s("p"),tr=l("Il secondo passo consiste nel convertire i token in numeri, in modo da poterne costruire un tensore e darlo in pasto al modello. Per fare questo, il tokenizer ha un "),bi=s("em"),ar=l("vocabolario"),sr=l(", che \xE8 la parte che scarichiamo quando lo istanziamo con il metodo "),$i=s("code"),rr=l("from_pretrained()"),nr=l(". Anche in questo caso, dobbiamo utilizzare lo stesso vocabolario usato quando il modello \xE8 stato preaddestrato."),Lt=c(),To=s("p"),lr=l("Per comprendere meglio le due fasi, le esploreremo separatamente. Si noti che utilizzeremo alcuni metodi che eseguono parti della pipeline di tokenizzazione separatamente per mostrare i risultati intermedi di tali passaggi, ma in pratica si dovrebbe chiamare il tokenizzatore direttamente sui propri input (come mostrato nella sezione 2)."),Ut=c(),Z=s("h3"),he=s("a"),_i=s("span"),h(We.$$.fragment),pr=c(),gi=s("span"),cr=l("Processo di tokenizzazione"),Mt=c(),ke=s("p"),ur=l("Il processo di tokenizzazione viene eseguito dal metodo "),Ei=s("code"),mr=l("tokenize()"),dr=l(" del tokenizer:"),Ot=c(),h(Xe.$$.fragment),Ht=c(),xo=s("p"),fr=l("L\u2019output di questo metodo \xE8 un elenco di stringhe, o token:"),Qt=c(),h(Ze.$$.fragment),Bt=c(),x=s("p"),vr=l("Questo tokenizzatore \xE8 un tokenizzatore di sottoparole: divide le parole fino a ottenere token che possono essere rappresentati dal suo vocabolario. \xC8 il caso di "),wi=s("code"),zr=l("trasformatore"),hr=l(", che viene diviso in due token: "),qi=s("code"),kr=l("trasforma"),br=l(" e "),Pi=s("code"),$r=l("##tore"),_r=l("."),Gt=c(),ee=s("h3"),be=s("a"),ji=s("span"),h(eo.$$.fragment),gr=c(),Ii=s("span"),Er=l("Dai token agli input IDS"),Rt=c(),$e=s("p"),wr=l("La conversione in ID di input \xE8 gestita dal metodo del tokenizer "),yi=s("code"),qr=l("convert_tokens_to_ids()"),Pr=l(":"),Jt=c(),h(oo.$$.fragment),Vt=c(),h(io.$$.fragment),Ft=c(),Co=s("p"),jr=l("Questi risultati, una volta convertiti nel tensore quadro appropriato, possono essere successivamente utilizzati come input per un modello, come visto in precedenza in questo capitolo."),Kt=c(),h(_e.$$.fragment),Yt=c(),oe=s("h2"),ge=s("a"),Ai=s("span"),h(to.$$.fragment),Ir=c(),Di=s("span"),yr=l("Decodifica"),Wt=c(),M=s("p"),Ar=l("La "),Ti=s("em"),Dr=l("decodifica"),Tr=l(" avviene al contrario: dagli indici del vocabolario si vuole ottenere una stringa. Questo pu\xF2 essere fatto con il metodo "),xi=s("code"),xr=l("decode()"),Cr=l(" come segue:"),Xt=c(),h(ao.$$.fragment),Zt=c(),h(so.$$.fragment),ea=c(),Ee=s("p"),Sr=l("Si noti che il metodo "),Ci=s("code"),Nr=l("decode"),Lr=l(" non solo converte gli indici in token, ma raggruppa anche i token che fanno parte delle stesse parole per produrre una frase leggibile. Questo comportamento sar\xE0 estremamente utile quando utilizzeremo modelli che prevedono un nuovo testo (o un testo generato da un prompt, o per problemi di sequenza-sequenza come la traduzione o il riassunto)."),oa=c(),So=s("p"),Ur=l("A questo punto si dovrebbero comprendere le operazioni atomiche che un tokenizer pu\xF2 gestire: tokenizzazione, conversione in ID e conversione degli ID in stringhe. Tuttavia, abbiamo solo raschiato la punta dell\u2019iceberg. Nella sezione che segue, vedremo i limiti del nostro approccio e vedremo come superarli."),this.h()},l(e){const t=yl('[data-svelte="svelte-1phssyn"]',document.head);d=r(t,"META",{name:!0,content:!0}),t.forEach(o),_=u(e),k(f.$$.fragment,e),w=u(e),q=r(e,"H1",{class:!0});var no=n(q);g=r(no,"A",{id:!0,class:!0,href:!0});var No=n(g);P=r(No,"SPAN",{});var Si=n(P);k(j.$$.fragment,Si),Si.forEach(o),No.forEach(o),I=u(no),E=r(no,"SPAN",{});var Kr=n(E);B=p(Kr,"Tokenizers"),Kr.forEach(o),no.forEach(o),N=u(e),S.l(e),lo=u(e),k(qe.$$.fragment,e),Li=u(e),po=r(e,"P",{});var Yr=n(po);ja=p(Yr,"I tokenizer sono uno dei componenti fondamentali della pipeline NLP. Servono a uno scopo: tradurre il testo in dati che possono essere elaborati dal modello. I modelli possono elaborare solo numeri, quindi i tokenizer devono convertire i nostri input testuali in dati numerici. In questa sezione analizzeremo cosa succede esattamente nella pipeline di tokenizzazione."),Yr.forEach(o),Ui=u(e),co=r(e,"P",{});var Wr=n(co);Ia=p(Wr,"Nelle attivit\xE0 di NLP, i dati che vengono generalmente processati sono testi non elaborati, grezzi. Ecco un esempio di testo grezzo:"),Wr.forEach(o),Mi=u(e),k(Pe.$$.fragment,e),Oi=u(e),uo=r(e,"P",{});var Xr=n(uo);ya=p(Xr,"Tuttavia, i modelli possono elaborare solo numeri, quindi dobbiamo trovare un modo per convertire il testo non elaborato in numeri. Questo \xE8 ci\xF2 che fanno i tokenizer, e ci sono molti modi per farlo. L\u2019obiettivo \xE8 trovare la rappresentazione pi\xF9 significativa, cio\xE8 quella che ha pi\xF9 senso per il modello, e, se possibile, la rappresentazione pi\xF9 piccola."),Xr.forEach(o),Hi=u(e),mo=r(e,"P",{});var Zr=n(mo);Aa=p(Zr,"Vediamo alcuni esempi di algoritmi di tokenizzazione e cerchiamo di rispondere ad alcune domande sulla tokenizzazione."),Zr.forEach(o),Qi=u(e),G=r(e,"H2",{class:!0});var aa=n(G);te=r(aa,"A",{id:!0,class:!0,href:!0});var en=n(te);Ho=r(en,"SPAN",{});var on=n(Ho);k(je.$$.fragment,on),on.forEach(o),en.forEach(o),Da=u(aa),Qo=r(aa,"SPAN",{});var tn=n(Qo);Ta=p(tn,"Tokenizer basati sulle parole"),tn.forEach(o),aa.forEach(o),Bi=u(e),k(Ie.$$.fragment,e),Gi=u(e),ae=r(e,"P",{});var sa=n(ae);xa=p(sa,"Il primo tipo di tokenizzatore che viene in mente \xE8 quello "),Bo=r(sa,"EM",{});var an=n(Bo);Ca=p(an,"basato sulle parole"),an.forEach(o),Sa=p(sa,". In genere \xE8 molto facile da configurare e utilizzare con poche regole e spesso produce risultati decenti. Ad esempio, nell\u2019immagine qui sotto, l\u2019obiettivo \xE8 dividere il testo non elaborato in parole e trovare una rappresentazione numerica per ciascuna di esse:"),sa.forEach(o),Ri=u(e),R=r(e,"DIV",{class:!0});var ra=n(R);ye=r(ra,"IMG",{class:!0,src:!0,alt:!0}),Na=u(ra),Ae=r(ra,"IMG",{class:!0,src:!0,alt:!0}),ra.forEach(o),Ji=u(e),se=r(e,"P",{});var na=n(se);La=p(na,"Esistono diversi modi per dividere il testo. Ad esempio, si possono usare gli spazi bianchi per suddividere il testo in parole, applicando la funzione "),Go=r(na,"CODE",{});var sn=n(Go);Ua=p(sn,"split()"),sn.forEach(o),Ma=p(na," di Python:"),na.forEach(o),Vi=u(e),k(De.$$.fragment,e),Fi=u(e),k(Te.$$.fragment,e),Ki=u(e),fo=r(e,"P",{});var rn=n(fo);Oa=p(rn,"Esistono anche varianti di tokenizzatori di parole che prevedono regole aggiuntive per la punteggiatura. Con questo tipo di tokenizer, possiamo ritrovarci con \u201Cvocabolari\u201D piuttosto grandi, dove un vocabolario \xE8 definito dal numero totale di token indipendenti che abbiamo nel nostro corpus."),rn.forEach(o),Yi=u(e),vo=r(e,"P",{});var nn=n(vo);Ha=p(nn,"A ogni parola viene assegnato un ID, a partire da 0 fino alla dimensione del vocabolario. Il modello utilizza questi ID per identificare ogni parola."),nn.forEach(o),Wi=u(e),zo=r(e,"P",{});var ln=n(zo);Qa=p(ln,"Se vogliamo coprire completamente una lingua con un tokenizzatore basato sulle parole, dovremo avere un identificatore per ogni parola della lingua, il che generer\xE0 un\u2019enorme quantit\xE0 di token. Per esempio, nella lingua inglese ci sono pi\xF9 di 500.000 parole, quindi per costruire una mappa da ogni parola a un ID di input dovremmo tenere traccia di cos\xEC tanti ID. Inoltre, parole come \u201Ccane\u201D sono rappresentate in modo diverso da parole come \u201Ccani\u201D, e il modello inizialmente non avr\xE0 modo di sapere che \u201Ccane\u201D e \u201Ccani\u201D sono simili: identificher\xE0 le due parole come non correlate. Lo stesso vale per altre parole simili, come \u201Ccorrere\u201D e \u201Ccorrendo\u201D, che il modello non vedr\xE0 inizialmente come simili."),ln.forEach(o),Xi=u(e),xe=r(e,"P",{});var Mr=n(xe);Ba=p(Mr,"Infine, abbiamo bisogno di un token personalizzato per rappresentare le parole che non fanno parte del nostro vocabolario. Questo \xE8 noto come token \u201Cunknown\u201D, spesso rappresentato come \u201D[UNK]\u201D o \u201D"),Ro=r(Mr,"UNK",{});var pn=n(Ro);Ga=p(pn,"\u201D. Se il tokenizer produce molti token di questo tipo \xE8 generalmente un brutto segno, perch\xE9 non \xE8 riuscito a trovare una rappresentazione sensata della parola e si stanno perdendo informazioni. L\u2019obiettivo della creazione del vocabolario \xE8 quello di fare in modo che il tokenizzatore inserisca il minor numero possibile di parole nel token sconosciuto."),pn.forEach(o),Mr.forEach(o),Zi=u(e),re=r(e,"P",{});var la=n(re);Ra=p(la,"Un modo per ridurre la quantit\xE0 di token sconosciuti \xE8 quello di andare un livello pi\xF9 in profondit\xE0, usando un tokenizer "),Jo=r(la,"EM",{});var cn=n(Jo);Ja=p(cn,"character-based"),cn.forEach(o),Va=p(la,"."),la.forEach(o),et=u(e),J=r(e,"H2",{class:!0});var pa=n(J);ne=r(pa,"A",{id:!0,class:!0,href:!0});var un=n(ne);Vo=r(un,"SPAN",{});var mn=n(Vo);k(Ce.$$.fragment,mn),mn.forEach(o),un.forEach(o),Fa=u(pa),Fo=r(pa,"SPAN",{});var dn=n(Fo);Ka=p(dn,"Character-based"),dn.forEach(o),pa.forEach(o),ot=u(e),k(Se.$$.fragment,e),it=u(e),ho=r(e,"P",{});var fn=n(ho);Ya=p(fn,"I tokenizer basati sui caratteri dividono il testo in caratteri, anzich\xE9 in parole. Ci\xF2 comporta due vantaggi principali:"),fn.forEach(o),tt=u(e),le=r(e,"UL",{});var ca=n(le);Ko=r(ca,"LI",{});var vn=n(Ko);Wa=p(vn,"Il vocabolario \xE8 molto pi\xF9 ridotto."),vn.forEach(o),Xa=u(ca),Yo=r(ca,"LI",{});var zn=n(Yo);Za=p(zn,"I token fuori vocabolario (sconosciuti) sono molto meno numerosi, poich\xE9 ogni parola pu\xF2 essere costruita a partire dai caratteri."),zn.forEach(o),ca.forEach(o),at=u(e),ko=r(e,"P",{});var hn=n(ko);es=p(hn,"Ma anche in questo caso sorgono alcune questioni relative agli spazi e alla punteggiatura:"),hn.forEach(o),st=u(e),V=r(e,"DIV",{class:!0});var ua=n(V);Ne=r(ua,"IMG",{class:!0,src:!0,alt:!0}),os=u(ua),Le=r(ua,"IMG",{class:!0,src:!0,alt:!0}),ua.forEach(o),rt=u(e),bo=r(e,"P",{});var kn=n(bo);is=p(kn,"Anche questo approccio non \xE8 perfetto. Poich\xE9 la rappresentazione \xE8 ora basata su caratteri anzich\xE9 su parole, si potrebbe sostenere che, intuitivamente, \xE8 meno significativa: ogni carattere non significa molto da solo, mentre \xE8 cos\xEC per le parole. Tuttavia, anche in questo caso il significato varia a seconda della lingua; in cinese, ad esempio, ogni carattere porta con s\xE9 pi\xF9 informazioni di un carattere in una lingua latina."),kn.forEach(o),nt=u(e),$o=r(e,"P",{});var bn=n($o);ts=p(bn,"Un\u2019altra cosa da considerare \xE8 che ci ritroveremo con una quantit\xE0 molto elevata di token da elaborare da parte del nostro modello: mentre una parola sarebbe un singolo token con un tokenizzatore basato sulle parole, pu\xF2 facilmente trasformarsi in 10 o pi\xF9 token quando viene convertita in caratteri."),bn.forEach(o),lt=u(e),pe=r(e,"P",{});var ma=n(pe);as=p(ma,"Per ottenere il meglio dei due mondi, possiamo utilizzare una terza tecnica che combina i due approcci: la "),Wo=r(ma,"EM",{});var $n=n(Wo);ss=p($n,"tokenizzazione delle sottoparole"),$n.forEach(o),rs=p(ma,"."),ma.forEach(o),pt=u(e),F=r(e,"H2",{class:!0});var da=n(F);ce=r(da,"A",{id:!0,class:!0,href:!0});var _n=n(ce);Xo=r(_n,"SPAN",{});var gn=n(Xo);k(Ue.$$.fragment,gn),gn.forEach(o),_n.forEach(o),ns=u(da),Zo=r(da,"SPAN",{});var En=n(Zo);ls=p(En,"Tokenizzazione delle sottoparole"),En.forEach(o),da.forEach(o),ct=u(e),k(Me.$$.fragment,e),ut=u(e),_o=r(e,"P",{});var wn=n(_o);ps=p(wn,"Gli algoritmi di tokenizzazione delle sottoparole si basano sul principio che le parole di uso frequente non devono essere suddivise in sottoparole pi\xF9 piccole, ma le parole rare devono essere scomposte in sottoparole significative."),wn.forEach(o),mt=u(e),go=r(e,"P",{});var qn=n(go);cs=p(qn,"Ad esempio, \u201Cfastidiosamente\u201D potrebbe essere considerata una parola rara e potrebbe essere scomposta in \u201Cfastidioso\u201D e \u201Cmente\u201D. \xC8 probabile che queste due parole compaiano pi\xF9 frequentemente come sottoparole a s\xE9 stanti, mentre il significato di \u201Cfastidiosamente\u201D viene mantenuto dal significato composito di \u201Cfastidioso\u201D e \u201Cmente\u201D."),qn.forEach(o),dt=u(e),Eo=r(e,"P",{});var Pn=n(Eo);us=p(Pn,"Ecco un esempio che mostra come un algoritmo di tokenizzazione delle sottoparole tokenizzerebbe la sequenza \u201CLet\u2019s do tokenization!\u201C:"),Pn.forEach(o),ft=u(e),K=r(e,"DIV",{class:!0});var fa=n(K);Oe=r(fa,"IMG",{class:!0,src:!0,alt:!0}),ms=u(fa),He=r(fa,"IMG",{class:!0,src:!0,alt:!0}),fa.forEach(o),vt=u(e),wo=r(e,"P",{});var jn=n(wo);ds=p(jn,"Queste sottoparole finiscono per fornire un significato semantico: per esempio, nell\u2019esempio precedente \u201Ctokenization\u201D \xE8 stato diviso in \u201Ctoken\u201D e \u201Cization\u201D, due token che hanno un significato semantico pur essendo efficienti dal punto di vista dello spazio (sono necessari solo due token per rappresentare una parola lunga). Questo ci permette di avere una copertura relativamente buona con vocabolari piccoli e quasi nessun token sconosciuto."),jn.forEach(o),zt=u(e),qo=r(e,"P",{});var In=n(qo);fs=p(In,"Questo approccio \xE8 particolarmente utile nelle lingue agglutinanti come il turco, dove \xE8 possibile formare parole complesse (quasi) arbitrariamente lunghe mettendo insieme sottoparole."),In.forEach(o),ht=u(e),Y=r(e,"H3",{class:!0});var va=n(Y);ue=r(va,"A",{id:!0,class:!0,href:!0});var yn=n(ue);ei=r(yn,"SPAN",{});var An=n(ei);k(Qe.$$.fragment,An),An.forEach(o),yn.forEach(o),vs=u(va),oi=r(va,"SPAN",{});var Dn=n(oi);zs=p(Dn,"E non solo!"),Dn.forEach(o),va.forEach(o),kt=u(e),Po=r(e,"P",{});var Tn=n(Po);hs=p(Tn,"Non sorprende che esistano molte altre tecniche. Per citarne alcune:"),Tn.forEach(o),bt=u(e),L=r(e,"UL",{});var Lo=n(L);ii=r(Lo,"LI",{});var xn=n(ii);ks=p(xn,"Byte-level BPE, utilizzato in GPT-2"),xn.forEach(o),bs=u(Lo),ti=r(Lo,"LI",{});var Cn=n(ti);$s=p(Cn,"WordPiece, utilizzato in BERT"),Cn.forEach(o),_s=u(Lo),ai=r(Lo,"LI",{});var Sn=n(ai);gs=p(Sn,"SentencePiece o Unigram, utilizzato in diversi modelli multilingua."),Sn.forEach(o),Lo.forEach(o),$t=u(e),jo=r(e,"P",{});var Nn=n(jo);Es=p(Nn,"A questo punto dovresti avere una conoscenza sufficiente di come funzionano i tokenizer per iniziare a usare l\u2019API."),Nn.forEach(o),_t=u(e),W=r(e,"H2",{class:!0});var za=n(W);me=r(za,"A",{id:!0,class:!0,href:!0});var Ln=n(me);si=r(Ln,"SPAN",{});var Un=n(si);k(Be.$$.fragment,Un),Un.forEach(o),Ln.forEach(o),ws=u(za),ri=r(za,"SPAN",{});var Mn=n(ri);qs=p(Mn,"Caricamento e salvataggio"),Mn.forEach(o),za.forEach(o),gt=u(e),y=r(e,"P",{});var O=n(y);Ps=p(O,"Caricare e salvare i tokenizer \xE8 semplice come per i modelli. In realt\xE0, si basa sugli stessi due metodi: "),ni=r(O,"CODE",{});var On=n(ni);js=p(On,"from_pretrained()"),On.forEach(o),Is=p(O," e "),li=r(O,"CODE",{});var Hn=n(li);ys=p(Hn,"save_pretrained()"),Hn.forEach(o),As=p(O,". Questi metodi caricano o salvano l\u2019algoritmo usato dal tokenizer (un po\u2019 come l\u2019"),pi=r(O,"EM",{});var Qn=n(pi);Ds=p(Qn,"architettura"),Qn.forEach(o),Ts=p(O," del modello) ed il suo vocabolario (un po\u2019 come i "),ci=r(O,"EM",{});var Bn=n(ci);xs=p(Bn,"pesi"),Bn.forEach(o),Cs=p(O," del modello)."),O.forEach(o),Et=u(e),de=r(e,"P",{});var ha=n(de);Ss=p(ha,"Il caricamento del tokenizer di BERT, addestrato con lo stesso checkpoint di BERT, avviene nello stesso modo in cui si carica il modello, con la differenza che si usa la classe "),ui=r(ha,"CODE",{});var Gn=n(ui);Ns=p(Gn,"BertTokenizer"),Gn.forEach(o),Ls=p(ha,":"),ha.forEach(o),wt=u(e),k(Ge.$$.fragment,e),qt=u(e),ie.l(e),Io=u(e),k(Re.$$.fragment,e),Pt=u(e),yo=r(e,"P",{});var Rn=n(yo);Us=p(Rn,"Ora possiamo usare il tokenizer come mostrato nella sezione precedente:"),Rn.forEach(o),jt=u(e),k(Je.$$.fragment,e),It=u(e),k(Ve.$$.fragment,e),yt=u(e),Ao=r(e,"P",{});var Jn=n(Ao);Ms=p(Jn,"Salvare un tokenizer \xE8 identico a salvare un modello:"),Jn.forEach(o),At=u(e),k(Fe.$$.fragment,e),Dt=u(e),A=r(e,"P",{});var H=n(A);Os=p(H,"Parleremo meglio dei "),mi=r(H,"CODE",{});var Vn=n(mi);Hs=p(Vn,"token_type_ids"),Vn.forEach(o),Qs=p(H," nel "),Do=r(H,"A",{href:!0});var Fn=n(Do);Bs=p(Fn,"Capitolo 3"),Fn.forEach(o),Gs=p(H," e spiegheremo la chiave "),di=r(H,"CODE",{});var Kn=n(di);Rs=p(Kn,"attention_mask"),Kn.forEach(o),Js=p(H," un po\u2019 pi\xF9 avanti. Per prima cosa, vediamo come vengono generati gli "),fi=r(H,"CODE",{});var Yn=n(fi);Vs=p(Yn,"input_ids"),Yn.forEach(o),Fs=p(H,". Per farlo, dobbiamo esaminare i metodi intermedi del tokenizer."),H.forEach(o),Tt=u(e),X=r(e,"H2",{class:!0});var ka=n(X);fe=r(ka,"A",{id:!0,class:!0,href:!0});var Wn=n(fe);vi=r(Wn,"SPAN",{});var Xn=n(vi);k(Ke.$$.fragment,Xn),Xn.forEach(o),Wn.forEach(o),Ks=u(ka),zi=r(ka,"SPAN",{});var Zn=n(zi);Ys=p(Zn,"Codifica"),Zn.forEach(o),ka.forEach(o),xt=u(e),k(Ye.$$.fragment,e),Ct=u(e),ve=r(e,"P",{});var ba=n(ve);Ws=p(ba,"La traduzione del testo in numeri \xE8 nota come "),hi=r(ba,"EM",{});var el=n(hi);Xs=p(el,"codifica"),el.forEach(o),Zs=p(ba,". La codifica avviene in due fasi: la tokenizzazione, seguita dalla conversione in input ID."),ba.forEach(o),St=u(e),ze=r(e,"P",{});var $a=n(ze);er=p($a,"Come abbiamo visto, il primo passo consiste nel dividere il testo in parole (o parti di parole, simboli di punteggiatura, ecc.), solitamente chiamate "),ki=r($a,"EM",{});var ol=n(ki);or=p(ol,"token"),ol.forEach(o),ir=p($a,". Ci sono diverse regole che possono governare questo processo, ed \xE8 per questo che dobbiamo istanziare il tokenizer usando il nome del modello, per assicurarci di usare le stesse regole che sono state usate quando il modello \xE8 stato preaddestrato."),$a.forEach(o),Nt=u(e),U=r(e,"P",{});var Uo=n(U);tr=p(Uo,"Il secondo passo consiste nel convertire i token in numeri, in modo da poterne costruire un tensore e darlo in pasto al modello. Per fare questo, il tokenizer ha un "),bi=r(Uo,"EM",{});var il=n(bi);ar=p(il,"vocabolario"),il.forEach(o),sr=p(Uo,", che \xE8 la parte che scarichiamo quando lo istanziamo con il metodo "),$i=r(Uo,"CODE",{});var tl=n($i);rr=p(tl,"from_pretrained()"),tl.forEach(o),nr=p(Uo,". Anche in questo caso, dobbiamo utilizzare lo stesso vocabolario usato quando il modello \xE8 stato preaddestrato."),Uo.forEach(o),Lt=u(e),To=r(e,"P",{});var al=n(To);lr=p(al,"Per comprendere meglio le due fasi, le esploreremo separatamente. Si noti che utilizzeremo alcuni metodi che eseguono parti della pipeline di tokenizzazione separatamente per mostrare i risultati intermedi di tali passaggi, ma in pratica si dovrebbe chiamare il tokenizzatore direttamente sui propri input (come mostrato nella sezione 2)."),al.forEach(o),Ut=u(e),Z=r(e,"H3",{class:!0});var _a=n(Z);he=r(_a,"A",{id:!0,class:!0,href:!0});var sl=n(he);_i=r(sl,"SPAN",{});var rl=n(_i);k(We.$$.fragment,rl),rl.forEach(o),sl.forEach(o),pr=u(_a),gi=r(_a,"SPAN",{});var nl=n(gi);cr=p(nl,"Processo di tokenizzazione"),nl.forEach(o),_a.forEach(o),Mt=u(e),ke=r(e,"P",{});var ga=n(ke);ur=p(ga,"Il processo di tokenizzazione viene eseguito dal metodo "),Ei=r(ga,"CODE",{});var ll=n(Ei);mr=p(ll,"tokenize()"),ll.forEach(o),dr=p(ga," del tokenizer:"),ga.forEach(o),Ot=u(e),k(Xe.$$.fragment,e),Ht=u(e),xo=r(e,"P",{});var pl=n(xo);fr=p(pl,"L\u2019output di questo metodo \xE8 un elenco di stringhe, o token:"),pl.forEach(o),Qt=u(e),k(Ze.$$.fragment,e),Bt=u(e),x=r(e,"P",{});var we=n(x);vr=p(we,"Questo tokenizzatore \xE8 un tokenizzatore di sottoparole: divide le parole fino a ottenere token che possono essere rappresentati dal suo vocabolario. \xC8 il caso di "),wi=r(we,"CODE",{});var cl=n(wi);zr=p(cl,"trasformatore"),cl.forEach(o),hr=p(we,", che viene diviso in due token: "),qi=r(we,"CODE",{});var ul=n(qi);kr=p(ul,"trasforma"),ul.forEach(o),br=p(we," e "),Pi=r(we,"CODE",{});var ml=n(Pi);$r=p(ml,"##tore"),ml.forEach(o),_r=p(we,"."),we.forEach(o),Gt=u(e),ee=r(e,"H3",{class:!0});var Ea=n(ee);be=r(Ea,"A",{id:!0,class:!0,href:!0});var dl=n(be);ji=r(dl,"SPAN",{});var fl=n(ji);k(eo.$$.fragment,fl),fl.forEach(o),dl.forEach(o),gr=u(Ea),Ii=r(Ea,"SPAN",{});var vl=n(Ii);Er=p(vl,"Dai token agli input IDS"),vl.forEach(o),Ea.forEach(o),Rt=u(e),$e=r(e,"P",{});var wa=n($e);wr=p(wa,"La conversione in ID di input \xE8 gestita dal metodo del tokenizer "),yi=r(wa,"CODE",{});var zl=n(yi);qr=p(zl,"convert_tokens_to_ids()"),zl.forEach(o),Pr=p(wa,":"),wa.forEach(o),Jt=u(e),k(oo.$$.fragment,e),Vt=u(e),k(io.$$.fragment,e),Ft=u(e),Co=r(e,"P",{});var hl=n(Co);jr=p(hl,"Questi risultati, una volta convertiti nel tensore quadro appropriato, possono essere successivamente utilizzati come input per un modello, come visto in precedenza in questo capitolo."),hl.forEach(o),Kt=u(e),k(_e.$$.fragment,e),Yt=u(e),oe=r(e,"H2",{class:!0});var qa=n(oe);ge=r(qa,"A",{id:!0,class:!0,href:!0});var kl=n(ge);Ai=r(kl,"SPAN",{});var bl=n(Ai);k(to.$$.fragment,bl),bl.forEach(o),kl.forEach(o),Ir=u(qa),Di=r(qa,"SPAN",{});var $l=n(Di);yr=p($l,"Decodifica"),$l.forEach(o),qa.forEach(o),Wt=u(e),M=r(e,"P",{});var Mo=n(M);Ar=p(Mo,"La "),Ti=r(Mo,"EM",{});var _l=n(Ti);Dr=p(_l,"decodifica"),_l.forEach(o),Tr=p(Mo," avviene al contrario: dagli indici del vocabolario si vuole ottenere una stringa. Questo pu\xF2 essere fatto con il metodo "),xi=r(Mo,"CODE",{});var gl=n(xi);xr=p(gl,"decode()"),gl.forEach(o),Cr=p(Mo," come segue:"),Mo.forEach(o),Xt=u(e),k(ao.$$.fragment,e),Zt=u(e),k(so.$$.fragment,e),ea=u(e),Ee=r(e,"P",{});var Pa=n(Ee);Sr=p(Pa,"Si noti che il metodo "),Ci=r(Pa,"CODE",{});var El=n(Ci);Nr=p(El,"decode"),El.forEach(o),Lr=p(Pa," non solo converte gli indici in token, ma raggruppa anche i token che fanno parte delle stesse parole per produrre una frase leggibile. Questo comportamento sar\xE0 estremamente utile quando utilizzeremo modelli che prevedono un nuovo testo (o un testo generato da un prompt, o per problemi di sequenza-sequenza come la traduzione o il riassunto)."),Pa.forEach(o),oa=u(e),So=r(e,"P",{});var wl=n(So);Ur=p(wl,"A questo punto si dovrebbero comprendere le operazioni atomiche che un tokenizer pu\xF2 gestire: tokenizzazione, conversione in ID e conversione degli ID in stringhe. Tuttavia, abbiamo solo raschiato la punta dell\u2019iceberg. Nella sezione che segue, vedremo i limiti del nostro approccio e vedremo come superarli."),wl.forEach(o),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(Hl)),m(g,"id","tokenizers"),m(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(g,"href","#tokenizers"),m(q,"class","relative group"),m(te,"id","tokenizer-basati-sulle-parole"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#tokenizer-basati-sulle-parole"),m(G,"class","relative group"),m(ye,"class","block dark:hidden"),Oo(ye.src,Or="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||m(ye,"src",Or),m(ye,"alt","Un esempio di tokenizzazione basata sulle parole."),m(Ae,"class","hidden dark:block"),Oo(Ae.src,Hr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||m(Ae,"src",Hr),m(Ae,"alt","Un esempio di tokenizzazione basata sulle parole."),m(R,"class","flex justify-center"),m(ne,"id","characterbased"),m(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ne,"href","#characterbased"),m(J,"class","relative group"),m(Ne,"class","block dark:hidden"),Oo(Ne.src,Qr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||m(Ne,"src",Qr),m(Ne,"alt","Un esempio di tokenizzazione basata sui caratteri."),m(Le,"class","hidden dark:block"),Oo(Le.src,Br="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||m(Le,"src",Br),m(Le,"alt","Un esempio di tokenizzazione basata sui caratteri"),m(V,"class","flex justify-center"),m(ce,"id","tokenizzazione-delle-sottoparole"),m(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ce,"href","#tokenizzazione-delle-sottoparole"),m(F,"class","relative group"),m(Oe,"class","block dark:hidden"),Oo(Oe.src,Gr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||m(Oe,"src",Gr),m(Oe,"alt","Un algoritmo di tokenizzazione delle sottoparole."),m(He,"class","hidden dark:block"),Oo(He.src,Rr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||m(He,"src",Rr),m(He,"alt","Un algoritmo di tokenizzazione delle sottoparole."),m(K,"class","flex justify-center"),m(ue,"id","e-non-solo"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#e-non-solo"),m(Y,"class","relative group"),m(me,"id","caricamento-e-salvataggio"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#caricamento-e-salvataggio"),m(W,"class","relative group"),m(Do,"href","/course/chapter3"),m(fe,"id","codifica"),m(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fe,"href","#codifica"),m(X,"class","relative group"),m(he,"id","processo-di-tokenizzazione"),m(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(he,"href","#processo-di-tokenizzazione"),m(Z,"class","relative group"),m(be,"id","dai-token-agli-input-ids"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#dai-token-agli-input-ids"),m(ee,"class","relative group"),m(ge,"id","decodifica"),m(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ge,"href","#decodifica"),m(oe,"class","relative group")},m(e,t){i(document.head,d),a(e,_,t),b(f,e,t),a(e,w,t),a(e,q,t),i(q,g),i(g,P),b(j,P,null),i(q,I),i(q,E),i(E,B),a(e,N,t),ro[C].m(e,t),a(e,lo,t),b(qe,e,t),a(e,Li,t),a(e,po,t),i(po,ja),a(e,Ui,t),a(e,co,t),i(co,Ia),a(e,Mi,t),b(Pe,e,t),a(e,Oi,t),a(e,uo,t),i(uo,ya),a(e,Hi,t),a(e,mo,t),i(mo,Aa),a(e,Qi,t),a(e,G,t),i(G,te),i(te,Ho),b(je,Ho,null),i(G,Da),i(G,Qo),i(Qo,Ta),a(e,Bi,t),b(Ie,e,t),a(e,Gi,t),a(e,ae,t),i(ae,xa),i(ae,Bo),i(Bo,Ca),i(ae,Sa),a(e,Ri,t),a(e,R,t),i(R,ye),i(R,Na),i(R,Ae),a(e,Ji,t),a(e,se,t),i(se,La),i(se,Go),i(Go,Ua),i(se,Ma),a(e,Vi,t),b(De,e,t),a(e,Fi,t),b(Te,e,t),a(e,Ki,t),a(e,fo,t),i(fo,Oa),a(e,Yi,t),a(e,vo,t),i(vo,Ha),a(e,Wi,t),a(e,zo,t),i(zo,Qa),a(e,Xi,t),a(e,xe,t),i(xe,Ba),i(xe,Ro),i(Ro,Ga),a(e,Zi,t),a(e,re,t),i(re,Ra),i(re,Jo),i(Jo,Ja),i(re,Va),a(e,et,t),a(e,J,t),i(J,ne),i(ne,Vo),b(Ce,Vo,null),i(J,Fa),i(J,Fo),i(Fo,Ka),a(e,ot,t),b(Se,e,t),a(e,it,t),a(e,ho,t),i(ho,Ya),a(e,tt,t),a(e,le,t),i(le,Ko),i(Ko,Wa),i(le,Xa),i(le,Yo),i(Yo,Za),a(e,at,t),a(e,ko,t),i(ko,es),a(e,st,t),a(e,V,t),i(V,Ne),i(V,os),i(V,Le),a(e,rt,t),a(e,bo,t),i(bo,is),a(e,nt,t),a(e,$o,t),i($o,ts),a(e,lt,t),a(e,pe,t),i(pe,as),i(pe,Wo),i(Wo,ss),i(pe,rs),a(e,pt,t),a(e,F,t),i(F,ce),i(ce,Xo),b(Ue,Xo,null),i(F,ns),i(F,Zo),i(Zo,ls),a(e,ct,t),b(Me,e,t),a(e,ut,t),a(e,_o,t),i(_o,ps),a(e,mt,t),a(e,go,t),i(go,cs),a(e,dt,t),a(e,Eo,t),i(Eo,us),a(e,ft,t),a(e,K,t),i(K,Oe),i(K,ms),i(K,He),a(e,vt,t),a(e,wo,t),i(wo,ds),a(e,zt,t),a(e,qo,t),i(qo,fs),a(e,ht,t),a(e,Y,t),i(Y,ue),i(ue,ei),b(Qe,ei,null),i(Y,vs),i(Y,oi),i(oi,zs),a(e,kt,t),a(e,Po,t),i(Po,hs),a(e,bt,t),a(e,L,t),i(L,ii),i(ii,ks),i(L,bs),i(L,ti),i(ti,$s),i(L,_s),i(L,ai),i(ai,gs),a(e,$t,t),a(e,jo,t),i(jo,Es),a(e,_t,t),a(e,W,t),i(W,me),i(me,si),b(Be,si,null),i(W,ws),i(W,ri),i(ri,qs),a(e,gt,t),a(e,y,t),i(y,Ps),i(y,ni),i(ni,js),i(y,Is),i(y,li),i(li,ys),i(y,As),i(y,pi),i(pi,Ds),i(y,Ts),i(y,ci),i(ci,xs),i(y,Cs),a(e,Et,t),a(e,de,t),i(de,Ss),i(de,ui),i(ui,Ns),i(de,Ls),a(e,wt,t),b(Ge,e,t),a(e,qt,t),ie.m(e,t),a(e,Io,t),b(Re,e,t),a(e,Pt,t),a(e,yo,t),i(yo,Us),a(e,jt,t),b(Je,e,t),a(e,It,t),b(Ve,e,t),a(e,yt,t),a(e,Ao,t),i(Ao,Ms),a(e,At,t),b(Fe,e,t),a(e,Dt,t),a(e,A,t),i(A,Os),i(A,mi),i(mi,Hs),i(A,Qs),i(A,Do),i(Do,Bs),i(A,Gs),i(A,di),i(di,Rs),i(A,Js),i(A,fi),i(fi,Vs),i(A,Fs),a(e,Tt,t),a(e,X,t),i(X,fe),i(fe,vi),b(Ke,vi,null),i(X,Ks),i(X,zi),i(zi,Ys),a(e,xt,t),b(Ye,e,t),a(e,Ct,t),a(e,ve,t),i(ve,Ws),i(ve,hi),i(hi,Xs),i(ve,Zs),a(e,St,t),a(e,ze,t),i(ze,er),i(ze,ki),i(ki,or),i(ze,ir),a(e,Nt,t),a(e,U,t),i(U,tr),i(U,bi),i(bi,ar),i(U,sr),i(U,$i),i($i,rr),i(U,nr),a(e,Lt,t),a(e,To,t),i(To,lr),a(e,Ut,t),a(e,Z,t),i(Z,he),i(he,_i),b(We,_i,null),i(Z,pr),i(Z,gi),i(gi,cr),a(e,Mt,t),a(e,ke,t),i(ke,ur),i(ke,Ei),i(Ei,mr),i(ke,dr),a(e,Ot,t),b(Xe,e,t),a(e,Ht,t),a(e,xo,t),i(xo,fr),a(e,Qt,t),b(Ze,e,t),a(e,Bt,t),a(e,x,t),i(x,vr),i(x,wi),i(wi,zr),i(x,hr),i(x,qi),i(qi,kr),i(x,br),i(x,Pi),i(Pi,$r),i(x,_r),a(e,Gt,t),a(e,ee,t),i(ee,be),i(be,ji),b(eo,ji,null),i(ee,gr),i(ee,Ii),i(Ii,Er),a(e,Rt,t),a(e,$e,t),i($e,wr),i($e,yi),i(yi,qr),i($e,Pr),a(e,Jt,t),b(oo,e,t),a(e,Vt,t),b(io,e,t),a(e,Ft,t),a(e,Co,t),i(Co,jr),a(e,Kt,t),b(_e,e,t),a(e,Yt,t),a(e,oe,t),i(oe,ge),i(ge,Ai),b(to,Ai,null),i(oe,Ir),i(oe,Di),i(Di,yr),a(e,Wt,t),a(e,M,t),i(M,Ar),i(M,Ti),i(Ti,Dr),i(M,Tr),i(M,xi),i(xi,xr),i(M,Cr),a(e,Xt,t),b(ao,e,t),a(e,Zt,t),b(so,e,t),a(e,ea,t),a(e,Ee,t),i(Ee,Sr),i(Ee,Ci),i(Ci,Nr),i(Ee,Lr),a(e,oa,t),a(e,So,t),i(So,Ur),ia=!0},p(e,[t]){const no={};t&1&&(no.fw=e[0]),f.$set(no);let No=C;C=Vr(e),C!==No&&(Tl(),v(ro[No],1,1,()=>{ro[No]=null}),Al(),S=ro[C],S||(S=ro[C]=Jr[C](e),S.c()),z(S,1),S.m(lo.parentNode,lo)),ta!==(ta=Fr(e))&&(ie.d(1),ie=ta(e),ie&&(ie.c(),ie.m(Io.parentNode,Io)));const Si={};t&2&&(Si.$$scope={dirty:t,ctx:e}),_e.$set(Si)},i(e){ia||(z(f.$$.fragment,e),z(j.$$.fragment,e),z(S),z(qe.$$.fragment,e),z(Pe.$$.fragment,e),z(je.$$.fragment,e),z(Ie.$$.fragment,e),z(De.$$.fragment,e),z(Te.$$.fragment,e),z(Ce.$$.fragment,e),z(Se.$$.fragment,e),z(Ue.$$.fragment,e),z(Me.$$.fragment,e),z(Qe.$$.fragment,e),z(Be.$$.fragment,e),z(Ge.$$.fragment,e),z(Re.$$.fragment,e),z(Je.$$.fragment,e),z(Ve.$$.fragment,e),z(Fe.$$.fragment,e),z(Ke.$$.fragment,e),z(Ye.$$.fragment,e),z(We.$$.fragment,e),z(Xe.$$.fragment,e),z(Ze.$$.fragment,e),z(eo.$$.fragment,e),z(oo.$$.fragment,e),z(io.$$.fragment,e),z(_e.$$.fragment,e),z(to.$$.fragment,e),z(ao.$$.fragment,e),z(so.$$.fragment,e),ia=!0)},o(e){v(f.$$.fragment,e),v(j.$$.fragment,e),v(S),v(qe.$$.fragment,e),v(Pe.$$.fragment,e),v(je.$$.fragment,e),v(Ie.$$.fragment,e),v(De.$$.fragment,e),v(Te.$$.fragment,e),v(Ce.$$.fragment,e),v(Se.$$.fragment,e),v(Ue.$$.fragment,e),v(Me.$$.fragment,e),v(Qe.$$.fragment,e),v(Be.$$.fragment,e),v(Ge.$$.fragment,e),v(Re.$$.fragment,e),v(Je.$$.fragment,e),v(Ve.$$.fragment,e),v(Fe.$$.fragment,e),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(We.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),v(eo.$$.fragment,e),v(oo.$$.fragment,e),v(io.$$.fragment,e),v(_e.$$.fragment,e),v(to.$$.fragment,e),v(ao.$$.fragment,e),v(so.$$.fragment,e),ia=!1},d(e){o(d),e&&o(_),$(f,e),e&&o(w),e&&o(q),$(j),e&&o(N),ro[C].d(e),e&&o(lo),$(qe,e),e&&o(Li),e&&o(po),e&&o(Ui),e&&o(co),e&&o(Mi),$(Pe,e),e&&o(Oi),e&&o(uo),e&&o(Hi),e&&o(mo),e&&o(Qi),e&&o(G),$(je),e&&o(Bi),$(Ie,e),e&&o(Gi),e&&o(ae),e&&o(Ri),e&&o(R),e&&o(Ji),e&&o(se),e&&o(Vi),$(De,e),e&&o(Fi),$(Te,e),e&&o(Ki),e&&o(fo),e&&o(Yi),e&&o(vo),e&&o(Wi),e&&o(zo),e&&o(Xi),e&&o(xe),e&&o(Zi),e&&o(re),e&&o(et),e&&o(J),$(Ce),e&&o(ot),$(Se,e),e&&o(it),e&&o(ho),e&&o(tt),e&&o(le),e&&o(at),e&&o(ko),e&&o(st),e&&o(V),e&&o(rt),e&&o(bo),e&&o(nt),e&&o($o),e&&o(lt),e&&o(pe),e&&o(pt),e&&o(F),$(Ue),e&&o(ct),$(Me,e),e&&o(ut),e&&o(_o),e&&o(mt),e&&o(go),e&&o(dt),e&&o(Eo),e&&o(ft),e&&o(K),e&&o(vt),e&&o(wo),e&&o(zt),e&&o(qo),e&&o(ht),e&&o(Y),$(Qe),e&&o(kt),e&&o(Po),e&&o(bt),e&&o(L),e&&o($t),e&&o(jo),e&&o(_t),e&&o(W),$(Be),e&&o(gt),e&&o(y),e&&o(Et),e&&o(de),e&&o(wt),$(Ge,e),e&&o(qt),ie.d(e),e&&o(Io),$(Re,e),e&&o(Pt),e&&o(yo),e&&o(jt),$(Je,e),e&&o(It),$(Ve,e),e&&o(yt),e&&o(Ao),e&&o(At),$(Fe,e),e&&o(Dt),e&&o(A),e&&o(Tt),e&&o(X),$(Ke),e&&o(xt),$(Ye,e),e&&o(Ct),e&&o(ve),e&&o(St),e&&o(ze),e&&o(Nt),e&&o(U),e&&o(Lt),e&&o(To),e&&o(Ut),e&&o(Z),$(We),e&&o(Mt),e&&o(ke),e&&o(Ot),$(Xe,e),e&&o(Ht),e&&o(xo),e&&o(Qt),$(Ze,e),e&&o(Bt),e&&o(x),e&&o(Gt),e&&o(ee),$(eo),e&&o(Rt),e&&o($e),e&&o(Jt),$(oo,e),e&&o(Vt),$(io,e),e&&o(Ft),e&&o(Co),e&&o(Kt),$(_e,e),e&&o(Yt),e&&o(oe),$(to),e&&o(Wt),e&&o(M),e&&o(Xt),$(ao,e),e&&o(Zt),$(so,e),e&&o(ea),e&&o(Ee),e&&o(oa),e&&o(So)}}}const Hl={local:"tokenizers",sections:[{local:"tokenizer-basati-sulle-parole",title:"Tokenizer basati sulle parole"},{local:"characterbased",title:"Character-based"},{local:"tokenizzazione-delle-sottoparole",sections:[{local:"e-non-solo",title:"E non solo!"}],title:"Tokenizzazione delle sottoparole"},{local:"caricamento-e-salvataggio",title:"Caricamento e salvataggio"},{local:"codifica",sections:[{local:"processo-di-tokenizzazione",title:"Processo di tokenizzazione"},{local:"dai-token-agli-input-ids",title:"Dai token agli input IDS"}],title:"Codifica"},{local:"decodifica",title:"Decodifica"}],title:"Tokenizers"};function Ql(T,d,_){let f="pt";return Dl(()=>{const w=new URLSearchParams(window.location.search);_(0,f=w.get("fw")||"pt")}),[f]}class Yl extends Pl{constructor(d){super();jl(this,d,Ql,Ol,Il,{})}}export{Yl as default,Hl as metadata};
