import{S as he,i as ge,s as be,e as o,k as f,w as G,t as n,M as we,c as r,d as s,m as h,a as l,x as O,h as p,b as m,G as t,g as i,y as Q,L as ke,q as W,o as D,B as F,v as ve}from"../../chunks/vendor-hf-doc-builder.js";import{I as _e}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as fe}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as $e}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function ze(oe){let u,B,d,g,y,w,H,E,J,C,k,A,q,U,P,b,V,S,X,Y,T,v,N,_,M,c,Z,$,ee,se,z,ae,te,I,x,ie,L;return w=new _e({}),k=new $e({props:{chapter:1,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"}]}}),v=new fe({props:{code:`from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

unmasker = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
result = unmasker(<span class="hljs-string">&quot;This man works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])

result = unmasker(<span class="hljs-string">&quot;This woman works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])`}}),_=new fe({props:{code:`['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']`,highlighted:`[<span class="hljs-string">&#x27;lawyer&#x27;</span>, <span class="hljs-string">&#x27;carpenter&#x27;</span>, <span class="hljs-string">&#x27;doctor&#x27;</span>, <span class="hljs-string">&#x27;waiter&#x27;</span>, <span class="hljs-string">&#x27;mechanic&#x27;</span>]
[<span class="hljs-string">&#x27;nurse&#x27;</span>, <span class="hljs-string">&#x27;waitress&#x27;</span>, <span class="hljs-string">&#x27;teacher&#x27;</span>, <span class="hljs-string">&#x27;maid&#x27;</span>, <span class="hljs-string">&#x27;prostitute&#x27;</span>]`}}),{c(){u=o("meta"),B=f(),d=o("h1"),g=o("a"),y=o("span"),G(w.$$.fragment),H=f(),E=o("span"),J=n("Bias e limiti"),C=f(),G(k.$$.fragment),A=f(),q=o("p"),U=n("Se intendi utilizzare un modello pre-addestrato o una versione affinata in produzione, sii consapevole che i modelli sono degli strumenti potenti, ma hanno dei limiti. Il pi\xF9 grande limite \xE8 che, per permettere un pre-addestramento su una quantit\xE0 importante di dati, i ricercatori spesso includono tutti i contenuti ai quali riescono ad accedere, prendendo nel contempo il meglio e il peggio di ci\xF2 che Intenet offre."),P=f(),b=o("p"),V=n("Per vederne una rappresentazione rapida, torniamo all\u2019esempio della pipeline "),S=o("code"),X=n("fill-mask"),Y=n(" con il modello BERT:"),T=f(),G(v.$$.fragment),N=f(),G(_.$$.fragment),M=f(),c=o("p"),Z=n("Quando domandiamo al modello di trovare la parola mancante in queste due frasi, questo produce solo una risposta senza genere predeterminato (\u2018waiter/waitress\u2019). Le altre parole si riferiscono a professioni che sono solitamente associate ad un genere specifico; inoltre, come potete vedere, \u2018prostitute\u2019 finisce tra le 5 associazioni pi\xF9 probabili che il modello predice per \u201Cwoman\u201D e \u201Cwork\u201D. Ci\xF2 succede nonostante BERT sia uno dei rari modelli Transformer che non sono costruiti recuperando dati di ogni sorta da internet, ma utilizzando dati apparentemente neutri (\xE8 addestrato sui dataset "),$=o("a"),ee=n("English Wikipedia"),se=n(" e "),z=o("a"),ae=n("BookCorpus"),te=n(")."),I=f(),x=o("p"),ie=n("Nell\u2019utilizzare questi strumenti, \xE8 perci\xF2 necessario tenere a mente che il modello d\u2019origine in corso di utilizzazione potrebbe facilmente generare contenuti sessisti, razzisti oppure omofobici. Nemmeno l\u2019affinamento del modello su dati personali riesce a far sparire questo bias intrinseco."),this.h()},l(e){const a=we('[data-svelte="svelte-1phssyn"]',document.head);u=r(a,"META",{name:!0,content:!0}),a.forEach(s),B=h(e),d=r(e,"H1",{class:!0});var R=l(d);g=r(R,"A",{id:!0,class:!0,href:!0});var re=l(g);y=r(re,"SPAN",{});var ne=l(y);O(w.$$.fragment,ne),ne.forEach(s),re.forEach(s),H=h(R),E=r(R,"SPAN",{});var le=l(E);J=p(le,"Bias e limiti"),le.forEach(s),R.forEach(s),C=h(e),O(k.$$.fragment,e),A=h(e),q=r(e,"P",{});var pe=l(q);U=p(pe,"Se intendi utilizzare un modello pre-addestrato o una versione affinata in produzione, sii consapevole che i modelli sono degli strumenti potenti, ma hanno dei limiti. Il pi\xF9 grande limite \xE8 che, per permettere un pre-addestramento su una quantit\xE0 importante di dati, i ricercatori spesso includono tutti i contenuti ai quali riescono ad accedere, prendendo nel contempo il meglio e il peggio di ci\xF2 che Intenet offre."),pe.forEach(s),P=h(e),b=r(e,"P",{});var K=l(b);V=p(K,"Per vederne una rappresentazione rapida, torniamo all\u2019esempio della pipeline "),S=r(K,"CODE",{});var ce=l(S);X=p(ce,"fill-mask"),ce.forEach(s),Y=p(K," con il modello BERT:"),K.forEach(s),T=h(e),O(v.$$.fragment,e),N=h(e),O(_.$$.fragment,e),M=h(e),c=r(e,"P",{});var j=l(c);Z=p(j,"Quando domandiamo al modello di trovare la parola mancante in queste due frasi, questo produce solo una risposta senza genere predeterminato (\u2018waiter/waitress\u2019). Le altre parole si riferiscono a professioni che sono solitamente associate ad un genere specifico; inoltre, come potete vedere, \u2018prostitute\u2019 finisce tra le 5 associazioni pi\xF9 probabili che il modello predice per \u201Cwoman\u201D e \u201Cwork\u201D. Ci\xF2 succede nonostante BERT sia uno dei rari modelli Transformer che non sono costruiti recuperando dati di ogni sorta da internet, ma utilizzando dati apparentemente neutri (\xE8 addestrato sui dataset "),$=r(j,"A",{href:!0,rel:!0});var me=l($);ee=p(me,"English Wikipedia"),me.forEach(s),se=p(j," e "),z=r(j,"A",{href:!0,rel:!0});var ue=l(z);ae=p(ue,"BookCorpus"),ue.forEach(s),te=p(j,")."),j.forEach(s),I=h(e),x=r(e,"P",{});var de=l(x);ie=p(de,"Nell\u2019utilizzare questi strumenti, \xE8 perci\xF2 necessario tenere a mente che il modello d\u2019origine in corso di utilizzazione potrebbe facilmente generare contenuti sessisti, razzisti oppure omofobici. Nemmeno l\u2019affinamento del modello su dati personali riesce a far sparire questo bias intrinseco."),de.forEach(s),this.h()},h(){m(u,"name","hf:doc:metadata"),m(u,"content",JSON.stringify(qe)),m(g,"id","bias-e-limiti"),m(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(g,"href","#bias-e-limiti"),m(d,"class","relative group"),m($,"href","https://huggingface.co/datasets/wikipedia"),m($,"rel","nofollow"),m(z,"href","https://huggingface.co/datasets/bookcorpus"),m(z,"rel","nofollow")},m(e,a){t(document.head,u),i(e,B,a),i(e,d,a),t(d,g),t(g,y),Q(w,y,null),t(d,H),t(d,E),t(E,J),i(e,C,a),Q(k,e,a),i(e,A,a),i(e,q,a),t(q,U),i(e,P,a),i(e,b,a),t(b,V),t(b,S),t(S,X),t(b,Y),i(e,T,a),Q(v,e,a),i(e,N,a),Q(_,e,a),i(e,M,a),i(e,c,a),t(c,Z),t(c,$),t($,ee),t(c,se),t(c,z),t(z,ae),t(c,te),i(e,I,a),i(e,x,a),t(x,ie),L=!0},p:ke,i(e){L||(W(w.$$.fragment,e),W(k.$$.fragment,e),W(v.$$.fragment,e),W(_.$$.fragment,e),L=!0)},o(e){D(w.$$.fragment,e),D(k.$$.fragment,e),D(v.$$.fragment,e),D(_.$$.fragment,e),L=!1},d(e){s(u),e&&s(B),e&&s(d),F(w),e&&s(C),F(k,e),e&&s(A),e&&s(q),e&&s(P),e&&s(b),e&&s(T),F(v,e),e&&s(N),F(_,e),e&&s(M),e&&s(c),e&&s(I),e&&s(x)}}}const qe={local:"bias-e-limiti",title:"Bias e limiti"};function xe(oe){return ve(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ce extends he{constructor(u){super();ge(this,u,xe,ze,be,{})}}export{Ce as default,qe as metadata};
