import{S as ec,i as ac,s as sc,e as r,t,k as m,w as $,c as p,a as c,h as n,d as s,m as f,x as j,g as d,G as a,y as E,q as g,o as v,B as x,l as Yp,M as tc,b as C,p as Da,v as nc,n as Pa}from"../../chunks/vendor-hf-doc-builder.js";import{T as lr}from"../../chunks/Tip-hf-doc-builder.js";import{Y as xt}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Qn}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Xp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as ic}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function oc(w){let i,u;return i=new Xp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function lc(w){let i,u;return i=new Xp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function rc(w){let i,u,o,b,k,h,_,y;return _=new T({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># Same as before</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># This is new</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){i=r("p"),u=t("Continuando l\u2019esempio del "),o=r("a"),b=t("capitolo precedente"),k=t(", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in TensorFlow:"),h=m(),$(_.$$.fragment),this.h()},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"Continuando l\u2019esempio del "),o=p(q,"A",{href:!0});var I=c(o);b=n(I,"capitolo precedente"),I.forEach(s),k=n(q,", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in TensorFlow:"),q.forEach(s),h=f(z),j(_.$$.fragment,z),this.h()},h(){C(o,"href","/course/chapter2")},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),d(z,h,q),E(_,z,q),y=!0},i(z){y||(g(_.$$.fragment,z),y=!0)},o(z){v(_.$$.fragment,z),y=!1},d(z){z&&s(i),z&&s(h),x(_,z)}}}function pc(w){let i,u,o,b,k,h,_,y;return _=new T({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># Same as before</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># This is new</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){i=r("p"),u=t("Continuando l\u2019esempio del "),o=r("a"),b=t("capitolo precedente"),k=t(", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in PyTorch:"),h=m(),$(_.$$.fragment),this.h()},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"Continuando l\u2019esempio del "),o=p(q,"A",{href:!0});var I=c(o);b=n(I,"capitolo precedente"),I.forEach(s),k=n(q,", ecco come addestrare un classificatore di sequenze su un\u2019unica batch in PyTorch:"),q.forEach(s),h=f(z),j(_.$$.fragment,z),this.h()},h(){C(o,"href","/course/chapter2")},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),d(z,h,q),E(_,z,q),y=!0},i(z){y||(g(_.$$.fragment,z),y=!0)},o(z){v(_.$$.fragment,z),y=!1},d(z){z&&s(i),z&&s(h),x(_,z)}}}function cc(w){let i,u;return i=new xt({props:{id:"W_gMJF0xomE"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function dc(w){let i,u;return i=new xt({props:{id:"_BZearw7f0w"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function uc(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Prova tu!"),k=t(" Quali sono le label dell\u2019elemento 15 del training set, e 87 del validation set?")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var y=c(o);b=n(y,"Prova tu!"),y.forEach(s),k=n(_," Quali sono le label dell\u2019elemento 15 del training set, e 87 del validation set?"),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function mc(w){let i,u;return i=new xt({props:{id:"P-rZWqcB6CE"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function fc(w){let i,u;return i=new xt({props:{id:"0u3ioSwev3s"}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function hc(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Prova tu!"),k=t(" Prendere l\u2019element 15 del training set e tokenizzare le due frasi sia separatamente, sia come coppia. Qual \xE8 la differenza tra i due risultati?")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var y=c(o);b=n(y,"Prova tu!"),y.forEach(s),k=n(_," Prendere l\u2019element 15 del training set e tokenizzare le due frasi sia separatamente, sia come coppia. Qual \xE8 la differenza tra i due risultati?"),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function _c(w){let i,u,o,b,k,h,_,y;return{c(){i=r("p"),u=t("La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=r("em"),b=t("collate function"),k=t(" ("),h=r("em"),_=t("funzione di raccolta"),y=t("). Il collator (raccoglitore) di default \xE8 la funzione che converte semplicemente i campioni in tf.Tensor e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9.")},l(z){i=p(z,"P",{});var q=c(i);u=n(q,"La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=p(q,"EM",{});var I=c(o);b=n(I,"collate function"),I.forEach(s),k=n(q," ("),h=p(q,"EM",{});var N=c(h);_=n(N,"funzione di raccolta"),N.forEach(s),y=n(q,"). Il collator (raccoglitore) di default \xE8 la funzione che converte semplicemente i campioni in tf.Tensor e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9."),q.forEach(s)},m(z,q){d(z,i,q),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,y)},d(z){z&&s(i)}}}function bc(w){let i,u,o,b,k,h,_,y,z,q,I;return{c(){i=r("p"),u=t("La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=r("em"),b=t("collate function"),k=t(" ("),h=r("em"),_=t("funzione di raccolta"),y=t("). \xC8 uno dei parametri che si possono passare quando si costruisce un "),z=r("code"),q=t("DataLoader"),I=t(", e il default \xE8 la funzione che converte semplicemente i campioni in tensori PyTorch e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9.")},l(N){i=p(N,"P",{});var D=c(i);u=n(D,"La funzione responsabile dell\u2019assembramento dei campioni in una batch si chiama "),o=p(D,"EM",{});var M=c(o);b=n(M,"collate function"),M.forEach(s),k=n(D," ("),h=p(D,"EM",{});var U=c(h);_=n(U,"funzione di raccolta"),U.forEach(s),y=n(D,"). \xC8 uno dei parametri che si possono passare quando si costruisce un "),z=p(D,"CODE",{});var O=c(z);q=n(O,"DataLoader"),O.forEach(s),I=n(D,", e il default \xE8 la funzione che converte semplicemente i campioni in tensori PyTorch e li concatena (ricorsivamente nel caso in cui gli elementi siano liste, tuple o dizionari). Ci\xF2 non sar\xE0 possibile nel nostro caso poich\xE9 gli input non hanno tutti la stessa lunghezza. Abbiamo rimandato il padding apposta, per poterlo applicare secondo necessit\xE0 ad ogni batch, evitando quindi input troppo lunghi con molto padding. Ci\xF2 accelerer\xE0 l\u2019addestramento di un bel po\u2019, ma pu\xF2 causare problemi se l\u2019addestramento avviene su TPU \u2014 le TPU preferiscono dimensioni fisse, anche se ci\xF2 richiede del padding in pi\xF9."),D.forEach(s)},m(N,D){d(N,i,D),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,y),a(i,z),a(z,q),a(i,I)},d(N){N&&s(i)}}}function gc(w){let i,u;return i=new T({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function vc(w){let i,u;return i=new T({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function zc(w){let i,u,o,b,k;return i=new T({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment),u=m(),o=r("p"),b=t("Ottimo! Adesso che siamo passati dal testo grezzo a dei batch che il modello \xE8 in grado di trattare, siamo pronti per affinarlo!")},l(h){j(i.$$.fragment,h),u=f(h),o=p(h,"P",{});var _=c(o);b=n(_,"Ottimo! Adesso che siamo passati dal testo grezzo a dei batch che il modello \xE8 in grado di trattare, siamo pronti per affinarlo!"),_.forEach(s)},m(h,_){E(i,h,_),d(h,u,_),d(h,o,_),a(o,b),k=!0},i(h){k||(g(i.$$.fragment,h),k=!0)},o(h){v(i.$$.fragment,h),k=!1},d(h){x(i,h),h&&s(u),h&&s(o)}}}function kc(w){let i,u;return i=new T({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){$(i.$$.fragment)},l(o){j(i.$$.fragment,o)},m(o,b){E(i,o,b),u=!0},i(o){u||(g(i.$$.fragment,o),u=!0)},o(o){v(i.$$.fragment,o),u=!1},d(o){x(i,o)}}}function $c(w){let i,u,o,b,k;return{c(){i=r("p"),u=t("\u270F\uFE0F "),o=r("strong"),b=t("Prova tu!"),k=t(" Replicare il preprocessing sul dataset GLUE SST-2. \xC8 leggermente diverso poiche \xE8 composto da frasi singole e non da coppie di frasi, ma il resto della procedura dovrebbe essere simile. Per una sfida pi\xF9 complessa, provare a scrivere una funzione di preprocessing che funzioni per qualsiasi dei compiti in GLUE.")},l(h){i=p(h,"P",{});var _=c(i);u=n(_,"\u270F\uFE0F "),o=p(_,"STRONG",{});var y=c(o);b=n(y,"Prova tu!"),y.forEach(s),k=n(_," Replicare il preprocessing sul dataset GLUE SST-2. \xC8 leggermente diverso poiche \xE8 composto da frasi singole e non da coppie di frasi, ma il resto della procedura dovrebbe essere simile. Per una sfida pi\xF9 complessa, provare a scrivere una funzione di preprocessing che funzioni per qualsiasi dei compiti in GLUE."),_.forEach(s)},m(h,_){d(h,i,_),a(i,u),a(i,o),a(o,b),a(i,k)},d(h){h&&s(i)}}}function Zp(w){let i,u,o,b,k,h,_,y,z,q,I,N,D,M,U,O,Q,Y,ce,Ce;return O=new T({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){i=r("p"),u=t("Ora che sono stati definiti un dataset e un collator di dati, dobbiamo metterli insieme. Si potrebbero caricare e raccogliere i batch manualmente, ma significherebbe molto lavoro e probabilmente una bassa performance. Invece, vi \xE8 un metodo semplice che offre una soluzione con buona performance a questo problema: "),o=r("code"),b=t("to_tf_dataset()"),k=t(". Questo impacchetter\xE0 il dataset con "),h=r("code"),_=t("tf.data.Dataset"),y=t(`, con una collate function opzionale.
`),z=r("code"),q=t("tf.data.Dataset"),I=t(" \xE8 un formato nativo di TensorFlow che Keras pu\xF2 utilizzare durante "),N=r("code"),D=t("model.fit()"),M=t(", cosicch\xE9 questo metodo converte immediatamente un \u{1F917} Dataset in un formato pronto per l\u2019addestramento. Vediamolo in azione col nostro dataset!"),U=m(),$(O.$$.fragment),Q=m(),Y=r("p"),ce=t("Fine! Questi dataset verranno utilizzati nelle prossime lezioni, dove l\u2019addestramento sar\xE0 reso piacevolmente immediato dopo tutto questo duro lavoro di preprocessing.")},l(P){i=p(P,"P",{});var S=c(i);u=n(S,"Ora che sono stati definiti un dataset e un collator di dati, dobbiamo metterli insieme. Si potrebbero caricare e raccogliere i batch manualmente, ma significherebbe molto lavoro e probabilmente una bassa performance. Invece, vi \xE8 un metodo semplice che offre una soluzione con buona performance a questo problema: "),o=p(S,"CODE",{});var Ta=c(o);b=n(Ta,"to_tf_dataset()"),Ta.forEach(s),k=n(S,". Questo impacchetter\xE0 il dataset con "),h=p(S,"CODE",{});var de=c(h);_=n(de,"tf.data.Dataset"),de.forEach(s),y=n(S,`, con una collate function opzionale.
`),z=p(S,"CODE",{});var Sa=c(z);q=n(Sa,"tf.data.Dataset"),Sa.forEach(s),I=n(S," \xE8 un formato nativo di TensorFlow che Keras pu\xF2 utilizzare durante "),N=p(S,"CODE",{});var Oa=c(N);D=n(Oa,"model.fit()"),Oa.forEach(s),M=n(S,", cosicch\xE9 questo metodo converte immediatamente un \u{1F917} Dataset in un formato pronto per l\u2019addestramento. Vediamolo in azione col nostro dataset!"),S.forEach(s),U=f(P),j(O.$$.fragment,P),Q=f(P),Y=p(P,"P",{});var Ge=c(Y);ce=n(Ge,"Fine! Questi dataset verranno utilizzati nelle prossime lezioni, dove l\u2019addestramento sar\xE0 reso piacevolmente immediato dopo tutto questo duro lavoro di preprocessing."),Ge.forEach(s)},m(P,S){d(P,i,S),a(i,u),a(i,o),a(o,b),a(i,k),a(i,h),a(h,_),a(i,y),a(i,z),a(z,q),a(i,I),a(i,N),a(N,D),a(i,M),d(P,U,S),E(O,P,S),d(P,Q,S),d(P,Y,S),a(Y,ce),Ce=!0},i(P){Ce||(g(O.$$.fragment,P),Ce=!0)},o(P){v(O.$$.fragment,P),Ce=!1},d(P){P&&s(i),P&&s(U),x(O,P),P&&s(Q),P&&s(Y)}}}function jc(w){let i,u,o,b,k,h,_,y,z,q,I,N,D,M,U,O,Q,Y,ce,Ce,P,S,Ta,de,Sa,Oa,Ge,Ee,ye,ps,Ve,Bn,cs,Hn,qt,se,te,Aa,Z,Wn,Je,Un,Gn,Ke,Vn,Jn,Ye,Kn,Yn,wt,Ia,Zn,Ct,Ze,yt,Xe,Dt,B,Xn,ds,ei,ai,us,si,ti,ms,ni,ii,fs,oi,li,hs,ri,pi,Pt,ue,ci,_s,di,ui,bs,mi,fi,Tt,De,hi,gs,_i,bi,St,ea,Ot,aa,At,me,gi,vs,vi,zi,zs,ki,$i,It,sa,Lt,ta,Nt,L,ji,ks,Ei,xi,$s,qi,wi,js,Ci,yi,Es,Di,Pi,xs,Ti,Si,qs,Oi,Ai,ws,Ii,Li,Ft,Pe,Mt,xe,Te,Cs,na,Ni,ys,Fi,Rt,ne,ie,La,Se,Mi,Na,Ri,Qi,Qt,ia,Bt,Fa,Bi,Ht,oa,Wt,la,Ut,G,Hi,Ma,Wi,Ui,Ds,Gi,Vi,Ps,Ji,Ki,Ts,Yi,Zi,Gt,Oe,Vt,Ae,Xi,Ss,eo,ao,Jt,ra,Kt,Ra,so,Yt,pa,Zt,fe,to,Os,no,io,As,oo,lo,Xt,ca,en,V,ro,Is,po,co,Ls,uo,mo,Ns,fo,ho,Fs,_o,bo,an,he,go,Ms,vo,zo,Rs,ko,$o,sn,J,jo,Qs,Eo,xo,Qa,qo,wo,Bs,Co,yo,Hs,Do,Po,tn,Ba,To,nn,Ie,So,Ws,Oo,Ao,on,_e,Io,Ha,Lo,No,Wa,Fo,Mo,ln,da,rn,K,Ro,Us,Qo,Bo,Gs,Ho,Wo,Vs,Uo,Go,ua,Vo,Jo,pn,be,Ko,ma,Js,Yo,Zo,Ks,Xo,el,cn,fa,dn,A,al,Ys,sl,tl,Zs,nl,il,Xs,ol,ll,et,rl,pl,at,cl,dl,st,ul,ml,tt,fl,hl,ha,_l,bl,un,Le,gl,nt,vl,zl,mn,ge,kl,it,$l,jl,ot,El,xl,fn,_a,hn,Ua,ql,_n,ba,bn,ve,wl,lt,Cl,yl,rt,Dl,Pl,gn,H,Tl,pt,Sl,Ol,ct,Al,Il,dt,Ll,Nl,ut,Fl,Ml,mt,Rl,Ql,vn,Ne,Bl,ft,Hl,Wl,zn,qe,Fe,ht,ga,Ul,_t,Gl,kn,va,$n,Ga,Me,Vl,bt,Jl,Kl,jn,oe,le,Va,X,Yl,gt,Zl,Xl,vt,er,ar,zt,sr,tr,En,za,xn,ka,qn,Re,nr,kt,ir,or,wn,$a,Cn,re,pe,Ja,Qe,yn,Ka,Dn;o=new ic({props:{fw:w[0]}}),y=new Qn({});const rr=[lc,oc],ja=[];function pr(e,l){return e[0]==="pt"?0:1}D=pr(w),M=ja[D]=rr[D](w);const cr=[pc,rc],Ea=[];function dr(e,l){return e[0]==="pt"?0:1}O=dr(w),Q=Ea[O]=cr[O](w),Ve=new Qn({});const ur=[dc,cc],xa=[];function mr(e,l){return e[0]==="pt"?0:1}se=mr(w),te=xa[se]=ur[se](w),Ze=new T({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),Xe=new T({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),ea=new T({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),aa=new T({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>}`}}),sa=new T({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),ta=new T({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Pe=new lr({props:{$$slots:{default:[uc]},$$scope:{ctx:w}}}),na=new Qn({});const fr=[fc,mc],qa=[];function hr(e,l){return e[0]==="pt"?0:1}ne=hr(w),ie=qa[ne]=fr[ne](w),ia=new T({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),oa=new T({props:{code:`inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>)
inputs`}}),la=new T({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Oe=new lr({props:{$$slots:{default:[hc]},$$scope:{ctx:w}}}),ra=new T({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),pa=new T({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),ca=new T({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),da=new T({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),fa=new T({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),_a=new T({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),ba=new T({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),ga=new Qn({}),va=new xt({props:{id:"7q5NyFT8REg"}});function _r(e,l){return e[0]==="pt"?bc:_c}let Pn=_r(w),we=Pn(w);const br=[vc,gc],wa=[];function gr(e,l){return e[0]==="pt"?0:1}oe=gr(w),le=wa[oe]=br[oe](w),za=new T({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),ka=new T({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),$a=new T({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const vr=[kc,zc],Ca=[];function zr(e,l){return e[0]==="tf"?0:1}re=zr(w),pe=Ca[re]=vr[re](w),Qe=new lr({props:{$$slots:{default:[$c]},$$scope:{ctx:w}}});let F=w[0]==="tf"&&Zp();return{c(){i=r("meta"),u=m(),$(o.$$.fragment),b=m(),k=r("h1"),h=r("a"),_=r("span"),$(y.$$.fragment),z=m(),q=r("span"),I=t("Processare i dati"),N=m(),M.c(),U=m(),Q.c(),Y=m(),ce=r("p"),Ce=t("Ovviamente, addestrare il modello su due frasi non porter\xE0 a dei risultati molto buoni. Per ottenere risultati migliori, si deve preparare un dataset pi\xF9 grande."),P=m(),S=r("p"),Ta=t("In questa sezione verr\xE0 usato come esempio il dataset MRPC (Microsoft Research Paraphrase Corpus), presentato nell\u2019"),de=r("a"),Sa=t("articolo"),Oa=t(" di William B. Dolan e Chris Brockett. Il dataset contiene 5801 coppie di frasi, con una label che indica se l\u2019una \xE8 una parafrasi dell\u2019altra (i.e. se hanno lo stesso significato). \xC8 stato selezionato per questo capitolo perch\xE9 \xE8 un dataset piccolo, con cui \xE8 facile sperimentare durante l\u2019addestramento."),Ge=m(),Ee=r("h3"),ye=r("a"),ps=r("span"),$(Ve.$$.fragment),Bn=m(),cs=r("span"),Hn=t("Caricare un dataset dall'Hub"),qt=m(),te.c(),Aa=m(),Z=r("p"),Wn=t("L\u2019Hub non contiene solo modelli; contiene anche molti dataset in tante lingue diverse. I dataset possono essere esplorati "),Je=r("a"),Un=t("qui"),Gn=t(", ed \xE8 consigliato tentare di caricare e processare un nuovo dataset dopo aver completato questa sezione (cfr. la "),Ke=r("a"),Vn=t("documentazione"),Jn=t("). Per ora, focalizziamoci sul dataset MRPC! Questo \xE8 uno dei 10 dataset che fanno parte del "),Ye=r("a"),Kn=t("GLUE benchmark"),Yn=t(", che \xE8 un benchmark accademico usato per misurare la performance di modelli ML su 10 compiti di classificazione del testo."),wt=m(),Ia=r("p"),Zn=t("La libreria \u{1F917} Datasets fornisce un comando molto semplice per scaricare e mettere nella cache un dataset sull\u2019Hub. Il dataset MRPC pu\xF2 essere scaricato cos\xEC:"),Ct=m(),$(Ze.$$.fragment),yt=m(),$(Xe.$$.fragment),Dt=m(),B=r("p"),Xn=t("Il risultato \xE8 un oggetto di tipo "),ds=r("code"),ei=t("DatasetDict"),ai=t(" che contiene il training set, il validation set, e il test set. Ciascuno di questi contiene svariate colonne, ("),us=r("code"),si=t("sentence1"),ti=t(", "),ms=r("code"),ni=t("sentence2"),ii=t(", "),fs=r("code"),oi=t("label"),li=t(", e "),hs=r("code"),ri=t("idx"),pi=t(") a un numero variabile di righe, corrispondenti al numero di elementi in ogni set (quindi, vi sono 3668 coppie di frasi nel training set, 408 nel validation set, e 1725 nel test set)."),Pt=m(),ue=r("p"),ci=t("Questo comando scarica il dataset e lo mette in cache, in "),_s=r("em"),di=t("~/.cache/huggingface/dataset"),ui=t(" secondo l\u2019impostazione predefinita. Nel Capitolo 2 \xE8 stato spiegato come personalizzare la cartella di cache impostando la variabile d\u2019ambiente "),bs=r("code"),mi=t("HF_HOME"),fi=t("."),Tt=m(),De=r("p"),hi=t("Ogni coppia di frasi nell\u2019oggetto "),gs=r("code"),_i=t("raw_datasets"),bi=t(" pu\xF2 essere ottenuta tramite il suo indice, come in un dizionario:"),St=m(),$(ea.$$.fragment),Ot=m(),$(aa.$$.fragment),At=m(),me=r("p"),gi=t("Le label sono gi\xE0 numeri interi, quindi non \xE8 necessario alcun preprocessing. Per sapere a quale numero corrisponde quale tipo di label, si possono analizzare le "),vs=r("code"),vi=t("features"),zi=t(" del "),zs=r("code"),ki=t("raw_train_dataset"),$i=t(". Ci\xF2 permette di capire la tipologia di ogni colonna:"),It=m(),$(sa.$$.fragment),Lt=m(),$(ta.$$.fragment),Nt=m(),L=r("p"),ji=t("Dietro le quinte, "),ks=r("code"),Ei=t("label"),xi=t(" \xE8 del tipo  "),$s=r("code"),qi=t("ClassLabel"),wi=t(", e la corrispondenza tra i numeri e i nomi delle label \xE8 contenuta nella cartella "),js=r("em"),Ci=t("names"),yi=t(". "),Es=r("code"),Di=t("0"),Pi=t(" corrisponde a "),xs=r("code"),Ti=t("not_equivalent"),Si=t(" (significato diverso), e "),qs=r("code"),Oi=t("1"),Ai=t(" corrisponde a "),ws=r("code"),Ii=t("equivalent"),Li=t(" (stesso significato)."),Ft=m(),$(Pe.$$.fragment),Mt=m(),xe=r("h3"),Te=r("a"),Cs=r("span"),$(na.$$.fragment),Ni=m(),ys=r("span"),Fi=t("Preprocessing del dataset"),Rt=m(),ie.c(),La=m(),Se=r("p"),Mi=t("Per preprocessare il dataset, \xE8 necessario convertire il testo in numeri comprensibili al modello. Come dimostrato nel "),Na=r("a"),Ri=t("capitolo precedente"),Qi=t(", ci\xF2 viene fatto con un tokenizer (tokenizzatore). Il tokenizer prende come input sia una frase sia una lista di frasi, quindi \xE8 possibile effettuare la tokenizzazione di tutte le prime e seconde frasi di ogni coppia in questo modo:"),Qt=m(),$(ia.$$.fragment),Bt=m(),Fa=r("p"),Bi=t("Tuttavia, non si possono semplicemente passare al modello due frasi e sperare di predire se l\u2019una \xE8 una parafrasi dell\u2019altra o no. Bisogna gestire le due frasi come una coppia, e applicare il preprocessing necessario. Fortunatamente, il tokenizer pu\xF2 anche prendere come input una coppia di frasi e prepararla nel formato atteso dal modello BERT:"),Ht=m(),$(oa.$$.fragment),Wt=m(),$(la.$$.fragment),Ut=m(),G=r("p"),Hi=t("Sono state gi\xE0 discusse nel "),Ma=r("a"),Wi=t("Capitolo 2"),Ui=t(" le chiavi "),Ds=r("code"),Gi=t("input_ids"),Vi=t(" e "),Ps=r("code"),Ji=t("attention_mask"),Ki=t(", ma il discorso su "),Ts=r("code"),Yi=t("token_type_ids"),Zi=t(" era stato rimandato. In questo esempio, ci\xF2 pu\xF2 essere usato per indicare al modello quale parte dell\u2019input \xE8 la prima frase, e quale la seconda."),Gt=m(),$(Oe.$$.fragment),Vt=m(),Ae=r("p"),Xi=t("Decodificando gli ID in "),Ss=r("code"),eo=t("input_ids"),ao=t(" per ritrasformarli in parole:"),Jt=m(),$(ra.$$.fragment),Kt=m(),Ra=r("p"),so=t("si ottiene:"),Yt=m(),$(pa.$$.fragment),Zt=m(),fe=r("p"),to=t("Perci\xF2 \xE8 chiaro che il modello si aspetta gli input nella forma "),Os=r("code"),no=t("[CLS] frase1 [SEP] frase2 [SEP]"),io=t(" quando vi sono due frasi. Allineando con "),As=r("code"),oo=t("token_type_ids"),lo=t(" si ottiene:"),Xt=m(),$(ca.$$.fragment),en=m(),V=r("p"),ro=t("Le parti dell\u2019input corrispondenti a "),Is=r("code"),po=t("[CLS] frase1 [SEP]"),co=t(" hanno tutte un token type ID di "),Ls=r("code"),uo=t("0"),mo=t(", mentre le altre parti, corrispondenti quindi a "),Ns=r("code"),fo=t("frase2 [SEP]"),ho=t(", hanno tutte un token type ID di "),Fs=r("code"),_o=t("1"),bo=t("."),an=m(),he=r("p"),go=t("Da notare che se viene selezionato un altro checkpoint, gli input tokenizzati non conterranno necessariamente i "),Ms=r("code"),vo=t("token_type_ids"),zo=t(" (ad esempio, non si ottengono usando un modello DistilBERT). I "),Rs=r("code"),ko=t("token_type_ids"),$o=t(" si ottengono solo quando il modello saprebbe che farne, avendole gi\xE0 viste in fase di pre-addestramento."),sn=m(),J=r("p"),jo=t("In questo caso, BERT \xE8 stato pre-addestrato con i token type IDs, e in aggiunta all\u2019obiettivo di "),Qs=r("em"),Eo=t("masked language modeling"),xo=t(" di cui si era parlato nel "),Qa=r("a"),qo=t("Capitolo 1"),wo=t(", vi \xE8 un altro obiettivo che si chiama "),Bs=r("em"),Co=t("next sentence prediction"),yo=t(" ("),Hs=r("em"),Do=t("predire la prossima frase"),Po=t("). Lo scopo di questo task \xE8 modellizzare la relazione tra coppie di frasi."),tn=m(),Ba=r("p"),To=t("Durante un task di next sentence prediction, il modello riceve una coppia di frasi (con token mascherati in maniera aleatoria) e deve predire se la seconda segue la prima. Per rendere il task meno banale, la met\xE0 delle volte le frasi si susseguono nel documento da cui erano state estratte originariamente, l\u2019altra met\xE0 delle volte le frasi provengono da due documenti diversi."),nn=m(),Ie=r("p"),So=t("In generale, non bisogna preoccuparsi se i "),Ws=r("code"),Oo=t("token_type_ids"),Ao=t(" sono presenti o no negli input tokenizzati: finch\xE9 viene usato lo stesso checkpoint per il tokenizer e il modello, tutto andr\xE0 bene poich\xE9 il tokenizer sa cosa fornire al modello."),on=m(),_e=r("p"),Io=t("Ora che abbiamo visto come il tokenizer pu\xF2 gestire una coppia di frasi, possiamo usarlo per tokenizzare l\u2019intero dataset: come nel "),Ha=r("a"),Lo=t("capitolo precedente"),No=t(", si pu\xF2 fornire al tokenizer una lista di coppie di frasi dando prima la lista delle prime frasi, e poi la lista delle seconde frasi. Questo approcchio \xE8 anche compatibile le opzioni di padding e truncation gi\xE0 viste nel "),Wa=r("a"),Fo=t("Capitolo 2"),Mo=t(". Perci\xF2, un modo per preprocessare il dataset di addestramento \xE8:"),ln=m(),$(da.$$.fragment),rn=m(),K=r("p"),Ro=t("Questo metodo funziona, ma ha lo svantaggio di restituire un dizionario (avente "),Us=r("code"),Qo=t("input_ids"),Bo=t(", "),Gs=r("code"),Ho=t("attention_mask"),Wo=t(", e "),Vs=r("code"),Uo=t("token_type_ids"),Go=t(" come chiavi, e delle liste di liste come valori). Oltretutto, questo metodo funziona solo se si ha a disposizione RAM sufficiente per contenere l\u2019intero dataset durante la tokenizzazione (mentre i dataset dalla libreria \u{1F917} Datasets sono file "),ua=r("a"),Vo=t("Apache Arrow"),Jo=t(" archiviati su disco, perci\xF2 in memoria vengono caricati solo i campioni richiesti)."),pn=m(),be=r("p"),Ko=t("Per tenere i dati come dataset, utilizzare il metodo "),ma=r("a"),Js=r("code"),Yo=t("Dataset.map()"),Zo=t(". Ci\xF2 permette anche della flessibilit\xE0 extra, qualora fosse necessario del preprocessing aggiuntivo oltre alla tokenizzazione. Il metodo "),Ks=r("code"),Xo=t("map()"),el=t(" applica una funziona ad ogni elemento del dataset, perci\xF2 bisogna definire una funzione che tokenizzi gli input:"),cn=m(),$(fa.$$.fragment),dn=m(),A=r("p"),al=t("Questa funzione riceve un dizionario (come gli elementi del nostro dataset) e restituisce un nuovo dizionario con input_ids"),Ys=r("code"),sl=t(", "),tl=t("attention_mask"),Zs=r("code"),nl=t(", e "),il=t("token_type_ids"),Xs=r("code"),ol=t("come chiavi. Funziona anche se il dizionario"),ll=t("example"),et=r("code"),rl=t("contiene svariati campioni (ad una chiave corrisponde una lista di frasi) poich\xE9 il"),pl=t("tokenizer"),at=r("code"),cl=t("funziona con liste di coppie di frasi, come gi\xE0 visto. Ci\xF2 permette di usare l'opzione"),dl=t("batched=True"),st=r("code"),ul=t("nella chiamata a"),ml=t("map()"),tt=r("code"),fl=t(", che accelerer\xE0 di molto la tokenizzazione. Il "),hl=t("tokenizer` si appoggia ad un tokenizer scritto in Rust della libreria "),ha=r("a"),_l=t("\u{1F917} Tokenizers"),bl=t(". Questo tokenizer pu\xF2 essere molto veloce, ma solo se gli vengono forniti molti input insieme."),un=m(),Le=r("p"),gl=t("Per ora non ci siamo preoccupati del parametro "),nt=r("code"),vl=t("padding"),zl=t(" nella nostra funzione di tokenizzazione. Questo perch\xE9 il padding di tutti i campioni fino a lunghezza massima non \xE8 efficiente: \xE8 meglio fare il padding dei campioni quando stiamo assemblando una batch, poich\xE9 in quel momento \xE8 necessario il padding solo fino alla lunghezza massima nel batch, non la lunghezza massima nell\u2019intero dataset. Ci\xF2 permette di risparmiare molto tempo e potenza di calcolo nel caso in cui gli input abbiano lunghezze molto varie!"),mn=m(),ge=r("p"),kl=t("Ecco come si applica la funzione di tokenizzazione sull\u2019intero dataset. Bisogna usare "),it=r("code"),$l=t("batched=True"),jl=t(" nella chiamata a "),ot=r("code"),El=t("map"),xl=t(" in modo tale che la funzione venga applicata a vari elementi del dataset insieme, e non ad ogni elemento separatamente. Ci\xF2 permette un preprocessing pi\xF9 rapido."),fn=m(),$(_a.$$.fragment),hn=m(),Ua=r("p"),ql=t("La libreria \u{1F917} Datasets aggiunge nuovi campi ai dataset, uno per ogni chiave nel dizionario restituito dalla funzione di preprocessing:"),_n=m(),$(ba.$$.fragment),bn=m(),ve=r("p"),wl=t("Si pu\xF2 anche applicare il multiprocessing durante il preprocessing con la funzione "),lt=r("code"),Cl=t("map()"),yl=t(" utilizzando il parametro "),rt=r("code"),Dl=t("num_proc"),Pl=t(". Ci\xF2 non \xE8 stato dimostrato qui perch\xE9 la libreria \u{1F917} Tokenizers gi\xE0 utilizza vari thread per tokenizzare i campioni pi\xF9 rapidamente, ma nel caso in cui non venga usato un tokenizer rapido di questa libreria, ci\xF2 potrebbe velocizzare il preprocessing."),gn=m(),H=r("p"),Tl=t("La funzione "),pt=r("code"),Sl=t("tokenize_function"),Ol=t(" restituisce un dizionario con "),ct=r("code"),Al=t("input_ids"),Il=t(", "),dt=r("code"),Ll=t("attention_mask"),Nl=t(", e "),ut=r("code"),Fl=t("token_type_ids"),Ml=t(" come chiavi, quindi quei tre campi vengono aggiunti a tutti gli split (le parti) del dataset. Si possono anche cambiare i campi esistenti nel caso in cui la funzione di preprocessing restituisca un nuovo valore per una chiave gi\xE0 esistente nel dataset a cui viene applicato "),mt=r("code"),Rl=t("map()"),Ql=t("."),vn=m(),Ne=r("p"),Bl=t("L\u2019ultima cosa da fare \xE8 il padding di tutti i campioni alla lunghezza dell\u2019elemento pi\xF9 lungo quando sono inseriti in una batch \u2014 una tecnica che si chiama "),ft=r("em"),Hl=t("dynamic padding"),Wl=t("."),zn=m(),qe=r("h3"),Fe=r("a"),ht=r("span"),$(ga.$$.fragment),Ul=m(),_t=r("span"),Gl=t("Dynamic padding"),kn=m(),$(va.$$.fragment),$n=m(),we.c(),Ga=m(),Me=r("p"),Vl=t("In pratica, bisogna definire una collate function che applichi la giusta quantit\xE0 di padding agli elementi del dataset in una stessa batch. Fortunatamente, la libreria \u{1F917} Transformers fornisce questa funziona tramite "),bt=r("code"),Jl=t("DataCollatorWithPadding"),Kl=t(". Essa prende in input un tokenizer quando viene istanziata (per individuare quale token da usare per il padding, e se il modello si aspetta padding a sinistra o a destra dell\u2019input) e far\xE0 tutto il necessario:"),jn=m(),le.c(),Va=m(),X=r("p"),Yl=t("Per testare questo nuovo gioco, analizziamo alcuni campioni dal set di addestramento da raggruppare in un batch. Adesso togliamo le colonne  "),gt=r("code"),Zl=t("idx"),Xl=t(", "),vt=r("code"),er=t("sentence1"),ar=t(", e "),zt=r("code"),sr=t("sentence2"),tr=t(" poich\xE9 non saranno necessarie e contengono stringhe (e non si possono creare tensori con stringhe), e controlliamo le lunghezze di ogni elemento nel batch:"),En=m(),$(za.$$.fragment),xn=m(),$(ka.$$.fragment),qn=m(),Re=r("p"),nr=t("Nulla di sorprendente, i campioni hanno lunghezza variabile da 32 a 67. Il padding dinamico significa che i campioni in questo batch dovrebbero tutti ricevere un padding fino alla lunghezza di 67, il massimo nel batch. Senza padding dinamico, tutti i campioni dovrebbero ricevere un padding fino alla lunghezza massima nell\u2019intero dataset, o la lunghezza massima processabile dal modello. Bisogna controllare che il "),kt=r("code"),ir=t("data_collator"),or=t(" stia applicando un padding dinamico al batch in maniera corretta:"),wn=m(),$($a.$$.fragment),Cn=m(),pe.c(),Ja=m(),$(Qe.$$.fragment),yn=m(),F&&F.c(),Ka=Yp(),this.h()},l(e){const l=tc('[data-svelte="svelte-1phssyn"]',document.head);i=p(l,"META",{name:!0,content:!0}),l.forEach(s),u=f(e),j(o.$$.fragment,e),b=f(e),k=p(e,"H1",{class:!0});var ya=c(k);h=p(ya,"A",{id:!0,class:!0,href:!0});var Ya=c(h);_=p(Ya,"SPAN",{});var Za=c(_);j(y.$$.fragment,Za),Za.forEach(s),Ya.forEach(s),z=f(ya),q=p(ya,"SPAN",{});var Xa=c(q);I=n(Xa,"Processare i dati"),Xa.forEach(s),ya.forEach(s),N=f(e),M.l(e),U=f(e),Q.l(e),Y=f(e),ce=p(e,"P",{});var $t=c(ce);Ce=n($t,"Ovviamente, addestrare il modello su due frasi non porter\xE0 a dei risultati molto buoni. Per ottenere risultati migliori, si deve preparare un dataset pi\xF9 grande."),$t.forEach(s),P=f(e),S=p(e,"P",{});var Be=c(S);Ta=n(Be,"In questa sezione verr\xE0 usato come esempio il dataset MRPC (Microsoft Research Paraphrase Corpus), presentato nell\u2019"),de=p(Be,"A",{href:!0,rel:!0});var jt=c(de);Sa=n(jt,"articolo"),jt.forEach(s),Oa=n(Be," di William B. Dolan e Chris Brockett. Il dataset contiene 5801 coppie di frasi, con una label che indica se l\u2019una \xE8 una parafrasi dell\u2019altra (i.e. se hanno lo stesso significato). \xC8 stato selezionato per questo capitolo perch\xE9 \xE8 un dataset piccolo, con cui \xE8 facile sperimentare durante l\u2019addestramento."),Be.forEach(s),Ge=f(e),Ee=p(e,"H3",{class:!0});var He=c(Ee);ye=p(He,"A",{id:!0,class:!0,href:!0});var es=c(ye);ps=p(es,"SPAN",{});var Et=c(ps);j(Ve.$$.fragment,Et),Et.forEach(s),es.forEach(s),Bn=f(He),cs=p(He,"SPAN",{});var kr=c(cs);Hn=n(kr,"Caricare un dataset dall'Hub"),kr.forEach(s),He.forEach(s),qt=f(e),te.l(e),Aa=f(e),Z=p(e,"P",{});var We=c(Z);Wn=n(We,"L\u2019Hub non contiene solo modelli; contiene anche molti dataset in tante lingue diverse. I dataset possono essere esplorati "),Je=p(We,"A",{href:!0,rel:!0});var $r=c(Je);Un=n($r,"qui"),$r.forEach(s),Gn=n(We,", ed \xE8 consigliato tentare di caricare e processare un nuovo dataset dopo aver completato questa sezione (cfr. la "),Ke=p(We,"A",{href:!0,rel:!0});var jr=c(Ke);Vn=n(jr,"documentazione"),jr.forEach(s),Jn=n(We,"). Per ora, focalizziamoci sul dataset MRPC! Questo \xE8 uno dei 10 dataset che fanno parte del "),Ye=p(We,"A",{href:!0,rel:!0});var Er=c(Ye);Kn=n(Er,"GLUE benchmark"),Er.forEach(s),Yn=n(We,", che \xE8 un benchmark accademico usato per misurare la performance di modelli ML su 10 compiti di classificazione del testo."),We.forEach(s),wt=f(e),Ia=p(e,"P",{});var xr=c(Ia);Zn=n(xr,"La libreria \u{1F917} Datasets fornisce un comando molto semplice per scaricare e mettere nella cache un dataset sull\u2019Hub. Il dataset MRPC pu\xF2 essere scaricato cos\xEC:"),xr.forEach(s),Ct=f(e),j(Ze.$$.fragment,e),yt=f(e),j(Xe.$$.fragment,e),Dt=f(e),B=p(e,"P",{});var ee=c(B);Xn=n(ee,"Il risultato \xE8 un oggetto di tipo "),ds=p(ee,"CODE",{});var qr=c(ds);ei=n(qr,"DatasetDict"),qr.forEach(s),ai=n(ee," che contiene il training set, il validation set, e il test set. Ciascuno di questi contiene svariate colonne, ("),us=p(ee,"CODE",{});var wr=c(us);si=n(wr,"sentence1"),wr.forEach(s),ti=n(ee,", "),ms=p(ee,"CODE",{});var Cr=c(ms);ni=n(Cr,"sentence2"),Cr.forEach(s),ii=n(ee,", "),fs=p(ee,"CODE",{});var yr=c(fs);oi=n(yr,"label"),yr.forEach(s),li=n(ee,", e "),hs=p(ee,"CODE",{});var Dr=c(hs);ri=n(Dr,"idx"),Dr.forEach(s),pi=n(ee,") a un numero variabile di righe, corrispondenti al numero di elementi in ogni set (quindi, vi sono 3668 coppie di frasi nel training set, 408 nel validation set, e 1725 nel test set)."),ee.forEach(s),Pt=f(e),ue=p(e,"P",{});var as=c(ue);ci=n(as,"Questo comando scarica il dataset e lo mette in cache, in "),_s=p(as,"EM",{});var Pr=c(_s);di=n(Pr,"~/.cache/huggingface/dataset"),Pr.forEach(s),ui=n(as," secondo l\u2019impostazione predefinita. Nel Capitolo 2 \xE8 stato spiegato come personalizzare la cartella di cache impostando la variabile d\u2019ambiente "),bs=p(as,"CODE",{});var Tr=c(bs);mi=n(Tr,"HF_HOME"),Tr.forEach(s),fi=n(as,"."),as.forEach(s),Tt=f(e),De=p(e,"P",{});var Tn=c(De);hi=n(Tn,"Ogni coppia di frasi nell\u2019oggetto "),gs=p(Tn,"CODE",{});var Sr=c(gs);_i=n(Sr,"raw_datasets"),Sr.forEach(s),bi=n(Tn," pu\xF2 essere ottenuta tramite il suo indice, come in un dizionario:"),Tn.forEach(s),St=f(e),j(ea.$$.fragment,e),Ot=f(e),j(aa.$$.fragment,e),At=f(e),me=p(e,"P",{});var ss=c(me);gi=n(ss,"Le label sono gi\xE0 numeri interi, quindi non \xE8 necessario alcun preprocessing. Per sapere a quale numero corrisponde quale tipo di label, si possono analizzare le "),vs=p(ss,"CODE",{});var Or=c(vs);vi=n(Or,"features"),Or.forEach(s),zi=n(ss," del "),zs=p(ss,"CODE",{});var Ar=c(zs);ki=n(Ar,"raw_train_dataset"),Ar.forEach(s),$i=n(ss,". Ci\xF2 permette di capire la tipologia di ogni colonna:"),ss.forEach(s),It=f(e),j(sa.$$.fragment,e),Lt=f(e),j(ta.$$.fragment,e),Nt=f(e),L=p(e,"P",{});var W=c(L);ji=n(W,"Dietro le quinte, "),ks=p(W,"CODE",{});var Ir=c(ks);Ei=n(Ir,"label"),Ir.forEach(s),xi=n(W," \xE8 del tipo  "),$s=p(W,"CODE",{});var Lr=c($s);qi=n(Lr,"ClassLabel"),Lr.forEach(s),wi=n(W,", e la corrispondenza tra i numeri e i nomi delle label \xE8 contenuta nella cartella "),js=p(W,"EM",{});var Nr=c(js);Ci=n(Nr,"names"),Nr.forEach(s),yi=n(W,". "),Es=p(W,"CODE",{});var Fr=c(Es);Di=n(Fr,"0"),Fr.forEach(s),Pi=n(W," corrisponde a "),xs=p(W,"CODE",{});var Mr=c(xs);Ti=n(Mr,"not_equivalent"),Mr.forEach(s),Si=n(W," (significato diverso), e "),qs=p(W,"CODE",{});var Rr=c(qs);Oi=n(Rr,"1"),Rr.forEach(s),Ai=n(W," corrisponde a "),ws=p(W,"CODE",{});var Qr=c(ws);Ii=n(Qr,"equivalent"),Qr.forEach(s),Li=n(W," (stesso significato)."),W.forEach(s),Ft=f(e),j(Pe.$$.fragment,e),Mt=f(e),xe=p(e,"H3",{class:!0});var Sn=c(xe);Te=p(Sn,"A",{id:!0,class:!0,href:!0});var Br=c(Te);Cs=p(Br,"SPAN",{});var Hr=c(Cs);j(na.$$.fragment,Hr),Hr.forEach(s),Br.forEach(s),Ni=f(Sn),ys=p(Sn,"SPAN",{});var Wr=c(ys);Fi=n(Wr,"Preprocessing del dataset"),Wr.forEach(s),Sn.forEach(s),Rt=f(e),ie.l(e),La=f(e),Se=p(e,"P",{});var On=c(Se);Mi=n(On,"Per preprocessare il dataset, \xE8 necessario convertire il testo in numeri comprensibili al modello. Come dimostrato nel "),Na=p(On,"A",{href:!0});var Ur=c(Na);Ri=n(Ur,"capitolo precedente"),Ur.forEach(s),Qi=n(On,", ci\xF2 viene fatto con un tokenizer (tokenizzatore). Il tokenizer prende come input sia una frase sia una lista di frasi, quindi \xE8 possibile effettuare la tokenizzazione di tutte le prime e seconde frasi di ogni coppia in questo modo:"),On.forEach(s),Qt=f(e),j(ia.$$.fragment,e),Bt=f(e),Fa=p(e,"P",{});var Gr=c(Fa);Bi=n(Gr,"Tuttavia, non si possono semplicemente passare al modello due frasi e sperare di predire se l\u2019una \xE8 una parafrasi dell\u2019altra o no. Bisogna gestire le due frasi come una coppia, e applicare il preprocessing necessario. Fortunatamente, il tokenizer pu\xF2 anche prendere come input una coppia di frasi e prepararla nel formato atteso dal modello BERT:"),Gr.forEach(s),Ht=f(e),j(oa.$$.fragment,e),Wt=f(e),j(la.$$.fragment,e),Ut=f(e),G=p(e,"P",{});var ze=c(G);Hi=n(ze,"Sono state gi\xE0 discusse nel "),Ma=p(ze,"A",{href:!0});var Vr=c(Ma);Wi=n(Vr,"Capitolo 2"),Vr.forEach(s),Ui=n(ze," le chiavi "),Ds=p(ze,"CODE",{});var Jr=c(Ds);Gi=n(Jr,"input_ids"),Jr.forEach(s),Vi=n(ze," e "),Ps=p(ze,"CODE",{});var Kr=c(Ps);Ji=n(Kr,"attention_mask"),Kr.forEach(s),Ki=n(ze,", ma il discorso su "),Ts=p(ze,"CODE",{});var Yr=c(Ts);Yi=n(Yr,"token_type_ids"),Yr.forEach(s),Zi=n(ze," era stato rimandato. In questo esempio, ci\xF2 pu\xF2 essere usato per indicare al modello quale parte dell\u2019input \xE8 la prima frase, e quale la seconda."),ze.forEach(s),Gt=f(e),j(Oe.$$.fragment,e),Vt=f(e),Ae=p(e,"P",{});var An=c(Ae);Xi=n(An,"Decodificando gli ID in "),Ss=p(An,"CODE",{});var Zr=c(Ss);eo=n(Zr,"input_ids"),Zr.forEach(s),ao=n(An," per ritrasformarli in parole:"),An.forEach(s),Jt=f(e),j(ra.$$.fragment,e),Kt=f(e),Ra=p(e,"P",{});var Xr=c(Ra);so=n(Xr,"si ottiene:"),Xr.forEach(s),Yt=f(e),j(pa.$$.fragment,e),Zt=f(e),fe=p(e,"P",{});var ts=c(fe);to=n(ts,"Perci\xF2 \xE8 chiaro che il modello si aspetta gli input nella forma "),Os=p(ts,"CODE",{});var ep=c(Os);no=n(ep,"[CLS] frase1 [SEP] frase2 [SEP]"),ep.forEach(s),io=n(ts," quando vi sono due frasi. Allineando con "),As=p(ts,"CODE",{});var ap=c(As);oo=n(ap,"token_type_ids"),ap.forEach(s),lo=n(ts," si ottiene:"),ts.forEach(s),Xt=f(e),j(ca.$$.fragment,e),en=f(e),V=p(e,"P",{});var ke=c(V);ro=n(ke,"Le parti dell\u2019input corrispondenti a "),Is=p(ke,"CODE",{});var sp=c(Is);po=n(sp,"[CLS] frase1 [SEP]"),sp.forEach(s),co=n(ke," hanno tutte un token type ID di "),Ls=p(ke,"CODE",{});var tp=c(Ls);uo=n(tp,"0"),tp.forEach(s),mo=n(ke,", mentre le altre parti, corrispondenti quindi a "),Ns=p(ke,"CODE",{});var np=c(Ns);fo=n(np,"frase2 [SEP]"),np.forEach(s),ho=n(ke,", hanno tutte un token type ID di "),Fs=p(ke,"CODE",{});var ip=c(Fs);_o=n(ip,"1"),ip.forEach(s),bo=n(ke,"."),ke.forEach(s),an=f(e),he=p(e,"P",{});var ns=c(he);go=n(ns,"Da notare che se viene selezionato un altro checkpoint, gli input tokenizzati non conterranno necessariamente i "),Ms=p(ns,"CODE",{});var op=c(Ms);vo=n(op,"token_type_ids"),op.forEach(s),zo=n(ns," (ad esempio, non si ottengono usando un modello DistilBERT). I "),Rs=p(ns,"CODE",{});var lp=c(Rs);ko=n(lp,"token_type_ids"),lp.forEach(s),$o=n(ns," si ottengono solo quando il modello saprebbe che farne, avendole gi\xE0 viste in fase di pre-addestramento."),ns.forEach(s),sn=f(e),J=p(e,"P",{});var $e=c(J);jo=n($e,"In questo caso, BERT \xE8 stato pre-addestrato con i token type IDs, e in aggiunta all\u2019obiettivo di "),Qs=p($e,"EM",{});var rp=c(Qs);Eo=n(rp,"masked language modeling"),rp.forEach(s),xo=n($e," di cui si era parlato nel "),Qa=p($e,"A",{href:!0});var pp=c(Qa);qo=n(pp,"Capitolo 1"),pp.forEach(s),wo=n($e,", vi \xE8 un altro obiettivo che si chiama "),Bs=p($e,"EM",{});var cp=c(Bs);Co=n(cp,"next sentence prediction"),cp.forEach(s),yo=n($e," ("),Hs=p($e,"EM",{});var dp=c(Hs);Do=n(dp,"predire la prossima frase"),dp.forEach(s),Po=n($e,"). Lo scopo di questo task \xE8 modellizzare la relazione tra coppie di frasi."),$e.forEach(s),tn=f(e),Ba=p(e,"P",{});var up=c(Ba);To=n(up,"Durante un task di next sentence prediction, il modello riceve una coppia di frasi (con token mascherati in maniera aleatoria) e deve predire se la seconda segue la prima. Per rendere il task meno banale, la met\xE0 delle volte le frasi si susseguono nel documento da cui erano state estratte originariamente, l\u2019altra met\xE0 delle volte le frasi provengono da due documenti diversi."),up.forEach(s),nn=f(e),Ie=p(e,"P",{});var In=c(Ie);So=n(In,"In generale, non bisogna preoccuparsi se i "),Ws=p(In,"CODE",{});var mp=c(Ws);Oo=n(mp,"token_type_ids"),mp.forEach(s),Ao=n(In," sono presenti o no negli input tokenizzati: finch\xE9 viene usato lo stesso checkpoint per il tokenizer e il modello, tutto andr\xE0 bene poich\xE9 il tokenizer sa cosa fornire al modello."),In.forEach(s),on=f(e),_e=p(e,"P",{});var is=c(_e);Io=n(is,"Ora che abbiamo visto come il tokenizer pu\xF2 gestire una coppia di frasi, possiamo usarlo per tokenizzare l\u2019intero dataset: come nel "),Ha=p(is,"A",{href:!0});var fp=c(Ha);Lo=n(fp,"capitolo precedente"),fp.forEach(s),No=n(is,", si pu\xF2 fornire al tokenizer una lista di coppie di frasi dando prima la lista delle prime frasi, e poi la lista delle seconde frasi. Questo approcchio \xE8 anche compatibile le opzioni di padding e truncation gi\xE0 viste nel "),Wa=p(is,"A",{href:!0});var hp=c(Wa);Fo=n(hp,"Capitolo 2"),hp.forEach(s),Mo=n(is,". Perci\xF2, un modo per preprocessare il dataset di addestramento \xE8:"),is.forEach(s),ln=f(e),j(da.$$.fragment,e),rn=f(e),K=p(e,"P",{});var je=c(K);Ro=n(je,"Questo metodo funziona, ma ha lo svantaggio di restituire un dizionario (avente "),Us=p(je,"CODE",{});var _p=c(Us);Qo=n(_p,"input_ids"),_p.forEach(s),Bo=n(je,", "),Gs=p(je,"CODE",{});var bp=c(Gs);Ho=n(bp,"attention_mask"),bp.forEach(s),Wo=n(je,", e "),Vs=p(je,"CODE",{});var gp=c(Vs);Uo=n(gp,"token_type_ids"),gp.forEach(s),Go=n(je," come chiavi, e delle liste di liste come valori). Oltretutto, questo metodo funziona solo se si ha a disposizione RAM sufficiente per contenere l\u2019intero dataset durante la tokenizzazione (mentre i dataset dalla libreria \u{1F917} Datasets sono file "),ua=p(je,"A",{href:!0,rel:!0});var vp=c(ua);Vo=n(vp,"Apache Arrow"),vp.forEach(s),Jo=n(je," archiviati su disco, perci\xF2 in memoria vengono caricati solo i campioni richiesti)."),je.forEach(s),pn=f(e),be=p(e,"P",{});var os=c(be);Ko=n(os,"Per tenere i dati come dataset, utilizzare il metodo "),ma=p(os,"A",{href:!0,rel:!0});var zp=c(ma);Js=p(zp,"CODE",{});var kp=c(Js);Yo=n(kp,"Dataset.map()"),kp.forEach(s),zp.forEach(s),Zo=n(os,". Ci\xF2 permette anche della flessibilit\xE0 extra, qualora fosse necessario del preprocessing aggiuntivo oltre alla tokenizzazione. Il metodo "),Ks=p(os,"CODE",{});var $p=c(Ks);Xo=n($p,"map()"),$p.forEach(s),el=n(os," applica una funziona ad ogni elemento del dataset, perci\xF2 bisogna definire una funzione che tokenizzi gli input:"),os.forEach(s),cn=f(e),j(fa.$$.fragment,e),dn=f(e),A=p(e,"P",{});var R=c(A);al=n(R,"Questa funzione riceve un dizionario (come gli elementi del nostro dataset) e restituisce un nuovo dizionario con input_ids"),Ys=p(R,"CODE",{});var jp=c(Ys);sl=n(jp,", "),jp.forEach(s),tl=n(R,"attention_mask"),Zs=p(R,"CODE",{});var Ep=c(Zs);nl=n(Ep,", e "),Ep.forEach(s),il=n(R,"token_type_ids"),Xs=p(R,"CODE",{});var xp=c(Xs);ol=n(xp,"come chiavi. Funziona anche se il dizionario"),xp.forEach(s),ll=n(R,"example"),et=p(R,"CODE",{});var qp=c(et);rl=n(qp,"contiene svariati campioni (ad una chiave corrisponde una lista di frasi) poich\xE9 il"),qp.forEach(s),pl=n(R,"tokenizer"),at=p(R,"CODE",{});var wp=c(at);cl=n(wp,"funziona con liste di coppie di frasi, come gi\xE0 visto. Ci\xF2 permette di usare l'opzione"),wp.forEach(s),dl=n(R,"batched=True"),st=p(R,"CODE",{});var Cp=c(st);ul=n(Cp,"nella chiamata a"),Cp.forEach(s),ml=n(R,"map()"),tt=p(R,"CODE",{});var yp=c(tt);fl=n(yp,", che accelerer\xE0 di molto la tokenizzazione. Il "),yp.forEach(s),hl=n(R,"tokenizer` si appoggia ad un tokenizer scritto in Rust della libreria "),ha=p(R,"A",{href:!0,rel:!0});var Dp=c(ha);_l=n(Dp,"\u{1F917} Tokenizers"),Dp.forEach(s),bl=n(R,". Questo tokenizer pu\xF2 essere molto veloce, ma solo se gli vengono forniti molti input insieme."),R.forEach(s),un=f(e),Le=p(e,"P",{});var Ln=c(Le);gl=n(Ln,"Per ora non ci siamo preoccupati del parametro "),nt=p(Ln,"CODE",{});var Pp=c(nt);vl=n(Pp,"padding"),Pp.forEach(s),zl=n(Ln," nella nostra funzione di tokenizzazione. Questo perch\xE9 il padding di tutti i campioni fino a lunghezza massima non \xE8 efficiente: \xE8 meglio fare il padding dei campioni quando stiamo assemblando una batch, poich\xE9 in quel momento \xE8 necessario il padding solo fino alla lunghezza massima nel batch, non la lunghezza massima nell\u2019intero dataset. Ci\xF2 permette di risparmiare molto tempo e potenza di calcolo nel caso in cui gli input abbiano lunghezze molto varie!"),Ln.forEach(s),mn=f(e),ge=p(e,"P",{});var ls=c(ge);kl=n(ls,"Ecco come si applica la funzione di tokenizzazione sull\u2019intero dataset. Bisogna usare "),it=p(ls,"CODE",{});var Tp=c(it);$l=n(Tp,"batched=True"),Tp.forEach(s),jl=n(ls," nella chiamata a "),ot=p(ls,"CODE",{});var Sp=c(ot);El=n(Sp,"map"),Sp.forEach(s),xl=n(ls," in modo tale che la funzione venga applicata a vari elementi del dataset insieme, e non ad ogni elemento separatamente. Ci\xF2 permette un preprocessing pi\xF9 rapido."),ls.forEach(s),fn=f(e),j(_a.$$.fragment,e),hn=f(e),Ua=p(e,"P",{});var Op=c(Ua);ql=n(Op,"La libreria \u{1F917} Datasets aggiunge nuovi campi ai dataset, uno per ogni chiave nel dizionario restituito dalla funzione di preprocessing:"),Op.forEach(s),_n=f(e),j(ba.$$.fragment,e),bn=f(e),ve=p(e,"P",{});var rs=c(ve);wl=n(rs,"Si pu\xF2 anche applicare il multiprocessing durante il preprocessing con la funzione "),lt=p(rs,"CODE",{});var Ap=c(lt);Cl=n(Ap,"map()"),Ap.forEach(s),yl=n(rs," utilizzando il parametro "),rt=p(rs,"CODE",{});var Ip=c(rt);Dl=n(Ip,"num_proc"),Ip.forEach(s),Pl=n(rs,". Ci\xF2 non \xE8 stato dimostrato qui perch\xE9 la libreria \u{1F917} Tokenizers gi\xE0 utilizza vari thread per tokenizzare i campioni pi\xF9 rapidamente, ma nel caso in cui non venga usato un tokenizer rapido di questa libreria, ci\xF2 potrebbe velocizzare il preprocessing."),rs.forEach(s),gn=f(e),H=p(e,"P",{});var ae=c(H);Tl=n(ae,"La funzione "),pt=p(ae,"CODE",{});var Lp=c(pt);Sl=n(Lp,"tokenize_function"),Lp.forEach(s),Ol=n(ae," restituisce un dizionario con "),ct=p(ae,"CODE",{});var Np=c(ct);Al=n(Np,"input_ids"),Np.forEach(s),Il=n(ae,", "),dt=p(ae,"CODE",{});var Fp=c(dt);Ll=n(Fp,"attention_mask"),Fp.forEach(s),Nl=n(ae,", e "),ut=p(ae,"CODE",{});var Mp=c(ut);Fl=n(Mp,"token_type_ids"),Mp.forEach(s),Ml=n(ae," come chiavi, quindi quei tre campi vengono aggiunti a tutti gli split (le parti) del dataset. Si possono anche cambiare i campi esistenti nel caso in cui la funzione di preprocessing restituisca un nuovo valore per una chiave gi\xE0 esistente nel dataset a cui viene applicato "),mt=p(ae,"CODE",{});var Rp=c(mt);Rl=n(Rp,"map()"),Rp.forEach(s),Ql=n(ae,"."),ae.forEach(s),vn=f(e),Ne=p(e,"P",{});var Nn=c(Ne);Bl=n(Nn,"L\u2019ultima cosa da fare \xE8 il padding di tutti i campioni alla lunghezza dell\u2019elemento pi\xF9 lungo quando sono inseriti in una batch \u2014 una tecnica che si chiama "),ft=p(Nn,"EM",{});var Qp=c(ft);Hl=n(Qp,"dynamic padding"),Qp.forEach(s),Wl=n(Nn,"."),Nn.forEach(s),zn=f(e),qe=p(e,"H3",{class:!0});var Fn=c(qe);Fe=p(Fn,"A",{id:!0,class:!0,href:!0});var Bp=c(Fe);ht=p(Bp,"SPAN",{});var Hp=c(ht);j(ga.$$.fragment,Hp),Hp.forEach(s),Bp.forEach(s),Ul=f(Fn),_t=p(Fn,"SPAN",{});var Wp=c(_t);Gl=n(Wp,"Dynamic padding"),Wp.forEach(s),Fn.forEach(s),kn=f(e),j(va.$$.fragment,e),$n=f(e),we.l(e),Ga=f(e),Me=p(e,"P",{});var Mn=c(Me);Vl=n(Mn,"In pratica, bisogna definire una collate function che applichi la giusta quantit\xE0 di padding agli elementi del dataset in una stessa batch. Fortunatamente, la libreria \u{1F917} Transformers fornisce questa funziona tramite "),bt=p(Mn,"CODE",{});var Up=c(bt);Jl=n(Up,"DataCollatorWithPadding"),Up.forEach(s),Kl=n(Mn,". Essa prende in input un tokenizer quando viene istanziata (per individuare quale token da usare per il padding, e se il modello si aspetta padding a sinistra o a destra dell\u2019input) e far\xE0 tutto il necessario:"),Mn.forEach(s),jn=f(e),le.l(e),Va=f(e),X=p(e,"P",{});var Ue=c(X);Yl=n(Ue,"Per testare questo nuovo gioco, analizziamo alcuni campioni dal set di addestramento da raggruppare in un batch. Adesso togliamo le colonne  "),gt=p(Ue,"CODE",{});var Gp=c(gt);Zl=n(Gp,"idx"),Gp.forEach(s),Xl=n(Ue,", "),vt=p(Ue,"CODE",{});var Vp=c(vt);er=n(Vp,"sentence1"),Vp.forEach(s),ar=n(Ue,", e "),zt=p(Ue,"CODE",{});var Jp=c(zt);sr=n(Jp,"sentence2"),Jp.forEach(s),tr=n(Ue," poich\xE9 non saranno necessarie e contengono stringhe (e non si possono creare tensori con stringhe), e controlliamo le lunghezze di ogni elemento nel batch:"),Ue.forEach(s),En=f(e),j(za.$$.fragment,e),xn=f(e),j(ka.$$.fragment,e),qn=f(e),Re=p(e,"P",{});var Rn=c(Re);nr=n(Rn,"Nulla di sorprendente, i campioni hanno lunghezza variabile da 32 a 67. Il padding dinamico significa che i campioni in questo batch dovrebbero tutti ricevere un padding fino alla lunghezza di 67, il massimo nel batch. Senza padding dinamico, tutti i campioni dovrebbero ricevere un padding fino alla lunghezza massima nell\u2019intero dataset, o la lunghezza massima processabile dal modello. Bisogna controllare che il "),kt=p(Rn,"CODE",{});var Kp=c(kt);ir=n(Kp,"data_collator"),Kp.forEach(s),or=n(Rn," stia applicando un padding dinamico al batch in maniera corretta:"),Rn.forEach(s),wn=f(e),j($a.$$.fragment,e),Cn=f(e),pe.l(e),Ja=f(e),j(Qe.$$.fragment,e),yn=f(e),F&&F.l(e),Ka=Yp(),this.h()},h(){C(i,"name","hf:doc:metadata"),C(i,"content",JSON.stringify(Ec)),C(h,"id","processare-i-dati"),C(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(h,"href","#processare-i-dati"),C(k,"class","relative group"),C(de,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),C(de,"rel","nofollow"),C(ye,"id","caricare-un-dataset-dallhub"),C(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(ye,"href","#caricare-un-dataset-dallhub"),C(Ee,"class","relative group"),C(Je,"href","https://huggingface.co/datasets"),C(Je,"rel","nofollow"),C(Ke,"href","https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub"),C(Ke,"rel","nofollow"),C(Ye,"href","https://gluebenchmark.com/"),C(Ye,"rel","nofollow"),C(Te,"id","preprocessing-del-dataset"),C(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(Te,"href","#preprocessing-del-dataset"),C(xe,"class","relative group"),C(Na,"href","/course/chapter2"),C(Ma,"href","/course/chapter2"),C(Qa,"href","/course/chapter1"),C(Ha,"href","/course/chapter2"),C(Wa,"href","/course/chapter2"),C(ua,"href","https://arrow.apache.org/"),C(ua,"rel","nofollow"),C(ma,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),C(ma,"rel","nofollow"),C(ha,"href","https://github.com/huggingface/tokenizers"),C(ha,"rel","nofollow"),C(Fe,"id","dynamic-padding"),C(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(Fe,"href","#dynamic-padding"),C(qe,"class","relative group")},m(e,l){a(document.head,i),d(e,u,l),E(o,e,l),d(e,b,l),d(e,k,l),a(k,h),a(h,_),E(y,_,null),a(k,z),a(k,q),a(q,I),d(e,N,l),ja[D].m(e,l),d(e,U,l),Ea[O].m(e,l),d(e,Y,l),d(e,ce,l),a(ce,Ce),d(e,P,l),d(e,S,l),a(S,Ta),a(S,de),a(de,Sa),a(S,Oa),d(e,Ge,l),d(e,Ee,l),a(Ee,ye),a(ye,ps),E(Ve,ps,null),a(Ee,Bn),a(Ee,cs),a(cs,Hn),d(e,qt,l),xa[se].m(e,l),d(e,Aa,l),d(e,Z,l),a(Z,Wn),a(Z,Je),a(Je,Un),a(Z,Gn),a(Z,Ke),a(Ke,Vn),a(Z,Jn),a(Z,Ye),a(Ye,Kn),a(Z,Yn),d(e,wt,l),d(e,Ia,l),a(Ia,Zn),d(e,Ct,l),E(Ze,e,l),d(e,yt,l),E(Xe,e,l),d(e,Dt,l),d(e,B,l),a(B,Xn),a(B,ds),a(ds,ei),a(B,ai),a(B,us),a(us,si),a(B,ti),a(B,ms),a(ms,ni),a(B,ii),a(B,fs),a(fs,oi),a(B,li),a(B,hs),a(hs,ri),a(B,pi),d(e,Pt,l),d(e,ue,l),a(ue,ci),a(ue,_s),a(_s,di),a(ue,ui),a(ue,bs),a(bs,mi),a(ue,fi),d(e,Tt,l),d(e,De,l),a(De,hi),a(De,gs),a(gs,_i),a(De,bi),d(e,St,l),E(ea,e,l),d(e,Ot,l),E(aa,e,l),d(e,At,l),d(e,me,l),a(me,gi),a(me,vs),a(vs,vi),a(me,zi),a(me,zs),a(zs,ki),a(me,$i),d(e,It,l),E(sa,e,l),d(e,Lt,l),E(ta,e,l),d(e,Nt,l),d(e,L,l),a(L,ji),a(L,ks),a(ks,Ei),a(L,xi),a(L,$s),a($s,qi),a(L,wi),a(L,js),a(js,Ci),a(L,yi),a(L,Es),a(Es,Di),a(L,Pi),a(L,xs),a(xs,Ti),a(L,Si),a(L,qs),a(qs,Oi),a(L,Ai),a(L,ws),a(ws,Ii),a(L,Li),d(e,Ft,l),E(Pe,e,l),d(e,Mt,l),d(e,xe,l),a(xe,Te),a(Te,Cs),E(na,Cs,null),a(xe,Ni),a(xe,ys),a(ys,Fi),d(e,Rt,l),qa[ne].m(e,l),d(e,La,l),d(e,Se,l),a(Se,Mi),a(Se,Na),a(Na,Ri),a(Se,Qi),d(e,Qt,l),E(ia,e,l),d(e,Bt,l),d(e,Fa,l),a(Fa,Bi),d(e,Ht,l),E(oa,e,l),d(e,Wt,l),E(la,e,l),d(e,Ut,l),d(e,G,l),a(G,Hi),a(G,Ma),a(Ma,Wi),a(G,Ui),a(G,Ds),a(Ds,Gi),a(G,Vi),a(G,Ps),a(Ps,Ji),a(G,Ki),a(G,Ts),a(Ts,Yi),a(G,Zi),d(e,Gt,l),E(Oe,e,l),d(e,Vt,l),d(e,Ae,l),a(Ae,Xi),a(Ae,Ss),a(Ss,eo),a(Ae,ao),d(e,Jt,l),E(ra,e,l),d(e,Kt,l),d(e,Ra,l),a(Ra,so),d(e,Yt,l),E(pa,e,l),d(e,Zt,l),d(e,fe,l),a(fe,to),a(fe,Os),a(Os,no),a(fe,io),a(fe,As),a(As,oo),a(fe,lo),d(e,Xt,l),E(ca,e,l),d(e,en,l),d(e,V,l),a(V,ro),a(V,Is),a(Is,po),a(V,co),a(V,Ls),a(Ls,uo),a(V,mo),a(V,Ns),a(Ns,fo),a(V,ho),a(V,Fs),a(Fs,_o),a(V,bo),d(e,an,l),d(e,he,l),a(he,go),a(he,Ms),a(Ms,vo),a(he,zo),a(he,Rs),a(Rs,ko),a(he,$o),d(e,sn,l),d(e,J,l),a(J,jo),a(J,Qs),a(Qs,Eo),a(J,xo),a(J,Qa),a(Qa,qo),a(J,wo),a(J,Bs),a(Bs,Co),a(J,yo),a(J,Hs),a(Hs,Do),a(J,Po),d(e,tn,l),d(e,Ba,l),a(Ba,To),d(e,nn,l),d(e,Ie,l),a(Ie,So),a(Ie,Ws),a(Ws,Oo),a(Ie,Ao),d(e,on,l),d(e,_e,l),a(_e,Io),a(_e,Ha),a(Ha,Lo),a(_e,No),a(_e,Wa),a(Wa,Fo),a(_e,Mo),d(e,ln,l),E(da,e,l),d(e,rn,l),d(e,K,l),a(K,Ro),a(K,Us),a(Us,Qo),a(K,Bo),a(K,Gs),a(Gs,Ho),a(K,Wo),a(K,Vs),a(Vs,Uo),a(K,Go),a(K,ua),a(ua,Vo),a(K,Jo),d(e,pn,l),d(e,be,l),a(be,Ko),a(be,ma),a(ma,Js),a(Js,Yo),a(be,Zo),a(be,Ks),a(Ks,Xo),a(be,el),d(e,cn,l),E(fa,e,l),d(e,dn,l),d(e,A,l),a(A,al),a(A,Ys),a(Ys,sl),a(A,tl),a(A,Zs),a(Zs,nl),a(A,il),a(A,Xs),a(Xs,ol),a(A,ll),a(A,et),a(et,rl),a(A,pl),a(A,at),a(at,cl),a(A,dl),a(A,st),a(st,ul),a(A,ml),a(A,tt),a(tt,fl),a(A,hl),a(A,ha),a(ha,_l),a(A,bl),d(e,un,l),d(e,Le,l),a(Le,gl),a(Le,nt),a(nt,vl),a(Le,zl),d(e,mn,l),d(e,ge,l),a(ge,kl),a(ge,it),a(it,$l),a(ge,jl),a(ge,ot),a(ot,El),a(ge,xl),d(e,fn,l),E(_a,e,l),d(e,hn,l),d(e,Ua,l),a(Ua,ql),d(e,_n,l),E(ba,e,l),d(e,bn,l),d(e,ve,l),a(ve,wl),a(ve,lt),a(lt,Cl),a(ve,yl),a(ve,rt),a(rt,Dl),a(ve,Pl),d(e,gn,l),d(e,H,l),a(H,Tl),a(H,pt),a(pt,Sl),a(H,Ol),a(H,ct),a(ct,Al),a(H,Il),a(H,dt),a(dt,Ll),a(H,Nl),a(H,ut),a(ut,Fl),a(H,Ml),a(H,mt),a(mt,Rl),a(H,Ql),d(e,vn,l),d(e,Ne,l),a(Ne,Bl),a(Ne,ft),a(ft,Hl),a(Ne,Wl),d(e,zn,l),d(e,qe,l),a(qe,Fe),a(Fe,ht),E(ga,ht,null),a(qe,Ul),a(qe,_t),a(_t,Gl),d(e,kn,l),E(va,e,l),d(e,$n,l),we.m(e,l),d(e,Ga,l),d(e,Me,l),a(Me,Vl),a(Me,bt),a(bt,Jl),a(Me,Kl),d(e,jn,l),wa[oe].m(e,l),d(e,Va,l),d(e,X,l),a(X,Yl),a(X,gt),a(gt,Zl),a(X,Xl),a(X,vt),a(vt,er),a(X,ar),a(X,zt),a(zt,sr),a(X,tr),d(e,En,l),E(za,e,l),d(e,xn,l),E(ka,e,l),d(e,qn,l),d(e,Re,l),a(Re,nr),a(Re,kt),a(kt,ir),a(Re,or),d(e,wn,l),E($a,e,l),d(e,Cn,l),Ca[re].m(e,l),d(e,Ja,l),E(Qe,e,l),d(e,yn,l),F&&F.m(e,l),d(e,Ka,l),Dn=!0},p(e,[l]){const ya={};l&1&&(ya.fw=e[0]),o.$set(ya);let Ya=D;D=pr(e),D!==Ya&&(Pa(),v(ja[Ya],1,1,()=>{ja[Ya]=null}),Da(),M=ja[D],M||(M=ja[D]=rr[D](e),M.c()),g(M,1),M.m(U.parentNode,U));let Za=O;O=dr(e),O!==Za&&(Pa(),v(Ea[Za],1,1,()=>{Ea[Za]=null}),Da(),Q=Ea[O],Q||(Q=Ea[O]=cr[O](e),Q.c()),g(Q,1),Q.m(Y.parentNode,Y));let Xa=se;se=mr(e),se!==Xa&&(Pa(),v(xa[Xa],1,1,()=>{xa[Xa]=null}),Da(),te=xa[se],te||(te=xa[se]=ur[se](e),te.c()),g(te,1),te.m(Aa.parentNode,Aa));const $t={};l&2&&($t.$$scope={dirty:l,ctx:e}),Pe.$set($t);let Be=ne;ne=hr(e),ne!==Be&&(Pa(),v(qa[Be],1,1,()=>{qa[Be]=null}),Da(),ie=qa[ne],ie||(ie=qa[ne]=fr[ne](e),ie.c()),g(ie,1),ie.m(La.parentNode,La));const jt={};l&2&&(jt.$$scope={dirty:l,ctx:e}),Oe.$set(jt),Pn!==(Pn=_r(e))&&(we.d(1),we=Pn(e),we&&(we.c(),we.m(Ga.parentNode,Ga)));let He=oe;oe=gr(e),oe!==He&&(Pa(),v(wa[He],1,1,()=>{wa[He]=null}),Da(),le=wa[oe],le||(le=wa[oe]=br[oe](e),le.c()),g(le,1),le.m(Va.parentNode,Va));let es=re;re=zr(e),re!==es&&(Pa(),v(Ca[es],1,1,()=>{Ca[es]=null}),Da(),pe=Ca[re],pe||(pe=Ca[re]=vr[re](e),pe.c()),g(pe,1),pe.m(Ja.parentNode,Ja));const Et={};l&2&&(Et.$$scope={dirty:l,ctx:e}),Qe.$set(Et),e[0]==="tf"?F?l&1&&g(F,1):(F=Zp(),F.c(),g(F,1),F.m(Ka.parentNode,Ka)):F&&(Pa(),v(F,1,1,()=>{F=null}),Da())},i(e){Dn||(g(o.$$.fragment,e),g(y.$$.fragment,e),g(M),g(Q),g(Ve.$$.fragment,e),g(te),g(Ze.$$.fragment,e),g(Xe.$$.fragment,e),g(ea.$$.fragment,e),g(aa.$$.fragment,e),g(sa.$$.fragment,e),g(ta.$$.fragment,e),g(Pe.$$.fragment,e),g(na.$$.fragment,e),g(ie),g(ia.$$.fragment,e),g(oa.$$.fragment,e),g(la.$$.fragment,e),g(Oe.$$.fragment,e),g(ra.$$.fragment,e),g(pa.$$.fragment,e),g(ca.$$.fragment,e),g(da.$$.fragment,e),g(fa.$$.fragment,e),g(_a.$$.fragment,e),g(ba.$$.fragment,e),g(ga.$$.fragment,e),g(va.$$.fragment,e),g(le),g(za.$$.fragment,e),g(ka.$$.fragment,e),g($a.$$.fragment,e),g(pe),g(Qe.$$.fragment,e),g(F),Dn=!0)},o(e){v(o.$$.fragment,e),v(y.$$.fragment,e),v(M),v(Q),v(Ve.$$.fragment,e),v(te),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(ea.$$.fragment,e),v(aa.$$.fragment,e),v(sa.$$.fragment,e),v(ta.$$.fragment,e),v(Pe.$$.fragment,e),v(na.$$.fragment,e),v(ie),v(ia.$$.fragment,e),v(oa.$$.fragment,e),v(la.$$.fragment,e),v(Oe.$$.fragment,e),v(ra.$$.fragment,e),v(pa.$$.fragment,e),v(ca.$$.fragment,e),v(da.$$.fragment,e),v(fa.$$.fragment,e),v(_a.$$.fragment,e),v(ba.$$.fragment,e),v(ga.$$.fragment,e),v(va.$$.fragment,e),v(le),v(za.$$.fragment,e),v(ka.$$.fragment,e),v($a.$$.fragment,e),v(pe),v(Qe.$$.fragment,e),v(F),Dn=!1},d(e){s(i),e&&s(u),x(o,e),e&&s(b),e&&s(k),x(y),e&&s(N),ja[D].d(e),e&&s(U),Ea[O].d(e),e&&s(Y),e&&s(ce),e&&s(P),e&&s(S),e&&s(Ge),e&&s(Ee),x(Ve),e&&s(qt),xa[se].d(e),e&&s(Aa),e&&s(Z),e&&s(wt),e&&s(Ia),e&&s(Ct),x(Ze,e),e&&s(yt),x(Xe,e),e&&s(Dt),e&&s(B),e&&s(Pt),e&&s(ue),e&&s(Tt),e&&s(De),e&&s(St),x(ea,e),e&&s(Ot),x(aa,e),e&&s(At),e&&s(me),e&&s(It),x(sa,e),e&&s(Lt),x(ta,e),e&&s(Nt),e&&s(L),e&&s(Ft),x(Pe,e),e&&s(Mt),e&&s(xe),x(na),e&&s(Rt),qa[ne].d(e),e&&s(La),e&&s(Se),e&&s(Qt),x(ia,e),e&&s(Bt),e&&s(Fa),e&&s(Ht),x(oa,e),e&&s(Wt),x(la,e),e&&s(Ut),e&&s(G),e&&s(Gt),x(Oe,e),e&&s(Vt),e&&s(Ae),e&&s(Jt),x(ra,e),e&&s(Kt),e&&s(Ra),e&&s(Yt),x(pa,e),e&&s(Zt),e&&s(fe),e&&s(Xt),x(ca,e),e&&s(en),e&&s(V),e&&s(an),e&&s(he),e&&s(sn),e&&s(J),e&&s(tn),e&&s(Ba),e&&s(nn),e&&s(Ie),e&&s(on),e&&s(_e),e&&s(ln),x(da,e),e&&s(rn),e&&s(K),e&&s(pn),e&&s(be),e&&s(cn),x(fa,e),e&&s(dn),e&&s(A),e&&s(un),e&&s(Le),e&&s(mn),e&&s(ge),e&&s(fn),x(_a,e),e&&s(hn),e&&s(Ua),e&&s(_n),x(ba,e),e&&s(bn),e&&s(ve),e&&s(gn),e&&s(H),e&&s(vn),e&&s(Ne),e&&s(zn),e&&s(qe),x(ga),e&&s(kn),x(va,e),e&&s($n),we.d(e),e&&s(Ga),e&&s(Me),e&&s(jn),wa[oe].d(e),e&&s(Va),e&&s(X),e&&s(En),x(za,e),e&&s(xn),x(ka,e),e&&s(qn),e&&s(Re),e&&s(wn),x($a,e),e&&s(Cn),Ca[re].d(e),e&&s(Ja),x(Qe,e),e&&s(yn),F&&F.d(e),e&&s(Ka)}}}const Ec={local:"processare-i-dati",sections:[{local:"caricare-un-dataset-dallhub",title:"Caricare un dataset dall'Hub"},{local:"preprocessing-del-dataset",title:"Preprocessing del dataset"},{local:"dynamic-padding",title:"Dynamic padding"}],title:"Processare i dati"};function xc(w,i,u){let o="pt";return nc(()=>{const b=new URLSearchParams(window.location.search);u(0,o=b.get("fw")||"pt")}),[o]}class Sc extends ec{constructor(i){super();ac(this,i,xc,jc,sc,{})}}export{Sc as default,Ec as metadata};
