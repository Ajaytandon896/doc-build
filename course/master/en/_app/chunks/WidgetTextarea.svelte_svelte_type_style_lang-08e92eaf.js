import{S as h,i as g,s as p,P as l,Q as d,a as m,d as c,b as s,g as f,F as b,L as u}from"./vendor-e7c81d8a.js";function w(e){let t,r;return{c(){t=l("svg"),r=l("path"),this.h()},l(n){t=d(n,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var o=m(t);r=d(o,"path",{d:!0,fill:!0}),m(r).forEach(c),o.forEach(c),this.h()},h(){s(r,"d","M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"),s(r,"fill","currentColor"),s(t,"class",e[0]),s(t,"xmlns","http://www.w3.org/2000/svg"),s(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),s(t,"aria-hidden","true"),s(t,"role","img"),s(t,"width","1em"),s(t,"height","1em"),s(t,"preserveAspectRatio","xMidYMid meet"),s(t,"viewBox","0 0 256 256")},m(n,o){f(n,t,o),b(t,r)},p(n,[o]){o&1&&s(t,"class",n[0])},i:u,o:u,d(n){n&&c(t)}}}function y(e,t,r){let{classNames:n=""}=t;return e.$$set=o=>{"classNames"in o&&r(0,n=o.classNames)},[n]}class x extends h{constructor(t){super();g(this,t,y,w,p,{classNames:0})}}var a=(e=>(e["text-classification"]="Text Classification",e["token-classification"]="Token Classification",e["table-question-answering"]="Table Question Answering",e["question-answering"]="Question Answering",e["zero-shot-classification"]="Zero-Shot Classification",e.translation="Translation",e.summarization="Summarization",e.conversational="Conversational",e["feature-extraction"]="Feature Extraction",e["text-generation"]="Text Generation",e["text2text-generation"]="Text2Text Generation",e["fill-mask"]="Fill-Mask",e["sentence-similarity"]="Sentence Similarity",e["text-to-speech"]="Text-to-Speech",e["automatic-speech-recognition"]="Automatic Speech Recognition",e["audio-to-audio"]="Audio-to-Audio",e["audio-classification"]="Audio Classification",e["voice-activity-detection"]="Voice Activity Detection",e["image-classification"]="Image Classification",e["object-detection"]="Object Detection",e["image-segmentation"]="Image Segmentation",e["text-to-image"]="Text-to-Image",e["image-to-text"]="Image-to-Text",e["structured-data-classification"]="Structured Data Classification",e["reinforcement-learning"]="Reinforcement Learning",e))(a||{});const i={"text-classification":["adapter-transformers","spacy","transformers"],"token-classification":["adapter-transformers","flair","spacy","stanza","transformers"],"table-question-answering":["transformers"],"question-answering":["adapter-transformers","allennlp","transformers"],"zero-shot-classification":["transformers"],translation:["transformers"],summarization:["transformers"],conversational:["transformers"],"feature-extraction":["sentence-transformers","transformers"],"text-generation":["transformers"],"text2text-generation":["transformers"],"fill-mask":["transformers"],"sentence-similarity":["sentence-transformers","spacy"],"text-to-speech":["espnet","tensorflowtts"],"automatic-speech-recognition":["espnet","speechbrain","transformers"],"audio-to-audio":["asteroid","speechbrain"],"audio-classification":["speechbrain","transformers"],"voice-activity-detection":[],"image-classification":["keras","timm","transformers"],"object-detection":["transformers"],"image-segmentation":["transformers"],"text-to-image":[],"image-to-text":[],"structured-data-classification":["sklearn"],"reinforcement-learning":["stable-baselines3"]},v={datasets:[{description:"A benchmark of 10 different audio tasks.",id:"superb"}],demo:{inputs:[{filename:"audio.wav",type:"audio"}],outputs:[{data:[{label:"Up",score:.2},{label:"Down",score:.8}],type:"chart"}]},id:"audio-classification",label:a["audio-classification"],libraries:i["audio-classification"],metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"An easy-to-use model for Command Recognition.",id:"speechbrain/google_speech_command_xvector"},{description:"An Emotion Recognition model.",id:"superb/hubert-large-superb-er"}],summary:"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.",widgetModels:["speechbrain/google_speech_command_xvector"],youtubeId:"KWwzcmG98Ds"},A={datasets:[],demo:{inputs:[{filename:"input.wav",type:"audio"}],outputs:[{filename:"label-0.wav",type:"audio"},{filename:"label-1.wav",type:"audio"}]},id:"audio-to-audio",label:a["audio-to-audio"],libraries:i["audio-to-audio"],metrics:[{description:"The Signal-to-Noise ratio is the relationship between the target signal level and the background noise level. It is calculated as the logarithm of the target signal divided by the background noise, in decibels.",id:"snri"},{description:"The Signal-to-Distortion ratio is the relationship between the target signal and the sum of noise, interference, and artifact errors",id:"sdri"}],models:[{description:"A solid model of audio source separation.",id:"speechbrain/sepformer-wham"},{description:"A speech enhancement model.",id:"speechbrain/metricgan-plus-voicebank"}],summary:"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.",widgetModels:["speechbrain/sepformer-wham"],youtubeId:"iohj7nCCYoM"},I={datasets:[{description:"An English dataset with 1,000 hours of data.",id:"librispeech_asr"},{description:"Dataset in 60 languages including demographic information.",id:"common_voice"}],demo:{inputs:[{filename:"input.flac",type:"audio"}],outputs:[{label:"Transcript",content:"Going along slushy country roads and speaking to damp audiences in...",type:"text"}]},id:"automatic-speech-recognition",label:a["automatic-speech-recognition"],libraries:i["automatic-speech-recognition"],metrics:[{description:"",id:"wer"},{description:"",id:"cer"}],models:[{description:"A good generic ASR model.",id:"facebook/wav2vec2-base-960h"},{description:"An end-to-end model that performs Automatic Speech Recognition and Speech Translation.",id:"facebook/s2t-small-mustc-en-fr-st"}],summary:"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.",widgetModels:["facebook/wav2vec2-base-960h"],youtubeId:"TksaY_FDgnk"},S={datasets:[{description:"A common dataset that is used to train models for many languages.",id:"wikipedia"},{description:"A large English dataset with text crawled from the web.",id:"c4"}],demo:{inputs:[{label:"Input",content:"The <mask> barked at me",type:"text"}],outputs:[{type:"chart",data:[{label:"wolf",score:.487},{label:"dog",score:.061},{label:"cat",score:.058},{label:"fox",score:.047},{label:"squirrel",score:.025}]}]},id:"fill-mask",label:a["fill-mask"],libraries:i["fill-mask"],metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"cross_entropy"},{description:"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"perplexity"}],models:[{description:"A faster and smaller model than the famous BERT model.",id:"distilbert-base-uncased"},{description:"A multilingual model trained on 100 languages.",id:"xlm-roberta-base"}],summary:"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.",widgetModels:["distilroberta-base"],youtubeId:"mqElG5QJWUg"},E={datasets:[{description:"Benchmark dataset used for image classification with images that belong to 100 classes.",id:"cifar100"},{description:"Dataset consisting of images of garments.",id:"fashion-mnist"}],demo:{inputs:[{filename:"image-classification-input.jpeg",type:"img"}],outputs:[{type:"chart",data:[{label:"Egyptian cat",score:.514},{label:"Tabby cat",score:.193},{label:"Tiger cat",score:.068}]}]},id:"image-classification",label:a["image-classification"],libraries:i["image-classification"],metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"Strong Image Classification model trained on the ImageNet dataset.",id:"google/vit-base-patch16-224"},{description:"Strong Image Classification model trained on the ImageNet dataset.",id:"facebook/deit-base-distilled-patch16-224"}],summary:"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image.Image classification models take an image as input and return a prediction about which class the image belongs to.",widgetModels:["google/vit-base-patch16-224"],youtubeId:"tjAIM7BOYhw"},M={datasets:[{description:"Widely used benchmark dataset for multiple Vision tasks.",id:"merve/coco2017"}],demo:{inputs:[{filename:"image-segmentation-input.jpeg",type:"img"}],outputs:[{filename:"image-segmentation-output.png",type:"img"}]},id:"image-segmentation",label:a["image-segmentation"],libraries:i["image-segmentation"],metrics:[{description:"Average Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for each semantic class separately",id:"Average Precision"},{description:"Mean Average Precision (mAP) is the overall average of the AP values",id:"Mean Average Precision"},{description:"Intersection over Union (IoU) is the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes",id:"Mean Intersection over Union"},{description:"AP\u03B1 is the Average Precision at the IoU threshold of a \u03B1 value, for example, AP50 and AP75",id:"AP\u03B1"}],models:[{description:"Solid panoptic segmentation model trained on the COCO 2017 benchmark dataset.",id:"facebook/detr-resnet-50-panoptic"}],summary:"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.",widgetModels:["facebook/detr-resnet-50-panoptic"],youtubeId:"dKE8SIt9C-w"},T={datasets:[{description:"Widely used benchmark dataset for multiple Vision tasks.",id:"merve/coco2017"}],demo:{inputs:[{filename:"object-detection-input.jpg",type:"img"}],outputs:[{filename:"object-detection-output.jpg",type:"img"}]},id:"object-detection",label:a["object-detection"],libraries:i["object-detection"],metrics:[{description:"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",id:"Average Precision"},{description:"The Mean Average Precision (mAP) metric is the overall average of the AP values",id:"Mean Average Precision"},{description:"The AP\u03B1 metric is the Average Precision at the IoU threshold of a \u03B1 value, for example, AP50 and AP75",id:"AP\u03B1"}],models:[{description:"Solid object detection model trained on the benchmark dataset COCO 2017.",id:"facebook/detr-resnet-50"}],summary:"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.",widgetModels:["facebook/detr-resnet-50"],youtubeId:"WdAeKSOpxhw"},_={datasets:[{description:"A famous question answering dataset based on English articles from Wikipedia.",id:"squad_v2"},{description:"A dataset of aggregated anonymized actual queries issued to the Google search engine.",id:"natural_questions"}],demo:{inputs:[{label:"Question",content:"Which name is also used to describe the Amazon rainforest in English?",type:"text"},{label:"Context",content:"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle",type:"text"}],outputs:[{label:"Answer",content:"Amazonia",type:"text"}]},id:"question-answering",label:a["question-answering"],libraries:i["question-answering"],metrics:[{description:"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",id:"exact-match"},{description:" The F1-Score metric is useful if we value both false positives and false negatives equally. The F1-Score is calculated on each word in the predicted sequence against the correct answer",id:"f1"}],models:[{description:"A robust baseline model for most question answering domains.",id:"deepset/roberta-base-squad2"},{description:"A special model that can answer questions from tables!",id:"google/tapas-base-finetuned-wtq"}],summary:"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!",widgetModels:["deepset/roberta-base-squad2"],youtubeId:"ajPx5LwJD-I"},C={datasets:[{description:"Bing queries with relevant passages from various web sources.",id:"ms_marco"}],demo:{inputs:[{label:"Source sentence",content:"Machine learning is so easy.",type:"text"},{label:"Sentences to compare to",content:"Deep learning is so straightforward.",type:"text"},{label:"",content:"This is so difficult, like rocket science.",type:"text"},{label:"",content:"I can't believe how much I struggled with this.",type:"text"}],outputs:[{type:"chart",data:[{label:"Deep learning is so straightforward.",score:.623},{label:"This is so difficult, like rocket science.",score:.413},{label:"I can't believe how much I struggled with this.",score:.256}]}]},id:"sentence-similarity",label:a["sentence-similarity"],libraries:i["sentence-similarity"],metrics:[{description:"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",id:"Mean Reciprocal Rank"},{description:"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length",id:"Cosine Similarity"}],models:[{description:"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.",id:"sentence-transformers/all-mpnet-base-v2"},{description:"A multilingual model trained for FAQ retrieval.",id:"clips/mfaq"}],summary:"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.",widgetModels:["sentence-transformers/all-MiniLM-L6-v2"],youtubeId:"VCZq5AkbNEU"},R={datasets:[{description:"News articles in five different languages along with their summaries. Widely used for benchmarking multilingual summarization models.",id:"mlsum"},{description:"English conversations and their summaries. Useful for benchmarking conversational agents.",id:"samsum"}],demo:{inputs:[{label:"Input",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",type:"text"}],outputs:[{label:"Output",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It was the first structure to reach a height of 300 metres.",type:"text"}]},id:"summarization",label:a.summarization,libraries:i.summarization,metrics:[{description:"The generated sequence is compared against its summary, and the overlap of tokens are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.",id:"rouge"}],models:[{description:"A strong summarization model trained on English news articles. Excels at generating factual summaries.",id:"facebook/bart-large-cnn"},{description:"A summarization model trained on medical articles.",id:"google/bigbird-pegasus-large-pubmed"}],summary:"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.",widgetModels:["sshleifer/distilbart-cnn-12-6"],youtubeId:"yHnr5Dk2zCI"},q={datasets:[{description:"Thousands of short audio clips of a single speaker.",id:"LJ Speech Dataset"},{description:"Multi-speaker English dataset.",id:"LibriTTS"}],demo:{inputs:[{label:"Input",content:"I love audio models on the Hub!",type:"text"}],outputs:[{filename:"audio.wav",type:"audio"}]},id:"text-to-speech",label:a["text-to-speech"],libraries:i["text-to-speech"],metrics:[{description:"The Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated speech.",id:"mel cepstral distortion"}],models:[{description:"An end-to-end TTS model trained for a single speaker.",id:"espnet/kan-bayashi_ljspeech_vits"}],summary:"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.",widgetModels:["espnet/kan-bayashi_ljspeech_vits"],youtubeId:"NW62DpzJ274"},D={datasets:[{description:"A widely used dataset useful to benchmark named entity recognition models.",id:"conll2003"},{description:"A multilingual dataset of Wikipedia articles annotated for named entity recognition in over 150 different languages.",id:"wikiann"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Z\xFCrich.",type:"text"}],outputs:[{text:"My name is Omar and I live in Z\xFCrich.",tokens:[{type:"PERSON",start:11,end:15},{type:"GPE",start:30,end:36}],type:"text-with-tokens"}]},id:"token-classification",label:a["token-classification"],libraries:i["token-classification"],metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"A robust performance model to identify people, locations, organizations and names of miscellaneous entities.",id:"dslim/bert-base-NER"},{description:"Flair models are typically the state of the art in named entity recognition tasks.",id:"flair/ner-english"}],summary:"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.",widgetModels:["dslim/bert-base-NER"],youtubeId:"wVHdVlPScxA"},z={datasets:[{description:"A dataset of copyright-free books translated into 16 different languages.",id:"opus_books"},{description:"An example of translation between programming languages. This dataset consists of functions in Java and C#.",id:"code_x_glue_cc_code_to_code_trans"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Z\xFCrich.",type:"text"}],outputs:[{label:"Output",content:"Mein Name ist Omar und ich wohne in Z\xFCrich.",type:"text"}]},id:"translation",label:a.translation,libraries:i.translation,metrics:[{description:"BLEU score is calculated by counting the number of shared single or subsequent tokens between the generated sequence and the reference. Subsequent n tokens are called \u201Cn-grams\u201D. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. The score ranges from 0 to 1, where 1 means the translation perfectly matched and 0 did not match at all",id:"bleu"},{description:"",id:"sacrebleu"}],models:[{description:"A model that translates from English to French.",id:"Helsinki-NLP/opus-mt-en-fr"},{description:"A general-purpose Transformer that can be used to translate from English to German, French, or Romanian.",id:"t5-base"}],summary:"Translation is the task of converting text from one language to another.",widgetModels:["t5-small"],youtubeId:"1JvfrvZgi6c"},O={datasets:[{description:"A widely used dataset used to benchmark multiple variants of text classification.",id:"glue"},{description:"A text classification dataset used to benchmark natural language inference models",id:"snli"}],demo:{inputs:[{label:"Input",content:"I love Hugging Face!",type:"text"}],outputs:[{type:"chart",data:[{label:"POSITIVE",score:.9},{label:"NEUTRAL",score:.1},{label:"NEGATIVE",score:0}]}]},id:"text-classification",label:a["text-classification"],libraries:i["text-classification"],metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)",id:"f1"}],models:[{description:"A robust model trained for sentiment analysis.",id:"distilbert-base-uncased-finetuned-sst-2-english"},{description:"Multi-genre natural language inference model.",id:"roberta-large-mnli"}],summary:"Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.",widgetModels:["distilbert-base-uncased-finetuned-sst-2-english"],youtubeId:"leNG9fN9FQU"},j={datasets:[{description:"A large multilingual dataset of text crawled from the web.",id:"mc4"},{description:"Diverse open-source data consisting of 22 smaller high-quality datasets. It was used to train GPT-Neo.",id:"the_pile"}],demo:{inputs:[{label:"Input",content:"Once upon a time,",type:"text"}],outputs:[{label:"Output",content:"Once upon a time, we knew that our ancestors were on the verge of extinction. The great explorers and poets of the Old World, from Alexander the Great to Chaucer, are dead and gone. A good many of our ancient explorers and poets have",type:"text"}]},id:"text-generation",label:a["text-generation"],libraries:i["text-generation"],metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"Cross Entropy"},{description:"The Perplexity metric is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"Perplexity"}],models:[{description:"The model from OpenAI that helped usher in the Transformer revolution.",id:"gpt2"},{description:"A special Transformer model that can generate high-quality text for various tasks.",id:"bigscience/T0pp"}],summary:"Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.",widgetModels:["gpt2"],youtubeId:"Vpjb1lu0MDk"};export{x as I,a as P,D as a,_ as b,z as c,R as d,j as e,S as f,C as g,q as h,I as i,A as j,v as k,E as l,T as m,M as n,O as t};
