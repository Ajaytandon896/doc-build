import{S as lp,i as pp,s as mp,e as m,k as u,w as g,t as r,V as tp,M as hp,c as h,d as s,m as f,x as k,a as c,h as i,W as sp,b as N,N as cp,F as a,g as l,y as $,o as d,p as G,q as _,B as v,v as up,O as fp,n as J}from"../../chunks/vendor-e7c81d8a.js";import{T as ao}from"../../chunks/Tip-989931f5.js";import{Y as ap}from"../../chunks/Youtube-365ea064.js";import{I as oo}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";import{C as j}from"../../chunks/CodeBlock-105940ae.js";import{D as op}from"../../chunks/DocNotebookDropdown-928568b4.js";import{F as dp}from"../../chunks/FrameworkSwitch-287292d8.js";const{document:np}=fp;function _p(x){let n,p;return n=new op({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"}]}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function wp(x){let n,p;return n=new op({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"}]}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function bp(x){let n,p;return n=new ap({props:{id:"b3u8RzBCX9Y"}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function gp(x){let n,p;return n=new ap({props:{id:"_wxyB3j3mk4"}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function kp(x){let n,p;return n=new j({props:{codee:`from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function $p(x){let n,p;return n=new j({props:{codee:`from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function vp(x){let n,p;return n=new j({props:{codee:"(1, 66) (1, 66),",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">66</span>) (<span class="hljs-number">1</span>, <span class="hljs-number">66</span>)'}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function xp(x){let n,p;return n=new j({props:{codee:"torch.Size([1, 66]) torch.Size([1, 66]),",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>]) torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>])'}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function yp(x){let n,p;return n=new j({props:{codee:`import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = tf.constant(mask)[<span class="hljs-literal">None</span>]

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function jp(x){let n,p;return n=new j({props:{codee:`import torch

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000,`,highlighted:`<span class="hljs-keyword">import</span> torch

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = torch.tensor(mask)[<span class="hljs-literal">None</span>]

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function qp(x){let n,p;return n=new j({props:{codee:`start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy(),`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Ep(x){let n,p;return n=new j({props:{codee:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0],`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Tp(x){let n,p,t,w,b,y,E,S,C,P,L,D,z,A;return z=new j({props:{codee:"scores = np.triu(scores),",highlighted:"scores = np.triu(scores)"}}),{c(){n=m("p"),p=r("Then we\u2019ll mask the values where "),t=m("code"),w=r("start_index > end_index"),b=r(" by setting them to "),y=m("code"),E=r("0"),S=r(" (the other probabilities are all positive numbers). The "),C=m("code"),P=r("np.triu()"),L=r(" function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),D=u(),g(z.$$.fragment)},l(q){n=h(q,"P",{});var T=c(n);p=i(T,"Then we\u2019ll mask the values where "),t=h(T,"CODE",{});var xe=c(t);w=i(xe,"start_index > end_index"),xe.forEach(s),b=i(T," by setting them to "),y=h(T,"CODE",{});var Q=c(y);E=i(Q,"0"),Q.forEach(s),S=i(T," (the other probabilities are all positive numbers). The "),C=h(T,"CODE",{});var ye=c(C);P=i(ye,"np.triu()"),ye.forEach(s),L=i(T," function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),T.forEach(s),D=f(q),k(z.$$.fragment,q)},m(q,T){l(q,n,T),a(n,p),a(n,t),a(t,w),a(n,b),a(n,y),a(y,E),a(n,S),a(n,C),a(C,P),a(n,L),l(q,D,T),$(z,q,T),A=!0},i(q){A||(_(z.$$.fragment,q),A=!0)},o(q){d(z.$$.fragment,q),A=!1},d(q){q&&s(n),q&&s(D),v(z,q)}}}function Pp(x){let n,p,t,w,b,y,E,S,C,P,L,D,z,A;return z=new j({props:{codee:"scores = torch.triu(scores),",highlighted:"scores = torch.triu(scores)"}}),{c(){n=m("p"),p=r("Then we\u2019ll mask the values where "),t=m("code"),w=r("start_index > end_index"),b=r(" by setting them to "),y=m("code"),E=r("0"),S=r(" (the other probabilities are all positive numbers). The "),C=m("code"),P=r("torch.triu()"),L=r(" function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),D=u(),g(z.$$.fragment)},l(q){n=h(q,"P",{});var T=c(n);p=i(T,"Then we\u2019ll mask the values where "),t=h(T,"CODE",{});var xe=c(t);w=i(xe,"start_index > end_index"),xe.forEach(s),b=i(T," by setting them to "),y=h(T,"CODE",{});var Q=c(y);E=i(Q,"0"),Q.forEach(s),S=i(T," (the other probabilities are all positive numbers). The "),C=h(T,"CODE",{});var ye=c(C);P=i(ye,"torch.triu()"),ye.forEach(s),L=i(T," function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),T.forEach(s),D=f(q),k(z.$$.fragment,q)},m(q,T){l(q,n,T),a(n,p),a(n,t),a(t,w),a(n,b),a(n,y),a(y,E),a(n,S),a(n,C),a(C,P),a(n,L),l(q,D,T),$(z,q,T),A=!0},i(q){A||(_(z.$$.fragment,q),A=!0)},o(q){d(z.$$.fragment,q),A=!1},d(q){q&&s(n),q&&s(D),v(z,q)}}}function Sp(x){let n,p,t,w,b;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),b=r(" Compute the start and end indices for the five most likely answers.")},l(y){n=h(y,"P",{});var E=c(n);p=i(E,"\u270F\uFE0F "),t=h(E,"STRONG",{});var S=c(t);w=i(S,"Try it out!"),S.forEach(s),b=i(E," Compute the start and end indices for the five most likely answers."),E.forEach(s)},m(y,E){l(y,n,E),a(n,p),a(n,t),a(t,w),a(n,b)},d(y){y&&s(n)}}}function Cp(x){let n,p,t,w,b,y,E,S;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),b=r(" Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in "),y=m("code"),E=r("top_k=5"),S=r(" when calling it.")},l(C){n=h(C,"P",{});var P=c(n);p=i(P,"\u270F\uFE0F "),t=h(P,"STRONG",{});var L=c(t);w=i(L,"Try it out!"),L.forEach(s),b=i(P," Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in "),y=h(P,"CODE",{});var D=c(y);E=i(D,"top_k=5"),D.forEach(s),S=i(P," when calling it."),P.forEach(s)},m(C,P){l(C,n,P),a(n,p),a(n,t),a(t,w),a(n,b),a(n,y),a(y,E),a(n,S)},d(C){C&&s(n)}}}function zp(x){let n,p,t,w;return n=new j({props:{codee:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape),`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),t=new j({props:{codee:"(2, 384),",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){g(n.$$.fragment),p=u(),g(t.$$.fragment)},l(b){k(n.$$.fragment,b),p=f(b),k(t.$$.fragment,b)},m(b,y){$(n,b,y),l(b,p,y),$(t,b,y),w=!0},i(b){w||(_(n.$$.fragment,b),_(t.$$.fragment,b),w=!0)},o(b){d(n.$$.fragment,b),d(t.$$.fragment,b),w=!1},d(b){v(n,b),b&&s(p),v(t,b)}}}function Np(x){let n,p,t,w;return n=new j({props:{codee:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape),`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),t=new j({props:{codee:"torch.Size([2, 384]),",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){g(n.$$.fragment),p=u(),g(t.$$.fragment)},l(b){k(n.$$.fragment,b),p=f(b),k(t.$$.fragment,b)},m(b,y){$(n,b,y),l(b,p,y),$(t,b,y),w=!0},i(b){w||(_(n.$$.fragment,b),_(t.$$.fragment,b),w=!0)},o(b){d(n.$$.fragment,b),d(t.$$.fragment,b),w=!1},d(b){v(n,b),b&&s(p),v(t,b)}}}function Ap(x){let n,p;return n=new j({props:{codee:"(2, 384) (2, 384),",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>) (<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Dp(x){let n,p;return n=new j({props:{codee:"torch.Size([2, 384]) torch.Size([2, 384]),",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>]) torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Lp(x){let n,p;return n=new j({props:{codee:`sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits),`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = tf.math.logical_or(tf.constant(mask)[<span class="hljs-literal">None</span>], inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>)

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Op(x){let n,p;return n=new j({props:{codee:`sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000,`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = torch.logical_or(torch.tensor(mask)[<span class="hljs-literal">None</span>], (inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>))

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Fp(x){let n,p;return n=new j({props:{codee:`start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy(),`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>).numpy()`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Mp(x){let n,p;return n=new j({props:{codee:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1),`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Wp(x){let n,p;return n=new j({props:{codee:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates),`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">0</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">0</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Up(x){let n,p;return n=new j({props:{codee:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates),`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">0</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">0</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){g(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Ip(x){let n,p,t,w,b;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),b=r(" Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk).")},l(y){n=h(y,"P",{});var E=c(n);p=i(E,"\u270F\uFE0F "),t=h(E,"STRONG",{});var S=c(t);w=i(S,"Try it out!"),S.forEach(s),b=i(E," Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk)."),E.forEach(s)},m(y,E){l(y,n,E),a(n,p),a(n,t),a(t,w),a(n,b)},d(y){y&&s(n)}}}function Hp(x){let n,p,t,w,b,y,E,S;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),b=r(" Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in "),y=m("code"),E=r("top_k=5"),S=r(" when calling it.")},l(C){n=h(C,"P",{});var P=c(n);p=i(P,"\u270F\uFE0F "),t=h(P,"STRONG",{});var L=c(t);w=i(L,"Try it out!"),L.forEach(s),b=i(P," Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in "),y=h(P,"CODE",{});var D=c(y);E=i(D,"top_k=5"),D.forEach(s),S=i(P," when calling it."),P.forEach(s)},m(C,P){l(C,n,P),a(n,p),a(n,t),a(t,w),a(n,b),a(n,y),a(y,E),a(n,S)},d(C){C&&s(n)}}}function Gp(x){let n,p,t,w,b,y,E,S,C,P,L,D,z,A,q,T,xe,Q,ye,ro,qn,R,K,Gt,je,Te,Ds,Ke,io,Be,lo,Ls,po,mo,En,_e,ho,Jt,co,uo,Os,fo,_o,Tn,Ye,Pn,Ve,Sn,Qt,wo,Cn,Xe,zn,Ze,Nn,Rt,bo,An,qe,Pe,Fs,et,go,Ms,ko,Dn,W,$o,Ws,vo,xo,tt,Us,yo,jo,Kt,qo,Eo,Ln,B,Y,Bt,Yt,To,On,st,nt,vi,Fn,Vt,Po,Mn,at,Wn,V,X,Xt,U,So,Is,Co,zo,Hs,No,Ao,Gs,Do,Lo,Un,Se,Oo,Js,Fo,Mo,In,Z,ee,Zt,es,Wo,Hn,te,se,ts,F,Uo,Qs,Io,Ho,Rs,Go,Jo,Ks,Qo,Ro,Bs,Ko,Bo,Gn,O,Yo,Ys,Vo,Xo,Vs,Zo,er,Xs,tr,sr,Zs,nr,ar,Jn,rp='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span></span>',Qn,we,or,Rn,ip='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span>',Kn,en,rr,ir,Bn,ss,lr,Yn,ot,Vn,ne,ae,ns,M,pr,tn,mr,hr,sn,cr,ur,nn,fr,dr,an,_r,wr,Xn,rt,Zn,as,br,ea,it,ta,Ce,sa,be,gr,on,kr,$r,rn,vr,xr,na,lt,aa,os,yr,oa,pt,ra,mt,ia,rs,jr,la,ze,pa,Ee,Ne,ln,ht,qr,pn,Er,ma,Ae,Tr,mn,Pr,Sr,ha,ct,ca,ut,ua,De,Cr,hn,zr,Nr,fa,ft,da,dt,_a,Le,Ar,cn,Dr,Lr,wa,ge,Or,un,Fr,Mr,fn,Wr,Ur,ba,_t,ga,wt,ka,Oe,Ir,dn,Hr,Gr,$a,is,Jr,va,bt,xa,gt,ya,Fe,Qr,_n,Rr,Kr,ja,kt,qa,$t,Ea,ls,Br,Ta,vt,Pa,ps,Yr,Sa,xt,Ca,ms,Vr,za,I,Xr,wn,Zr,ei,bn,ti,si,gn,ni,ai,Na,yt,Aa,H,oi,kn,ri,ii,$n,li,pi,vn,mi,hi,Da,oe,re,hs,cs,ci,La,jt,Oa,ie,le,us,fs,ui,Fa,pe,me,ds,_s,fi,Ma,he,ce,ws,bs,di,Wa,ue,fe,gs,qt,Ua,ks,_i,Ia,Me,Ha,We,wi,xn,bi,gi,Ga,Et,Ja,Tt,Qa,$s,ki,Ra,Ue,Ka,vs,$i,Ba;t=new dp({props:{fw:x[0]}}),S=new oo({});const xi=[wp,_p],Pt=[];function yi(e,o){return e[0]==="pt"?0:1}z=yi(x),A=Pt[z]=xi[z](x);const ji=[gp,bp],St=[];function qi(e,o){return e[0]==="pt"?0:1}R=qi(x),K=St[R]=ji[R](x),Ke=new oo({}),Ye=new j({props:{codee:`from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back \u{1F917} Transformers?"
question_answerer(question=question, context=context),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>)
context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question = <span class="hljs-string">&quot;Which deep learning libraries back \u{1F917} Transformers?&quot;</span>
question_answerer(question=question, context=context)`}}),Ve=new j({props:{codee:`{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'},`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),Xe=new j({props:{codee:`long_context = """
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context),`,highlighted:`long_context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question_answerer(question=question, context=long_context)`}}),Ze=new j({props:{codee:`{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'},`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),et=new oo({});const Ei=[$p,kp],Ct=[];function Ti(e,o){return e[0]==="pt"?0:1}B=Ti(x),Y=Ct[B]=Ei[B](x),at=new j({props:{codee:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape),`,highlighted:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Pi=[xp,vp],zt=[];function Si(e,o){return e[0]==="pt"?0:1}V=Si(x),X=zt[V]=Pi[V](x);const Ci=[jp,yp],Nt=[];function zi(e,o){return e[0]==="pt"?0:1}Z=zi(x),ee=Nt[Z]=Ci[Z](x);const Ni=[Ep,qp],At=[];function Ai(e,o){return e[0]==="pt"?0:1}te=Ai(x),se=At[te]=Ni[te](x),ot=new j({props:{codee:"scores = start_probabilities[:, None] * end_probabilities[None, :],",highlighted:'scores = start_probabilities[:, <span class="hljs-literal">None</span>] * end_probabilities[<span class="hljs-literal">None</span>, :]'}});const Di=[Pp,Tp],Dt=[];function Li(e,o){return e[0]==="pt"?0:1}ne=Li(x),ae=Dt[ne]=Di[ne](x),rt=new j({props:{codee:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index]),`,highlighted:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[<span class="hljs-number">1</span>]
end_index = max_index % scores.shape[<span class="hljs-number">1</span>]
<span class="hljs-built_in">print</span>(scores[start_index, end_index])`}}),it=new j({props:{codee:"0.97773,",highlighted:'<span class="hljs-number">0.97773</span>'}}),Ce=new ao({props:{$$slots:{default:[Sp]},$$scope:{ctx:x}}}),lt=new j({props:{codee:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char],`,highlighted:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="hljs-literal">True</span>)
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`}}),pt=new j({props:{codee:`result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result),`,highlighted:`result = {
    <span class="hljs-string">&quot;answer&quot;</span>: answer,
    <span class="hljs-string">&quot;start&quot;</span>: start_char,
    <span class="hljs-string">&quot;end&quot;</span>: end_char,
    <span class="hljs-string">&quot;score&quot;</span>: scores[start_index, end_index],
}
<span class="hljs-built_in">print</span>(result)`}}),mt=new j({props:{codee:`{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773},`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>}`}}),ze=new ao({props:{$$slots:{default:[Cp]},$$scope:{ctx:x}}}),ht=new oo({}),ct=new j({props:{codee:`inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"])),`,highlighted:`inputs = tokenizer(question, long_context)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),ut=new j({props:{codee:"461,",highlighted:'<span class="hljs-number">461</span>'}}),ft=new j({props:{codee:`inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"])),`,highlighted:`inputs = tokenizer(question, long_context, max_length=<span class="hljs-number">384</span>, truncation=<span class="hljs-string">&quot;only_second&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),dt=new j({props:{codee:`"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
""",`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
&quot;&quot;&quot;</span>`}}),_t=new j({props:{codee:`sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids)),`,highlighted:`sentence = <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>
inputs = tokenizer(
    sentence, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),wt=new j({props:{codee:`'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]',`,highlighted:`<span class="hljs-string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] is not too long [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] too long but we [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] but we are going [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] are going to split [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] it anyway. [SEP]&#x27;</span>`}}),bt=new j({props:{codee:"print(inputs.keys()),",highlighted:'<span class="hljs-built_in">print</span>(inputs.keys())'}}),gt=new j({props:{codee:"dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping']),",highlighted:'dict_keys([<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>])'}}),kt=new j({props:{codee:'print(inputs["overflow_to_sample_mapping"]),',highlighted:'<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])'}}),$t=new j({props:{codee:"[0, 0, 0, 0, 0, 0, 0],",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),vt=new j({props:{codee:`sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"]),`,highlighted:`sentences = [
    <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,
    <span class="hljs-string">&quot;This sentence is shorter but will still get split.&quot;</span>,
]
inputs = tokenizer(
    sentences, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])`}}),xt=new j({props:{codee:"[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),yt=new j({props:{codee:`inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
),`,highlighted:`inputs = tokenizer(
    question,
    long_context,
    stride=<span class="hljs-number">128</span>,
    max_length=<span class="hljs-number">384</span>,
    padding=<span class="hljs-string">&quot;longest&quot;</span>,
    truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_offsets_mapping=<span class="hljs-literal">True</span>,
)`}});const Oi=[Np,zp],Lt=[];function Fi(e,o){return e[0]==="pt"?0:1}oe=Fi(x),re=Lt[oe]=Oi[oe](x),jt=new j({props:{codee:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape),`,highlighted:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Mi=[Dp,Ap],Ot=[];function Wi(e,o){return e[0]==="pt"?0:1}ie=Wi(x),le=Ot[ie]=Mi[ie](x);const Ui=[Op,Lp],Ft=[];function Ii(e,o){return e[0]==="pt"?0:1}pe=Ii(x),me=Ft[pe]=Ui[pe](x);const Hi=[Mp,Fp],Mt=[];function Gi(e,o){return e[0]==="pt"?0:1}he=Gi(x),ce=Mt[he]=Hi[he](x);const Ji=[Up,Wp],Wt=[];function Qi(e,o){return e[0]==="pt"?0:1}return ue=Qi(x),fe=Wt[ue]=Ji[ue](x),qt=new j({props:{codee:"[(0, 18, 0.33867), (173, 184, 0.97149)],",highlighted:'[(<span class="hljs-number">0</span>, <span class="hljs-number">18</span>, <span class="hljs-number">0.33867</span>), (<span class="hljs-number">173</span>, <span class="hljs-number">184</span>, <span class="hljs-number">0.97149</span>)]'}}),Me=new ao({props:{$$slots:{default:[Ip]},$$scope:{ctx:x}}}),Et=new j({props:{codee:`for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result),`,highlighted:`<span class="hljs-keyword">for</span> candidate, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {<span class="hljs-string">&quot;answer&quot;</span>: answer, <span class="hljs-string">&quot;start&quot;</span>: start_char, <span class="hljs-string">&quot;end&quot;</span>: end_char, <span class="hljs-string">&quot;score&quot;</span>: score}
    <span class="hljs-built_in">print</span>(result)`}}),Tt=new j({props:{codee:`{'answer': '\\n\u{1F917} Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149},`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;\\n\u{1F917} Transformers: State of the Art NLP&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">37</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.33867</span>}
{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>}`}}),Ue=new ao({props:{$$slots:{default:[Hp]},$$scope:{ctx:x}}}),{c(){n=m("meta"),p=u(),g(t.$$.fragment),w=u(),b=m("h1"),y=m("a"),E=m("span"),g(S.$$.fragment),C=u(),P=m("span"),L=r("Fast tokenizers in the QA pipeline"),D=u(),A.c(),q=u(),T=m("p"),xe=r("We will now dive into the "),Q=m("code"),ye=r("question-answering"),ro=r(" pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you\u2019re not interested in the question answering task."),qn=u(),K.c(),Gt=u(),je=m("h2"),Te=m("a"),Ds=m("span"),g(Ke.$$.fragment),io=u(),Be=m("span"),lo=r("Using the "),Ls=m("code"),po=r("question-answering"),mo=r(" pipeline"),En=u(),_e=m("p"),ho=r("As we saw in "),Jt=m("a"),co=r("Chapter 1"),uo=r(", we can use the "),Os=m("code"),fo=r("question-answering"),_o=r(" pipeline like this to get the answer to a question:"),Tn=u(),g(Ye.$$.fragment),Pn=u(),g(Ve.$$.fragment),Sn=u(),Qt=m("p"),wo=r("Unlike the other pipelines, which can\u2019t truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it\u2019s at the end:"),Cn=u(),g(Xe.$$.fragment),zn=u(),g(Ze.$$.fragment),Nn=u(),Rt=m("p"),bo=r("Let\u2019s see how it does all of this!"),An=u(),qe=m("h2"),Pe=m("a"),Fs=m("span"),g(et.$$.fragment),go=u(),Ms=m("span"),ko=r("Using a model for question answering"),Dn=u(),W=m("p"),$o=r("Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the "),Ws=m("code"),vo=r("question-answering"),xo=r(" pipeline is "),tt=m("a"),Us=m("code"),yo=r("distilbert-base-cased-distilled-squad"),jo=r(" (the \u201Csquad\u201D in the name comes from the dataset on which the model was fine-tuned; we\u2019ll talk more about the SQuAD dataset in "),Kt=m("a"),qo=r("Chapter 7"),Eo=r("):"),Ln=u(),Y.c(),Bt=u(),Yt=m("p"),To=r("Note that we tokenize the question and the context as a pair, with the question first."),On=u(),st=m("div"),nt=m("img"),Fn=u(),Vt=m("p"),Po=r("Models for question answering work a little differently from the models we\u2019ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don\u2019t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get:"),Mn=u(),g(at.$$.fragment),Wn=u(),X.c(),Xt=u(),U=m("p"),So=r("To convert those logits into probabilities, we will apply a softmax function \u2014 but before that, we need to make sure we mask the indices that are not part of the context. Our input is "),Is=m("code"),Co=r("[CLS] question [SEP] context [SEP]"),zo=r(", so we need to mask the tokens of the question as well as the "),Hs=m("code"),No=r("[SEP]"),Ao=r(" token. We\u2019ll keep the "),Gs=m("code"),Do=r("[CLS]"),Lo=r(" token, however, as some models use it to indicate that the answer is not in the context."),Un=u(),Se=m("p"),Oo=r("Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use "),Js=m("code"),Fo=r("-10000"),Mo=r(":"),In=u(),ee.c(),Zt=u(),es=m("p"),Wo=r("Now that we have properly masked the logits corresponding to positions we don\u2019t want to predict, we can apply the softmax:"),Hn=u(),se.c(),ts=u(),F=m("p"),Uo=r("At this stage, we could take the argmax of the start and end probabilities \u2014 but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible "),Qs=m("code"),Io=r("start_index"),Ho=r(" and "),Rs=m("code"),Go=r("end_index"),Jo=r(" where "),Ks=m("code"),Qo=r("start_index <= end_index"),Ro=r(", then take the tuple "),Bs=m("code"),Ko=r("(start_index, end_index)"),Bo=r(" with the highest probability."),Gn=u(),O=m("p"),Yo=r("Assuming the events \u201CThe answer starts at "),Ys=m("code"),Vo=r("start_index"),Xo=r("\u201D and \u201CThe answer ends at "),Vs=m("code"),Zo=r("end_index"),er=r("\u201D to be independent, the probability that the answer starts at "),Xs=m("code"),tr=r("start_index"),sr=r(" and ends at "),Zs=m("code"),nr=r("end_index"),ar=r(` is:
`),Jn=new tp,Qn=u(),we=m("p"),or=r("So, to compute all the scores, we just need to compute all the products "),Rn=new tp,Kn=r(" where "),en=m("code"),rr=r("start_index <= end_index"),ir=r("."),Bn=u(),ss=m("p"),lr=r("First let\u2019s compute all the possible products:"),Yn=u(),g(ot.$$.fragment),Vn=u(),ae.c(),ns=u(),M=m("p"),pr=r("Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division "),tn=m("code"),mr=r("//"),hr=r(" and modulus "),sn=m("code"),cr=r("%"),ur=r(" operations to get the "),nn=m("code"),fr=r("start_index"),dr=r(" and "),an=m("code"),_r=r("end_index"),wr=r(":"),Xn=u(),g(rt.$$.fragment),Zn=u(),as=m("p"),br=r("We\u2019re not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section):"),ea=u(),g(it.$$.fragment),ta=u(),g(Ce.$$.fragment),sa=u(),be=m("p"),gr=r("We have the "),on=m("code"),kr=r("start_index"),$r=r(" and "),rn=m("code"),vr=r("end_index"),xr=r(" of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task:"),na=u(),g(lt.$$.fragment),aa=u(),os=m("p"),yr=r("Now we just have to format everything to get our result:"),oa=u(),g(pt.$$.fragment),ra=u(),g(mt.$$.fragment),ia=u(),rs=m("p"),jr=r("Great! That\u2019s the same as in our first example!"),la=u(),g(ze.$$.fragment),pa=u(),Ee=m("h2"),Ne=m("a"),ln=m("span"),g(ht.$$.fragment),qr=u(),pn=m("span"),Er=r("Handling long contexts"),ma=u(),Ae=m("p"),Tr=r("If we try to tokenize the question and long context we used as an example previously, we\u2019ll get a number of tokens higher than the maximum length used in the "),mn=m("code"),Pr=r("question-answering"),Sr=r(" pipeline (which is 384):"),ha=u(),g(ct.$$.fragment),ca=u(),g(ut.$$.fragment),ua=u(),De=m("p"),Cr=r("So, we\u2019ll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don\u2019t want to truncate the question, only the context. Since the context is the second sentence, we\u2019ll use the "),hn=m("code"),zr=r('"only_second"'),Nr=r(" truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:"),fa=u(),g(ft.$$.fragment),da=u(),g(dt.$$.fragment),_a=u(),Le=m("p"),Ar=r("This means the model will have a hard time picking the correct answer. To fix this, the "),cn=m("code"),Dr=r("question-answering"),Lr=r(" pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don\u2019t split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks."),wa=u(),ge=m("p"),Or=r("We can have the tokenizer (fast or slow) do this for us by adding "),un=m("code"),Fr=r("return_overflowing_tokens=True"),Mr=r(", and we can specify the overlap we want with the "),fn=m("code"),Wr=r("stride"),Ur=r(" argument. Here is an example, using a smaller sentence:"),ba=u(),g(_t.$$.fragment),ga=u(),g(wt.$$.fragment),ka=u(),Oe=m("p"),Ir=r("As we can see, the sentence has been split into chunks in such a way that each entry in "),dn=m("code"),Hr=r('inputs["input_ids"]'),Gr=r(" has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries."),$a=u(),is=m("p"),Jr=r("Let\u2019s take a closer look at the result of the tokenization:"),va=u(),g(bt.$$.fragment),xa=u(),g(gt.$$.fragment),ya=u(),Fe=m("p"),Qr=r("As expected, we get input IDs and an attention mask. The last key, "),_n=m("code"),Rr=r("overflow_to_sample_mapping"),Kr=r(", is a map that tells us which sentence each of the results corresponds to \u2014 here we have 7 results that all come from the (only) sentence we passed the tokenizer:"),ja=u(),g(kt.$$.fragment),qa=u(),g($t.$$.fragment),Ea=u(),ls=m("p"),Br=r("This is more useful when we tokenize several sentences together. For instance, this:"),Ta=u(),g(vt.$$.fragment),Pa=u(),ps=m("p"),Yr=r("gets us:"),Sa=u(),g(xt.$$.fragment),Ca=u(),ms=m("p"),Vr=r("which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence."),za=u(),I=m("p"),Xr=r("Now let\u2019s go back to our long context. By default the "),wn=m("code"),Zr=r("question-answering"),ei=r(" pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing "),bn=m("code"),ti=r("max_seq_len"),si=r(" and "),gn=m("code"),ni=r("stride"),ai=r(" arguments when calling the pipeline). We will thus use those parameters when tokenizing. We\u2019ll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets:"),Na=u(),g(yt.$$.fragment),Aa=u(),H=m("p"),oi=r("Those "),kn=m("code"),ri=r("inputs"),ii=r(" will contain the input IDs and attention masks the model expects, as well as the offsets and the "),$n=m("code"),li=r("overflow_to_sample_mapping"),pi=r(" we just talked about. Since those two are not parameters used by the model, we\u2019ll pop them out of the "),vn=m("code"),mi=r("inputs"),hi=r(" (and we won\u2019t store the map, since it\u2019s not useful here) before converting it to a tensor:"),Da=u(),re.c(),hs=u(),cs=m("p"),ci=r("Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits:"),La=u(),g(jt.$$.fragment),Oa=u(),le.c(),us=u(),fs=m("p"),ui=r("Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask):"),Fa=u(),me.c(),ds=u(),_s=m("p"),fi=r("Then we can use the softmax to convert our logits to probabilities:"),Ma=u(),ce.c(),ws=u(),bs=m("p"),di=r("The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score:"),Wa=u(),fe.c(),gs=u(),g(qt.$$.fragment),Ua=u(),ks=m("p"),_i=r("Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it\u2019s interesting to see what the model has picked in the first chunk)."),Ia=u(),g(Me.$$.fragment),Ha=u(),We=m("p"),wi=r("The "),xn=m("code"),bi=r("offsets"),gi=r(" we grabbed earlier is actually a list of offsets, with one list per chunk of text:"),Ga=u(),g(Et.$$.fragment),Ja=u(),g(Tt.$$.fragment),Qa=u(),$s=m("p"),ki=r("If we ignore the first result, we get the same result as our pipeline for this long context \u2014 yay!"),Ra=u(),g(Ue.$$.fragment),Ka=u(),vs=m("p"),$i=r("This concludes our deep dive into the tokenizer\u2019s capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks."),this.h()},l(e){const o=hp('[data-svelte="svelte-1phssyn"]',np.head);n=h(o,"META",{name:!0,content:!0}),o.forEach(s),p=f(e),k(t.$$.fragment,e),w=f(e),b=h(e,"H1",{class:!0});var Ut=c(b);y=h(Ut,"A",{id:!0,class:!0,href:!0});var xs=c(y);E=h(xs,"SPAN",{});var ys=c(E);k(S.$$.fragment,ys),ys.forEach(s),xs.forEach(s),C=f(Ut),P=h(Ut,"SPAN",{});var js=c(P);L=i(js,"Fast tokenizers in the QA pipeline"),js.forEach(s),Ut.forEach(s),D=f(e),A.l(e),q=f(e),T=h(e,"P",{});var Ie=c(T);xe=i(Ie,"We will now dive into the "),Q=h(Ie,"CODE",{});var qs=c(Q);ye=i(qs,"question-answering"),qs.forEach(s),ro=i(Ie," pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you\u2019re not interested in the question answering task."),Ie.forEach(s),qn=f(e),K.l(e),Gt=f(e),je=h(e,"H2",{class:!0});var He=c(je);Te=h(He,"A",{id:!0,class:!0,href:!0});var Es=c(Te);Ds=h(Es,"SPAN",{});var yn=c(Ds);k(Ke.$$.fragment,yn),yn.forEach(s),Es.forEach(s),io=f(He),Be=h(He,"SPAN",{});var It=c(Be);lo=i(It,"Using the "),Ls=h(It,"CODE",{});var Ts=c(Ls);po=i(Ts,"question-answering"),Ts.forEach(s),mo=i(It," pipeline"),It.forEach(s),He.forEach(s),En=f(e),_e=h(e,"P",{});var ke=c(_e);ho=i(ke,"As we saw in "),Jt=h(ke,"A",{href:!0});var Ps=c(Jt);co=i(Ps,"Chapter 1"),Ps.forEach(s),uo=i(ke,", we can use the "),Os=h(ke,"CODE",{});var Ss=c(Os);fo=i(Ss,"question-answering"),Ss.forEach(s),_o=i(ke," pipeline like this to get the answer to a question:"),ke.forEach(s),Tn=f(e),k(Ye.$$.fragment,e),Pn=f(e),k(Ve.$$.fragment,e),Sn=f(e),Qt=h(e,"P",{});var Cs=c(Qt);wo=i(Cs,"Unlike the other pipelines, which can\u2019t truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it\u2019s at the end:"),Cs.forEach(s),Cn=f(e),k(Xe.$$.fragment,e),zn=f(e),k(Ze.$$.fragment,e),Nn=f(e),Rt=h(e,"P",{});var jn=c(Rt);bo=i(jn,"Let\u2019s see how it does all of this!"),jn.forEach(s),An=f(e),qe=h(e,"H2",{class:!0});var Ht=c(qe);Pe=h(Ht,"A",{id:!0,class:!0,href:!0});var Ri=c(Pe);Fs=h(Ri,"SPAN",{});var Ki=c(Fs);k(et.$$.fragment,Ki),Ki.forEach(s),Ri.forEach(s),go=f(Ht),Ms=h(Ht,"SPAN",{});var Bi=c(Ms);ko=i(Bi,"Using a model for question answering"),Bi.forEach(s),Ht.forEach(s),Dn=f(e),W=h(e,"P",{});var Ge=c(W);$o=i(Ge,"Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the "),Ws=h(Ge,"CODE",{});var Yi=c(Ws);vo=i(Yi,"question-answering"),Yi.forEach(s),xo=i(Ge," pipeline is "),tt=h(Ge,"A",{href:!0,rel:!0});var Vi=c(tt);Us=h(Vi,"CODE",{});var Xi=c(Us);yo=i(Xi,"distilbert-base-cased-distilled-squad"),Xi.forEach(s),Vi.forEach(s),jo=i(Ge," (the \u201Csquad\u201D in the name comes from the dataset on which the model was fine-tuned; we\u2019ll talk more about the SQuAD dataset in "),Kt=h(Ge,"A",{href:!0});var Zi=c(Kt);qo=i(Zi,"Chapter 7"),Zi.forEach(s),Eo=i(Ge,"):"),Ge.forEach(s),Ln=f(e),Y.l(e),Bt=f(e),Yt=h(e,"P",{});var el=c(Yt);To=i(el,"Note that we tokenize the question and the context as a pair, with the question first."),el.forEach(s),On=f(e),st=h(e,"DIV",{class:!0});var tl=c(st);nt=h(tl,"IMG",{src:!0,alt:!0,width:!0}),tl.forEach(s),Fn=f(e),Vt=h(e,"P",{});var sl=c(Vt);Po=i(sl,"Models for question answering work a little differently from the models we\u2019ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don\u2019t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get:"),sl.forEach(s),Mn=f(e),k(at.$$.fragment,e),Wn=f(e),X.l(e),Xt=f(e),U=h(e,"P",{});var Je=c(U);So=i(Je,"To convert those logits into probabilities, we will apply a softmax function \u2014 but before that, we need to make sure we mask the indices that are not part of the context. Our input is "),Is=h(Je,"CODE",{});var nl=c(Is);Co=i(nl,"[CLS] question [SEP] context [SEP]"),nl.forEach(s),zo=i(Je,", so we need to mask the tokens of the question as well as the "),Hs=h(Je,"CODE",{});var al=c(Hs);No=i(al,"[SEP]"),al.forEach(s),Ao=i(Je," token. We\u2019ll keep the "),Gs=h(Je,"CODE",{});var ol=c(Gs);Do=i(ol,"[CLS]"),ol.forEach(s),Lo=i(Je," token, however, as some models use it to indicate that the answer is not in the context."),Je.forEach(s),Un=f(e),Se=h(e,"P",{});var Ya=c(Se);Oo=i(Ya,"Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use "),Js=h(Ya,"CODE",{});var rl=c(Js);Fo=i(rl,"-10000"),rl.forEach(s),Mo=i(Ya,":"),Ya.forEach(s),In=f(e),ee.l(e),Zt=f(e),es=h(e,"P",{});var il=c(es);Wo=i(il,"Now that we have properly masked the logits corresponding to positions we don\u2019t want to predict, we can apply the softmax:"),il.forEach(s),Hn=f(e),se.l(e),ts=f(e),F=h(e,"P",{});var $e=c(F);Uo=i($e,"At this stage, we could take the argmax of the start and end probabilities \u2014 but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible "),Qs=h($e,"CODE",{});var ll=c(Qs);Io=i(ll,"start_index"),ll.forEach(s),Ho=i($e," and "),Rs=h($e,"CODE",{});var pl=c(Rs);Go=i(pl,"end_index"),pl.forEach(s),Jo=i($e," where "),Ks=h($e,"CODE",{});var ml=c(Ks);Qo=i(ml,"start_index <= end_index"),ml.forEach(s),Ro=i($e,", then take the tuple "),Bs=h($e,"CODE",{});var hl=c(Bs);Ko=i(hl,"(start_index, end_index)"),hl.forEach(s),Bo=i($e," with the highest probability."),$e.forEach(s),Gn=f(e),O=h(e,"P",{});var de=c(O);Yo=i(de,"Assuming the events \u201CThe answer starts at "),Ys=h(de,"CODE",{});var cl=c(Ys);Vo=i(cl,"start_index"),cl.forEach(s),Xo=i(de,"\u201D and \u201CThe answer ends at "),Vs=h(de,"CODE",{});var ul=c(Vs);Zo=i(ul,"end_index"),ul.forEach(s),er=i(de,"\u201D to be independent, the probability that the answer starts at "),Xs=h(de,"CODE",{});var fl=c(Xs);tr=i(fl,"start_index"),fl.forEach(s),sr=i(de," and ends at "),Zs=h(de,"CODE",{});var dl=c(Zs);nr=i(dl,"end_index"),dl.forEach(s),ar=i(de,` is:
`),Jn=sp(de),de.forEach(s),Qn=f(e),we=h(e,"P",{});var zs=c(we);or=i(zs,"So, to compute all the scores, we just need to compute all the products "),Rn=sp(zs),Kn=i(zs," where "),en=h(zs,"CODE",{});var _l=c(en);rr=i(_l,"start_index <= end_index"),_l.forEach(s),ir=i(zs,"."),zs.forEach(s),Bn=f(e),ss=h(e,"P",{});var wl=c(ss);lr=i(wl,"First let\u2019s compute all the possible products:"),wl.forEach(s),Yn=f(e),k(ot.$$.fragment,e),Vn=f(e),ae.l(e),ns=f(e),M=h(e,"P",{});var ve=c(M);pr=i(ve,"Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division "),tn=h(ve,"CODE",{});var bl=c(tn);mr=i(bl,"//"),bl.forEach(s),hr=i(ve," and modulus "),sn=h(ve,"CODE",{});var gl=c(sn);cr=i(gl,"%"),gl.forEach(s),ur=i(ve," operations to get the "),nn=h(ve,"CODE",{});var kl=c(nn);fr=i(kl,"start_index"),kl.forEach(s),dr=i(ve," and "),an=h(ve,"CODE",{});var $l=c(an);_r=i($l,"end_index"),$l.forEach(s),wr=i(ve,":"),ve.forEach(s),Xn=f(e),k(rt.$$.fragment,e),Zn=f(e),as=h(e,"P",{});var vl=c(as);br=i(vl,"We\u2019re not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section):"),vl.forEach(s),ea=f(e),k(it.$$.fragment,e),ta=f(e),k(Ce.$$.fragment,e),sa=f(e),be=h(e,"P",{});var Ns=c(be);gr=i(Ns,"We have the "),on=h(Ns,"CODE",{});var xl=c(on);kr=i(xl,"start_index"),xl.forEach(s),$r=i(Ns," and "),rn=h(Ns,"CODE",{});var yl=c(rn);vr=i(yl,"end_index"),yl.forEach(s),xr=i(Ns," of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task:"),Ns.forEach(s),na=f(e),k(lt.$$.fragment,e),aa=f(e),os=h(e,"P",{});var jl=c(os);yr=i(jl,"Now we just have to format everything to get our result:"),jl.forEach(s),oa=f(e),k(pt.$$.fragment,e),ra=f(e),k(mt.$$.fragment,e),ia=f(e),rs=h(e,"P",{});var ql=c(rs);jr=i(ql,"Great! That\u2019s the same as in our first example!"),ql.forEach(s),la=f(e),k(ze.$$.fragment,e),pa=f(e),Ee=h(e,"H2",{class:!0});var Va=c(Ee);Ne=h(Va,"A",{id:!0,class:!0,href:!0});var El=c(Ne);ln=h(El,"SPAN",{});var Tl=c(ln);k(ht.$$.fragment,Tl),Tl.forEach(s),El.forEach(s),qr=f(Va),pn=h(Va,"SPAN",{});var Pl=c(pn);Er=i(Pl,"Handling long contexts"),Pl.forEach(s),Va.forEach(s),ma=f(e),Ae=h(e,"P",{});var Xa=c(Ae);Tr=i(Xa,"If we try to tokenize the question and long context we used as an example previously, we\u2019ll get a number of tokens higher than the maximum length used in the "),mn=h(Xa,"CODE",{});var Sl=c(mn);Pr=i(Sl,"question-answering"),Sl.forEach(s),Sr=i(Xa," pipeline (which is 384):"),Xa.forEach(s),ha=f(e),k(ct.$$.fragment,e),ca=f(e),k(ut.$$.fragment,e),ua=f(e),De=h(e,"P",{});var Za=c(De);Cr=i(Za,"So, we\u2019ll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don\u2019t want to truncate the question, only the context. Since the context is the second sentence, we\u2019ll use the "),hn=h(Za,"CODE",{});var Cl=c(hn);zr=i(Cl,'"only_second"'),Cl.forEach(s),Nr=i(Za," truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:"),Za.forEach(s),fa=f(e),k(ft.$$.fragment,e),da=f(e),k(dt.$$.fragment,e),_a=f(e),Le=h(e,"P",{});var eo=c(Le);Ar=i(eo,"This means the model will have a hard time picking the correct answer. To fix this, the "),cn=h(eo,"CODE",{});var zl=c(cn);Dr=i(zl,"question-answering"),zl.forEach(s),Lr=i(eo," pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don\u2019t split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks."),eo.forEach(s),wa=f(e),ge=h(e,"P",{});var As=c(ge);Or=i(As,"We can have the tokenizer (fast or slow) do this for us by adding "),un=h(As,"CODE",{});var Nl=c(un);Fr=i(Nl,"return_overflowing_tokens=True"),Nl.forEach(s),Mr=i(As,", and we can specify the overlap we want with the "),fn=h(As,"CODE",{});var Al=c(fn);Wr=i(Al,"stride"),Al.forEach(s),Ur=i(As," argument. Here is an example, using a smaller sentence:"),As.forEach(s),ba=f(e),k(_t.$$.fragment,e),ga=f(e),k(wt.$$.fragment,e),ka=f(e),Oe=h(e,"P",{});var to=c(Oe);Ir=i(to,"As we can see, the sentence has been split into chunks in such a way that each entry in "),dn=h(to,"CODE",{});var Dl=c(dn);Hr=i(Dl,'inputs["input_ids"]'),Dl.forEach(s),Gr=i(to," has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries."),to.forEach(s),$a=f(e),is=h(e,"P",{});var Ll=c(is);Jr=i(Ll,"Let\u2019s take a closer look at the result of the tokenization:"),Ll.forEach(s),va=f(e),k(bt.$$.fragment,e),xa=f(e),k(gt.$$.fragment,e),ya=f(e),Fe=h(e,"P",{});var so=c(Fe);Qr=i(so,"As expected, we get input IDs and an attention mask. The last key, "),_n=h(so,"CODE",{});var Ol=c(_n);Rr=i(Ol,"overflow_to_sample_mapping"),Ol.forEach(s),Kr=i(so,", is a map that tells us which sentence each of the results corresponds to \u2014 here we have 7 results that all come from the (only) sentence we passed the tokenizer:"),so.forEach(s),ja=f(e),k(kt.$$.fragment,e),qa=f(e),k($t.$$.fragment,e),Ea=f(e),ls=h(e,"P",{});var Fl=c(ls);Br=i(Fl,"This is more useful when we tokenize several sentences together. For instance, this:"),Fl.forEach(s),Ta=f(e),k(vt.$$.fragment,e),Pa=f(e),ps=h(e,"P",{});var Ml=c(ps);Yr=i(Ml,"gets us:"),Ml.forEach(s),Sa=f(e),k(xt.$$.fragment,e),Ca=f(e),ms=h(e,"P",{});var Wl=c(ms);Vr=i(Wl,"which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence."),Wl.forEach(s),za=f(e),I=h(e,"P",{});var Qe=c(I);Xr=i(Qe,"Now let\u2019s go back to our long context. By default the "),wn=h(Qe,"CODE",{});var Ul=c(wn);Zr=i(Ul,"question-answering"),Ul.forEach(s),ei=i(Qe," pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing "),bn=h(Qe,"CODE",{});var Il=c(bn);ti=i(Il,"max_seq_len"),Il.forEach(s),si=i(Qe," and "),gn=h(Qe,"CODE",{});var Hl=c(gn);ni=i(Hl,"stride"),Hl.forEach(s),ai=i(Qe," arguments when calling the pipeline). We will thus use those parameters when tokenizing. We\u2019ll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets:"),Qe.forEach(s),Na=f(e),k(yt.$$.fragment,e),Aa=f(e),H=h(e,"P",{});var Re=c(H);oi=i(Re,"Those "),kn=h(Re,"CODE",{});var Gl=c(kn);ri=i(Gl,"inputs"),Gl.forEach(s),ii=i(Re," will contain the input IDs and attention masks the model expects, as well as the offsets and the "),$n=h(Re,"CODE",{});var Jl=c($n);li=i(Jl,"overflow_to_sample_mapping"),Jl.forEach(s),pi=i(Re," we just talked about. Since those two are not parameters used by the model, we\u2019ll pop them out of the "),vn=h(Re,"CODE",{});var Ql=c(vn);mi=i(Ql,"inputs"),Ql.forEach(s),hi=i(Re," (and we won\u2019t store the map, since it\u2019s not useful here) before converting it to a tensor:"),Re.forEach(s),Da=f(e),re.l(e),hs=f(e),cs=h(e,"P",{});var Rl=c(cs);ci=i(Rl,"Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits:"),Rl.forEach(s),La=f(e),k(jt.$$.fragment,e),Oa=f(e),le.l(e),us=f(e),fs=h(e,"P",{});var Kl=c(fs);ui=i(Kl,"Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask):"),Kl.forEach(s),Fa=f(e),me.l(e),ds=f(e),_s=h(e,"P",{});var Bl=c(_s);fi=i(Bl,"Then we can use the softmax to convert our logits to probabilities:"),Bl.forEach(s),Ma=f(e),ce.l(e),ws=f(e),bs=h(e,"P",{});var Yl=c(bs);di=i(Yl,"The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score:"),Yl.forEach(s),Wa=f(e),fe.l(e),gs=f(e),k(qt.$$.fragment,e),Ua=f(e),ks=h(e,"P",{});var Vl=c(ks);_i=i(Vl,"Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it\u2019s interesting to see what the model has picked in the first chunk)."),Vl.forEach(s),Ia=f(e),k(Me.$$.fragment,e),Ha=f(e),We=h(e,"P",{});var no=c(We);wi=i(no,"The "),xn=h(no,"CODE",{});var Xl=c(xn);bi=i(Xl,"offsets"),Xl.forEach(s),gi=i(no," we grabbed earlier is actually a list of offsets, with one list per chunk of text:"),no.forEach(s),Ga=f(e),k(Et.$$.fragment,e),Ja=f(e),k(Tt.$$.fragment,e),Qa=f(e),$s=h(e,"P",{});var Zl=c($s);ki=i(Zl,"If we ignore the first result, we get the same result as our pipeline for this long context \u2014 yay!"),Zl.forEach(s),Ra=f(e),k(Ue.$$.fragment,e),Ka=f(e),vs=h(e,"P",{});var ep=c(vs);$i=i(ep,"This concludes our deep dive into the tokenizer\u2019s capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks."),ep.forEach(s),this.h()},h(){N(n,"name","hf:doc:metadata"),N(n,"content",JSON.stringify(Jp)),N(y,"id","fast-tokenizers-in-the-qa-pipeline"),N(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),N(y,"href","#fast-tokenizers-in-the-qa-pipeline"),N(b,"class","relative group"),N(Te,"id","using-the-questionanswering-pipeline"),N(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),N(Te,"href","#using-the-questionanswering-pipeline"),N(je,"class","relative group"),N(Jt,"href","/course/chapter1"),N(Pe,"id","using-a-model-for-question-answering"),N(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),N(Pe,"href","#using-a-model-for-question-answering"),N(qe,"class","relative group"),N(tt,"href","https://huggingface.co/distilbert-base-cased-distilled-squad"),N(tt,"rel","nofollow"),N(Kt,"href","/course/chapter7/7"),cp(nt.src,vi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.PNG")||N(nt,"src",vi),N(nt,"alt","An example of tokenization of question and context"),N(nt,"width","80%"),N(st,"class","flex justify-center"),Jn.a=null,Rn.a=Kn,N(Ne,"id","handling-long-contexts"),N(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),N(Ne,"href","#handling-long-contexts"),N(Ee,"class","relative group")},m(e,o){a(np.head,n),l(e,p,o),$(t,e,o),l(e,w,o),l(e,b,o),a(b,y),a(y,E),$(S,E,null),a(b,C),a(b,P),a(P,L),l(e,D,o),Pt[z].m(e,o),l(e,q,o),l(e,T,o),a(T,xe),a(T,Q),a(Q,ye),a(T,ro),l(e,qn,o),St[R].m(e,o),l(e,Gt,o),l(e,je,o),a(je,Te),a(Te,Ds),$(Ke,Ds,null),a(je,io),a(je,Be),a(Be,lo),a(Be,Ls),a(Ls,po),a(Be,mo),l(e,En,o),l(e,_e,o),a(_e,ho),a(_e,Jt),a(Jt,co),a(_e,uo),a(_e,Os),a(Os,fo),a(_e,_o),l(e,Tn,o),$(Ye,e,o),l(e,Pn,o),$(Ve,e,o),l(e,Sn,o),l(e,Qt,o),a(Qt,wo),l(e,Cn,o),$(Xe,e,o),l(e,zn,o),$(Ze,e,o),l(e,Nn,o),l(e,Rt,o),a(Rt,bo),l(e,An,o),l(e,qe,o),a(qe,Pe),a(Pe,Fs),$(et,Fs,null),a(qe,go),a(qe,Ms),a(Ms,ko),l(e,Dn,o),l(e,W,o),a(W,$o),a(W,Ws),a(Ws,vo),a(W,xo),a(W,tt),a(tt,Us),a(Us,yo),a(W,jo),a(W,Kt),a(Kt,qo),a(W,Eo),l(e,Ln,o),Ct[B].m(e,o),l(e,Bt,o),l(e,Yt,o),a(Yt,To),l(e,On,o),l(e,st,o),a(st,nt),l(e,Fn,o),l(e,Vt,o),a(Vt,Po),l(e,Mn,o),$(at,e,o),l(e,Wn,o),zt[V].m(e,o),l(e,Xt,o),l(e,U,o),a(U,So),a(U,Is),a(Is,Co),a(U,zo),a(U,Hs),a(Hs,No),a(U,Ao),a(U,Gs),a(Gs,Do),a(U,Lo),l(e,Un,o),l(e,Se,o),a(Se,Oo),a(Se,Js),a(Js,Fo),a(Se,Mo),l(e,In,o),Nt[Z].m(e,o),l(e,Zt,o),l(e,es,o),a(es,Wo),l(e,Hn,o),At[te].m(e,o),l(e,ts,o),l(e,F,o),a(F,Uo),a(F,Qs),a(Qs,Io),a(F,Ho),a(F,Rs),a(Rs,Go),a(F,Jo),a(F,Ks),a(Ks,Qo),a(F,Ro),a(F,Bs),a(Bs,Ko),a(F,Bo),l(e,Gn,o),l(e,O,o),a(O,Yo),a(O,Ys),a(Ys,Vo),a(O,Xo),a(O,Vs),a(Vs,Zo),a(O,er),a(O,Xs),a(Xs,tr),a(O,sr),a(O,Zs),a(Zs,nr),a(O,ar),Jn.m(rp,O),l(e,Qn,o),l(e,we,o),a(we,or),Rn.m(ip,we),a(we,Kn),a(we,en),a(en,rr),a(we,ir),l(e,Bn,o),l(e,ss,o),a(ss,lr),l(e,Yn,o),$(ot,e,o),l(e,Vn,o),Dt[ne].m(e,o),l(e,ns,o),l(e,M,o),a(M,pr),a(M,tn),a(tn,mr),a(M,hr),a(M,sn),a(sn,cr),a(M,ur),a(M,nn),a(nn,fr),a(M,dr),a(M,an),a(an,_r),a(M,wr),l(e,Xn,o),$(rt,e,o),l(e,Zn,o),l(e,as,o),a(as,br),l(e,ea,o),$(it,e,o),l(e,ta,o),$(Ce,e,o),l(e,sa,o),l(e,be,o),a(be,gr),a(be,on),a(on,kr),a(be,$r),a(be,rn),a(rn,vr),a(be,xr),l(e,na,o),$(lt,e,o),l(e,aa,o),l(e,os,o),a(os,yr),l(e,oa,o),$(pt,e,o),l(e,ra,o),$(mt,e,o),l(e,ia,o),l(e,rs,o),a(rs,jr),l(e,la,o),$(ze,e,o),l(e,pa,o),l(e,Ee,o),a(Ee,Ne),a(Ne,ln),$(ht,ln,null),a(Ee,qr),a(Ee,pn),a(pn,Er),l(e,ma,o),l(e,Ae,o),a(Ae,Tr),a(Ae,mn),a(mn,Pr),a(Ae,Sr),l(e,ha,o),$(ct,e,o),l(e,ca,o),$(ut,e,o),l(e,ua,o),l(e,De,o),a(De,Cr),a(De,hn),a(hn,zr),a(De,Nr),l(e,fa,o),$(ft,e,o),l(e,da,o),$(dt,e,o),l(e,_a,o),l(e,Le,o),a(Le,Ar),a(Le,cn),a(cn,Dr),a(Le,Lr),l(e,wa,o),l(e,ge,o),a(ge,Or),a(ge,un),a(un,Fr),a(ge,Mr),a(ge,fn),a(fn,Wr),a(ge,Ur),l(e,ba,o),$(_t,e,o),l(e,ga,o),$(wt,e,o),l(e,ka,o),l(e,Oe,o),a(Oe,Ir),a(Oe,dn),a(dn,Hr),a(Oe,Gr),l(e,$a,o),l(e,is,o),a(is,Jr),l(e,va,o),$(bt,e,o),l(e,xa,o),$(gt,e,o),l(e,ya,o),l(e,Fe,o),a(Fe,Qr),a(Fe,_n),a(_n,Rr),a(Fe,Kr),l(e,ja,o),$(kt,e,o),l(e,qa,o),$($t,e,o),l(e,Ea,o),l(e,ls,o),a(ls,Br),l(e,Ta,o),$(vt,e,o),l(e,Pa,o),l(e,ps,o),a(ps,Yr),l(e,Sa,o),$(xt,e,o),l(e,Ca,o),l(e,ms,o),a(ms,Vr),l(e,za,o),l(e,I,o),a(I,Xr),a(I,wn),a(wn,Zr),a(I,ei),a(I,bn),a(bn,ti),a(I,si),a(I,gn),a(gn,ni),a(I,ai),l(e,Na,o),$(yt,e,o),l(e,Aa,o),l(e,H,o),a(H,oi),a(H,kn),a(kn,ri),a(H,ii),a(H,$n),a($n,li),a(H,pi),a(H,vn),a(vn,mi),a(H,hi),l(e,Da,o),Lt[oe].m(e,o),l(e,hs,o),l(e,cs,o),a(cs,ci),l(e,La,o),$(jt,e,o),l(e,Oa,o),Ot[ie].m(e,o),l(e,us,o),l(e,fs,o),a(fs,ui),l(e,Fa,o),Ft[pe].m(e,o),l(e,ds,o),l(e,_s,o),a(_s,fi),l(e,Ma,o),Mt[he].m(e,o),l(e,ws,o),l(e,bs,o),a(bs,di),l(e,Wa,o),Wt[ue].m(e,o),l(e,gs,o),$(qt,e,o),l(e,Ua,o),l(e,ks,o),a(ks,_i),l(e,Ia,o),$(Me,e,o),l(e,Ha,o),l(e,We,o),a(We,wi),a(We,xn),a(xn,bi),a(We,gi),l(e,Ga,o),$(Et,e,o),l(e,Ja,o),$(Tt,e,o),l(e,Qa,o),l(e,$s,o),a($s,ki),l(e,Ra,o),$(Ue,e,o),l(e,Ka,o),l(e,vs,o),a(vs,$i),Ba=!0},p(e,[o]){const Ut={};o&1&&(Ut.fw=e[0]),t.$set(Ut);let xs=z;z=yi(e),z!==xs&&(J(),d(Pt[xs],1,1,()=>{Pt[xs]=null}),G(),A=Pt[z],A||(A=Pt[z]=xi[z](e),A.c()),_(A,1),A.m(q.parentNode,q));let ys=R;R=qi(e),R!==ys&&(J(),d(St[ys],1,1,()=>{St[ys]=null}),G(),K=St[R],K||(K=St[R]=ji[R](e),K.c()),_(K,1),K.m(Gt.parentNode,Gt));let js=B;B=Ti(e),B!==js&&(J(),d(Ct[js],1,1,()=>{Ct[js]=null}),G(),Y=Ct[B],Y||(Y=Ct[B]=Ei[B](e),Y.c()),_(Y,1),Y.m(Bt.parentNode,Bt));let Ie=V;V=Si(e),V!==Ie&&(J(),d(zt[Ie],1,1,()=>{zt[Ie]=null}),G(),X=zt[V],X||(X=zt[V]=Pi[V](e),X.c()),_(X,1),X.m(Xt.parentNode,Xt));let qs=Z;Z=zi(e),Z!==qs&&(J(),d(Nt[qs],1,1,()=>{Nt[qs]=null}),G(),ee=Nt[Z],ee||(ee=Nt[Z]=Ci[Z](e),ee.c()),_(ee,1),ee.m(Zt.parentNode,Zt));let He=te;te=Ai(e),te!==He&&(J(),d(At[He],1,1,()=>{At[He]=null}),G(),se=At[te],se||(se=At[te]=Ni[te](e),se.c()),_(se,1),se.m(ts.parentNode,ts));let Es=ne;ne=Li(e),ne!==Es&&(J(),d(Dt[Es],1,1,()=>{Dt[Es]=null}),G(),ae=Dt[ne],ae||(ae=Dt[ne]=Di[ne](e),ae.c()),_(ae,1),ae.m(ns.parentNode,ns));const yn={};o&2&&(yn.$$scope={dirty:o,ctx:e}),Ce.$set(yn);const It={};o&2&&(It.$$scope={dirty:o,ctx:e}),ze.$set(It);let Ts=oe;oe=Fi(e),oe!==Ts&&(J(),d(Lt[Ts],1,1,()=>{Lt[Ts]=null}),G(),re=Lt[oe],re||(re=Lt[oe]=Oi[oe](e),re.c()),_(re,1),re.m(hs.parentNode,hs));let ke=ie;ie=Wi(e),ie!==ke&&(J(),d(Ot[ke],1,1,()=>{Ot[ke]=null}),G(),le=Ot[ie],le||(le=Ot[ie]=Mi[ie](e),le.c()),_(le,1),le.m(us.parentNode,us));let Ps=pe;pe=Ii(e),pe!==Ps&&(J(),d(Ft[Ps],1,1,()=>{Ft[Ps]=null}),G(),me=Ft[pe],me||(me=Ft[pe]=Ui[pe](e),me.c()),_(me,1),me.m(ds.parentNode,ds));let Ss=he;he=Gi(e),he!==Ss&&(J(),d(Mt[Ss],1,1,()=>{Mt[Ss]=null}),G(),ce=Mt[he],ce||(ce=Mt[he]=Hi[he](e),ce.c()),_(ce,1),ce.m(ws.parentNode,ws));let Cs=ue;ue=Qi(e),ue!==Cs&&(J(),d(Wt[Cs],1,1,()=>{Wt[Cs]=null}),G(),fe=Wt[ue],fe||(fe=Wt[ue]=Ji[ue](e),fe.c()),_(fe,1),fe.m(gs.parentNode,gs));const jn={};o&2&&(jn.$$scope={dirty:o,ctx:e}),Me.$set(jn);const Ht={};o&2&&(Ht.$$scope={dirty:o,ctx:e}),Ue.$set(Ht)},i(e){Ba||(_(t.$$.fragment,e),_(S.$$.fragment,e),_(A),_(K),_(Ke.$$.fragment,e),_(Ye.$$.fragment,e),_(Ve.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),_(Y),_(at.$$.fragment,e),_(X),_(ee),_(se),_(ot.$$.fragment,e),_(ae),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(Ce.$$.fragment,e),_(lt.$$.fragment,e),_(pt.$$.fragment,e),_(mt.$$.fragment,e),_(ze.$$.fragment,e),_(ht.$$.fragment,e),_(ct.$$.fragment,e),_(ut.$$.fragment,e),_(ft.$$.fragment,e),_(dt.$$.fragment,e),_(_t.$$.fragment,e),_(wt.$$.fragment,e),_(bt.$$.fragment,e),_(gt.$$.fragment,e),_(kt.$$.fragment,e),_($t.$$.fragment,e),_(vt.$$.fragment,e),_(xt.$$.fragment,e),_(yt.$$.fragment,e),_(re),_(jt.$$.fragment,e),_(le),_(me),_(ce),_(fe),_(qt.$$.fragment,e),_(Me.$$.fragment,e),_(Et.$$.fragment,e),_(Tt.$$.fragment,e),_(Ue.$$.fragment,e),Ba=!0)},o(e){d(t.$$.fragment,e),d(S.$$.fragment,e),d(A),d(K),d(Ke.$$.fragment,e),d(Ye.$$.fragment,e),d(Ve.$$.fragment,e),d(Xe.$$.fragment,e),d(Ze.$$.fragment,e),d(et.$$.fragment,e),d(Y),d(at.$$.fragment,e),d(X),d(ee),d(se),d(ot.$$.fragment,e),d(ae),d(rt.$$.fragment,e),d(it.$$.fragment,e),d(Ce.$$.fragment,e),d(lt.$$.fragment,e),d(pt.$$.fragment,e),d(mt.$$.fragment,e),d(ze.$$.fragment,e),d(ht.$$.fragment,e),d(ct.$$.fragment,e),d(ut.$$.fragment,e),d(ft.$$.fragment,e),d(dt.$$.fragment,e),d(_t.$$.fragment,e),d(wt.$$.fragment,e),d(bt.$$.fragment,e),d(gt.$$.fragment,e),d(kt.$$.fragment,e),d($t.$$.fragment,e),d(vt.$$.fragment,e),d(xt.$$.fragment,e),d(yt.$$.fragment,e),d(re),d(jt.$$.fragment,e),d(le),d(me),d(ce),d(fe),d(qt.$$.fragment,e),d(Me.$$.fragment,e),d(Et.$$.fragment,e),d(Tt.$$.fragment,e),d(Ue.$$.fragment,e),Ba=!1},d(e){s(n),e&&s(p),v(t,e),e&&s(w),e&&s(b),v(S),e&&s(D),Pt[z].d(e),e&&s(q),e&&s(T),e&&s(qn),St[R].d(e),e&&s(Gt),e&&s(je),v(Ke),e&&s(En),e&&s(_e),e&&s(Tn),v(Ye,e),e&&s(Pn),v(Ve,e),e&&s(Sn),e&&s(Qt),e&&s(Cn),v(Xe,e),e&&s(zn),v(Ze,e),e&&s(Nn),e&&s(Rt),e&&s(An),e&&s(qe),v(et),e&&s(Dn),e&&s(W),e&&s(Ln),Ct[B].d(e),e&&s(Bt),e&&s(Yt),e&&s(On),e&&s(st),e&&s(Fn),e&&s(Vt),e&&s(Mn),v(at,e),e&&s(Wn),zt[V].d(e),e&&s(Xt),e&&s(U),e&&s(Un),e&&s(Se),e&&s(In),Nt[Z].d(e),e&&s(Zt),e&&s(es),e&&s(Hn),At[te].d(e),e&&s(ts),e&&s(F),e&&s(Gn),e&&s(O),e&&s(Qn),e&&s(we),e&&s(Bn),e&&s(ss),e&&s(Yn),v(ot,e),e&&s(Vn),Dt[ne].d(e),e&&s(ns),e&&s(M),e&&s(Xn),v(rt,e),e&&s(Zn),e&&s(as),e&&s(ea),v(it,e),e&&s(ta),v(Ce,e),e&&s(sa),e&&s(be),e&&s(na),v(lt,e),e&&s(aa),e&&s(os),e&&s(oa),v(pt,e),e&&s(ra),v(mt,e),e&&s(ia),e&&s(rs),e&&s(la),v(ze,e),e&&s(pa),e&&s(Ee),v(ht),e&&s(ma),e&&s(Ae),e&&s(ha),v(ct,e),e&&s(ca),v(ut,e),e&&s(ua),e&&s(De),e&&s(fa),v(ft,e),e&&s(da),v(dt,e),e&&s(_a),e&&s(Le),e&&s(wa),e&&s(ge),e&&s(ba),v(_t,e),e&&s(ga),v(wt,e),e&&s(ka),e&&s(Oe),e&&s($a),e&&s(is),e&&s(va),v(bt,e),e&&s(xa),v(gt,e),e&&s(ya),e&&s(Fe),e&&s(ja),v(kt,e),e&&s(qa),v($t,e),e&&s(Ea),e&&s(ls),e&&s(Ta),v(vt,e),e&&s(Pa),e&&s(ps),e&&s(Sa),v(xt,e),e&&s(Ca),e&&s(ms),e&&s(za),e&&s(I),e&&s(Na),v(yt,e),e&&s(Aa),e&&s(H),e&&s(Da),Lt[oe].d(e),e&&s(hs),e&&s(cs),e&&s(La),v(jt,e),e&&s(Oa),Ot[ie].d(e),e&&s(us),e&&s(fs),e&&s(Fa),Ft[pe].d(e),e&&s(ds),e&&s(_s),e&&s(Ma),Mt[he].d(e),e&&s(ws),e&&s(bs),e&&s(Wa),Wt[ue].d(e),e&&s(gs),v(qt,e),e&&s(Ua),e&&s(ks),e&&s(Ia),v(Me,e),e&&s(Ha),e&&s(We),e&&s(Ga),v(Et,e),e&&s(Ja),v(Tt,e),e&&s(Qa),e&&s($s),e&&s(Ra),v(Ue,e),e&&s(Ka),e&&s(vs)}}}const Jp={local:"fast-tokenizers-in-the-qa-pipeline",sections:[{local:"using-the-questionanswering-pipeline",title:"Using the `question-answering` pipeline"},{local:"using-a-model-for-question-answering",title:"Using a model for question answering"},{local:"handling-long-contexts",title:"Handling long contexts"}],title:"Fast tokenizers in the QA pipeline"};function Qp(x,n,p){let t="pt";return up(()=>{const w=new URL(document.location).searchParams;p(0,t=w.get("fw")||"pt")}),[t]}class em extends lp{constructor(n){super();pp(this,n,Qp,Gp,mp,{})}}export{em as default,Jp as metadata};
