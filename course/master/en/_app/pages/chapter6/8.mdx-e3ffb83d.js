import{S as Dw,i as Cw,s as Ow,e as a,k as c,w as f,t as n,M as Lw,c as i,d as s,m as h,a as l,x as d,h as o,b as $,N as Sw,F as t,g as p,y as u,q as m,o as k,B as w,v as Bw,O as Aw}from"../../chunks/vendor-e7c81d8a.js";import{T as Nw}from"../../chunks/Tip-989931f5.js";import{Y as Ww}from"../../chunks/Youtube-365ea064.js";import{I as ta}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";import{C as _}from"../../chunks/CodeBlock-105940ae.js";import{D as Fw}from"../../chunks/DocNotebookDropdown-928568b4.js";const{document:Pw}=Aw;function Iw(sa){let z,ie,A,F,Z,Q,pt,ee,ct,qe,N,Pe,le,ve,De,y,te,ht,ft,se,dt,ut,ne,mt,kt;return{c(){z=a("p"),ie=a("strong"),A=n("To go further"),F=n(" If you test the two versions of the previous normalizers on a string containing the unicode character "),Z=a("code"),Q=n('u"\\u0085"'),pt=n(` you will surely notice that these two normalizers are not exactly equivalent.
To not over-complicate the version with `),ee=a("code"),ct=n("normalizers.Sequence"),qe=n(" too much , we haven\u2019t included the Regex replacements that the "),N=a("code"),Pe=n("BertNormalizer"),le=n(" requires when the "),ve=a("code"),De=n("clean_text"),y=n(" argument is set to "),te=a("code"),ht=n("True"),ft=n(" - which is the default behavior. But don\u2019t worry: it is possible to get exactly the same normalization without using the handy "),se=a("code"),dt=n("BertNormalizer"),ut=n(" by adding two "),ne=a("code"),mt=n("normalizers.Replace"),kt=n("\u2019s to the normalizers sequence.")},l(oe){z=i(oe,"P",{});var g=l(z);ie=i(g,"STRONG",{});var wt=l(ie);A=o(wt,"To go further"),wt.forEach(s),F=o(g," If you test the two versions of the previous normalizers on a string containing the unicode character "),Z=i(g,"CODE",{});var Ee=l(Z);Q=o(Ee,'u"\\u0085"'),Ee.forEach(s),pt=o(g,` you will surely notice that these two normalizers are not exactly equivalent.
To not over-complicate the version with `),ee=i(g,"CODE",{});var Zs=l(ee);ct=o(Zs,"normalizers.Sequence"),Zs.forEach(s),qe=o(g," too much , we haven\u2019t included the Regex replacements that the "),N=i(g,"CODE",{});var _t=l(N);Pe=o(_t,"BertNormalizer"),_t.forEach(s),le=o(g," requires when the "),ve=i(g,"CODE",{});var re=l(ve);De=o(re,"clean_text"),re.forEach(s),y=o(g," argument is set to "),te=i(g,"CODE",{});var Fn=l(te);ht=o(Fn,"True"),Fn.forEach(s),ft=o(g," - which is the default behavior. But don\u2019t worry: it is possible to get exactly the same normalization without using the handy "),se=i(g,"CODE",{});var $t=l(se);dt=o($t,"BertNormalizer"),$t.forEach(s),ut=o(g," by adding two "),ne=i(g,"CODE",{});var ae=l(ne);mt=o(ae,"normalizers.Replace"),ae.forEach(s),kt=o(g,"\u2019s to the normalizers sequence."),g.forEach(s)},m(oe,g){p(oe,z,g),t(z,ie),t(ie,A),t(z,F),t(z,Z),t(Z,Q),t(z,pt),t(z,ee),t(ee,ct),t(z,qe),t(z,N),t(N,Pe),t(z,le),t(z,ve),t(ve,De),t(z,y),t(z,te),t(te,ht),t(z,ft),t(z,se),t(se,dt),t(z,ut),t(z,ne),t(ne,mt),t(z,kt)},d(oe){oe&&s(z)}}}function Rw(sa){let z,ie,A,F,Z,Q,pt,ee,ct,qe,N,Pe,le,ve,De,y,te,ht,ft,se,dt,ut,ne,mt,kt,oe,g,wt,Ee,Zs,_t,re,Fn,$t,ae,cp,Qs,hp,fp,na,zt,oa,Ce,dp,In,up,mp,ra,T,pe,Rn,kp,wp,Un,_p,$p,bt,zp,bp,gp,ce,Gn,vp,Ep,Mn,jp,xp,gt,yp,Tp,qp,q,Xn,Pp,Dp,Kn,Cp,Op,Yn,Lp,Sp,Hn,Bp,Ap,Jn,Np,Wp,vt,Fp,Ip,Rp,he,Vn,Up,Gp,Zn,Mp,Xp,Et,Kp,Yp,Hp,fe,Qn,Jp,Vp,eo,Zp,Qp,jt,ec,tc,sc,de,to,nc,oc,so,rc,ac,xt,ic,lc,aa,Oe,pc,yt,cc,hc,ia,je,Le,no,Tt,fc,oo,dc,la,ue,uc,en,mc,kc,qt,wc,_c,pa,Pt,ca,Se,$c,ro,zc,bc,ha,tn,gc,fa,Dt,da,sn,vc,ua,xe,Be,ao,Ct,Ec,io,jc,ma,E,xc,lo,yc,Tc,po,qc,Pc,co,Dc,Cc,ho,Oc,Lc,fo,Sc,Bc,uo,Ac,Nc,ka,Ae,Wc,mo,Fc,Ic,wa,Ot,_a,I,Rc,ko,Uc,Gc,wo,Mc,Xc,_o,Kc,Yc,$a,j,Hc,$o,Jc,Vc,zo,Zc,Qc,bo,eh,th,go,sh,nh,vo,oh,rh,Eo,ah,ih,za,Lt,ba,R,lh,jo,ph,ch,xo,hh,fh,yo,dh,uh,ga,St,va,me,mh,To,kh,wh,qo,_h,$h,Ea,ke,zh,Po,bh,gh,Do,vh,Eh,ja,Bt,xa,At,ya,Ne,Ta,We,jh,Co,xh,yh,qa,Nt,Pa,nn,Th,Da,Wt,Ca,Fe,qh,Oo,Ph,Dh,Oa,Ft,La,It,Sa,Ie,Ch,Lo,Oh,Lh,Ba,Rt,Aa,Ut,Na,Re,Sh,So,Bh,Ah,Wa,Gt,Fa,Mt,Ia,Ue,Nh,Bo,Wh,Fh,Ra,Xt,Ua,P,Ih,Ao,Rh,Uh,No,Gh,Mh,Wo,Xh,Kh,Fo,Yh,Hh,Io,Jh,Vh,Ga,on,Zh,Ma,Kt,Xa,Ge,Qh,Ro,ef,tf,Ka,Yt,Ya,Me,sf,Uo,nf,of,Ha,Ht,Ja,Jt,Va,b,rf,Go,af,lf,Mo,pf,cf,Xo,hf,ff,Ko,df,uf,Yo,mf,kf,Ho,wf,_f,Jo,$f,zf,Vo,bf,gf,Zo,vf,Ef,Za,D,jf,Qo,xf,yf,er,Tf,qf,tr,Pf,Df,sr,Cf,Of,nr,Lf,Sf,Qa,Vt,ei,Zt,ti,U,Bf,or,Af,Nf,rr,Wf,Ff,ar,If,Rf,si,rn,Uf,ni,Qt,oi,an,Gf,ri,ln,Mf,ai,es,ii,ts,li,pn,Xf,pi,ss,ci,ns,hi,cn,Kf,fi,os,di,Xe,Yf,ir,Hf,Jf,ui,rs,mi,as,ki,hn,Vf,wi,is,_i,we,Zf,lr,Qf,ed,pr,td,sd,$i,ls,zi,_e,nd,cr,od,rd,hr,ad,id,bi,C,ld,fr,pd,cd,dr,hd,fd,ur,dd,ud,mr,md,kd,kr,wd,_d,gi,ps,vi,Ke,$d,wr,zd,bd,Ei,cs,ji,$e,gd,_r,vd,Ed,$r,jd,xd,xi,fn,yd,yi,ye,Ye,zr,hs,Td,br,qd,Ti,He,Pd,gr,Dd,Cd,qi,fs,Pi,G,Od,vr,Ld,Sd,Er,Bd,Ad,jr,Nd,Wd,Di,dn,Fd,Ci,ds,Oi,Je,Id,xr,Rd,Ud,Li,us,Si,ms,Bi,un,Gd,Ai,ks,Ni,x,Md,yr,Xd,Kd,Tr,Yd,Hd,qr,Jd,Vd,Pr,Zd,Qd,Dr,eu,tu,Cr,su,nu,Wi,mn,ou,Fi,ws,Ii,kn,ru,Ri,_s,Ui,$s,Gi,wn,au,Mi,zs,Xi,ze,iu,Or,lu,pu,Lr,cu,hu,Ki,bs,Yi,gs,Hi,_n,fu,Ji,vs,Vi,$n,du,Zi,Es,Qi,js,el,be,uu,Sr,mu,ku,Br,wu,_u,tl,xs,sl,zn,$u,nl,ys,ol,bn,zu,rl,Te,Ve,Ar,Ts,bu,Nr,gu,al,Ze,vu,Wr,Eu,ju,il,qs,ll,gn,xu,pl,vn,yu,cl,Ps,hl,M,Tu,Fr,qu,Pu,Ir,Du,Cu,Rr,Ou,Lu,fl,Qe,Su,Ur,Bu,Au,dl,Ds,ul,En,Nu,ml,Cs,kl,Os,wl,jn,Wu,_l,Ls,$l,O,Fu,Gr,Iu,Ru,Mr,Uu,Gu,Xr,Mu,Xu,Kr,Ku,Yu,zl,xn,Hu,bl,Ss,gl,yn,Ju,vl,Bs,El,As,jl,X,Vu,Yr,Zu,Qu,Hr,em,tm,Jr,sm,nm,xl,Ns,yl,Ws,Tl,Tn,om,ql,Fs,Pl,qn,rm,Dl,Is,Cl,Rs,Ol,et,am,Vr,im,lm,Ll,Us,Sl,K,pm,Zr,cm,hm,Qr,fm,dm,ea,um,mm,Bl,Gs,Al,Pn,km,Nl,Ms,Wl,Dn,wm,Fl;return Q=new ta({}),N=new Fw({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"}]}}),zt=new Ww({props:{id:"MR8tZm5ViWU"}}),Tt=new ta({}),Pt=new _({props:{codee:`from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"],`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, name=<span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_training_corpus</span>():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">1000</span>):
        <span class="hljs-keyword">yield</span> dataset[i : i + <span class="hljs-number">1000</span>][<span class="hljs-string">&quot;text&quot;</span>]`}}),Dt=new _({props:{codee:`with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\\n"),`,highlighted:`<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset)):
        f.write(dataset[i][<span class="hljs-string">&quot;text&quot;</span>] + <span class="hljs-string">&quot;\\n&quot;</span>)`}}),Ct=new ta({}),Ot=new _({props:{codee:`from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]")),`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),Lt=new _({props:{codee:"tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True),",highlighted:'tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="hljs-literal">True</span>)'}}),St=new _({props:{codee:`tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
),`,highlighted:`tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)`}}),Bt=new _({props:{codee:'print(tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?")),',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),At=new _({props:{codee:"hello how are u?,",highlighted:"hello how are u?"}}),Ne=new Nw({props:{$$slots:{default:[Iw]},$$scope:{ctx:sa}}}),Nt=new _({props:{codee:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer(),",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"}}),Wt=new _({props:{codee:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace(),",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"}}),Ft=new _({props:{codee:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer."),`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)'}}),It=new _({props:{codee:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))],`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Rt=new _({props:{codee:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer."),`,highlighted:`pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),Ut=new _({props:{codee:`[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))],`,highlighted:'[(<span class="hljs-string">&quot;Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">28</span>))]'}}),Gt=new _({props:{codee:`pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer."),`,highlighted:`pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)`}}),Mt=new _({props:{codee:`[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))],`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), (<span class="hljs-string">&#x27;s&#x27;</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;test&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;my&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">13</span>)), (<span class="hljs-string">&#x27;pre&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">17</span>)),
 (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">17</span>, <span class="hljs-number">18</span>)), (<span class="hljs-string">&#x27;tokenizer&#x27;</span>, (<span class="hljs-number">18</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;.&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),Xt=new _({props:{codee:`special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens),`,highlighted:`special_tokens = [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
trainer = trainers.WordPieceTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens)`}}),Kt=new _({props:{codee:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer),",highlighted:"tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"}}),Yt=new _({props:{codee:`tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer),`,highlighted:`tokenizer.model = models.WordPiece(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>)
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Ht=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),Jt=new _({props:{codee:`['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.'],`,highlighted:'[<span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Vt=new _({props:{codee:`cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id),`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Zt=new _({props:{codee:"(2, 3),",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)'}}),Qt=new _({props:{codee:`tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
),`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,
    pair=<span class="hljs-string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="hljs-string">&quot;[SEP]&quot;</span>, sep_token_id)],
)`}}),es=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),ts=new _({props:{codee:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]'],`,highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),ss=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),ns=new _({props:{codee:`['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;tok&#x27;</span>, <span class="hljs-string">&#x27;##eni&#x27;</span>, <span class="hljs-string">&#x27;##zer&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;pair&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;sentences&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),os=new _({props:{codee:'tokenizer.decoder = decoders.WordPiece(prefix="##"),',highlighted:'tokenizer.decoder = decoders.WordPiece(prefix=<span class="hljs-string">&quot;##&quot;</span>)'}}),rs=new _({props:{codee:"tokenizer.decode(encoding.ids),",highlighted:"tokenizer.decode(encoding.ids)"}}),as=new _({props:{codee:`"let's test this tokenizer... on a pair of sentences.",`,highlighted:'<span class="hljs-string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span>'}}),is=new _({props:{codee:'tokenizer.save("tokenizer.json"),',highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),ls=new _({props:{codee:'new_tokenizer = Tokenizer.from_file("tokenizer.json"),',highlighted:'new_tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),ps=new _({props:{codee:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    <span class="hljs-comment"># tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively</span>
    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,
    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,
    cls_token=<span class="hljs-string">&quot;[CLS]&quot;</span>,
    sep_token=<span class="hljs-string">&quot;[SEP]&quot;</span>,
    mask_token=<span class="hljs-string">&quot;[MASK]&quot;</span>,
)`}}),cs=new _({props:{codee:`from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)`}}),hs=new ta({}),fs=new _({props:{codee:"tokenizer = Tokenizer(models.BPE()),",highlighted:"tokenizer = Tokenizer(models.BPE())"}}),ds=new _({props:{codee:"tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False),",highlighted:'tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>)'}}),us=new _({props:{codee:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!"),`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)'}}),ms=new _({props:{codee:`[('Let', (0, 3)), ("'s", (3, 5)), ('\u0120test', (5, 10)), ('\u0120pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))],`,highlighted:`[(<span class="hljs-string">&#x27;Let&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">3</span>)), (<span class="hljs-string">&quot;&#x27;s&quot;</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u0120test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120pre&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;-&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
 (<span class="hljs-string">&#x27;tokenization&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">27</span>)), (<span class="hljs-string">&#x27;!&#x27;</span>, (<span class="hljs-number">27</span>, <span class="hljs-number">28</span>))]`}}),ks=new _({props:{codee:`trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer),`,highlighted:`trainer = trainers.BpeTrainer(vocab_size=<span class="hljs-number">25000</span>, special_tokens=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),ws=new _({props:{codee:`tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer),`,highlighted:`tokenizer.model = models.BPE()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),_s=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),$s=new _({props:{codee:`['L', 'et', "'", 's', '\u0120test', '\u0120this', '\u0120to', 'ken', 'izer', '.'],`,highlighted:'[<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-string">&#x27;et&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u0120test&#x27;</span>, <span class="hljs-string">&#x27;\u0120this&#x27;</span>, <span class="hljs-string">&#x27;\u0120to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),zs=new _({props:{codee:"tokenizer.post_processor = processors.ByteLevel(trim_offsets=False),",highlighted:'tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="hljs-literal">False</span>)'}}),bs=new _({props:{codee:`sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end],`,highlighted:`sentence = <span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[<span class="hljs-number">4</span>]
sentence[start:end]`}}),gs=new _({props:{codee:"' test',",highlighted:'<span class="hljs-string">&#x27; test&#x27;</span>'}}),vs=new _({props:{codee:"tokenizer.decoder = decoders.ByteLevel(),",highlighted:"tokenizer.decoder = decoders.ByteLevel()"}}),Es=new _({props:{codee:"tokenizer.decode(encoding.ids),",highlighted:"tokenizer.decode(encoding.ids)"}}),js=new _({props:{codee:`"Let's test this tokenizer.",`,highlighted:'<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>'}}),xs=new _({props:{codee:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>,
)`}}),ys=new _({props:{codee:`from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)`}}),Ts=new ta({}),qs=new _({props:{codee:"tokenizer = Tokenizer(models.Unigram()),",highlighted:"tokenizer = Tokenizer(models.Unigram())"}}),Ps=new _({props:{codee:`from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("\`\`", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
),`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Regex

tokenizer.normalizer = normalizers.<span class="hljs-type">Sequence</span>(
    [
        normalizers.Replace(<span class="hljs-string">&quot;\`\`&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.Replace(<span class="hljs-string">&quot;&#x27;&#x27;&quot;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(<span class="hljs-string">&quot; {2,}&quot;</span>), <span class="hljs-string">&quot; &quot;</span>),
    ]
)`}}),Ds=new _({props:{codee:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(),",highlighted:"tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"}}),Cs=new _({props:{codee:`tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!"),`,highlighted:'tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)'}}),Os=new _({props:{codee:`[("\u2581Let's", (0, 5)), ('\u2581test', (5, 10)), ('\u2581the', (10, 14)), ('\u2581pre-tokenizer!', (14, 29))],`,highlighted:'[(<span class="hljs-string">&quot;\u2581Let&#x27;s&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;\u2581test&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581the&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581pre-tokenizer!&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">29</span>))]'}}),Ls=new _({props:{codee:`special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer),`,highlighted:`special_tokens = [<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>]
trainer = trainers.UnigramTrainer(
    vocab_size=<span class="hljs-number">25000</span>, special_tokens=special_tokens, unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`}}),Ss=new _({props:{codee:`tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer),`,highlighted:`tokenizer.model = models.Unigram()
tokenizer.train([<span class="hljs-string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)`}}),Bs=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)`}}),As=new _({props:{codee:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.'],`,highlighted:'[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),Ns=new _({props:{codee:`cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id),`,highlighted:`cls_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>)
sep_token_id = tokenizer.token_to_id(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>)
<span class="hljs-built_in">print</span>(cls_token_id, sep_token_id)`}}),Ws=new _({props:{codee:"0 1,",highlighted:'<span class="hljs-number">0</span> <span class="hljs-number">1</span>'}}),Fs=new _({props:{codee:`tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
),`,highlighted:`tokenizer.post_processor = processors.TemplateProcessing(
    single=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,
    pair=<span class="hljs-string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],
)`}}),Is=new _({props:{codee:`encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids),`,highlighted:`encoding = tokenizer.encode(<span class="hljs-string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="hljs-string">&quot;on a pair of sentences!&quot;</span>)
<span class="hljs-built_in">print</span>(encoding.tokens)
<span class="hljs-built_in">print</span>(encoding.type_ids)`}}),Rs=new _({props:{codee:`['\u2581Let', "'", 's', '\u2581test', '\u2581this', '\u2581to', 'ken', 'izer', '.', '.', '.', '<sep>', '\u2581', 'on', '\u2581', 'a', '\u2581pair', 
  '\u2581of', '\u2581sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],`,highlighted:`[<span class="hljs-string">&#x27;\u2581Let&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;\u2581test&#x27;</span>, <span class="hljs-string">&#x27;\u2581this&#x27;</span>, <span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-string">&#x27;ken&#x27;</span>, <span class="hljs-string">&#x27;izer&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;\u2581pair&#x27;</span>, 
  <span class="hljs-string">&#x27;\u2581of&#x27;</span>, <span class="hljs-string">&#x27;\u2581sentence&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;cls&gt;&#x27;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]`}}),Us=new _({props:{codee:"tokenizer.decoder = decoders.Metaspace(),",highlighted:"tokenizer.decoder = decoders.Metaspace()"}}),Gs=new _({props:{codee:`from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>,
    eos_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>,
    unk_token=<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>,
    pad_token=<span class="hljs-string">&quot;&lt;pad&gt;&quot;</span>,
    cls_token=<span class="hljs-string">&quot;&lt;cls&gt;&quot;</span>,
    sep_token=<span class="hljs-string">&quot;&lt;sep&gt;&quot;</span>,
    mask_token=<span class="hljs-string">&quot;&lt;mask&gt;&quot;</span>,
    padding_side=<span class="hljs-string">&quot;left&quot;</span>,
)`}}),Ms=new _({props:{codee:`from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)`}}),{c(){z=a("meta"),ie=c(),A=a("h1"),F=a("a"),Z=a("span"),f(Q.$$.fragment),pt=c(),ee=a("span"),ct=n("Building a tokenizer, block by block"),qe=c(),f(N.$$.fragment),Pe=c(),le=a("p"),ve=n("As we\u2019ve seen in the previous sections, tokenization comprises several steps:"),De=c(),y=a("ul"),te=a("li"),ht=n("Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)"),ft=c(),se=a("li"),dt=n("Pre-tokenization (splitting the input into words)"),ut=c(),ne=a("li"),mt=n("Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)"),kt=c(),oe=a("li"),g=n("Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)"),wt=c(),Ee=a("p"),Zs=n("As a reminder, here\u2019s another look at the overall process:"),_t=c(),re=a("img"),$t=c(),ae=a("p"),cp=n("The \u{1F917} Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together. In this section we\u2019ll see how we can build a tokenizer from scratch, as opposed to training a new tokenizer from an old one as we did in "),Qs=a("a"),hp=n("section 2"),fp=n(". You\u2019ll then be able to build any kind of tokenizer you can think of!"),na=c(),f(zt.$$.fragment),oa=c(),Ce=a("p"),dp=n("More precisely, the library is built around a central "),In=a("code"),up=n("Tokenizer"),mp=n(" class with the building blocks regrouped in submodules:"),ra=c(),T=a("ul"),pe=a("li"),Rn=a("code"),kp=n("normalizers"),wp=n(" contains all the possible types of "),Un=a("code"),_p=n("Normalizer"),$p=n(" you can use (complete list "),bt=a("a"),zp=n("here"),bp=n(")."),gp=c(),ce=a("li"),Gn=a("code"),vp=n("pre_tokenizers"),Ep=n(" contains all the possible types of "),Mn=a("code"),jp=n("PreTokenizer"),xp=n(" you can use (complete list "),gt=a("a"),yp=n("here"),Tp=n(")."),qp=c(),q=a("li"),Xn=a("code"),Pp=n("models"),Dp=n(" contains the various types of "),Kn=a("code"),Cp=n("Model"),Op=n(" you can use, like "),Yn=a("code"),Lp=n("BPE"),Sp=n(", "),Hn=a("code"),Bp=n("WordPiece"),Ap=n(", and "),Jn=a("code"),Np=n("Unigram"),Wp=n(" (complete list "),vt=a("a"),Fp=n("here"),Ip=n(")."),Rp=c(),he=a("li"),Vn=a("code"),Up=n("trainers"),Gp=n(" contains all the different types of "),Zn=a("code"),Mp=n("Trainer"),Xp=n(" you can use to train your model on a corpus (one per type of model; complete list "),Et=a("a"),Kp=n("here"),Yp=n(")."),Hp=c(),fe=a("li"),Qn=a("code"),Jp=n("post_processors"),Vp=n(" contains the various types of "),eo=a("code"),Zp=n("PostProcessor"),Qp=n(" you can use (complete list "),jt=a("a"),ec=n("here"),tc=n(")."),sc=c(),de=a("li"),to=a("code"),nc=n("decoders"),oc=n(" contains the various types of "),so=a("code"),rc=n("Decoder"),ac=n(" you can use to decode the outputs of tokenization (complete list "),xt=a("a"),ic=n("here"),lc=n(")."),aa=c(),Oe=a("p"),pc=n("You can find the whole list of building blocks "),yt=a("a"),cc=n("here"),hc=n("."),ia=c(),je=a("h2"),Le=a("a"),no=a("span"),f(Tt.$$.fragment),fc=c(),oo=a("span"),dc=n("Acquiring a corpus"),la=c(),ue=a("p"),uc=n("To train our new tokenizer, we will use a small corpus of text (so the examples run fast). The steps for acquiring the corpus are similar to the ones we took at the "),en=a("a"),mc=n("beginning of this chapter"),kc=n(", but this time we\u2019ll use the "),qt=a("a"),wc=n("WikiText-2"),_c=n(" dataset:"),pa=c(),f(Pt.$$.fragment),ca=c(),Se=a("p"),$c=n("The function "),ro=a("code"),zc=n("get_training_corpus()"),bc=n(" is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer."),ha=c(),tn=a("p"),gc=n("\u{1F917} Tokenizers can also be trained on text files directly. Here\u2019s how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally:"),fa=c(),f(Dt.$$.fragment),da=c(),sn=a("p"),vc=n("Next we\u2019ll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Let\u2019s start with BERT!"),ua=c(),xe=a("h2"),Be=a("a"),ao=a("span"),f(Ct.$$.fragment),Ec=c(),io=a("span"),jc=n("Building a WordPiece tokenizer from scratch"),ma=c(),E=a("p"),xc=n("To build a tokenizer with the \u{1F917} Tokenizers library, we start by instantiating a "),lo=a("code"),yc=n("Tokenizer"),Tc=n(" object with a "),po=a("code"),qc=n("model"),Pc=n(", then set its "),co=a("code"),Dc=n("normalizer"),Cc=n(", "),ho=a("code"),Oc=n("pre_tokenizer"),Lc=n(", "),fo=a("code"),Sc=n("post_processor"),Bc=n(", and "),uo=a("code"),Ac=n("decoder"),Nc=n(" attributes to the values we want."),ka=c(),Ae=a("p"),Wc=n("For this example, we\u2019ll create a "),mo=a("code"),Fc=n("Tokenizer"),Ic=n(" with a WordPiece model:"),wa=c(),f(Ot.$$.fragment),_a=c(),I=a("p"),Rc=n("We have to specify the "),ko=a("code"),Uc=n("unk_token"),Gc=n(" so the model knows what to return when it encounters characters it hasn\u2019t seen before. Other arguments we can set here include the "),wo=a("code"),Mc=n("vocab"),Xc=n(" of our model (we\u2019re going to train the model, so we don\u2019t need to set this) and "),_o=a("code"),Kc=n("max_input_chars_per_word"),Yc=n(", which specifies a maximum length for each word (words longer than the value passed will be split)."),$a=c(),j=a("p"),Hc=n("The first step of tokenization is normalization, so let\u2019s begin with that. Since BERT is widely used, there is a "),$o=a("code"),Jc=n("BertNormalizer"),Vc=n(" with the classic options we can set for BERT: "),zo=a("code"),Zc=n("lowercase"),Qc=n(" and "),bo=a("code"),eh=n("strip_accents"),th=n(", which are self-explanatory; "),go=a("code"),sh=n("clean_text"),nh=n(" to remove all control characters and replace repeating spaces with a single one; and "),vo=a("code"),oh=n("handle_chinese_chars"),rh=n(", which places spaces around Chinese characters. To replicate the "),Eo=a("code"),ah=n("bert-base-uncased"),ih=n(" tokenizer, we can just set this normalizer:"),za=c(),f(Lt.$$.fragment),ba=c(),R=a("p"),lh=n("Generally speaking, however, when building a new tokenizer you won\u2019t have access to such a handy normalizer already implemented in the \u{1F917} Tokenizers library \u2014 so let\u2019s see how to create the BERT normalizer by hand. The library provides a "),jo=a("code"),ph=n("Lowercase"),ch=n(" normalizer and a "),xo=a("code"),hh=n("StripAccents"),fh=n(" normalizer, and you can compose several normalizers using a "),yo=a("code"),dh=n("Sequence"),uh=n(":"),ga=c(),f(St.$$.fragment),va=c(),me=a("p"),mh=n("We\u2019re also using an "),To=a("code"),kh=n("NFD"),wh=n(" Unicode normalizer, as otherwise the "),qo=a("code"),_h=n("StripAccents"),$h=n(" normalizer won\u2019t properly recognize the accented characters and thus won\u2019t strip them out."),Ea=c(),ke=a("p"),zh=n("As we\u2019ve seen before, we can use the "),Po=a("code"),bh=n("normalize_str()"),gh=n(" method of the "),Do=a("code"),vh=n("normalizer"),Eh=n(" to check out the effects it has on a given text:"),ja=c(),f(Bt.$$.fragment),xa=c(),f(At.$$.fragment),ya=c(),f(Ne.$$.fragment),Ta=c(),We=a("p"),jh=n("Next is the pre-tokenization step. Again, there is a prebuilt "),Co=a("code"),xh=n("BertPreTokenizer"),yh=n(" that we can use:"),qa=c(),f(Nt.$$.fragment),Pa=c(),nn=a("p"),Th=n("Or we can build it from scratch:"),Da=c(),f(Wt.$$.fragment),Ca=c(),Fe=a("p"),qh=n("Note that the "),Oo=a("code"),Ph=n("Whitespace"),Dh=n(" pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation:"),Oa=c(),f(Ft.$$.fragment),La=c(),f(It.$$.fragment),Sa=c(),Ie=a("p"),Ch=n("If you only want to split on whitespace, you should use the "),Lo=a("code"),Oh=n("WhitespaceSplit"),Lh=n(" pre-tokenizer instead:"),Ba=c(),f(Rt.$$.fragment),Aa=c(),f(Ut.$$.fragment),Na=c(),Re=a("p"),Sh=n("Like with normalizers, you can use a "),So=a("code"),Bh=n("Sequence"),Ah=n(" to compose several pre-tokenizers:"),Wa=c(),f(Gt.$$.fragment),Fa=c(),f(Mt.$$.fragment),Ia=c(),Ue=a("p"),Nh=n("The next step in the tokenization pipeline is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a "),Bo=a("code"),Wh=n("WordPieceTrainer"),Fh=n(". The main thing to remember when instantiating a trainer in \u{1F917} Tokenizers is that you need to pass it all the special tokens you intend to use \u2014 otherwise it won\u2019t add them to the vocabulary, since they are not in the training corpus:"),Ra=c(),f(Xt.$$.fragment),Ua=c(),P=a("p"),Ih=n("As well as specifying the "),Ao=a("code"),Rh=n("vocab_size"),Uh=n(" and "),No=a("code"),Gh=n("special_tokens"),Mh=n(", we can set the "),Wo=a("code"),Xh=n("min_frequency"),Kh=n(" (the number of times a token must appear to be included in the vocabulary) or change the "),Fo=a("code"),Yh=n("continuing_subword_prefix"),Hh=n(" (if we want to use something different from "),Io=a("code"),Jh=n("##"),Vh=n(")."),Ga=c(),on=a("p"),Zh=n("To train our model using the iterator we defined earlier, we just have to execute this command:"),Ma=c(),f(Kt.$$.fragment),Xa=c(),Ge=a("p"),Qh=n("We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with an empty "),Ro=a("code"),ef=n("WordPiece"),tf=n(" beforehand):"),Ka=c(),f(Yt.$$.fragment),Ya=c(),Me=a("p"),sf=n("In both cases, we can then test the tokenizer on a text by calling the "),Uo=a("code"),nf=n("encode()"),of=n(" method:"),Ha=c(),f(Ht.$$.fragment),Ja=c(),f(Jt.$$.fragment),Va=c(),b=a("p"),rf=n("The "),Go=a("code"),af=n("encoding"),lf=n(" obtained is an "),Mo=a("code"),pf=n("Encoding"),cf=n(", which contains all the necessary outputs of the tokenizer in its various attributes: "),Xo=a("code"),hf=n("ids"),ff=n(", "),Ko=a("code"),df=n("type_ids"),uf=n(", "),Yo=a("code"),mf=n("tokens"),kf=n(", "),Ho=a("code"),wf=n("offsets"),_f=n(", "),Jo=a("code"),$f=n("attention_mask"),zf=n(", "),Vo=a("code"),bf=n("special_tokens_mask"),gf=n(", and "),Zo=a("code"),vf=n("overflowing"),Ef=n("."),Za=c(),D=a("p"),jf=n("The last step in the tokenization pipeline is post-processing. We need to add the "),Qo=a("code"),xf=n("[CLS]"),yf=n(" token at the beginning and the "),er=a("code"),Tf=n("[SEP]"),qf=n(" token at the end (or after each sentence, if we have a pair of sentences). We will use a "),tr=a("code"),Pf=n("TemplateProcessor"),Df=n(" for this, but first we need to know the IDs of the "),sr=a("code"),Cf=n("[CLS]"),Of=n(" and "),nr=a("code"),Lf=n("[SEP]"),Sf=n(" tokens in the vocabulary:"),Qa=c(),f(Vt.$$.fragment),ei=c(),f(Zt.$$.fragment),ti=c(),U=a("p"),Bf=n("To write the template for the "),or=a("code"),Af=n("TemplateProcessor"),Nf=n(", we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by "),rr=a("code"),Wf=n("$A"),Ff=n(", while the second sentence (if encoding a pair) is represented by "),ar=a("code"),If=n("$B"),Rf=n(". For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon."),si=c(),rn=a("p"),Uf=n("The classic BERT template is thus defined as follows:"),ni=c(),f(Qt.$$.fragment),oi=c(),an=a("p"),Gf=n("Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs."),ri=c(),ln=a("p"),Mf=n("Once this is added, going back to our previous example will give:"),ai=c(),f(es.$$.fragment),ii=c(),f(ts.$$.fragment),li=c(),pn=a("p"),Xf=n("And on a pair of sentences, we get the proper result:"),pi=c(),f(ss.$$.fragment),ci=c(),f(ns.$$.fragment),hi=c(),cn=a("p"),Kf=n("We\u2019ve almost finished building this tokenizer from scratch \u2014 the last step is to include a decoder:"),fi=c(),f(os.$$.fragment),di=c(),Xe=a("p"),Yf=n("Let\u2019s test it on our previous "),ir=a("code"),Hf=n("encoding"),Jf=n(":"),ui=c(),f(rs.$$.fragment),mi=c(),f(as.$$.fragment),ki=c(),hn=a("p"),Vf=n("Great! We can save our tokenizer in a single JSON file like this:"),wi=c(),f(is.$$.fragment),_i=c(),we=a("p"),Zf=n("We can then reload that file in a "),lr=a("code"),Qf=n("Tokenizer"),ed=n(" object with the "),pr=a("code"),td=n("from_file()"),sd=n(" method:"),$i=c(),f(ls.$$.fragment),zi=c(),_e=a("p"),nd=n("To use this tokenizer in \u{1F917} Transformers, we have to wrap it in a "),cr=a("code"),od=n("PreTrainedTokenizerFast"),rd=n(". We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, "),hr=a("code"),ad=n("BertTokenizerFast"),id=n("). If you apply this lesson to build a brand new tokenizer, you will have to use the first option."),bi=c(),C=a("p"),ld=n("To wrap the tokenizer in a "),fr=a("code"),pd=n("PreTrainedTokenizerFast"),cd=n(", we can either pass the tokenizer we built as a "),dr=a("code"),hd=n("tokenizer_object"),fd=n(" or pass the tokenizer file we saved as "),ur=a("code"),dd=n("tokenizer_file"),ud=n(". The key thing to remember is that we have to manually set all the special tokens, since that class can\u2019t infer from the "),mr=a("code"),md=n("tokenizer"),kd=n(" object which token is the mask token, the "),kr=a("code"),wd=n("[CLS]"),_d=n(" token, etc.:"),gi=c(),f(ps.$$.fragment),vi=c(),Ke=a("p"),$d=n("If you are using a specific tokenizer class (like "),wr=a("code"),zd=n("BertTokenizerFast"),bd=n("), you will only need to specify the special tokens that are different from the default ones (here, none):"),Ei=c(),f(cs.$$.fragment),ji=c(),$e=a("p"),gd=n("You can then use this tokenizer like any other \u{1F917} Transformers tokenizer. You can save it with the "),_r=a("code"),vd=n("save_pretrained()"),Ed=n(" method, or upload it to the Hub with the "),$r=a("code"),jd=n("push_to_hub()"),xd=n(" method."),xi=c(),fn=a("p"),yd=n("Now that we\u2019ve seen how to build a WordPiece tokenizer, let\u2019s do the same for a BPE tokenizer. We\u2019ll go a bit faster since you know all the steps, and only highlight the differences."),yi=c(),ye=a("h2"),Ye=a("a"),zr=a("span"),f(hs.$$.fragment),Td=c(),br=a("span"),qd=n("Building a BPE tokenizer from scratch"),Ti=c(),He=a("p"),Pd=n("Let\u2019s now build a GPT-2 tokenizer. Like for the BERT tokenizer, we start by initializing a "),gr=a("code"),Dd=n("Tokenizer"),Cd=n(" with a BPE model:"),qi=c(),f(fs.$$.fragment),Pi=c(),G=a("p"),Od=n("Also like for BERT, we could initialize this model with a vocabulary if we had one (we would need to pass the "),vr=a("code"),Ld=n("vocab"),Sd=n(" and "),Er=a("code"),Bd=n("merges"),Ad=n(" in this case), but since we will train from scratch, we don\u2019t need to do that. We also don\u2019t need to specify an "),jr=a("code"),Nd=n("unk_token"),Wd=n(" because GPT-2 uses byte-level BPE, which doesn\u2019t require it."),Di=c(),dn=a("p"),Fd=n("GPT-2 does not use a normalizer, so we skip that step and go directly to the pre-tokenization:"),Ci=c(),f(ds.$$.fragment),Oi=c(),Je=a("p"),Id=n("The option we added to "),xr=a("code"),Rd=n("ByteLevel"),Ud=n(" here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before:"),Li=c(),f(us.$$.fragment),Si=c(),f(ms.$$.fragment),Bi=c(),un=a("p"),Gd=n("Next is the model, which needs training. For GPT-2, the only special token is the end-of-text token:"),Ai=c(),f(ks.$$.fragment),Ni=c(),x=a("p"),Md=n("Like with the "),yr=a("code"),Xd=n("WordPieceTrainer"),Kd=n(", as well as the "),Tr=a("code"),Yd=n("vocab_size"),Hd=n(" and "),qr=a("code"),Jd=n("special_tokens"),Vd=n(", we can specify the "),Pr=a("code"),Zd=n("min_frequency"),Qd=n(" if we want to, or if we have an end-of-word suffix (like "),Dr=a("code"),eu=n("</w>"),tu=n("), we can set it with "),Cr=a("code"),su=n("end_of_word_suffix"),nu=n("."),Wi=c(),mn=a("p"),ou=n("This tokenizer can also be trained on text files:"),Fi=c(),f(ws.$$.fragment),Ii=c(),kn=a("p"),ru=n("Let\u2019s have a look at the tokenization of a sample text:"),Ri=c(),f(_s.$$.fragment),Ui=c(),f($s.$$.fragment),Gi=c(),wn=a("p"),au=n("We apply the byte-level post-processing for the GPT-2 tokenizer as follows:"),Mi=c(),f(zs.$$.fragment),Xi=c(),ze=a("p"),iu=n("The "),Or=a("code"),lu=n("trim_offsets = False"),pu=n(" option indicates to the post-processor that we should leave the offsets of tokens that begin with \u2018\u0120\u2019 as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let\u2019s have a look at the result with the text we just encoded, where "),Lr=a("code"),cu=n("'\u0120test'"),hu=n(" is the token at index 4:"),Ki=c(),f(bs.$$.fragment),Yi=c(),f(gs.$$.fragment),Hi=c(),_n=a("p"),fu=n("Finally, we add a byte-level decoder:"),Ji=c(),f(vs.$$.fragment),Vi=c(),$n=a("p"),du=n("and we can double-check it works properly:"),Zi=c(),f(Es.$$.fragment),Qi=c(),f(js.$$.fragment),el=c(),be=a("p"),uu=n("Great! Now that we\u2019re done, we can save the tokenizer like before, and wrap it in a "),Sr=a("code"),mu=n("PreTrainedTokenizerFast"),ku=n(" or "),Br=a("code"),wu=n("GPT2TokenizerFast"),_u=n(" if we want to use it in \u{1F917} Transformers:"),tl=c(),f(xs.$$.fragment),sl=c(),zn=a("p"),$u=n("or:"),nl=c(),f(ys.$$.fragment),ol=c(),bn=a("p"),zu=n("As the last example, we\u2019ll show you how to build a Unigram tokenizer from scratch."),rl=c(),Te=a("h2"),Ve=a("a"),Ar=a("span"),f(Ts.$$.fragment),bu=c(),Nr=a("span"),gu=n("Building a Unigram tokenizer from scratch"),al=c(),Ze=a("p"),vu=n("Let\u2019s now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a "),Wr=a("code"),Eu=n("Tokenizer"),ju=n(" with a Unigram model:"),il=c(),f(qs.$$.fragment),ll=c(),gn=a("p"),xu=n("Again, we could initialize this model with a vocabulary if we had one."),pl=c(),vn=a("p"),yu=n("For the normalization, XLNet uses a few replacements (which come from SentencePiece):"),cl=c(),f(Ps.$$.fragment),hl=c(),M=a("p"),Tu=n("This replaces "),Fr=a("code"),qu=n("\u201C"),Pu=n(" and "),Ir=a("code"),Du=n("\u201D"),Cu=n(" with "),Rr=a("code"),Ou=n("\u201D"),Lu=n(" and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize."),fl=c(),Qe=a("p"),Su=n("The pre-tokenizer to use for any SentencePiece tokenizer is "),Ur=a("code"),Bu=n("Metaspace"),Au=n(":"),dl=c(),f(Ds.$$.fragment),ul=c(),En=a("p"),Nu=n("We can have a look at the pre-tokenization of an example text like before:"),ml=c(),f(Cs.$$.fragment),kl=c(),f(Os.$$.fragment),wl=c(),jn=a("p"),Wu=n("Next is the model, which needs training. XLNet has quite a few special tokens:"),_l=c(),f(Ls.$$.fragment),$l=c(),O=a("p"),Fu=n("A very important argument not to forget for the "),Gr=a("code"),Iu=n("UnigramTrainer"),Ru=n(" is the "),Mr=a("code"),Uu=n("unk_token"),Gu=n(". We can also pass along other arguments specific to the Unigram algorithm, such as the "),Xr=a("code"),Mu=n("shrinking_factor"),Xu=n(" for each step where we remove tokens (defaults to 0.75) or the "),Kr=a("code"),Ku=n("max_piece_length"),Yu=n(" to specify the maximum length of a given token (defaults to 16)."),zl=c(),xn=a("p"),Hu=n("This tokenizer can also be trained on text files:"),bl=c(),f(Ss.$$.fragment),gl=c(),yn=a("p"),Ju=n("Let\u2019s have a look at the tokenization of a sample text:"),vl=c(),f(Bs.$$.fragment),El=c(),f(As.$$.fragment),jl=c(),X=a("p"),Vu=n("A peculiarity of XLNet is that it puts the "),Yr=a("code"),Zu=n("<cls>"),Qu=n(" token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It\u2019s padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the "),Hr=a("code"),em=n("<cls>"),tm=n(" and "),Jr=a("code"),sm=n("<sep>"),nm=n(" tokens:"),xl=c(),f(Ns.$$.fragment),yl=c(),f(Ws.$$.fragment),Tl=c(),Tn=a("p"),om=n("The template looks like this:"),ql=c(),f(Fs.$$.fragment),Pl=c(),qn=a("p"),rm=n("And we can test it works by encoding a pair of sentences:"),Dl=c(),f(Is.$$.fragment),Cl=c(),f(Rs.$$.fragment),Ol=c(),et=a("p"),am=n("Finally, we add a "),Vr=a("code"),im=n("Metaspace"),lm=n(" decoder:"),Ll=c(),f(Us.$$.fragment),Sl=c(),K=a("p"),pm=n("and we\u2019re done with this tokenizer! We can save the tokenizer like before, and wrap it in a "),Zr=a("code"),cm=n("PreTrainedTokenizerFast"),hm=n(" or "),Qr=a("code"),fm=n("XLNetTokenizerFast"),dm=n(" if we want to use it in \u{1F917} Transformers. One thing to note when using "),ea=a("code"),um=n("PreTrainedTokenizerFast"),mm=n(" is that on top of the special tokens, we need to tell the \u{1F917} Transformers library to pad on the left:"),Bl=c(),f(Gs.$$.fragment),Al=c(),Pn=a("p"),km=n("Or alternatively:"),Nl=c(),f(Ms.$$.fragment),Wl=c(),Dn=a("p"),wm=n("Now that you have seen how the various building blocks are used to build existing tokenizers, you should be able to write any tokenizer you want with the \u{1F917} Tokenizers library and be able to use it in \u{1F917} Transformers."),this.h()},l(e){const r=Lw('[data-svelte="svelte-1phssyn"]',Pw.head);z=i(r,"META",{name:!0,content:!0}),r.forEach(s),ie=h(e),A=i(e,"H1",{class:!0});var Xs=l(A);F=i(Xs,"A",{id:!0,class:!0,href:!0});var _m=l(F);Z=i(_m,"SPAN",{});var $m=l(Z);d(Q.$$.fragment,$m),$m.forEach(s),_m.forEach(s),pt=h(Xs),ee=i(Xs,"SPAN",{});var zm=l(ee);ct=o(zm,"Building a tokenizer, block by block"),zm.forEach(s),Xs.forEach(s),qe=h(e),d(N.$$.fragment,e),Pe=h(e),le=i(e,"P",{});var bm=l(le);ve=o(bm,"As we\u2019ve seen in the previous sections, tokenization comprises several steps:"),bm.forEach(s),De=h(e),y=i(e,"UL",{});var tt=l(y);te=i(tt,"LI",{});var gm=l(te);ht=o(gm,"Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)"),gm.forEach(s),ft=h(tt),se=i(tt,"LI",{});var vm=l(se);dt=o(vm,"Pre-tokenization (splitting the input into words)"),vm.forEach(s),ut=h(tt),ne=i(tt,"LI",{});var Em=l(ne);mt=o(Em,"Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)"),Em.forEach(s),kt=h(tt),oe=i(tt,"LI",{});var jm=l(oe);g=o(jm,"Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)"),jm.forEach(s),tt.forEach(s),wt=h(e),Ee=i(e,"P",{});var xm=l(Ee);Zs=o(xm,"As a reminder, here\u2019s another look at the overall process:"),xm.forEach(s),_t=h(e),re=i(e,"IMG",{src:!0,alt:!0,width:!0}),$t=h(e),ae=i(e,"P",{});var Il=l(ae);cp=o(Il,"The \u{1F917} Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together. In this section we\u2019ll see how we can build a tokenizer from scratch, as opposed to training a new tokenizer from an old one as we did in "),Qs=i(Il,"A",{href:!0});var ym=l(Qs);hp=o(ym,"section 2"),ym.forEach(s),fp=o(Il,". You\u2019ll then be able to build any kind of tokenizer you can think of!"),Il.forEach(s),na=h(e),d(zt.$$.fragment,e),oa=h(e),Ce=i(e,"P",{});var Rl=l(Ce);dp=o(Rl,"More precisely, the library is built around a central "),In=i(Rl,"CODE",{});var Tm=l(In);up=o(Tm,"Tokenizer"),Tm.forEach(s),mp=o(Rl," class with the building blocks regrouped in submodules:"),Rl.forEach(s),ra=h(e),T=i(e,"UL",{});var Y=l(T);pe=i(Y,"LI",{});var Ks=l(pe);Rn=i(Ks,"CODE",{});var qm=l(Rn);kp=o(qm,"normalizers"),qm.forEach(s),wp=o(Ks," contains all the possible types of "),Un=i(Ks,"CODE",{});var Pm=l(Un);_p=o(Pm,"Normalizer"),Pm.forEach(s),$p=o(Ks," you can use (complete list "),bt=i(Ks,"A",{href:!0,rel:!0});var Dm=l(bt);zp=o(Dm,"here"),Dm.forEach(s),bp=o(Ks,")."),Ks.forEach(s),gp=h(Y),ce=i(Y,"LI",{});var Ys=l(ce);Gn=i(Ys,"CODE",{});var Cm=l(Gn);vp=o(Cm,"pre_tokenizers"),Cm.forEach(s),Ep=o(Ys," contains all the possible types of "),Mn=i(Ys,"CODE",{});var Om=l(Mn);jp=o(Om,"PreTokenizer"),Om.forEach(s),xp=o(Ys," you can use (complete list "),gt=i(Ys,"A",{href:!0,rel:!0});var Lm=l(gt);yp=o(Lm,"here"),Lm.forEach(s),Tp=o(Ys,")."),Ys.forEach(s),qp=h(Y),q=i(Y,"LI",{});var W=l(q);Xn=i(W,"CODE",{});var Sm=l(Xn);Pp=o(Sm,"models"),Sm.forEach(s),Dp=o(W," contains the various types of "),Kn=i(W,"CODE",{});var Bm=l(Kn);Cp=o(Bm,"Model"),Bm.forEach(s),Op=o(W," you can use, like "),Yn=i(W,"CODE",{});var Am=l(Yn);Lp=o(Am,"BPE"),Am.forEach(s),Sp=o(W,", "),Hn=i(W,"CODE",{});var Nm=l(Hn);Bp=o(Nm,"WordPiece"),Nm.forEach(s),Ap=o(W,", and "),Jn=i(W,"CODE",{});var Wm=l(Jn);Np=o(Wm,"Unigram"),Wm.forEach(s),Wp=o(W," (complete list "),vt=i(W,"A",{href:!0,rel:!0});var Fm=l(vt);Fp=o(Fm,"here"),Fm.forEach(s),Ip=o(W,")."),W.forEach(s),Rp=h(Y),he=i(Y,"LI",{});var Hs=l(he);Vn=i(Hs,"CODE",{});var Im=l(Vn);Up=o(Im,"trainers"),Im.forEach(s),Gp=o(Hs," contains all the different types of "),Zn=i(Hs,"CODE",{});var Rm=l(Zn);Mp=o(Rm,"Trainer"),Rm.forEach(s),Xp=o(Hs," you can use to train your model on a corpus (one per type of model; complete list "),Et=i(Hs,"A",{href:!0,rel:!0});var Um=l(Et);Kp=o(Um,"here"),Um.forEach(s),Yp=o(Hs,")."),Hs.forEach(s),Hp=h(Y),fe=i(Y,"LI",{});var Js=l(fe);Qn=i(Js,"CODE",{});var Gm=l(Qn);Jp=o(Gm,"post_processors"),Gm.forEach(s),Vp=o(Js," contains the various types of "),eo=i(Js,"CODE",{});var Mm=l(eo);Zp=o(Mm,"PostProcessor"),Mm.forEach(s),Qp=o(Js," you can use (complete list "),jt=i(Js,"A",{href:!0,rel:!0});var Xm=l(jt);ec=o(Xm,"here"),Xm.forEach(s),tc=o(Js,")."),Js.forEach(s),sc=h(Y),de=i(Y,"LI",{});var Vs=l(de);to=i(Vs,"CODE",{});var Km=l(to);nc=o(Km,"decoders"),Km.forEach(s),oc=o(Vs," contains the various types of "),so=i(Vs,"CODE",{});var Ym=l(so);rc=o(Ym,"Decoder"),Ym.forEach(s),ac=o(Vs," you can use to decode the outputs of tokenization (complete list "),xt=i(Vs,"A",{href:!0,rel:!0});var Hm=l(xt);ic=o(Hm,"here"),Hm.forEach(s),lc=o(Vs,")."),Vs.forEach(s),Y.forEach(s),aa=h(e),Oe=i(e,"P",{});var Ul=l(Oe);pc=o(Ul,"You can find the whole list of building blocks "),yt=i(Ul,"A",{href:!0,rel:!0});var Jm=l(yt);cc=o(Jm,"here"),Jm.forEach(s),hc=o(Ul,"."),Ul.forEach(s),ia=h(e),je=i(e,"H2",{class:!0});var Gl=l(je);Le=i(Gl,"A",{id:!0,class:!0,href:!0});var Vm=l(Le);no=i(Vm,"SPAN",{});var Zm=l(no);d(Tt.$$.fragment,Zm),Zm.forEach(s),Vm.forEach(s),fc=h(Gl),oo=i(Gl,"SPAN",{});var Qm=l(oo);dc=o(Qm,"Acquiring a corpus"),Qm.forEach(s),Gl.forEach(s),la=h(e),ue=i(e,"P",{});var Cn=l(ue);uc=o(Cn,"To train our new tokenizer, we will use a small corpus of text (so the examples run fast). The steps for acquiring the corpus are similar to the ones we took at the "),en=i(Cn,"A",{href:!0});var ek=l(en);mc=o(ek,"beginning of this chapter"),ek.forEach(s),kc=o(Cn,", but this time we\u2019ll use the "),qt=i(Cn,"A",{href:!0,rel:!0});var tk=l(qt);wc=o(tk,"WikiText-2"),tk.forEach(s),_c=o(Cn," dataset:"),Cn.forEach(s),pa=h(e),d(Pt.$$.fragment,e),ca=h(e),Se=i(e,"P",{});var Ml=l(Se);$c=o(Ml,"The function "),ro=i(Ml,"CODE",{});var sk=l(ro);zc=o(sk,"get_training_corpus()"),sk.forEach(s),bc=o(Ml," is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer."),Ml.forEach(s),ha=h(e),tn=i(e,"P",{});var nk=l(tn);gc=o(nk,"\u{1F917} Tokenizers can also be trained on text files directly. Here\u2019s how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally:"),nk.forEach(s),fa=h(e),d(Dt.$$.fragment,e),da=h(e),sn=i(e,"P",{});var ok=l(sn);vc=o(ok,"Next we\u2019ll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Let\u2019s start with BERT!"),ok.forEach(s),ua=h(e),xe=i(e,"H2",{class:!0});var Xl=l(xe);Be=i(Xl,"A",{id:!0,class:!0,href:!0});var rk=l(Be);ao=i(rk,"SPAN",{});var ak=l(ao);d(Ct.$$.fragment,ak),ak.forEach(s),rk.forEach(s),Ec=h(Xl),io=i(Xl,"SPAN",{});var ik=l(io);jc=o(ik,"Building a WordPiece tokenizer from scratch"),ik.forEach(s),Xl.forEach(s),ma=h(e),E=i(e,"P",{});var L=l(E);xc=o(L,"To build a tokenizer with the \u{1F917} Tokenizers library, we start by instantiating a "),lo=i(L,"CODE",{});var lk=l(lo);yc=o(lk,"Tokenizer"),lk.forEach(s),Tc=o(L," object with a "),po=i(L,"CODE",{});var pk=l(po);qc=o(pk,"model"),pk.forEach(s),Pc=o(L,", then set its "),co=i(L,"CODE",{});var ck=l(co);Dc=o(ck,"normalizer"),ck.forEach(s),Cc=o(L,", "),ho=i(L,"CODE",{});var hk=l(ho);Oc=o(hk,"pre_tokenizer"),hk.forEach(s),Lc=o(L,", "),fo=i(L,"CODE",{});var fk=l(fo);Sc=o(fk,"post_processor"),fk.forEach(s),Bc=o(L,", and "),uo=i(L,"CODE",{});var dk=l(uo);Ac=o(dk,"decoder"),dk.forEach(s),Nc=o(L," attributes to the values we want."),L.forEach(s),ka=h(e),Ae=i(e,"P",{});var Kl=l(Ae);Wc=o(Kl,"For this example, we\u2019ll create a "),mo=i(Kl,"CODE",{});var uk=l(mo);Fc=o(uk,"Tokenizer"),uk.forEach(s),Ic=o(Kl," with a WordPiece model:"),Kl.forEach(s),wa=h(e),d(Ot.$$.fragment,e),_a=h(e),I=i(e,"P",{});var st=l(I);Rc=o(st,"We have to specify the "),ko=i(st,"CODE",{});var mk=l(ko);Uc=o(mk,"unk_token"),mk.forEach(s),Gc=o(st," so the model knows what to return when it encounters characters it hasn\u2019t seen before. Other arguments we can set here include the "),wo=i(st,"CODE",{});var kk=l(wo);Mc=o(kk,"vocab"),kk.forEach(s),Xc=o(st," of our model (we\u2019re going to train the model, so we don\u2019t need to set this) and "),_o=i(st,"CODE",{});var wk=l(_o);Kc=o(wk,"max_input_chars_per_word"),wk.forEach(s),Yc=o(st,", which specifies a maximum length for each word (words longer than the value passed will be split)."),st.forEach(s),$a=h(e),j=i(e,"P",{});var S=l(j);Hc=o(S,"The first step of tokenization is normalization, so let\u2019s begin with that. Since BERT is widely used, there is a "),$o=i(S,"CODE",{});var _k=l($o);Jc=o(_k,"BertNormalizer"),_k.forEach(s),Vc=o(S," with the classic options we can set for BERT: "),zo=i(S,"CODE",{});var $k=l(zo);Zc=o($k,"lowercase"),$k.forEach(s),Qc=o(S," and "),bo=i(S,"CODE",{});var zk=l(bo);eh=o(zk,"strip_accents"),zk.forEach(s),th=o(S,", which are self-explanatory; "),go=i(S,"CODE",{});var bk=l(go);sh=o(bk,"clean_text"),bk.forEach(s),nh=o(S," to remove all control characters and replace repeating spaces with a single one; and "),vo=i(S,"CODE",{});var gk=l(vo);oh=o(gk,"handle_chinese_chars"),gk.forEach(s),rh=o(S,", which places spaces around Chinese characters. To replicate the "),Eo=i(S,"CODE",{});var vk=l(Eo);ah=o(vk,"bert-base-uncased"),vk.forEach(s),ih=o(S," tokenizer, we can just set this normalizer:"),S.forEach(s),za=h(e),d(Lt.$$.fragment,e),ba=h(e),R=i(e,"P",{});var nt=l(R);lh=o(nt,"Generally speaking, however, when building a new tokenizer you won\u2019t have access to such a handy normalizer already implemented in the \u{1F917} Tokenizers library \u2014 so let\u2019s see how to create the BERT normalizer by hand. The library provides a "),jo=i(nt,"CODE",{});var Ek=l(jo);ph=o(Ek,"Lowercase"),Ek.forEach(s),ch=o(nt," normalizer and a "),xo=i(nt,"CODE",{});var jk=l(xo);hh=o(jk,"StripAccents"),jk.forEach(s),fh=o(nt," normalizer, and you can compose several normalizers using a "),yo=i(nt,"CODE",{});var xk=l(yo);dh=o(xk,"Sequence"),xk.forEach(s),uh=o(nt,":"),nt.forEach(s),ga=h(e),d(St.$$.fragment,e),va=h(e),me=i(e,"P",{});var On=l(me);mh=o(On,"We\u2019re also using an "),To=i(On,"CODE",{});var yk=l(To);kh=o(yk,"NFD"),yk.forEach(s),wh=o(On," Unicode normalizer, as otherwise the "),qo=i(On,"CODE",{});var Tk=l(qo);_h=o(Tk,"StripAccents"),Tk.forEach(s),$h=o(On," normalizer won\u2019t properly recognize the accented characters and thus won\u2019t strip them out."),On.forEach(s),Ea=h(e),ke=i(e,"P",{});var Ln=l(ke);zh=o(Ln,"As we\u2019ve seen before, we can use the "),Po=i(Ln,"CODE",{});var qk=l(Po);bh=o(qk,"normalize_str()"),qk.forEach(s),gh=o(Ln," method of the "),Do=i(Ln,"CODE",{});var Pk=l(Do);vh=o(Pk,"normalizer"),Pk.forEach(s),Eh=o(Ln," to check out the effects it has on a given text:"),Ln.forEach(s),ja=h(e),d(Bt.$$.fragment,e),xa=h(e),d(At.$$.fragment,e),ya=h(e),d(Ne.$$.fragment,e),Ta=h(e),We=i(e,"P",{});var Yl=l(We);jh=o(Yl,"Next is the pre-tokenization step. Again, there is a prebuilt "),Co=i(Yl,"CODE",{});var Dk=l(Co);xh=o(Dk,"BertPreTokenizer"),Dk.forEach(s),yh=o(Yl," that we can use:"),Yl.forEach(s),qa=h(e),d(Nt.$$.fragment,e),Pa=h(e),nn=i(e,"P",{});var Ck=l(nn);Th=o(Ck,"Or we can build it from scratch:"),Ck.forEach(s),Da=h(e),d(Wt.$$.fragment,e),Ca=h(e),Fe=i(e,"P",{});var Hl=l(Fe);qh=o(Hl,"Note that the "),Oo=i(Hl,"CODE",{});var Ok=l(Oo);Ph=o(Ok,"Whitespace"),Ok.forEach(s),Dh=o(Hl," pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation:"),Hl.forEach(s),Oa=h(e),d(Ft.$$.fragment,e),La=h(e),d(It.$$.fragment,e),Sa=h(e),Ie=i(e,"P",{});var Jl=l(Ie);Ch=o(Jl,"If you only want to split on whitespace, you should use the "),Lo=i(Jl,"CODE",{});var Lk=l(Lo);Oh=o(Lk,"WhitespaceSplit"),Lk.forEach(s),Lh=o(Jl," pre-tokenizer instead:"),Jl.forEach(s),Ba=h(e),d(Rt.$$.fragment,e),Aa=h(e),d(Ut.$$.fragment,e),Na=h(e),Re=i(e,"P",{});var Vl=l(Re);Sh=o(Vl,"Like with normalizers, you can use a "),So=i(Vl,"CODE",{});var Sk=l(So);Bh=o(Sk,"Sequence"),Sk.forEach(s),Ah=o(Vl," to compose several pre-tokenizers:"),Vl.forEach(s),Wa=h(e),d(Gt.$$.fragment,e),Fa=h(e),d(Mt.$$.fragment,e),Ia=h(e),Ue=i(e,"P",{});var Zl=l(Ue);Nh=o(Zl,"The next step in the tokenization pipeline is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a "),Bo=i(Zl,"CODE",{});var Bk=l(Bo);Wh=o(Bk,"WordPieceTrainer"),Bk.forEach(s),Fh=o(Zl,". The main thing to remember when instantiating a trainer in \u{1F917} Tokenizers is that you need to pass it all the special tokens you intend to use \u2014 otherwise it won\u2019t add them to the vocabulary, since they are not in the training corpus:"),Zl.forEach(s),Ra=h(e),d(Xt.$$.fragment,e),Ua=h(e),P=i(e,"P",{});var H=l(P);Ih=o(H,"As well as specifying the "),Ao=i(H,"CODE",{});var Ak=l(Ao);Rh=o(Ak,"vocab_size"),Ak.forEach(s),Uh=o(H," and "),No=i(H,"CODE",{});var Nk=l(No);Gh=o(Nk,"special_tokens"),Nk.forEach(s),Mh=o(H,", we can set the "),Wo=i(H,"CODE",{});var Wk=l(Wo);Xh=o(Wk,"min_frequency"),Wk.forEach(s),Kh=o(H," (the number of times a token must appear to be included in the vocabulary) or change the "),Fo=i(H,"CODE",{});var Fk=l(Fo);Yh=o(Fk,"continuing_subword_prefix"),Fk.forEach(s),Hh=o(H," (if we want to use something different from "),Io=i(H,"CODE",{});var Ik=l(Io);Jh=o(Ik,"##"),Ik.forEach(s),Vh=o(H,")."),H.forEach(s),Ga=h(e),on=i(e,"P",{});var Rk=l(on);Zh=o(Rk,"To train our model using the iterator we defined earlier, we just have to execute this command:"),Rk.forEach(s),Ma=h(e),d(Kt.$$.fragment,e),Xa=h(e),Ge=i(e,"P",{});var Ql=l(Ge);Qh=o(Ql,"We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with an empty "),Ro=i(Ql,"CODE",{});var Uk=l(Ro);ef=o(Uk,"WordPiece"),Uk.forEach(s),tf=o(Ql," beforehand):"),Ql.forEach(s),Ka=h(e),d(Yt.$$.fragment,e),Ya=h(e),Me=i(e,"P",{});var ep=l(Me);sf=o(ep,"In both cases, we can then test the tokenizer on a text by calling the "),Uo=i(ep,"CODE",{});var Gk=l(Uo);nf=o(Gk,"encode()"),Gk.forEach(s),of=o(ep," method:"),ep.forEach(s),Ha=h(e),d(Ht.$$.fragment,e),Ja=h(e),d(Jt.$$.fragment,e),Va=h(e),b=i(e,"P",{});var v=l(b);rf=o(v,"The "),Go=i(v,"CODE",{});var Mk=l(Go);af=o(Mk,"encoding"),Mk.forEach(s),lf=o(v," obtained is an "),Mo=i(v,"CODE",{});var Xk=l(Mo);pf=o(Xk,"Encoding"),Xk.forEach(s),cf=o(v,", which contains all the necessary outputs of the tokenizer in its various attributes: "),Xo=i(v,"CODE",{});var Kk=l(Xo);hf=o(Kk,"ids"),Kk.forEach(s),ff=o(v,", "),Ko=i(v,"CODE",{});var Yk=l(Ko);df=o(Yk,"type_ids"),Yk.forEach(s),uf=o(v,", "),Yo=i(v,"CODE",{});var Hk=l(Yo);mf=o(Hk,"tokens"),Hk.forEach(s),kf=o(v,", "),Ho=i(v,"CODE",{});var Jk=l(Ho);wf=o(Jk,"offsets"),Jk.forEach(s),_f=o(v,", "),Jo=i(v,"CODE",{});var Vk=l(Jo);$f=o(Vk,"attention_mask"),Vk.forEach(s),zf=o(v,", "),Vo=i(v,"CODE",{});var Zk=l(Vo);bf=o(Zk,"special_tokens_mask"),Zk.forEach(s),gf=o(v,", and "),Zo=i(v,"CODE",{});var Qk=l(Zo);vf=o(Qk,"overflowing"),Qk.forEach(s),Ef=o(v,"."),v.forEach(s),Za=h(e),D=i(e,"P",{});var J=l(D);jf=o(J,"The last step in the tokenization pipeline is post-processing. We need to add the "),Qo=i(J,"CODE",{});var e2=l(Qo);xf=o(e2,"[CLS]"),e2.forEach(s),yf=o(J," token at the beginning and the "),er=i(J,"CODE",{});var t2=l(er);Tf=o(t2,"[SEP]"),t2.forEach(s),qf=o(J," token at the end (or after each sentence, if we have a pair of sentences). We will use a "),tr=i(J,"CODE",{});var s2=l(tr);Pf=o(s2,"TemplateProcessor"),s2.forEach(s),Df=o(J," for this, but first we need to know the IDs of the "),sr=i(J,"CODE",{});var n2=l(sr);Cf=o(n2,"[CLS]"),n2.forEach(s),Of=o(J," and "),nr=i(J,"CODE",{});var o2=l(nr);Lf=o(o2,"[SEP]"),o2.forEach(s),Sf=o(J," tokens in the vocabulary:"),J.forEach(s),Qa=h(e),d(Vt.$$.fragment,e),ei=h(e),d(Zt.$$.fragment,e),ti=h(e),U=i(e,"P",{});var ot=l(U);Bf=o(ot,"To write the template for the "),or=i(ot,"CODE",{});var r2=l(or);Af=o(r2,"TemplateProcessor"),r2.forEach(s),Nf=o(ot,", we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by "),rr=i(ot,"CODE",{});var a2=l(rr);Wf=o(a2,"$A"),a2.forEach(s),Ff=o(ot,", while the second sentence (if encoding a pair) is represented by "),ar=i(ot,"CODE",{});var i2=l(ar);If=o(i2,"$B"),i2.forEach(s),Rf=o(ot,". For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon."),ot.forEach(s),si=h(e),rn=i(e,"P",{});var l2=l(rn);Uf=o(l2,"The classic BERT template is thus defined as follows:"),l2.forEach(s),ni=h(e),d(Qt.$$.fragment,e),oi=h(e),an=i(e,"P",{});var p2=l(an);Gf=o(p2,"Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs."),p2.forEach(s),ri=h(e),ln=i(e,"P",{});var c2=l(ln);Mf=o(c2,"Once this is added, going back to our previous example will give:"),c2.forEach(s),ai=h(e),d(es.$$.fragment,e),ii=h(e),d(ts.$$.fragment,e),li=h(e),pn=i(e,"P",{});var h2=l(pn);Xf=o(h2,"And on a pair of sentences, we get the proper result:"),h2.forEach(s),pi=h(e),d(ss.$$.fragment,e),ci=h(e),d(ns.$$.fragment,e),hi=h(e),cn=i(e,"P",{});var f2=l(cn);Kf=o(f2,"We\u2019ve almost finished building this tokenizer from scratch \u2014 the last step is to include a decoder:"),f2.forEach(s),fi=h(e),d(os.$$.fragment,e),di=h(e),Xe=i(e,"P",{});var tp=l(Xe);Yf=o(tp,"Let\u2019s test it on our previous "),ir=i(tp,"CODE",{});var d2=l(ir);Hf=o(d2,"encoding"),d2.forEach(s),Jf=o(tp,":"),tp.forEach(s),ui=h(e),d(rs.$$.fragment,e),mi=h(e),d(as.$$.fragment,e),ki=h(e),hn=i(e,"P",{});var u2=l(hn);Vf=o(u2,"Great! We can save our tokenizer in a single JSON file like this:"),u2.forEach(s),wi=h(e),d(is.$$.fragment,e),_i=h(e),we=i(e,"P",{});var Sn=l(we);Zf=o(Sn,"We can then reload that file in a "),lr=i(Sn,"CODE",{});var m2=l(lr);Qf=o(m2,"Tokenizer"),m2.forEach(s),ed=o(Sn," object with the "),pr=i(Sn,"CODE",{});var k2=l(pr);td=o(k2,"from_file()"),k2.forEach(s),sd=o(Sn," method:"),Sn.forEach(s),$i=h(e),d(ls.$$.fragment,e),zi=h(e),_e=i(e,"P",{});var Bn=l(_e);nd=o(Bn,"To use this tokenizer in \u{1F917} Transformers, we have to wrap it in a "),cr=i(Bn,"CODE",{});var w2=l(cr);od=o(w2,"PreTrainedTokenizerFast"),w2.forEach(s),rd=o(Bn,". We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, "),hr=i(Bn,"CODE",{});var _2=l(hr);ad=o(_2,"BertTokenizerFast"),_2.forEach(s),id=o(Bn,"). If you apply this lesson to build a brand new tokenizer, you will have to use the first option."),Bn.forEach(s),bi=h(e),C=i(e,"P",{});var V=l(C);ld=o(V,"To wrap the tokenizer in a "),fr=i(V,"CODE",{});var $2=l(fr);pd=o($2,"PreTrainedTokenizerFast"),$2.forEach(s),cd=o(V,", we can either pass the tokenizer we built as a "),dr=i(V,"CODE",{});var z2=l(dr);hd=o(z2,"tokenizer_object"),z2.forEach(s),fd=o(V," or pass the tokenizer file we saved as "),ur=i(V,"CODE",{});var b2=l(ur);dd=o(b2,"tokenizer_file"),b2.forEach(s),ud=o(V,". The key thing to remember is that we have to manually set all the special tokens, since that class can\u2019t infer from the "),mr=i(V,"CODE",{});var g2=l(mr);md=o(g2,"tokenizer"),g2.forEach(s),kd=o(V," object which token is the mask token, the "),kr=i(V,"CODE",{});var v2=l(kr);wd=o(v2,"[CLS]"),v2.forEach(s),_d=o(V," token, etc.:"),V.forEach(s),gi=h(e),d(ps.$$.fragment,e),vi=h(e),Ke=i(e,"P",{});var sp=l(Ke);$d=o(sp,"If you are using a specific tokenizer class (like "),wr=i(sp,"CODE",{});var E2=l(wr);zd=o(E2,"BertTokenizerFast"),E2.forEach(s),bd=o(sp,"), you will only need to specify the special tokens that are different from the default ones (here, none):"),sp.forEach(s),Ei=h(e),d(cs.$$.fragment,e),ji=h(e),$e=i(e,"P",{});var An=l($e);gd=o(An,"You can then use this tokenizer like any other \u{1F917} Transformers tokenizer. You can save it with the "),_r=i(An,"CODE",{});var j2=l(_r);vd=o(j2,"save_pretrained()"),j2.forEach(s),Ed=o(An," method, or upload it to the Hub with the "),$r=i(An,"CODE",{});var x2=l($r);jd=o(x2,"push_to_hub()"),x2.forEach(s),xd=o(An," method."),An.forEach(s),xi=h(e),fn=i(e,"P",{});var y2=l(fn);yd=o(y2,"Now that we\u2019ve seen how to build a WordPiece tokenizer, let\u2019s do the same for a BPE tokenizer. We\u2019ll go a bit faster since you know all the steps, and only highlight the differences."),y2.forEach(s),yi=h(e),ye=i(e,"H2",{class:!0});var np=l(ye);Ye=i(np,"A",{id:!0,class:!0,href:!0});var T2=l(Ye);zr=i(T2,"SPAN",{});var q2=l(zr);d(hs.$$.fragment,q2),q2.forEach(s),T2.forEach(s),Td=h(np),br=i(np,"SPAN",{});var P2=l(br);qd=o(P2,"Building a BPE tokenizer from scratch"),P2.forEach(s),np.forEach(s),Ti=h(e),He=i(e,"P",{});var op=l(He);Pd=o(op,"Let\u2019s now build a GPT-2 tokenizer. Like for the BERT tokenizer, we start by initializing a "),gr=i(op,"CODE",{});var D2=l(gr);Dd=o(D2,"Tokenizer"),D2.forEach(s),Cd=o(op," with a BPE model:"),op.forEach(s),qi=h(e),d(fs.$$.fragment,e),Pi=h(e),G=i(e,"P",{});var rt=l(G);Od=o(rt,"Also like for BERT, we could initialize this model with a vocabulary if we had one (we would need to pass the "),vr=i(rt,"CODE",{});var C2=l(vr);Ld=o(C2,"vocab"),C2.forEach(s),Sd=o(rt," and "),Er=i(rt,"CODE",{});var O2=l(Er);Bd=o(O2,"merges"),O2.forEach(s),Ad=o(rt," in this case), but since we will train from scratch, we don\u2019t need to do that. We also don\u2019t need to specify an "),jr=i(rt,"CODE",{});var L2=l(jr);Nd=o(L2,"unk_token"),L2.forEach(s),Wd=o(rt," because GPT-2 uses byte-level BPE, which doesn\u2019t require it."),rt.forEach(s),Di=h(e),dn=i(e,"P",{});var S2=l(dn);Fd=o(S2,"GPT-2 does not use a normalizer, so we skip that step and go directly to the pre-tokenization:"),S2.forEach(s),Ci=h(e),d(ds.$$.fragment,e),Oi=h(e),Je=i(e,"P",{});var rp=l(Je);Id=o(rp,"The option we added to "),xr=i(rp,"CODE",{});var B2=l(xr);Rd=o(B2,"ByteLevel"),B2.forEach(s),Ud=o(rp," here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before:"),rp.forEach(s),Li=h(e),d(us.$$.fragment,e),Si=h(e),d(ms.$$.fragment,e),Bi=h(e),un=i(e,"P",{});var A2=l(un);Gd=o(A2,"Next is the model, which needs training. For GPT-2, the only special token is the end-of-text token:"),A2.forEach(s),Ai=h(e),d(ks.$$.fragment,e),Ni=h(e),x=i(e,"P",{});var B=l(x);Md=o(B,"Like with the "),yr=i(B,"CODE",{});var N2=l(yr);Xd=o(N2,"WordPieceTrainer"),N2.forEach(s),Kd=o(B,", as well as the "),Tr=i(B,"CODE",{});var W2=l(Tr);Yd=o(W2,"vocab_size"),W2.forEach(s),Hd=o(B," and "),qr=i(B,"CODE",{});var F2=l(qr);Jd=o(F2,"special_tokens"),F2.forEach(s),Vd=o(B,", we can specify the "),Pr=i(B,"CODE",{});var I2=l(Pr);Zd=o(I2,"min_frequency"),I2.forEach(s),Qd=o(B," if we want to, or if we have an end-of-word suffix (like "),Dr=i(B,"CODE",{});var R2=l(Dr);eu=o(R2,"</w>"),R2.forEach(s),tu=o(B,"), we can set it with "),Cr=i(B,"CODE",{});var U2=l(Cr);su=o(U2,"end_of_word_suffix"),U2.forEach(s),nu=o(B,"."),B.forEach(s),Wi=h(e),mn=i(e,"P",{});var G2=l(mn);ou=o(G2,"This tokenizer can also be trained on text files:"),G2.forEach(s),Fi=h(e),d(ws.$$.fragment,e),Ii=h(e),kn=i(e,"P",{});var M2=l(kn);ru=o(M2,"Let\u2019s have a look at the tokenization of a sample text:"),M2.forEach(s),Ri=h(e),d(_s.$$.fragment,e),Ui=h(e),d($s.$$.fragment,e),Gi=h(e),wn=i(e,"P",{});var X2=l(wn);au=o(X2,"We apply the byte-level post-processing for the GPT-2 tokenizer as follows:"),X2.forEach(s),Mi=h(e),d(zs.$$.fragment,e),Xi=h(e),ze=i(e,"P",{});var Nn=l(ze);iu=o(Nn,"The "),Or=i(Nn,"CODE",{});var K2=l(Or);lu=o(K2,"trim_offsets = False"),K2.forEach(s),pu=o(Nn," option indicates to the post-processor that we should leave the offsets of tokens that begin with \u2018\u0120\u2019 as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let\u2019s have a look at the result with the text we just encoded, where "),Lr=i(Nn,"CODE",{});var Y2=l(Lr);cu=o(Y2,"'\u0120test'"),Y2.forEach(s),hu=o(Nn," is the token at index 4:"),Nn.forEach(s),Ki=h(e),d(bs.$$.fragment,e),Yi=h(e),d(gs.$$.fragment,e),Hi=h(e),_n=i(e,"P",{});var H2=l(_n);fu=o(H2,"Finally, we add a byte-level decoder:"),H2.forEach(s),Ji=h(e),d(vs.$$.fragment,e),Vi=h(e),$n=i(e,"P",{});var J2=l($n);du=o(J2,"and we can double-check it works properly:"),J2.forEach(s),Zi=h(e),d(Es.$$.fragment,e),Qi=h(e),d(js.$$.fragment,e),el=h(e),be=i(e,"P",{});var Wn=l(be);uu=o(Wn,"Great! Now that we\u2019re done, we can save the tokenizer like before, and wrap it in a "),Sr=i(Wn,"CODE",{});var V2=l(Sr);mu=o(V2,"PreTrainedTokenizerFast"),V2.forEach(s),ku=o(Wn," or "),Br=i(Wn,"CODE",{});var Z2=l(Br);wu=o(Z2,"GPT2TokenizerFast"),Z2.forEach(s),_u=o(Wn," if we want to use it in \u{1F917} Transformers:"),Wn.forEach(s),tl=h(e),d(xs.$$.fragment,e),sl=h(e),zn=i(e,"P",{});var Q2=l(zn);$u=o(Q2,"or:"),Q2.forEach(s),nl=h(e),d(ys.$$.fragment,e),ol=h(e),bn=i(e,"P",{});var ew=l(bn);zu=o(ew,"As the last example, we\u2019ll show you how to build a Unigram tokenizer from scratch."),ew.forEach(s),rl=h(e),Te=i(e,"H2",{class:!0});var ap=l(Te);Ve=i(ap,"A",{id:!0,class:!0,href:!0});var tw=l(Ve);Ar=i(tw,"SPAN",{});var sw=l(Ar);d(Ts.$$.fragment,sw),sw.forEach(s),tw.forEach(s),bu=h(ap),Nr=i(ap,"SPAN",{});var nw=l(Nr);gu=o(nw,"Building a Unigram tokenizer from scratch"),nw.forEach(s),ap.forEach(s),al=h(e),Ze=i(e,"P",{});var ip=l(Ze);vu=o(ip,"Let\u2019s now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a "),Wr=i(ip,"CODE",{});var ow=l(Wr);Eu=o(ow,"Tokenizer"),ow.forEach(s),ju=o(ip," with a Unigram model:"),ip.forEach(s),il=h(e),d(qs.$$.fragment,e),ll=h(e),gn=i(e,"P",{});var rw=l(gn);xu=o(rw,"Again, we could initialize this model with a vocabulary if we had one."),rw.forEach(s),pl=h(e),vn=i(e,"P",{});var aw=l(vn);yu=o(aw,"For the normalization, XLNet uses a few replacements (which come from SentencePiece):"),aw.forEach(s),cl=h(e),d(Ps.$$.fragment,e),hl=h(e),M=i(e,"P",{});var at=l(M);Tu=o(at,"This replaces "),Fr=i(at,"CODE",{});var iw=l(Fr);qu=o(iw,"\u201C"),iw.forEach(s),Pu=o(at," and "),Ir=i(at,"CODE",{});var lw=l(Ir);Du=o(lw,"\u201D"),lw.forEach(s),Cu=o(at," with "),Rr=i(at,"CODE",{});var pw=l(Rr);Ou=o(pw,"\u201D"),pw.forEach(s),Lu=o(at," and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize."),at.forEach(s),fl=h(e),Qe=i(e,"P",{});var lp=l(Qe);Su=o(lp,"The pre-tokenizer to use for any SentencePiece tokenizer is "),Ur=i(lp,"CODE",{});var cw=l(Ur);Bu=o(cw,"Metaspace"),cw.forEach(s),Au=o(lp,":"),lp.forEach(s),dl=h(e),d(Ds.$$.fragment,e),ul=h(e),En=i(e,"P",{});var hw=l(En);Nu=o(hw,"We can have a look at the pre-tokenization of an example text like before:"),hw.forEach(s),ml=h(e),d(Cs.$$.fragment,e),kl=h(e),d(Os.$$.fragment,e),wl=h(e),jn=i(e,"P",{});var fw=l(jn);Wu=o(fw,"Next is the model, which needs training. XLNet has quite a few special tokens:"),fw.forEach(s),_l=h(e),d(Ls.$$.fragment,e),$l=h(e),O=i(e,"P",{});var ge=l(O);Fu=o(ge,"A very important argument not to forget for the "),Gr=i(ge,"CODE",{});var dw=l(Gr);Iu=o(dw,"UnigramTrainer"),dw.forEach(s),Ru=o(ge," is the "),Mr=i(ge,"CODE",{});var uw=l(Mr);Uu=o(uw,"unk_token"),uw.forEach(s),Gu=o(ge,". We can also pass along other arguments specific to the Unigram algorithm, such as the "),Xr=i(ge,"CODE",{});var mw=l(Xr);Mu=o(mw,"shrinking_factor"),mw.forEach(s),Xu=o(ge," for each step where we remove tokens (defaults to 0.75) or the "),Kr=i(ge,"CODE",{});var kw=l(Kr);Ku=o(kw,"max_piece_length"),kw.forEach(s),Yu=o(ge," to specify the maximum length of a given token (defaults to 16)."),ge.forEach(s),zl=h(e),xn=i(e,"P",{});var ww=l(xn);Hu=o(ww,"This tokenizer can also be trained on text files:"),ww.forEach(s),bl=h(e),d(Ss.$$.fragment,e),gl=h(e),yn=i(e,"P",{});var _w=l(yn);Ju=o(_w,"Let\u2019s have a look at the tokenization of a sample text:"),_w.forEach(s),vl=h(e),d(Bs.$$.fragment,e),El=h(e),d(As.$$.fragment,e),jl=h(e),X=i(e,"P",{});var it=l(X);Vu=o(it,"A peculiarity of XLNet is that it puts the "),Yr=i(it,"CODE",{});var $w=l(Yr);Zu=o($w,"<cls>"),$w.forEach(s),Qu=o(it," token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It\u2019s padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the "),Hr=i(it,"CODE",{});var zw=l(Hr);em=o(zw,"<cls>"),zw.forEach(s),tm=o(it," and "),Jr=i(it,"CODE",{});var bw=l(Jr);sm=o(bw,"<sep>"),bw.forEach(s),nm=o(it," tokens:"),it.forEach(s),xl=h(e),d(Ns.$$.fragment,e),yl=h(e),d(Ws.$$.fragment,e),Tl=h(e),Tn=i(e,"P",{});var gw=l(Tn);om=o(gw,"The template looks like this:"),gw.forEach(s),ql=h(e),d(Fs.$$.fragment,e),Pl=h(e),qn=i(e,"P",{});var vw=l(qn);rm=o(vw,"And we can test it works by encoding a pair of sentences:"),vw.forEach(s),Dl=h(e),d(Is.$$.fragment,e),Cl=h(e),d(Rs.$$.fragment,e),Ol=h(e),et=i(e,"P",{});var pp=l(et);am=o(pp,"Finally, we add a "),Vr=i(pp,"CODE",{});var Ew=l(Vr);im=o(Ew,"Metaspace"),Ew.forEach(s),lm=o(pp," decoder:"),pp.forEach(s),Ll=h(e),d(Us.$$.fragment,e),Sl=h(e),K=i(e,"P",{});var lt=l(K);pm=o(lt,"and we\u2019re done with this tokenizer! We can save the tokenizer like before, and wrap it in a "),Zr=i(lt,"CODE",{});var jw=l(Zr);cm=o(jw,"PreTrainedTokenizerFast"),jw.forEach(s),hm=o(lt," or "),Qr=i(lt,"CODE",{});var xw=l(Qr);fm=o(xw,"XLNetTokenizerFast"),xw.forEach(s),dm=o(lt," if we want to use it in \u{1F917} Transformers. One thing to note when using "),ea=i(lt,"CODE",{});var yw=l(ea);um=o(yw,"PreTrainedTokenizerFast"),yw.forEach(s),mm=o(lt," is that on top of the special tokens, we need to tell the \u{1F917} Transformers library to pad on the left:"),lt.forEach(s),Bl=h(e),d(Gs.$$.fragment,e),Al=h(e),Pn=i(e,"P",{});var Tw=l(Pn);km=o(Tw,"Or alternatively:"),Tw.forEach(s),Nl=h(e),d(Ms.$$.fragment,e),Wl=h(e),Dn=i(e,"P",{});var qw=l(Dn);wm=o(qw,"Now that you have seen how the various building blocks are used to build existing tokenizers, you should be able to write any tokenizer you want with the \u{1F917} Tokenizers library and be able to use it in \u{1F917} Transformers."),qw.forEach(s),this.h()},h(){$(z,"name","hf:doc:metadata"),$(z,"content",JSON.stringify(Uw)),$(F,"id","building-a-tokenizer-block-by-block"),$(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(F,"href","#building-a-tokenizer-block-by-block"),$(A,"class","relative group"),Sw(re.src,Fn="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.PNG")||$(re,"src",Fn),$(re,"alt","The tokenization pipeline."),$(re,"width","100%"),$(Qs,"href","/course/chapter6/2"),$(bt,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"),$(bt,"rel","nofollow"),$(gt,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers"),$(gt,"rel","nofollow"),$(vt,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models"),$(vt,"rel","nofollow"),$(Et,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers"),$(Et,"rel","nofollow"),$(jt,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors"),$(jt,"rel","nofollow"),$(xt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders"),$(xt,"rel","nofollow"),$(yt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html"),$(yt,"rel","nofollow"),$(Le,"id","acquiring-a-corpus"),$(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Le,"href","#acquiring-a-corpus"),$(je,"class","relative group"),$(en,"href","/course/chapter6/2"),$(qt,"href","https://huggingface.co/datasets/wikitext"),$(qt,"rel","nofollow"),$(Be,"id","building-a-wordpiece-tokenizer-from-scratch"),$(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Be,"href","#building-a-wordpiece-tokenizer-from-scratch"),$(xe,"class","relative group"),$(Ye,"id","building-a-bpe-tokenizer-from-scratch"),$(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ye,"href","#building-a-bpe-tokenizer-from-scratch"),$(ye,"class","relative group"),$(Ve,"id","building-a-unigram-tokenizer-from-scratch"),$(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ve,"href","#building-a-unigram-tokenizer-from-scratch"),$(Te,"class","relative group")},m(e,r){t(Pw.head,z),p(e,ie,r),p(e,A,r),t(A,F),t(F,Z),u(Q,Z,null),t(A,pt),t(A,ee),t(ee,ct),p(e,qe,r),u(N,e,r),p(e,Pe,r),p(e,le,r),t(le,ve),p(e,De,r),p(e,y,r),t(y,te),t(te,ht),t(y,ft),t(y,se),t(se,dt),t(y,ut),t(y,ne),t(ne,mt),t(y,kt),t(y,oe),t(oe,g),p(e,wt,r),p(e,Ee,r),t(Ee,Zs),p(e,_t,r),p(e,re,r),p(e,$t,r),p(e,ae,r),t(ae,cp),t(ae,Qs),t(Qs,hp),t(ae,fp),p(e,na,r),u(zt,e,r),p(e,oa,r),p(e,Ce,r),t(Ce,dp),t(Ce,In),t(In,up),t(Ce,mp),p(e,ra,r),p(e,T,r),t(T,pe),t(pe,Rn),t(Rn,kp),t(pe,wp),t(pe,Un),t(Un,_p),t(pe,$p),t(pe,bt),t(bt,zp),t(pe,bp),t(T,gp),t(T,ce),t(ce,Gn),t(Gn,vp),t(ce,Ep),t(ce,Mn),t(Mn,jp),t(ce,xp),t(ce,gt),t(gt,yp),t(ce,Tp),t(T,qp),t(T,q),t(q,Xn),t(Xn,Pp),t(q,Dp),t(q,Kn),t(Kn,Cp),t(q,Op),t(q,Yn),t(Yn,Lp),t(q,Sp),t(q,Hn),t(Hn,Bp),t(q,Ap),t(q,Jn),t(Jn,Np),t(q,Wp),t(q,vt),t(vt,Fp),t(q,Ip),t(T,Rp),t(T,he),t(he,Vn),t(Vn,Up),t(he,Gp),t(he,Zn),t(Zn,Mp),t(he,Xp),t(he,Et),t(Et,Kp),t(he,Yp),t(T,Hp),t(T,fe),t(fe,Qn),t(Qn,Jp),t(fe,Vp),t(fe,eo),t(eo,Zp),t(fe,Qp),t(fe,jt),t(jt,ec),t(fe,tc),t(T,sc),t(T,de),t(de,to),t(to,nc),t(de,oc),t(de,so),t(so,rc),t(de,ac),t(de,xt),t(xt,ic),t(de,lc),p(e,aa,r),p(e,Oe,r),t(Oe,pc),t(Oe,yt),t(yt,cc),t(Oe,hc),p(e,ia,r),p(e,je,r),t(je,Le),t(Le,no),u(Tt,no,null),t(je,fc),t(je,oo),t(oo,dc),p(e,la,r),p(e,ue,r),t(ue,uc),t(ue,en),t(en,mc),t(ue,kc),t(ue,qt),t(qt,wc),t(ue,_c),p(e,pa,r),u(Pt,e,r),p(e,ca,r),p(e,Se,r),t(Se,$c),t(Se,ro),t(ro,zc),t(Se,bc),p(e,ha,r),p(e,tn,r),t(tn,gc),p(e,fa,r),u(Dt,e,r),p(e,da,r),p(e,sn,r),t(sn,vc),p(e,ua,r),p(e,xe,r),t(xe,Be),t(Be,ao),u(Ct,ao,null),t(xe,Ec),t(xe,io),t(io,jc),p(e,ma,r),p(e,E,r),t(E,xc),t(E,lo),t(lo,yc),t(E,Tc),t(E,po),t(po,qc),t(E,Pc),t(E,co),t(co,Dc),t(E,Cc),t(E,ho),t(ho,Oc),t(E,Lc),t(E,fo),t(fo,Sc),t(E,Bc),t(E,uo),t(uo,Ac),t(E,Nc),p(e,ka,r),p(e,Ae,r),t(Ae,Wc),t(Ae,mo),t(mo,Fc),t(Ae,Ic),p(e,wa,r),u(Ot,e,r),p(e,_a,r),p(e,I,r),t(I,Rc),t(I,ko),t(ko,Uc),t(I,Gc),t(I,wo),t(wo,Mc),t(I,Xc),t(I,_o),t(_o,Kc),t(I,Yc),p(e,$a,r),p(e,j,r),t(j,Hc),t(j,$o),t($o,Jc),t(j,Vc),t(j,zo),t(zo,Zc),t(j,Qc),t(j,bo),t(bo,eh),t(j,th),t(j,go),t(go,sh),t(j,nh),t(j,vo),t(vo,oh),t(j,rh),t(j,Eo),t(Eo,ah),t(j,ih),p(e,za,r),u(Lt,e,r),p(e,ba,r),p(e,R,r),t(R,lh),t(R,jo),t(jo,ph),t(R,ch),t(R,xo),t(xo,hh),t(R,fh),t(R,yo),t(yo,dh),t(R,uh),p(e,ga,r),u(St,e,r),p(e,va,r),p(e,me,r),t(me,mh),t(me,To),t(To,kh),t(me,wh),t(me,qo),t(qo,_h),t(me,$h),p(e,Ea,r),p(e,ke,r),t(ke,zh),t(ke,Po),t(Po,bh),t(ke,gh),t(ke,Do),t(Do,vh),t(ke,Eh),p(e,ja,r),u(Bt,e,r),p(e,xa,r),u(At,e,r),p(e,ya,r),u(Ne,e,r),p(e,Ta,r),p(e,We,r),t(We,jh),t(We,Co),t(Co,xh),t(We,yh),p(e,qa,r),u(Nt,e,r),p(e,Pa,r),p(e,nn,r),t(nn,Th),p(e,Da,r),u(Wt,e,r),p(e,Ca,r),p(e,Fe,r),t(Fe,qh),t(Fe,Oo),t(Oo,Ph),t(Fe,Dh),p(e,Oa,r),u(Ft,e,r),p(e,La,r),u(It,e,r),p(e,Sa,r),p(e,Ie,r),t(Ie,Ch),t(Ie,Lo),t(Lo,Oh),t(Ie,Lh),p(e,Ba,r),u(Rt,e,r),p(e,Aa,r),u(Ut,e,r),p(e,Na,r),p(e,Re,r),t(Re,Sh),t(Re,So),t(So,Bh),t(Re,Ah),p(e,Wa,r),u(Gt,e,r),p(e,Fa,r),u(Mt,e,r),p(e,Ia,r),p(e,Ue,r),t(Ue,Nh),t(Ue,Bo),t(Bo,Wh),t(Ue,Fh),p(e,Ra,r),u(Xt,e,r),p(e,Ua,r),p(e,P,r),t(P,Ih),t(P,Ao),t(Ao,Rh),t(P,Uh),t(P,No),t(No,Gh),t(P,Mh),t(P,Wo),t(Wo,Xh),t(P,Kh),t(P,Fo),t(Fo,Yh),t(P,Hh),t(P,Io),t(Io,Jh),t(P,Vh),p(e,Ga,r),p(e,on,r),t(on,Zh),p(e,Ma,r),u(Kt,e,r),p(e,Xa,r),p(e,Ge,r),t(Ge,Qh),t(Ge,Ro),t(Ro,ef),t(Ge,tf),p(e,Ka,r),u(Yt,e,r),p(e,Ya,r),p(e,Me,r),t(Me,sf),t(Me,Uo),t(Uo,nf),t(Me,of),p(e,Ha,r),u(Ht,e,r),p(e,Ja,r),u(Jt,e,r),p(e,Va,r),p(e,b,r),t(b,rf),t(b,Go),t(Go,af),t(b,lf),t(b,Mo),t(Mo,pf),t(b,cf),t(b,Xo),t(Xo,hf),t(b,ff),t(b,Ko),t(Ko,df),t(b,uf),t(b,Yo),t(Yo,mf),t(b,kf),t(b,Ho),t(Ho,wf),t(b,_f),t(b,Jo),t(Jo,$f),t(b,zf),t(b,Vo),t(Vo,bf),t(b,gf),t(b,Zo),t(Zo,vf),t(b,Ef),p(e,Za,r),p(e,D,r),t(D,jf),t(D,Qo),t(Qo,xf),t(D,yf),t(D,er),t(er,Tf),t(D,qf),t(D,tr),t(tr,Pf),t(D,Df),t(D,sr),t(sr,Cf),t(D,Of),t(D,nr),t(nr,Lf),t(D,Sf),p(e,Qa,r),u(Vt,e,r),p(e,ei,r),u(Zt,e,r),p(e,ti,r),p(e,U,r),t(U,Bf),t(U,or),t(or,Af),t(U,Nf),t(U,rr),t(rr,Wf),t(U,Ff),t(U,ar),t(ar,If),t(U,Rf),p(e,si,r),p(e,rn,r),t(rn,Uf),p(e,ni,r),u(Qt,e,r),p(e,oi,r),p(e,an,r),t(an,Gf),p(e,ri,r),p(e,ln,r),t(ln,Mf),p(e,ai,r),u(es,e,r),p(e,ii,r),u(ts,e,r),p(e,li,r),p(e,pn,r),t(pn,Xf),p(e,pi,r),u(ss,e,r),p(e,ci,r),u(ns,e,r),p(e,hi,r),p(e,cn,r),t(cn,Kf),p(e,fi,r),u(os,e,r),p(e,di,r),p(e,Xe,r),t(Xe,Yf),t(Xe,ir),t(ir,Hf),t(Xe,Jf),p(e,ui,r),u(rs,e,r),p(e,mi,r),u(as,e,r),p(e,ki,r),p(e,hn,r),t(hn,Vf),p(e,wi,r),u(is,e,r),p(e,_i,r),p(e,we,r),t(we,Zf),t(we,lr),t(lr,Qf),t(we,ed),t(we,pr),t(pr,td),t(we,sd),p(e,$i,r),u(ls,e,r),p(e,zi,r),p(e,_e,r),t(_e,nd),t(_e,cr),t(cr,od),t(_e,rd),t(_e,hr),t(hr,ad),t(_e,id),p(e,bi,r),p(e,C,r),t(C,ld),t(C,fr),t(fr,pd),t(C,cd),t(C,dr),t(dr,hd),t(C,fd),t(C,ur),t(ur,dd),t(C,ud),t(C,mr),t(mr,md),t(C,kd),t(C,kr),t(kr,wd),t(C,_d),p(e,gi,r),u(ps,e,r),p(e,vi,r),p(e,Ke,r),t(Ke,$d),t(Ke,wr),t(wr,zd),t(Ke,bd),p(e,Ei,r),u(cs,e,r),p(e,ji,r),p(e,$e,r),t($e,gd),t($e,_r),t(_r,vd),t($e,Ed),t($e,$r),t($r,jd),t($e,xd),p(e,xi,r),p(e,fn,r),t(fn,yd),p(e,yi,r),p(e,ye,r),t(ye,Ye),t(Ye,zr),u(hs,zr,null),t(ye,Td),t(ye,br),t(br,qd),p(e,Ti,r),p(e,He,r),t(He,Pd),t(He,gr),t(gr,Dd),t(He,Cd),p(e,qi,r),u(fs,e,r),p(e,Pi,r),p(e,G,r),t(G,Od),t(G,vr),t(vr,Ld),t(G,Sd),t(G,Er),t(Er,Bd),t(G,Ad),t(G,jr),t(jr,Nd),t(G,Wd),p(e,Di,r),p(e,dn,r),t(dn,Fd),p(e,Ci,r),u(ds,e,r),p(e,Oi,r),p(e,Je,r),t(Je,Id),t(Je,xr),t(xr,Rd),t(Je,Ud),p(e,Li,r),u(us,e,r),p(e,Si,r),u(ms,e,r),p(e,Bi,r),p(e,un,r),t(un,Gd),p(e,Ai,r),u(ks,e,r),p(e,Ni,r),p(e,x,r),t(x,Md),t(x,yr),t(yr,Xd),t(x,Kd),t(x,Tr),t(Tr,Yd),t(x,Hd),t(x,qr),t(qr,Jd),t(x,Vd),t(x,Pr),t(Pr,Zd),t(x,Qd),t(x,Dr),t(Dr,eu),t(x,tu),t(x,Cr),t(Cr,su),t(x,nu),p(e,Wi,r),p(e,mn,r),t(mn,ou),p(e,Fi,r),u(ws,e,r),p(e,Ii,r),p(e,kn,r),t(kn,ru),p(e,Ri,r),u(_s,e,r),p(e,Ui,r),u($s,e,r),p(e,Gi,r),p(e,wn,r),t(wn,au),p(e,Mi,r),u(zs,e,r),p(e,Xi,r),p(e,ze,r),t(ze,iu),t(ze,Or),t(Or,lu),t(ze,pu),t(ze,Lr),t(Lr,cu),t(ze,hu),p(e,Ki,r),u(bs,e,r),p(e,Yi,r),u(gs,e,r),p(e,Hi,r),p(e,_n,r),t(_n,fu),p(e,Ji,r),u(vs,e,r),p(e,Vi,r),p(e,$n,r),t($n,du),p(e,Zi,r),u(Es,e,r),p(e,Qi,r),u(js,e,r),p(e,el,r),p(e,be,r),t(be,uu),t(be,Sr),t(Sr,mu),t(be,ku),t(be,Br),t(Br,wu),t(be,_u),p(e,tl,r),u(xs,e,r),p(e,sl,r),p(e,zn,r),t(zn,$u),p(e,nl,r),u(ys,e,r),p(e,ol,r),p(e,bn,r),t(bn,zu),p(e,rl,r),p(e,Te,r),t(Te,Ve),t(Ve,Ar),u(Ts,Ar,null),t(Te,bu),t(Te,Nr),t(Nr,gu),p(e,al,r),p(e,Ze,r),t(Ze,vu),t(Ze,Wr),t(Wr,Eu),t(Ze,ju),p(e,il,r),u(qs,e,r),p(e,ll,r),p(e,gn,r),t(gn,xu),p(e,pl,r),p(e,vn,r),t(vn,yu),p(e,cl,r),u(Ps,e,r),p(e,hl,r),p(e,M,r),t(M,Tu),t(M,Fr),t(Fr,qu),t(M,Pu),t(M,Ir),t(Ir,Du),t(M,Cu),t(M,Rr),t(Rr,Ou),t(M,Lu),p(e,fl,r),p(e,Qe,r),t(Qe,Su),t(Qe,Ur),t(Ur,Bu),t(Qe,Au),p(e,dl,r),u(Ds,e,r),p(e,ul,r),p(e,En,r),t(En,Nu),p(e,ml,r),u(Cs,e,r),p(e,kl,r),u(Os,e,r),p(e,wl,r),p(e,jn,r),t(jn,Wu),p(e,_l,r),u(Ls,e,r),p(e,$l,r),p(e,O,r),t(O,Fu),t(O,Gr),t(Gr,Iu),t(O,Ru),t(O,Mr),t(Mr,Uu),t(O,Gu),t(O,Xr),t(Xr,Mu),t(O,Xu),t(O,Kr),t(Kr,Ku),t(O,Yu),p(e,zl,r),p(e,xn,r),t(xn,Hu),p(e,bl,r),u(Ss,e,r),p(e,gl,r),p(e,yn,r),t(yn,Ju),p(e,vl,r),u(Bs,e,r),p(e,El,r),u(As,e,r),p(e,jl,r),p(e,X,r),t(X,Vu),t(X,Yr),t(Yr,Zu),t(X,Qu),t(X,Hr),t(Hr,em),t(X,tm),t(X,Jr),t(Jr,sm),t(X,nm),p(e,xl,r),u(Ns,e,r),p(e,yl,r),u(Ws,e,r),p(e,Tl,r),p(e,Tn,r),t(Tn,om),p(e,ql,r),u(Fs,e,r),p(e,Pl,r),p(e,qn,r),t(qn,rm),p(e,Dl,r),u(Is,e,r),p(e,Cl,r),u(Rs,e,r),p(e,Ol,r),p(e,et,r),t(et,am),t(et,Vr),t(Vr,im),t(et,lm),p(e,Ll,r),u(Us,e,r),p(e,Sl,r),p(e,K,r),t(K,pm),t(K,Zr),t(Zr,cm),t(K,hm),t(K,Qr),t(Qr,fm),t(K,dm),t(K,ea),t(ea,um),t(K,mm),p(e,Bl,r),u(Gs,e,r),p(e,Al,r),p(e,Pn,r),t(Pn,km),p(e,Nl,r),u(Ms,e,r),p(e,Wl,r),p(e,Dn,r),t(Dn,wm),Fl=!0},p(e,[r]){const Xs={};r&2&&(Xs.$$scope={dirty:r,ctx:e}),Ne.$set(Xs)},i(e){Fl||(m(Q.$$.fragment,e),m(N.$$.fragment,e),m(zt.$$.fragment,e),m(Tt.$$.fragment,e),m(Pt.$$.fragment,e),m(Dt.$$.fragment,e),m(Ct.$$.fragment,e),m(Ot.$$.fragment,e),m(Lt.$$.fragment,e),m(St.$$.fragment,e),m(Bt.$$.fragment,e),m(At.$$.fragment,e),m(Ne.$$.fragment,e),m(Nt.$$.fragment,e),m(Wt.$$.fragment,e),m(Ft.$$.fragment,e),m(It.$$.fragment,e),m(Rt.$$.fragment,e),m(Ut.$$.fragment,e),m(Gt.$$.fragment,e),m(Mt.$$.fragment,e),m(Xt.$$.fragment,e),m(Kt.$$.fragment,e),m(Yt.$$.fragment,e),m(Ht.$$.fragment,e),m(Jt.$$.fragment,e),m(Vt.$$.fragment,e),m(Zt.$$.fragment,e),m(Qt.$$.fragment,e),m(es.$$.fragment,e),m(ts.$$.fragment,e),m(ss.$$.fragment,e),m(ns.$$.fragment,e),m(os.$$.fragment,e),m(rs.$$.fragment,e),m(as.$$.fragment,e),m(is.$$.fragment,e),m(ls.$$.fragment,e),m(ps.$$.fragment,e),m(cs.$$.fragment,e),m(hs.$$.fragment,e),m(fs.$$.fragment,e),m(ds.$$.fragment,e),m(us.$$.fragment,e),m(ms.$$.fragment,e),m(ks.$$.fragment,e),m(ws.$$.fragment,e),m(_s.$$.fragment,e),m($s.$$.fragment,e),m(zs.$$.fragment,e),m(bs.$$.fragment,e),m(gs.$$.fragment,e),m(vs.$$.fragment,e),m(Es.$$.fragment,e),m(js.$$.fragment,e),m(xs.$$.fragment,e),m(ys.$$.fragment,e),m(Ts.$$.fragment,e),m(qs.$$.fragment,e),m(Ps.$$.fragment,e),m(Ds.$$.fragment,e),m(Cs.$$.fragment,e),m(Os.$$.fragment,e),m(Ls.$$.fragment,e),m(Ss.$$.fragment,e),m(Bs.$$.fragment,e),m(As.$$.fragment,e),m(Ns.$$.fragment,e),m(Ws.$$.fragment,e),m(Fs.$$.fragment,e),m(Is.$$.fragment,e),m(Rs.$$.fragment,e),m(Us.$$.fragment,e),m(Gs.$$.fragment,e),m(Ms.$$.fragment,e),Fl=!0)},o(e){k(Q.$$.fragment,e),k(N.$$.fragment,e),k(zt.$$.fragment,e),k(Tt.$$.fragment,e),k(Pt.$$.fragment,e),k(Dt.$$.fragment,e),k(Ct.$$.fragment,e),k(Ot.$$.fragment,e),k(Lt.$$.fragment,e),k(St.$$.fragment,e),k(Bt.$$.fragment,e),k(At.$$.fragment,e),k(Ne.$$.fragment,e),k(Nt.$$.fragment,e),k(Wt.$$.fragment,e),k(Ft.$$.fragment,e),k(It.$$.fragment,e),k(Rt.$$.fragment,e),k(Ut.$$.fragment,e),k(Gt.$$.fragment,e),k(Mt.$$.fragment,e),k(Xt.$$.fragment,e),k(Kt.$$.fragment,e),k(Yt.$$.fragment,e),k(Ht.$$.fragment,e),k(Jt.$$.fragment,e),k(Vt.$$.fragment,e),k(Zt.$$.fragment,e),k(Qt.$$.fragment,e),k(es.$$.fragment,e),k(ts.$$.fragment,e),k(ss.$$.fragment,e),k(ns.$$.fragment,e),k(os.$$.fragment,e),k(rs.$$.fragment,e),k(as.$$.fragment,e),k(is.$$.fragment,e),k(ls.$$.fragment,e),k(ps.$$.fragment,e),k(cs.$$.fragment,e),k(hs.$$.fragment,e),k(fs.$$.fragment,e),k(ds.$$.fragment,e),k(us.$$.fragment,e),k(ms.$$.fragment,e),k(ks.$$.fragment,e),k(ws.$$.fragment,e),k(_s.$$.fragment,e),k($s.$$.fragment,e),k(zs.$$.fragment,e),k(bs.$$.fragment,e),k(gs.$$.fragment,e),k(vs.$$.fragment,e),k(Es.$$.fragment,e),k(js.$$.fragment,e),k(xs.$$.fragment,e),k(ys.$$.fragment,e),k(Ts.$$.fragment,e),k(qs.$$.fragment,e),k(Ps.$$.fragment,e),k(Ds.$$.fragment,e),k(Cs.$$.fragment,e),k(Os.$$.fragment,e),k(Ls.$$.fragment,e),k(Ss.$$.fragment,e),k(Bs.$$.fragment,e),k(As.$$.fragment,e),k(Ns.$$.fragment,e),k(Ws.$$.fragment,e),k(Fs.$$.fragment,e),k(Is.$$.fragment,e),k(Rs.$$.fragment,e),k(Us.$$.fragment,e),k(Gs.$$.fragment,e),k(Ms.$$.fragment,e),Fl=!1},d(e){s(z),e&&s(ie),e&&s(A),w(Q),e&&s(qe),w(N,e),e&&s(Pe),e&&s(le),e&&s(De),e&&s(y),e&&s(wt),e&&s(Ee),e&&s(_t),e&&s(re),e&&s($t),e&&s(ae),e&&s(na),w(zt,e),e&&s(oa),e&&s(Ce),e&&s(ra),e&&s(T),e&&s(aa),e&&s(Oe),e&&s(ia),e&&s(je),w(Tt),e&&s(la),e&&s(ue),e&&s(pa),w(Pt,e),e&&s(ca),e&&s(Se),e&&s(ha),e&&s(tn),e&&s(fa),w(Dt,e),e&&s(da),e&&s(sn),e&&s(ua),e&&s(xe),w(Ct),e&&s(ma),e&&s(E),e&&s(ka),e&&s(Ae),e&&s(wa),w(Ot,e),e&&s(_a),e&&s(I),e&&s($a),e&&s(j),e&&s(za),w(Lt,e),e&&s(ba),e&&s(R),e&&s(ga),w(St,e),e&&s(va),e&&s(me),e&&s(Ea),e&&s(ke),e&&s(ja),w(Bt,e),e&&s(xa),w(At,e),e&&s(ya),w(Ne,e),e&&s(Ta),e&&s(We),e&&s(qa),w(Nt,e),e&&s(Pa),e&&s(nn),e&&s(Da),w(Wt,e),e&&s(Ca),e&&s(Fe),e&&s(Oa),w(Ft,e),e&&s(La),w(It,e),e&&s(Sa),e&&s(Ie),e&&s(Ba),w(Rt,e),e&&s(Aa),w(Ut,e),e&&s(Na),e&&s(Re),e&&s(Wa),w(Gt,e),e&&s(Fa),w(Mt,e),e&&s(Ia),e&&s(Ue),e&&s(Ra),w(Xt,e),e&&s(Ua),e&&s(P),e&&s(Ga),e&&s(on),e&&s(Ma),w(Kt,e),e&&s(Xa),e&&s(Ge),e&&s(Ka),w(Yt,e),e&&s(Ya),e&&s(Me),e&&s(Ha),w(Ht,e),e&&s(Ja),w(Jt,e),e&&s(Va),e&&s(b),e&&s(Za),e&&s(D),e&&s(Qa),w(Vt,e),e&&s(ei),w(Zt,e),e&&s(ti),e&&s(U),e&&s(si),e&&s(rn),e&&s(ni),w(Qt,e),e&&s(oi),e&&s(an),e&&s(ri),e&&s(ln),e&&s(ai),w(es,e),e&&s(ii),w(ts,e),e&&s(li),e&&s(pn),e&&s(pi),w(ss,e),e&&s(ci),w(ns,e),e&&s(hi),e&&s(cn),e&&s(fi),w(os,e),e&&s(di),e&&s(Xe),e&&s(ui),w(rs,e),e&&s(mi),w(as,e),e&&s(ki),e&&s(hn),e&&s(wi),w(is,e),e&&s(_i),e&&s(we),e&&s($i),w(ls,e),e&&s(zi),e&&s(_e),e&&s(bi),e&&s(C),e&&s(gi),w(ps,e),e&&s(vi),e&&s(Ke),e&&s(Ei),w(cs,e),e&&s(ji),e&&s($e),e&&s(xi),e&&s(fn),e&&s(yi),e&&s(ye),w(hs),e&&s(Ti),e&&s(He),e&&s(qi),w(fs,e),e&&s(Pi),e&&s(G),e&&s(Di),e&&s(dn),e&&s(Ci),w(ds,e),e&&s(Oi),e&&s(Je),e&&s(Li),w(us,e),e&&s(Si),w(ms,e),e&&s(Bi),e&&s(un),e&&s(Ai),w(ks,e),e&&s(Ni),e&&s(x),e&&s(Wi),e&&s(mn),e&&s(Fi),w(ws,e),e&&s(Ii),e&&s(kn),e&&s(Ri),w(_s,e),e&&s(Ui),w($s,e),e&&s(Gi),e&&s(wn),e&&s(Mi),w(zs,e),e&&s(Xi),e&&s(ze),e&&s(Ki),w(bs,e),e&&s(Yi),w(gs,e),e&&s(Hi),e&&s(_n),e&&s(Ji),w(vs,e),e&&s(Vi),e&&s($n),e&&s(Zi),w(Es,e),e&&s(Qi),w(js,e),e&&s(el),e&&s(be),e&&s(tl),w(xs,e),e&&s(sl),e&&s(zn),e&&s(nl),w(ys,e),e&&s(ol),e&&s(bn),e&&s(rl),e&&s(Te),w(Ts),e&&s(al),e&&s(Ze),e&&s(il),w(qs,e),e&&s(ll),e&&s(gn),e&&s(pl),e&&s(vn),e&&s(cl),w(Ps,e),e&&s(hl),e&&s(M),e&&s(fl),e&&s(Qe),e&&s(dl),w(Ds,e),e&&s(ul),e&&s(En),e&&s(ml),w(Cs,e),e&&s(kl),w(Os,e),e&&s(wl),e&&s(jn),e&&s(_l),w(Ls,e),e&&s($l),e&&s(O),e&&s(zl),e&&s(xn),e&&s(bl),w(Ss,e),e&&s(gl),e&&s(yn),e&&s(vl),w(Bs,e),e&&s(El),w(As,e),e&&s(jl),e&&s(X),e&&s(xl),w(Ns,e),e&&s(yl),w(Ws,e),e&&s(Tl),e&&s(Tn),e&&s(ql),w(Fs,e),e&&s(Pl),e&&s(qn),e&&s(Dl),w(Is,e),e&&s(Cl),w(Rs,e),e&&s(Ol),e&&s(et),e&&s(Ll),w(Us,e),e&&s(Sl),e&&s(K),e&&s(Bl),w(Gs,e),e&&s(Al),e&&s(Pn),e&&s(Nl),w(Ms,e),e&&s(Wl),e&&s(Dn)}}}const Uw={local:"building-a-tokenizer-block-by-block",sections:[{local:"acquiring-a-corpus",title:"Acquiring a corpus"},{local:"building-a-wordpiece-tokenizer-from-scratch",title:"Building a WordPiece tokenizer from scratch"},{local:"building-a-bpe-tokenizer-from-scratch",title:"Building a BPE tokenizer from scratch"},{local:"building-a-unigram-tokenizer-from-scratch",title:"Building a Unigram tokenizer from scratch"}],title:"Building a tokenizer, block by block"};function Gw(sa){return Bw(()=>{new URL(document.location).searchParams.get("fw")}),[]}class Vw extends Dw{constructor(z){super();Cw(this,z,Gw,Rw,Ow,{})}}export{Vw as default,Uw as metadata};
