import{S as bd,i as jd,s as kd,e as o,k as c,w,t as n,M as _d,c as l,d as t,m as h,x as b,a as r,h as a,b as y,N as yd,F as s,g as p,y as j,o as m,p as xl,q as g,B as k,v as $d,O as vd,n as wl}from"../../chunks/vendor-e7c81d8a.js";import{T as Mc}from"../../chunks/Tip-989931f5.js";import{Y as bl}from"../../chunks/Youtube-365ea064.js";import{I as Dt}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";import{C as $}from"../../chunks/CodeBlock-105940ae.js";import{D as wd}from"../../chunks/DocNotebookDropdown-928568b4.js";import{F as Ed}from"../../chunks/FrameworkSwitch-287292d8.js";const{document:xd}=vd;function Od(D){let f,x;return f=new wd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"}]}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Id(D){let f,x;return f=new wd({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"}]}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Cd(D){let f,x;return{c(){f=o("p"),x=n("\u26A0\uFE0F When tokenizing a single sentence, you won\u2019t always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! It\u2019s only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference.")},l(d){f=l(d,"P",{});var _=r(f);x=a(_,"\u26A0\uFE0F When tokenizing a single sentence, you won\u2019t always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! It\u2019s only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference."),_.forEach(t)},m(d,_){p(d,f,_),s(f,x)},d(d){d&&t(f)}}}function zd(D){let f,x,d,_,P,E,F,A,N,S,Y,I,B,z;return{c(){f=o("p"),x=n("The notion of what a word is is complicated. For instance, does \u201CI\u2019ll\u201D (a contraction of \u201CI will\u201D) count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words."),d=c(),_=o("p"),P=n("\u270F\uFE0F "),E=o("strong"),F=n("Try it out!"),A=n(" Create a tokenizer from the "),N=o("code"),S=n("bert-base-cased"),Y=n(" and "),I=o("code"),B=n("roberta-base"),z=n(" checkpoints and tokenize \u201D81s\u201D with them. What do you observe? What are the word IDs?")},l(T){f=l(T,"P",{});var O=r(f);x=a(O,"The notion of what a word is is complicated. For instance, does \u201CI\u2019ll\u201D (a contraction of \u201CI will\u201D) count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words."),O.forEach(t),d=h(T),_=l(T,"P",{});var X=r(_);P=a(X,"\u270F\uFE0F "),E=l(X,"STRONG",{});var q=r(E);F=a(q,"Try it out!"),q.forEach(t),A=a(X," Create a tokenizer from the "),N=l(X,"CODE",{});var V=r(N);S=a(V,"bert-base-cased"),V.forEach(t),Y=a(X," and "),I=l(X,"CODE",{});var H=r(I);B=a(H,"roberta-base"),H.forEach(t),z=a(X," checkpoints and tokenize \u201D81s\u201D with them. What do you observe? What are the word IDs?"),X.forEach(t)},m(T,O){p(T,f,O),s(f,x),p(T,d,O),p(T,_,O),s(_,P),s(_,E),s(E,F),s(_,A),s(_,N),s(N,S),s(_,Y),s(_,I),s(I,B),s(_,z)},d(T){T&&t(f),T&&t(d),T&&t(_)}}}function Td(D){let f,x,d,_,P;return{c(){f=o("p"),x=n("\u270F\uFE0F "),d=o("strong"),_=n("Try it out!"),P=n(" Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you.")},l(E){f=l(E,"P",{});var F=r(f);x=a(F,"\u270F\uFE0F "),d=l(F,"STRONG",{});var A=r(d);_=a(A,"Try it out!"),A.forEach(t),P=a(F," Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you."),F.forEach(t)},m(E,F){p(E,f,F),s(f,x),s(f,d),s(d,_),s(f,P)},d(E){E&&t(f)}}}function Dd(D){let f,x;return f=new bl({props:{id:"PrX4CjrVnNc"}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Pd(D){let f,x;return f=new bl({props:{id:"0E7ltQB7fM8"}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Sd(D){let f,x,d,_,P,E,F,A,N,S,Y,I,B,z,T,O,X,q,V,H,Z;return S=new $({props:{codee:`from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),q=new $({props:{codee:`print(inputs["input_ids"].shape)
print(outputs.logits.shape),`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),H=new $({props:{codee:`(1, 19)
(1, 19, 9),`,highlighted:`(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>)
(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>)`}}),{c(){f=o("p"),x=n("First we need to tokenize our input and pass it through the model. This is done exactly as in "),d=o("a"),_=n("Chapter 2"),P=n("; we instantiate the tokenizer and the model using the "),E=o("code"),F=n("TFAutoXxx"),A=n(" classes and then use them on our example:"),N=c(),w(S.$$.fragment),Y=c(),I=o("p"),B=n("Since we\u2019re using "),z=o("code"),T=n("TFAutoModelForTokenClassification"),O=n(" here, we get one set of logits for each token in the input sequence:"),X=c(),w(q.$$.fragment),V=c(),w(H.$$.fragment),this.h()},l(u){f=l(u,"P",{});var v=r(f);x=a(v,"First we need to tokenize our input and pass it through the model. This is done exactly as in "),d=l(v,"A",{href:!0});var de=r(d);_=a(de,"Chapter 2"),de.forEach(t),P=a(v,"; we instantiate the tokenizer and the model using the "),E=l(v,"CODE",{});var Ee=r(E);F=a(Ee,"TFAutoXxx"),Ee.forEach(t),A=a(v," classes and then use them on our example:"),v.forEach(t),N=h(u),b(S.$$.fragment,u),Y=h(u),I=l(u,"P",{});var oe=r(I);B=a(oe,"Since we\u2019re using "),z=l(oe,"CODE",{});var re=r(z);T=a(re,"TFAutoModelForTokenClassification"),re.forEach(t),O=a(oe," here, we get one set of logits for each token in the input sequence:"),oe.forEach(t),X=h(u),b(q.$$.fragment,u),V=h(u),b(H.$$.fragment,u),this.h()},h(){y(d,"href","/course/chapter2")},m(u,v){p(u,f,v),s(f,x),s(f,d),s(d,_),s(f,P),s(f,E),s(E,F),s(f,A),p(u,N,v),j(S,u,v),p(u,Y,v),p(u,I,v),s(I,B),s(I,z),s(z,T),s(I,O),p(u,X,v),j(q,u,v),p(u,V,v),j(H,u,v),Z=!0},i(u){Z||(g(S.$$.fragment,u),g(q.$$.fragment,u),g(H.$$.fragment,u),Z=!0)},o(u){m(S.$$.fragment,u),m(q.$$.fragment,u),m(H.$$.fragment,u),Z=!1},d(u){u&&t(f),u&&t(N),k(S,u),u&&t(Y),u&&t(I),u&&t(X),k(q,u),u&&t(V),k(H,u)}}}function Fd(D){let f,x,d,_,P,E,F,A,N,S,Y,I,B,z,T,O,X,q,V,H,Z;return S=new $({props:{codee:`from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),q=new $({props:{codee:`print(inputs["input_ids"].shape)
print(outputs.logits.shape),`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),H=new $({props:{codee:`torch.Size([1, 19])
torch.Size([1, 19, 9]),`,highlighted:`torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>])
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>])`}}),{c(){f=o("p"),x=n("First we need to tokenize our input and pass it through the model. This is done exactly as in "),d=o("a"),_=n("Chapter 2"),P=n("; we instantiate the tokenizer and the model using the "),E=o("code"),F=n("AutoXxx"),A=n(" classes and then use them on our example:"),N=c(),w(S.$$.fragment),Y=c(),I=o("p"),B=n("Since we\u2019re using "),z=o("code"),T=n("AutoModelForTokenClassification"),O=n(" here, we get one set of logits for each token in the input sequence:"),X=c(),w(q.$$.fragment),V=c(),w(H.$$.fragment),this.h()},l(u){f=l(u,"P",{});var v=r(f);x=a(v,"First we need to tokenize our input and pass it through the model. This is done exactly as in "),d=l(v,"A",{href:!0});var de=r(d);_=a(de,"Chapter 2"),de.forEach(t),P=a(v,"; we instantiate the tokenizer and the model using the "),E=l(v,"CODE",{});var Ee=r(E);F=a(Ee,"AutoXxx"),Ee.forEach(t),A=a(v," classes and then use them on our example:"),v.forEach(t),N=h(u),b(S.$$.fragment,u),Y=h(u),I=l(u,"P",{});var oe=r(I);B=a(oe,"Since we\u2019re using "),z=l(oe,"CODE",{});var re=r(z);T=a(re,"AutoModelForTokenClassification"),re.forEach(t),O=a(oe," here, we get one set of logits for each token in the input sequence:"),oe.forEach(t),X=h(u),b(q.$$.fragment,u),V=h(u),b(H.$$.fragment,u),this.h()},h(){y(d,"href","/course/chapter2")},m(u,v){p(u,f,v),s(f,x),s(f,d),s(d,_),s(f,P),s(f,E),s(E,F),s(f,A),p(u,N,v),j(S,u,v),p(u,Y,v),p(u,I,v),s(I,B),s(I,z),s(z,T),s(I,O),p(u,X,v),j(q,u,v),p(u,V,v),j(H,u,v),Z=!0},i(u){Z||(g(S.$$.fragment,u),g(q.$$.fragment,u),g(H.$$.fragment,u),Z=!0)},o(u){m(S.$$.fragment,u),m(q.$$.fragment,u),m(H.$$.fragment,u),Z=!1},d(u){u&&t(f),u&&t(N),k(S,u),u&&t(Y),u&&t(I),u&&t(X),k(q,u),u&&t(V),k(H,u)}}}function qd(D){let f,x;return f=new $({props:{codee:`import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

probabilities = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
predictions = predictions.numpy().tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Rd(D){let f,x;return f=new $({props:{codee:`import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions),`,highlighted:`<span class="hljs-keyword">import</span> torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
predictions = outputs.logits.argmax(dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){w(f.$$.fragment)},l(d){b(f.$$.fragment,d)},m(d,_){j(f,d,_),x=!0},i(d){x||(g(f.$$.fragment,d),x=!0)},o(d){m(f.$$.fragment,d),x=!1},d(d){k(f,d)}}}function Bd(D){let f,x,d,_,P,E,F,A,N,S,Y,I,B,z,T,O,X,q,V,H,Z,u,v,de,Ee,oe,re,jl,kl,Ia,as,Ca,Fe,_l,Ks,yl,$l,za,qe,Pt,Oe,Zs,vl,El,et,Ol,Il,St,Cl,os,Ie,st,Ft,zl,Tl,tt,Dl,Pl,nt,Sl,Fl,Ce,at,qt,ql,Rl,ot,Bl,Al,lt,Xl,Ta,Re,Da,ze,Be,Rt,ls,Hl,Bt,Gl,Pa,rs,Sa,Ae,Nl,At,Ml,Wl,Fa,Xe,Ll,Xt,Ul,Yl,qa,rt,Vl,Ra,is,Ba,He,Ql,Ht,Jl,Kl,Aa,ps,Xa,ee,Zl,Gt,er,sr,Nt,tr,nr,Mt,ar,or,Wt,lr,rr,Ha,cs,Ga,hs,Na,Ge,ir,Lt,pr,cr,Ma,fs,Wa,ds,La,it,hr,Ua,us,Ya,ms,Va,je,fr,Ut,dr,ur,Yt,mr,gr,Qa,gs,Ja,xs,Ka,Q,xr,Vt,wr,br,Qt,jr,kr,Jt,_r,yr,Kt,$r,vr,Zt,Er,Or,Za,Ne,eo,ke,Ir,en,Cr,zr,sn,Tr,Dr,so,M,Pr,tn,Sr,Fr,nn,qr,Rr,an,Br,Ar,on,Xr,Hr,ln,Gr,Nr,rn,Mr,Wr,to,ws,no,bs,ao,_e,Lr,pn,Ur,Yr,cn,Vr,Qr,oo,Me,lo,Te,We,hn,js,Jr,ks,Kr,fn,Zr,ei,ro,se,si,pt,ti,ni,dn,ai,oi,ct,li,ri,un,ii,pi,io,ue,me,ht,De,Le,mn,_s,ci,gn,hi,po,Ue,fi,ys,xn,di,ui,co,$s,ho,vs,fo,ft,mi,uo,Es,mo,Os,go,W,gi,wn,xi,wi,bn,bi,ji,jn,ki,_i,kn,yi,$i,_n,vi,Ei,yn,Oi,Ii,xo,ye,Ye,$n,Ci,zi,vn,Ti,Di,Pi,dt,En,Si,Fi,qi,Ve,On,Ri,Bi,In,Ai,Xi,wo,Qe,Hi,Cn,Gi,Ni,bo,Pe,Je,zn,Is,Mi,Tn,Wi,jo,ge,xe,ut,mt,Li,ko,we,be,gt,Cs,_o,Ke,Ui,Dn,Yi,Vi,yo,zs,$o,Ts,vo,C,Qi,Pn,Ji,Ki,Sn,Zi,ep,Fn,sp,tp,qn,np,ap,Rn,op,lp,Bn,rp,ip,An,pp,cp,Xn,hp,fp,Hn,dp,up,Gn,mp,gp,Nn,xp,wp,Eo,G,bp,Mn,jp,kp,Wn,_p,yp,Ln,$p,vp,Un,Ep,Op,Yn,Ip,Cp,Vn,zp,Tp,Qn,Dp,Pp,Jn,Sp,Fp,Oo,Ds,Ps,Wc,Io,Ze,qp,Kn,Rp,Bp,Co,Ss,zo,Fs,To,ie,Ap,Zn,Xp,Hp,ea,Gp,Np,sa,Mp,Wp,Do,qs,Po,Rs,So,pe,Lp,ta,Up,Yp,na,Vp,Qp,aa,Jp,Kp,Fo,Bs,qo,es,Zp,oa,ec,sc,Ro,As,Bo,xt,tc,Ao,Xs,Xo,Hs,Ho,wt,nc,Go,Se,ss,la,Gs,ac,ra,oc,No,L,lc,ia,rc,ic,pa,pc,cc,ca,hc,fc,ha,dc,uc,fa,mc,gc,da,xc,wc,Mo,J,bc,ua,jc,kc,ma,_c,yc,ga,$c,vc,xa,Ec,Oc,wa,Ic,Cc,Wo,Ns,Lo,Ms,Uo,K,zc,ba,Tc,Dc,ja,Pc,Sc,ka,Fc,qc,_a,Rc,Bc,ya,Ac,Xc,Yo,Ws,Vo,bt,Hc,Qo,Ls,Jo,jt,Gc,Ko;d=new Ed({props:{fw:D[0]}}),A=new Dt({});const Lc=[Id,Od],Us=[];function Uc(e,i){return e[0]==="pt"?0:1}B=Uc(D),z=Us[B]=Lc[B](D),as=new bl({props:{id:"g8quOxoqhHQ"}}),Re=new Mc({props:{warning:!0,$$slots:{default:[Cd]},$$scope:{ctx:D}}}),ls=new Dt({}),rs=new bl({props:{id:"3umI3tm27Vw"}}),is=new $({props:{codee:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding)),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
encoding = tokenizer(example)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(encoding))`}}),ps=new $({props:{codee:"<class 'transformers.tokenization_utils_base.BatchEncoding'>,",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;</span>&gt;'}}),cs=new $({props:{codee:"tokenizer.is_fast,",highlighted:"tokenizer.is_fast"}}),hs=new $({props:{codee:"True,",highlighted:'<span class="hljs-literal">True</span>'}}),fs=new $({props:{codee:"encoding.is_fast,",highlighted:"encoding.is_fast"}}),ds=new $({props:{codee:"True,",highlighted:'<span class="hljs-literal">True</span>'}}),us=new $({props:{codee:"encoding.tokens(),",highlighted:"encoding.tokens()"}}),ms=new $({props:{codee:`['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]'],`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;My&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;and&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;work&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>,
 <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),gs=new $({props:{codee:"encoding.word_ids(),",highlighted:"encoding.word_ids()"}}),xs=new $({props:{codee:"[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None],",highlighted:'[<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-literal">None</span>]'}}),Ne=new Mc({props:{$$slots:{default:[zd]},$$scope:{ctx:D}}}),ws=new $({props:{codee:`start, end = encoding.word_to_chars(3)
example[start:end],`,highlighted:`start, end = encoding.word_to_chars(<span class="hljs-number">3</span>)
example[start:end]`}}),bs=new $({props:{codee:"Sylvain,",highlighted:"Sylvain"}}),Me=new Mc({props:{$$slots:{default:[Td]},$$scope:{ctx:D}}}),js=new Dt({});const Yc=[Pd,Dd],Ys=[];function Vc(e,i){return e[0]==="pt"?0:1}ue=Vc(D),me=Ys[ue]=Yc[ue](D),_s=new Dt({}),$s=new $({props:{codee:`from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn."),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),vs=new $({props:{codee:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}],`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Es=new $({props:{codee:`from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn."),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="hljs-string">&quot;simple&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),Os=new $({props:{codee:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}],`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Is=new Dt({});const Qc=[Fd,Sd],Vs=[];function Jc(e,i){return e[0]==="pt"?0:1}ge=Jc(D),xe=Vs[ge]=Qc[ge](D);const Kc=[Rd,qd],Qs=[];function Zc(e,i){return e[0]==="pt"?0:1}return we=Zc(D),be=Qs[we]=Kc[we](D),Cs=new $({props:{codee:"[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0],",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),zs=new $({props:{codee:"model.config.id2label,",highlighted:"model.config.id2label"}}),Ts=new $({props:{codee:`{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'},`,highlighted:`{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;O&#x27;</span>,
 <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;B-MISC&#x27;</span>,
 <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;I-MISC&#x27;</span>,
 <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;B-PER&#x27;</span>,
 <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>,
 <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;B-ORG&#x27;</span>,
 <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>,
 <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;B-LOC&#x27;</span>,
 <span class="hljs-number">8</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>}`}}),Ss=new $({props:{codee:`results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results),`,highlighted:`results = []
tokens = inputs.tokens()

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        results.append(
            {<span class="hljs-string">&quot;entity&quot;</span>: label, <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred], <span class="hljs-string">&quot;word&quot;</span>: tokens[idx]}
        )

<span class="hljs-built_in">print</span>(results)`}}),Fs=new $({props:{codee:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}],`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>}]`}}),qs=new $({props:{codee:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"],`,highlighted:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]`}}),Rs=new $({props:{codee:`[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)],`,highlighted:`[(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">7</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">11</span>, <span class="hljs-number">12</span>), (<span class="hljs-number">12</span>, <span class="hljs-number">14</span>), (<span class="hljs-number">14</span>, <span class="hljs-number">16</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">18</span>), (<span class="hljs-number">19</span>, <span class="hljs-number">22</span>), (<span class="hljs-number">23</span>, <span class="hljs-number">24</span>), (<span class="hljs-number">25</span>, <span class="hljs-number">29</span>), (<span class="hljs-number">30</span>, <span class="hljs-number">32</span>),
 (<span class="hljs-number">33</span>, <span class="hljs-number">35</span>), (<span class="hljs-number">35</span>, <span class="hljs-number">40</span>), (<span class="hljs-number">41</span>, <span class="hljs-number">45</span>), (<span class="hljs-number">46</span>, <span class="hljs-number">48</span>), (<span class="hljs-number">49</span>, <span class="hljs-number">57</span>), (<span class="hljs-number">57</span>, <span class="hljs-number">58</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)]`}}),Bs=new $({props:{codee:"example[12:14],",highlighted:'example[<span class="hljs-number">12</span>:<span class="hljs-number">14</span>]'}}),As=new $({props:{codee:"yl,",highlighted:"yl"}}),Xs=new $({props:{codee:`results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results),`,highlighted:`results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        start, end = offsets[idx]
        results.append(
            {
                <span class="hljs-string">&quot;entity&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred],
                <span class="hljs-string">&quot;word&quot;</span>: tokens[idx],
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )

<span class="hljs-built_in">print</span>(results)`}}),Hs=new $({props:{codee:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}],`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Gs=new Dt({}),Ns=new $({props:{codee:"example[33:45],",highlighted:'example[<span class="hljs-number">33</span>:<span class="hljs-number">45</span>]'}}),Ms=new $({props:{codee:"Hugging Face,",highlighted:"Hugging Face"}}),Ws=new $({props:{codee:`import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results),`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

idx = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> idx &lt; <span class="hljs-built_in">len</span>(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        <span class="hljs-comment"># Remove the B- or I-</span>
        label = label[<span class="hljs-number">2</span>:]
        start, _ = offsets[idx]

        <span class="hljs-comment"># Grab all the tokens labeled with I-label</span>
        all_scores = []
        <span class="hljs-keyword">while</span> (
            idx &lt; <span class="hljs-built_in">len</span>(predictions)
            <span class="hljs-keyword">and</span> model.config.id2label[predictions[idx]] == <span class="hljs-string">f&quot;I-<span class="hljs-subst">{label}</span>&quot;</span>
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += <span class="hljs-number">1</span>

        <span class="hljs-comment"># The score is the mean of all the scores of the tokens in that grouped entity</span>
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                <span class="hljs-string">&quot;entity_group&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: score,
                <span class="hljs-string">&quot;word&quot;</span>: word,
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )
    idx += <span class="hljs-number">1</span>

<span class="hljs-built_in">print</span>(results)`}}),Ls=new $({props:{codee:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}],`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),{c(){f=o("meta"),x=c(),w(d.$$.fragment),_=c(),P=o("h1"),E=o("a"),F=o("span"),w(A.$$.fragment),N=c(),S=o("span"),Y=n("Fast tokenizers' special powers"),I=c(),z.c(),T=c(),O=o("p"),X=n("In this section we will take a closer look at the capabilities of the tokenizers in \u{1F917} Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers \u2014 especially those backed by the \u{1F917} Tokenizers library \u2014 can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the "),q=o("code"),V=n("token-classification"),H=n(" (that we called "),Z=o("code"),u=n("ner"),v=n(") and "),de=o("code"),Ee=n("question-answering"),oe=n(" pipelines that we first encountered in "),re=o("a"),jl=n("Chapter 1"),kl=n("."),Ia=c(),w(as.$$.fragment),Ca=c(),Fe=o("p"),_l=n("In the following discussion, we will often make the distinction between \u201Cslow\u201D and \u201Cfast\u201D tokenizers. Slow tokenizers are those written in Python inside the \u{1F917} Transformers library, while the fast versions are the ones provided by \u{1F917} Tokenizers, which are written in Rust. If you remember the table from "),Ks=o("a"),yl=n("Chapter 5"),$l=n(" that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow:"),za=c(),qe=o("table"),Pt=o("thead"),Oe=o("tr"),Zs=o("th"),vl=n("Fast tokenizer"),El=c(),et=o("th"),Ol=n("Slow tokenizer"),Il=c(),St=o("th"),Cl=c(),os=o("tbody"),Ie=o("tr"),st=o("td"),Ft=o("code"),zl=n("batched=True"),Tl=c(),tt=o("td"),Dl=n("10.8s"),Pl=c(),nt=o("td"),Sl=n("4min41s"),Fl=c(),Ce=o("tr"),at=o("td"),qt=o("code"),ql=n("batched=False"),Rl=c(),ot=o("td"),Bl=n("59.2s"),Al=c(),lt=o("td"),Xl=n("5min3s"),Ta=c(),w(Re.$$.fragment),Da=c(),ze=o("h2"),Be=o("a"),Rt=o("span"),w(ls.$$.fragment),Hl=c(),Bt=o("span"),Gl=n("Batch encoding"),Pa=c(),w(rs.$$.fragment),Sa=c(),Ae=o("p"),Nl=n("The output of a tokenizer isn\u2019t a simple Python dictionary; what we get is actually a special "),At=o("code"),Ml=n("BatchEncoding"),Wl=n(" object. It\u2019s a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers."),Fa=c(),Xe=o("p"),Ll=n("Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from \u2014 a feature we call "),Xt=o("em"),Ul=n("offset mapping"),Yl=n(". This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it\u2019s inside, and vice versa."),qa=c(),rt=o("p"),Vl=n("Let\u2019s take a look at an example:"),Ra=c(),w(is.$$.fragment),Ba=c(),He=o("p"),Ql=n("As mentioned previously, we get a "),Ht=o("code"),Jl=n("BatchEncoding"),Kl=n(" object in the tokenizer\u2019s output:"),Aa=c(),w(ps.$$.fragment),Xa=c(),ee=o("p"),Zl=n("Since the "),Gt=o("code"),er=n("AutoTokenizer"),sr=n(" class picks a fast tokenizer by default, we can use the additional methods this "),Nt=o("code"),tr=n("BatchEncoding"),nr=n(" object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute "),Mt=o("code"),ar=n("is_fast"),or=n(" of the "),Wt=o("code"),lr=n("tokenizer"),rr=n(":"),Ha=c(),w(cs.$$.fragment),Ga=c(),w(hs.$$.fragment),Na=c(),Ge=o("p"),ir=n("or check the same attribute of our "),Lt=o("code"),pr=n("encoding"),cr=n(":"),Ma=c(),w(fs.$$.fragment),Wa=c(),w(ds.$$.fragment),La=c(),it=o("p"),hr=n("Let\u2019s see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens:"),Ua=c(),w(us.$$.fragment),Ya=c(),w(ms.$$.fragment),Va=c(),je=o("p"),fr=n("In this case the token at index 5 is "),Ut=o("code"),dr=n("##yl"),ur=n(", which is part of the word \u201CSylvain\u201D in the original sentence. We can also use the "),Yt=o("code"),mr=n("word_ids()"),gr=n(" method to get the index of the word each token comes from:"),Qa=c(),w(gs.$$.fragment),Ja=c(),w(xs.$$.fragment),Ka=c(),Q=o("p"),xr=n("We can see that the tokenizer\u2019s special tokens "),Vt=o("code"),wr=n("[CLS]"),br=n(" and "),Qt=o("code"),jr=n("[SEP]"),kr=n(" are mapped to "),Jt=o("code"),_r=n("None"),yr=n(", and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the "),Kt=o("code"),$r=n("##"),vr=n(" prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it\u2019s a fast one. In the next chapter, we\u2019ll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called "),Zt=o("em"),Er=n("whole word masking"),Or=n(")."),Za=c(),w(Ne.$$.fragment),eo=c(),ke=o("p"),Ir=n("Similarly, there is a "),en=o("code"),Cr=n("sentence_ids()"),zr=n(" method that we can use to map a token to the sentence it came from (though in this case, the "),sn=o("code"),Tr=n("token_type_ids"),Dr=n(" returned by the tokenizer can give us the same information)."),so=c(),M=o("p"),Pr=n("Lastly, we can map any word or token to characters in the original text, and vice versa, via the "),tn=o("code"),Sr=n("word_to_chars()"),Fr=n(" or "),nn=o("code"),qr=n("token_to_chars()"),Rr=n(" and "),an=o("code"),Br=n("char_to_word()"),Ar=n(" or "),on=o("code"),Xr=n("char_to_token()"),Hr=n(" methods. For instance, the "),ln=o("code"),Gr=n("word_ids()"),Nr=n(" method told us that "),rn=o("code"),Mr=n("##yl"),Wr=n(" is part of the word at index 3, but which word is it in the sentence? We can find out like this:"),to=c(),w(ws.$$.fragment),no=c(),w(bs.$$.fragment),ao=c(),_e=o("p"),Lr=n("As we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of "),pn=o("em"),Ur=n("offsets"),Yr=n(". To illustrate their use, next we\u2019ll show you how to replicate the results of the "),cn=o("code"),Vr=n("token-classification"),Qr=n(" pipeline manually."),oo=c(),w(Me.$$.fragment),lo=c(),Te=o("h2"),We=o("a"),hn=o("span"),w(js.$$.fragment),Jr=c(),ks=o("span"),Kr=n("Inside the "),fn=o("code"),Zr=n("token-classification"),ei=n(" pipeline"),ro=c(),se=o("p"),si=n("In "),pt=o("a"),ti=n("Chapter 1"),ni=n(" we got our first taste of applying NER \u2014 where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations \u2014 with the \u{1F917} Transformers "),dn=o("code"),ai=n("pipeline()"),oi=n(" function. Then, in "),ct=o("a"),li=n("Chapter 2"),ri=n(", we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: tokenization, passing the inputs through the model, and post-processing. The first two steps in the "),un=o("code"),ii=n("token-classification"),pi=n(" pipeline are the same as in any other pipeline, but the post-processing is a little more complex \u2014 let\u2019s see how!"),io=c(),me.c(),ht=c(),De=o("h3"),Le=o("a"),mn=o("span"),w(_s.$$.fragment),ci=c(),gn=o("span"),hi=n("Getting the base results with the pipeline"),po=c(),Ue=o("p"),fi=n("First, let\u2019s grab a token classification pipeline so we can get some results to compare manually. The model used by default is "),ys=o("a"),xn=o("code"),di=n("dbmdz/bert-large-cased-finetuned-conll03-english"),ui=n("; it performs NER on sentences:"),co=c(),w($s.$$.fragment),ho=c(),w(vs.$$.fragment),fo=c(),ft=o("p"),mi=n("The model properly identified each token generated by \u201CSylvain\u201D as a person, each token generated by \u201CHugging Face\u201D as an organization, and the token \u201CBrooklyn\u201D as a location. We can also ask the pipeline to group together the tokens that correspond to the same entity:"),uo=c(),w(Es.$$.fragment),mo=c(),w(Os.$$.fragment),go=c(),W=o("p"),gi=n("The "),wn=o("code"),xi=n("aggregation_strategy"),wi=n(" picked will change the scores computed for each grouped entity. With "),bn=o("code"),bi=n('"simple"'),ji=n(" the score is just the mean of the scores of each token in the given entity: for instance, the score of \u201CSylvain\u201D is the mean of the scores we saw in the previous example for the tokens "),jn=o("code"),ki=n("S"),_i=n(", "),kn=o("code"),yi=n("##yl"),$i=n(", "),_n=o("code"),vi=n("##va"),Ei=n(", and "),yn=o("code"),Oi=n("##in"),Ii=n(". Other strategies available are:"),xo=c(),ye=o("ul"),Ye=o("li"),$n=o("code"),Ci=n('"first"'),zi=n(", where the score of each entity is the score of the first token of that entity (so for \u201CSylvain\u201D it would be 0.993828, the score of the token "),vn=o("code"),Ti=n("S"),Di=n(")"),Pi=c(),dt=o("li"),En=o("code"),Si=n('"max"'),Fi=n(", where the score of each entity is the maximum score of the tokens in that entity (so for \u201CHugging Face\u201D it would be 0.98879766, the score of \u201CFace\u201D)"),qi=c(),Ve=o("li"),On=o("code"),Ri=n('"average"'),Bi=n(", where the score of each entity is the average of the scores of the words composing that entity (so for \u201CSylvain\u201D there would be no difference from the "),In=o("code"),Ai=n('"simple"'),Xi=n(" strategy, but \u201CHugging Face\u201D would have a score of 0.9819, the average of the scores for \u201CHugging\u201D, 0.975, and \u201CFace\u201D, 0.98879)"),wo=c(),Qe=o("p"),Hi=n("Now let\u2019s see how to obtain these results without using the "),Cn=o("code"),Gi=n("pipeline()"),Ni=n(" function!"),bo=c(),Pe=o("h3"),Je=o("a"),zn=o("span"),w(Is.$$.fragment),Mi=c(),Tn=o("span"),Wi=n("From inputs to predictions"),jo=c(),xe.c(),ut=c(),mt=o("p"),Li=n("We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9. Like for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order):"),ko=c(),be.c(),gt=c(),w(Cs.$$.fragment),_o=c(),Ke=o("p"),Ui=n("The "),Dn=o("code"),Yi=n("model.config.id2label"),Vi=n(" attribute contains the mapping of indexes to labels that we can use to make sense of the predictions:"),yo=c(),w(zs.$$.fragment),$o=c(),w(Ts.$$.fragment),vo=c(),C=o("p"),Qi=n("As we saw earlier, there are 9 labels: "),Pn=o("code"),Ji=n("O"),Ki=n(" is the label for the tokens that are not in any named entity (it stands for \u201Coutside\u201D), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label "),Sn=o("code"),Zi=n("B-XXX"),ep=n(" indicates the token is at the beginning of an entity "),Fn=o("code"),sp=n("XXX"),tp=n(" and the label "),qn=o("code"),np=n("I-XXX"),ap=n(" indicates the token is inside the entity "),Rn=o("code"),op=n("XXX"),lp=n(". For instance, in the current example we would expect our model to classify the token "),Bn=o("code"),rp=n("S"),ip=n(" as "),An=o("code"),pp=n("B-PER"),cp=n(" (beginning of a person entity) and the tokens "),Xn=o("code"),hp=n("##yl"),fp=n(", "),Hn=o("code"),dp=n("##va"),up=n(" and "),Gn=o("code"),mp=n("##in"),gp=n(" as "),Nn=o("code"),xp=n("I-PER"),wp=n(" (inside a person entity)."),Eo=c(),G=o("p"),bp=n("You might think the model was wrong in this case as it gave the label "),Mn=o("code"),jp=n("I-PER"),kp=n(" to all four of these tokens, but that\u2019s not entirely true. There are actually two formats for those "),Wn=o("code"),_p=n("B-"),yp=n(" and "),Ln=o("code"),$p=n("I-"),vp=n(" labels: "),Un=o("em"),Ep=n("IOB1"),Op=n(" and "),Yn=o("em"),Ip=n("IOB2"),Cp=n(". The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with "),Vn=o("code"),zp=n("B-"),Tp=n(" are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label "),Qn=o("code"),Dp=n("I-PER"),Pp=n(" to the "),Jn=o("code"),Sp=n("S"),Fp=n(" token."),Oo=c(),Ds=o("div"),Ps=o("img"),Io=c(),Ze=o("p"),qp=n("With this map, we are ready to reproduce (almost entirely) the results of the first pipeline \u2014 we can just grab the score and label of each token that was not classified as "),Kn=o("code"),Rp=n("O"),Bp=n(":"),Co=c(),w(Ss.$$.fragment),zo=c(),w(Fs.$$.fragment),To=c(),ie=o("p"),Ap=n("This is very similar to what we had before, with one exception: the pipeline also gave us information about the "),Zn=o("code"),Xp=n("start"),Hp=n(" and "),ea=o("code"),Gp=n("end"),Np=n(" of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set "),sa=o("code"),Mp=n("return_offsets_mapping=True"),Wp=n(" when we apply the tokenizer to our inputs:"),Do=c(),w(qs.$$.fragment),Po=c(),w(Rs.$$.fragment),So=c(),pe=o("p"),Lp=n("Each tuple is the span of text corresponding to each token, where "),ta=o("code"),Up=n("(0, 0)"),Yp=n(" is reserved for the special tokens. We saw before that the token at index 5 is "),na=o("code"),Vp=n("##yl"),Qp=n(", which has "),aa=o("code"),Jp=n("(12, 14)"),Kp=n(" as offsets here. If we grab the corresponding slice in our example:"),Fo=c(),w(Bs.$$.fragment),qo=c(),es=o("p"),Zp=n("we get the proper span of text without the "),oa=o("code"),ec=n("##"),sc=n(":"),Ro=c(),w(As.$$.fragment),Bo=c(),xt=o("p"),tc=n("Using this, we can now complete the previous results:"),Ao=c(),w(Xs.$$.fragment),Xo=c(),w(Hs.$$.fragment),Ho=c(),wt=o("p"),nc=n("This is the same as what we got from the first pipeline!"),Go=c(),Se=o("h3"),ss=o("a"),la=o("span"),w(Gs.$$.fragment),ac=c(),ra=o("span"),oc=n("Grouping entities"),No=c(),L=o("p"),lc=n("Using the offsets to determine the start and end keys for each entity is handy, but that information isn\u2019t strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens "),ia=o("code"),rc=n("Hu"),ic=n(", "),pa=o("code"),pc=n("##gging"),cc=n(", and "),ca=o("code"),hc=n("Face"),fc=n(", we could make special rules that say the first two should be attached while removing the "),ha=o("code"),dc=n("##"),uc=n(", and the "),fa=o("code"),mc=n("Face"),gc=n(" should be added with a space since it does not begin with "),da=o("code"),xc=n("##"),wc=n(" \u2014 but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter)."),Mo=c(),J=o("p"),bc=n("With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens "),ua=o("code"),jc=n("Hu"),kc=n(", "),ma=o("code"),_c=n("##gging"),yc=n(", and "),ga=o("code"),$c=n("Face"),vc=n(", we should start at character 33 (the beginning of "),xa=o("code"),Ec=n("Hu"),Oc=n(") and end before character 45 (the end of "),wa=o("code"),Ic=n("Face"),Cc=n("):"),Wo=c(),w(Ns.$$.fragment),Lo=c(),w(Ms.$$.fragment),Uo=c(),K=o("p"),zc=n("To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with "),ba=o("code"),Tc=n("I-XXX"),Dc=n(", except for the first one, which can be labeled as "),ja=o("code"),Pc=n("B-XXX"),Sc=n(" or "),ka=o("code"),Fc=n("I-XXX"),qc=n(" (so, we stop grouping an entity when we get a "),_a=o("code"),Rc=n("O"),Bc=n(", a new type of entity, or a "),ya=o("code"),Ac=n("B-XXX"),Xc=n(" that tells us an entity of the same type is starting):"),Yo=c(),w(Ws.$$.fragment),Vo=c(),bt=o("p"),Hc=n("And we get the same results as with our second pipeline!"),Qo=c(),w(Ls.$$.fragment),Jo=c(),jt=o("p"),Gc=n("Another example of a task where these offsets are extremely useful is question answering. Diving into that pipeline, which we\u2019ll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the \u{1F917} Transformers library: dealing with overflowing tokens when we truncate an input to a given length."),this.h()},l(e){const i=_d('[data-svelte="svelte-1phssyn"]',xd.head);f=l(i,"META",{name:!0,content:!0}),i.forEach(t),x=h(e),b(d.$$.fragment,e),_=h(e),P=l(e,"H1",{class:!0});var Js=r(P);E=l(Js,"A",{id:!0,class:!0,href:!0});var kt=r(E);F=l(kt,"SPAN",{});var $a=r(F);b(A.$$.fragment,$a),$a.forEach(t),kt.forEach(t),N=h(Js),S=l(Js,"SPAN",{});var va=r(S);Y=a(va,"Fast tokenizers' special powers"),va.forEach(t),Js.forEach(t),I=h(e),z.l(e),T=h(e),O=l(e,"P",{});var le=r(O);X=a(le,"In this section we will take a closer look at the capabilities of the tokenizers in \u{1F917} Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers \u2014 especially those backed by the \u{1F917} Tokenizers library \u2014 can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the "),q=l(le,"CODE",{});var _t=r(q);V=a(_t,"token-classification"),_t.forEach(t),H=a(le," (that we called "),Z=l(le,"CODE",{});var yt=r(Z);u=a(yt,"ner"),yt.forEach(t),v=a(le,") and "),de=l(le,"CODE",{});var $t=r(de);Ee=a($t,"question-answering"),$t.forEach(t),oe=a(le," pipelines that we first encountered in "),re=l(le,"A",{href:!0});var eh=r(re);jl=a(eh,"Chapter 1"),eh.forEach(t),kl=a(le,"."),le.forEach(t),Ia=h(e),b(as.$$.fragment,e),Ca=h(e),Fe=l(e,"P",{});var Zo=r(Fe);_l=a(Zo,"In the following discussion, we will often make the distinction between \u201Cslow\u201D and \u201Cfast\u201D tokenizers. Slow tokenizers are those written in Python inside the \u{1F917} Transformers library, while the fast versions are the ones provided by \u{1F917} Tokenizers, which are written in Rust. If you remember the table from "),Ks=l(Zo,"A",{href:!0});var sh=r(Ks);yl=a(sh,"Chapter 5"),sh.forEach(t),$l=a(Zo," that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow:"),Zo.forEach(t),za=h(e),qe=l(e,"TABLE",{});var el=r(qe);Pt=l(el,"THEAD",{});var th=r(Pt);Oe=l(th,"TR",{});var vt=r(Oe);Zs=l(vt,"TH",{align:!0});var nh=r(Zs);vl=a(nh,"Fast tokenizer"),nh.forEach(t),El=h(vt),et=l(vt,"TH",{align:!0});var ah=r(et);Ol=a(ah,"Slow tokenizer"),ah.forEach(t),Il=h(vt),St=l(vt,"TH",{align:!0}),r(St).forEach(t),vt.forEach(t),th.forEach(t),Cl=h(el),os=l(el,"TBODY",{});var sl=r(os);Ie=l(sl,"TR",{});var Et=r(Ie);st=l(Et,"TD",{align:!0});var oh=r(st);Ft=l(oh,"CODE",{});var lh=r(Ft);zl=a(lh,"batched=True"),lh.forEach(t),oh.forEach(t),Tl=h(Et),tt=l(Et,"TD",{align:!0});var rh=r(tt);Dl=a(rh,"10.8s"),rh.forEach(t),Pl=h(Et),nt=l(Et,"TD",{align:!0});var ih=r(nt);Sl=a(ih,"4min41s"),ih.forEach(t),Et.forEach(t),Fl=h(sl),Ce=l(sl,"TR",{});var Ot=r(Ce);at=l(Ot,"TD",{align:!0});var ph=r(at);qt=l(ph,"CODE",{});var ch=r(qt);ql=a(ch,"batched=False"),ch.forEach(t),ph.forEach(t),Rl=h(Ot),ot=l(Ot,"TD",{align:!0});var hh=r(ot);Bl=a(hh,"59.2s"),hh.forEach(t),Al=h(Ot),lt=l(Ot,"TD",{align:!0});var fh=r(lt);Xl=a(fh,"5min3s"),fh.forEach(t),Ot.forEach(t),sl.forEach(t),el.forEach(t),Ta=h(e),b(Re.$$.fragment,e),Da=h(e),ze=l(e,"H2",{class:!0});var tl=r(ze);Be=l(tl,"A",{id:!0,class:!0,href:!0});var dh=r(Be);Rt=l(dh,"SPAN",{});var uh=r(Rt);b(ls.$$.fragment,uh),uh.forEach(t),dh.forEach(t),Hl=h(tl),Bt=l(tl,"SPAN",{});var mh=r(Bt);Gl=a(mh,"Batch encoding"),mh.forEach(t),tl.forEach(t),Pa=h(e),b(rs.$$.fragment,e),Sa=h(e),Ae=l(e,"P",{});var nl=r(Ae);Nl=a(nl,"The output of a tokenizer isn\u2019t a simple Python dictionary; what we get is actually a special "),At=l(nl,"CODE",{});var gh=r(At);Ml=a(gh,"BatchEncoding"),gh.forEach(t),Wl=a(nl," object. It\u2019s a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers."),nl.forEach(t),Fa=h(e),Xe=l(e,"P",{});var al=r(Xe);Ll=a(al,"Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from \u2014 a feature we call "),Xt=l(al,"EM",{});var xh=r(Xt);Ul=a(xh,"offset mapping"),xh.forEach(t),Yl=a(al,". This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it\u2019s inside, and vice versa."),al.forEach(t),qa=h(e),rt=l(e,"P",{});var wh=r(rt);Vl=a(wh,"Let\u2019s take a look at an example:"),wh.forEach(t),Ra=h(e),b(is.$$.fragment,e),Ba=h(e),He=l(e,"P",{});var ol=r(He);Ql=a(ol,"As mentioned previously, we get a "),Ht=l(ol,"CODE",{});var bh=r(Ht);Jl=a(bh,"BatchEncoding"),bh.forEach(t),Kl=a(ol," object in the tokenizer\u2019s output:"),ol.forEach(t),Aa=h(e),b(ps.$$.fragment,e),Xa=h(e),ee=l(e,"P",{});var $e=r(ee);Zl=a($e,"Since the "),Gt=l($e,"CODE",{});var jh=r(Gt);er=a(jh,"AutoTokenizer"),jh.forEach(t),sr=a($e," class picks a fast tokenizer by default, we can use the additional methods this "),Nt=l($e,"CODE",{});var kh=r(Nt);tr=a(kh,"BatchEncoding"),kh.forEach(t),nr=a($e," object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute "),Mt=l($e,"CODE",{});var _h=r(Mt);ar=a(_h,"is_fast"),_h.forEach(t),or=a($e," of the "),Wt=l($e,"CODE",{});var yh=r(Wt);lr=a(yh,"tokenizer"),yh.forEach(t),rr=a($e,":"),$e.forEach(t),Ha=h(e),b(cs.$$.fragment,e),Ga=h(e),b(hs.$$.fragment,e),Na=h(e),Ge=l(e,"P",{});var ll=r(Ge);ir=a(ll,"or check the same attribute of our "),Lt=l(ll,"CODE",{});var $h=r(Lt);pr=a($h,"encoding"),$h.forEach(t),cr=a(ll,":"),ll.forEach(t),Ma=h(e),b(fs.$$.fragment,e),Wa=h(e),b(ds.$$.fragment,e),La=h(e),it=l(e,"P",{});var vh=r(it);hr=a(vh,"Let\u2019s see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens:"),vh.forEach(t),Ua=h(e),b(us.$$.fragment,e),Ya=h(e),b(ms.$$.fragment,e),Va=h(e),je=l(e,"P",{});var It=r(je);fr=a(It,"In this case the token at index 5 is "),Ut=l(It,"CODE",{});var Eh=r(Ut);dr=a(Eh,"##yl"),Eh.forEach(t),ur=a(It,", which is part of the word \u201CSylvain\u201D in the original sentence. We can also use the "),Yt=l(It,"CODE",{});var Oh=r(Yt);mr=a(Oh,"word_ids()"),Oh.forEach(t),gr=a(It," method to get the index of the word each token comes from:"),It.forEach(t),Qa=h(e),b(gs.$$.fragment,e),Ja=h(e),b(xs.$$.fragment,e),Ka=h(e),Q=l(e,"P",{});var ce=r(Q);xr=a(ce,"We can see that the tokenizer\u2019s special tokens "),Vt=l(ce,"CODE",{});var Ih=r(Vt);wr=a(Ih,"[CLS]"),Ih.forEach(t),br=a(ce," and "),Qt=l(ce,"CODE",{});var Ch=r(Qt);jr=a(Ch,"[SEP]"),Ch.forEach(t),kr=a(ce," are mapped to "),Jt=l(ce,"CODE",{});var zh=r(Jt);_r=a(zh,"None"),zh.forEach(t),yr=a(ce,", and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the "),Kt=l(ce,"CODE",{});var Th=r(Kt);$r=a(Th,"##"),Th.forEach(t),vr=a(ce," prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it\u2019s a fast one. In the next chapter, we\u2019ll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called "),Zt=l(ce,"EM",{});var Dh=r(Zt);Er=a(Dh,"whole word masking"),Dh.forEach(t),Or=a(ce,")."),ce.forEach(t),Za=h(e),b(Ne.$$.fragment,e),eo=h(e),ke=l(e,"P",{});var Ct=r(ke);Ir=a(Ct,"Similarly, there is a "),en=l(Ct,"CODE",{});var Ph=r(en);Cr=a(Ph,"sentence_ids()"),Ph.forEach(t),zr=a(Ct," method that we can use to map a token to the sentence it came from (though in this case, the "),sn=l(Ct,"CODE",{});var Sh=r(sn);Tr=a(Sh,"token_type_ids"),Sh.forEach(t),Dr=a(Ct," returned by the tokenizer can give us the same information)."),Ct.forEach(t),so=h(e),M=l(e,"P",{});var te=r(M);Pr=a(te,"Lastly, we can map any word or token to characters in the original text, and vice versa, via the "),tn=l(te,"CODE",{});var Fh=r(tn);Sr=a(Fh,"word_to_chars()"),Fh.forEach(t),Fr=a(te," or "),nn=l(te,"CODE",{});var qh=r(nn);qr=a(qh,"token_to_chars()"),qh.forEach(t),Rr=a(te," and "),an=l(te,"CODE",{});var Rh=r(an);Br=a(Rh,"char_to_word()"),Rh.forEach(t),Ar=a(te," or "),on=l(te,"CODE",{});var Bh=r(on);Xr=a(Bh,"char_to_token()"),Bh.forEach(t),Hr=a(te," methods. For instance, the "),ln=l(te,"CODE",{});var Ah=r(ln);Gr=a(Ah,"word_ids()"),Ah.forEach(t),Nr=a(te," method told us that "),rn=l(te,"CODE",{});var Xh=r(rn);Mr=a(Xh,"##yl"),Xh.forEach(t),Wr=a(te," is part of the word at index 3, but which word is it in the sentence? We can find out like this:"),te.forEach(t),to=h(e),b(ws.$$.fragment,e),no=h(e),b(bs.$$.fragment,e),ao=h(e),_e=l(e,"P",{});var zt=r(_e);Lr=a(zt,"As we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of "),pn=l(zt,"EM",{});var Hh=r(pn);Ur=a(Hh,"offsets"),Hh.forEach(t),Yr=a(zt,". To illustrate their use, next we\u2019ll show you how to replicate the results of the "),cn=l(zt,"CODE",{});var Gh=r(cn);Vr=a(Gh,"token-classification"),Gh.forEach(t),Qr=a(zt," pipeline manually."),zt.forEach(t),oo=h(e),b(Me.$$.fragment,e),lo=h(e),Te=l(e,"H2",{class:!0});var rl=r(Te);We=l(rl,"A",{id:!0,class:!0,href:!0});var Nh=r(We);hn=l(Nh,"SPAN",{});var Mh=r(hn);b(js.$$.fragment,Mh),Mh.forEach(t),Nh.forEach(t),Jr=h(rl),ks=l(rl,"SPAN",{});var il=r(ks);Kr=a(il,"Inside the "),fn=l(il,"CODE",{});var Wh=r(fn);Zr=a(Wh,"token-classification"),Wh.forEach(t),ei=a(il," pipeline"),il.forEach(t),rl.forEach(t),ro=h(e),se=l(e,"P",{});var ve=r(se);si=a(ve,"In "),pt=l(ve,"A",{href:!0});var Lh=r(pt);ti=a(Lh,"Chapter 1"),Lh.forEach(t),ni=a(ve," we got our first taste of applying NER \u2014 where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations \u2014 with the \u{1F917} Transformers "),dn=l(ve,"CODE",{});var Uh=r(dn);ai=a(Uh,"pipeline()"),Uh.forEach(t),oi=a(ve," function. Then, in "),ct=l(ve,"A",{href:!0});var Yh=r(ct);li=a(Yh,"Chapter 2"),Yh.forEach(t),ri=a(ve,", we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: tokenization, passing the inputs through the model, and post-processing. The first two steps in the "),un=l(ve,"CODE",{});var Vh=r(un);ii=a(Vh,"token-classification"),Vh.forEach(t),pi=a(ve," pipeline are the same as in any other pipeline, but the post-processing is a little more complex \u2014 let\u2019s see how!"),ve.forEach(t),io=h(e),me.l(e),ht=h(e),De=l(e,"H3",{class:!0});var pl=r(De);Le=l(pl,"A",{id:!0,class:!0,href:!0});var Qh=r(Le);mn=l(Qh,"SPAN",{});var Jh=r(mn);b(_s.$$.fragment,Jh),Jh.forEach(t),Qh.forEach(t),ci=h(pl),gn=l(pl,"SPAN",{});var Kh=r(gn);hi=a(Kh,"Getting the base results with the pipeline"),Kh.forEach(t),pl.forEach(t),po=h(e),Ue=l(e,"P",{});var cl=r(Ue);fi=a(cl,"First, let\u2019s grab a token classification pipeline so we can get some results to compare manually. The model used by default is "),ys=l(cl,"A",{href:!0,rel:!0});var Zh=r(ys);xn=l(Zh,"CODE",{});var ef=r(xn);di=a(ef,"dbmdz/bert-large-cased-finetuned-conll03-english"),ef.forEach(t),Zh.forEach(t),ui=a(cl,"; it performs NER on sentences:"),cl.forEach(t),co=h(e),b($s.$$.fragment,e),ho=h(e),b(vs.$$.fragment,e),fo=h(e),ft=l(e,"P",{});var sf=r(ft);mi=a(sf,"The model properly identified each token generated by \u201CSylvain\u201D as a person, each token generated by \u201CHugging Face\u201D as an organization, and the token \u201CBrooklyn\u201D as a location. We can also ask the pipeline to group together the tokens that correspond to the same entity:"),sf.forEach(t),uo=h(e),b(Es.$$.fragment,e),mo=h(e),b(Os.$$.fragment,e),go=h(e),W=l(e,"P",{});var ne=r(W);gi=a(ne,"The "),wn=l(ne,"CODE",{});var tf=r(wn);xi=a(tf,"aggregation_strategy"),tf.forEach(t),wi=a(ne," picked will change the scores computed for each grouped entity. With "),bn=l(ne,"CODE",{});var nf=r(bn);bi=a(nf,'"simple"'),nf.forEach(t),ji=a(ne," the score is just the mean of the scores of each token in the given entity: for instance, the score of \u201CSylvain\u201D is the mean of the scores we saw in the previous example for the tokens "),jn=l(ne,"CODE",{});var af=r(jn);ki=a(af,"S"),af.forEach(t),_i=a(ne,", "),kn=l(ne,"CODE",{});var of=r(kn);yi=a(of,"##yl"),of.forEach(t),$i=a(ne,", "),_n=l(ne,"CODE",{});var lf=r(_n);vi=a(lf,"##va"),lf.forEach(t),Ei=a(ne,", and "),yn=l(ne,"CODE",{});var rf=r(yn);Oi=a(rf,"##in"),rf.forEach(t),Ii=a(ne,". Other strategies available are:"),ne.forEach(t),xo=h(e),ye=l(e,"UL",{});var Tt=r(ye);Ye=l(Tt,"LI",{});var Ea=r(Ye);$n=l(Ea,"CODE",{});var pf=r($n);Ci=a(pf,'"first"'),pf.forEach(t),zi=a(Ea,", where the score of each entity is the score of the first token of that entity (so for \u201CSylvain\u201D it would be 0.993828, the score of the token "),vn=l(Ea,"CODE",{});var cf=r(vn);Ti=a(cf,"S"),cf.forEach(t),Di=a(Ea,")"),Ea.forEach(t),Pi=h(Tt),dt=l(Tt,"LI",{});var Nc=r(dt);En=l(Nc,"CODE",{});var hf=r(En);Si=a(hf,'"max"'),hf.forEach(t),Fi=a(Nc,", where the score of each entity is the maximum score of the tokens in that entity (so for \u201CHugging Face\u201D it would be 0.98879766, the score of \u201CFace\u201D)"),Nc.forEach(t),qi=h(Tt),Ve=l(Tt,"LI",{});var Oa=r(Ve);On=l(Oa,"CODE",{});var ff=r(On);Ri=a(ff,'"average"'),ff.forEach(t),Bi=a(Oa,", where the score of each entity is the average of the scores of the words composing that entity (so for \u201CSylvain\u201D there would be no difference from the "),In=l(Oa,"CODE",{});var df=r(In);Ai=a(df,'"simple"'),df.forEach(t),Xi=a(Oa," strategy, but \u201CHugging Face\u201D would have a score of 0.9819, the average of the scores for \u201CHugging\u201D, 0.975, and \u201CFace\u201D, 0.98879)"),Oa.forEach(t),Tt.forEach(t),wo=h(e),Qe=l(e,"P",{});var hl=r(Qe);Hi=a(hl,"Now let\u2019s see how to obtain these results without using the "),Cn=l(hl,"CODE",{});var uf=r(Cn);Gi=a(uf,"pipeline()"),uf.forEach(t),Ni=a(hl," function!"),hl.forEach(t),bo=h(e),Pe=l(e,"H3",{class:!0});var fl=r(Pe);Je=l(fl,"A",{id:!0,class:!0,href:!0});var mf=r(Je);zn=l(mf,"SPAN",{});var gf=r(zn);b(Is.$$.fragment,gf),gf.forEach(t),mf.forEach(t),Mi=h(fl),Tn=l(fl,"SPAN",{});var xf=r(Tn);Wi=a(xf,"From inputs to predictions"),xf.forEach(t),fl.forEach(t),jo=h(e),xe.l(e),ut=h(e),mt=l(e,"P",{});var wf=r(mt);Li=a(wf,"We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9. Like for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order):"),wf.forEach(t),ko=h(e),be.l(e),gt=h(e),b(Cs.$$.fragment,e),_o=h(e),Ke=l(e,"P",{});var dl=r(Ke);Ui=a(dl,"The "),Dn=l(dl,"CODE",{});var bf=r(Dn);Yi=a(bf,"model.config.id2label"),bf.forEach(t),Vi=a(dl," attribute contains the mapping of indexes to labels that we can use to make sense of the predictions:"),dl.forEach(t),yo=h(e),b(zs.$$.fragment,e),$o=h(e),b(Ts.$$.fragment,e),vo=h(e),C=l(e,"P",{});var R=r(C);Qi=a(R,"As we saw earlier, there are 9 labels: "),Pn=l(R,"CODE",{});var jf=r(Pn);Ji=a(jf,"O"),jf.forEach(t),Ki=a(R," is the label for the tokens that are not in any named entity (it stands for \u201Coutside\u201D), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label "),Sn=l(R,"CODE",{});var kf=r(Sn);Zi=a(kf,"B-XXX"),kf.forEach(t),ep=a(R," indicates the token is at the beginning of an entity "),Fn=l(R,"CODE",{});var _f=r(Fn);sp=a(_f,"XXX"),_f.forEach(t),tp=a(R," and the label "),qn=l(R,"CODE",{});var yf=r(qn);np=a(yf,"I-XXX"),yf.forEach(t),ap=a(R," indicates the token is inside the entity "),Rn=l(R,"CODE",{});var $f=r(Rn);op=a($f,"XXX"),$f.forEach(t),lp=a(R,". For instance, in the current example we would expect our model to classify the token "),Bn=l(R,"CODE",{});var vf=r(Bn);rp=a(vf,"S"),vf.forEach(t),ip=a(R," as "),An=l(R,"CODE",{});var Ef=r(An);pp=a(Ef,"B-PER"),Ef.forEach(t),cp=a(R," (beginning of a person entity) and the tokens "),Xn=l(R,"CODE",{});var Of=r(Xn);hp=a(Of,"##yl"),Of.forEach(t),fp=a(R,", "),Hn=l(R,"CODE",{});var If=r(Hn);dp=a(If,"##va"),If.forEach(t),up=a(R," and "),Gn=l(R,"CODE",{});var Cf=r(Gn);mp=a(Cf,"##in"),Cf.forEach(t),gp=a(R," as "),Nn=l(R,"CODE",{});var zf=r(Nn);xp=a(zf,"I-PER"),zf.forEach(t),wp=a(R," (inside a person entity)."),R.forEach(t),Eo=h(e),G=l(e,"P",{});var U=r(G);bp=a(U,"You might think the model was wrong in this case as it gave the label "),Mn=l(U,"CODE",{});var Tf=r(Mn);jp=a(Tf,"I-PER"),Tf.forEach(t),kp=a(U," to all four of these tokens, but that\u2019s not entirely true. There are actually two formats for those "),Wn=l(U,"CODE",{});var Df=r(Wn);_p=a(Df,"B-"),Df.forEach(t),yp=a(U," and "),Ln=l(U,"CODE",{});var Pf=r(Ln);$p=a(Pf,"I-"),Pf.forEach(t),vp=a(U," labels: "),Un=l(U,"EM",{});var Sf=r(Un);Ep=a(Sf,"IOB1"),Sf.forEach(t),Op=a(U," and "),Yn=l(U,"EM",{});var Ff=r(Yn);Ip=a(Ff,"IOB2"),Ff.forEach(t),Cp=a(U,". The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with "),Vn=l(U,"CODE",{});var qf=r(Vn);zp=a(qf,"B-"),qf.forEach(t),Tp=a(U," are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label "),Qn=l(U,"CODE",{});var Rf=r(Qn);Dp=a(Rf,"I-PER"),Rf.forEach(t),Pp=a(U," to the "),Jn=l(U,"CODE",{});var Bf=r(Jn);Sp=a(Bf,"S"),Bf.forEach(t),Fp=a(U," token."),U.forEach(t),Oo=h(e),Ds=l(e,"DIV",{class:!0});var Af=r(Ds);Ps=l(Af,"IMG",{src:!0,alt:!0,width:!0}),Af.forEach(t),Io=h(e),Ze=l(e,"P",{});var ul=r(Ze);qp=a(ul,"With this map, we are ready to reproduce (almost entirely) the results of the first pipeline \u2014 we can just grab the score and label of each token that was not classified as "),Kn=l(ul,"CODE",{});var Xf=r(Kn);Rp=a(Xf,"O"),Xf.forEach(t),Bp=a(ul,":"),ul.forEach(t),Co=h(e),b(Ss.$$.fragment,e),zo=h(e),b(Fs.$$.fragment,e),To=h(e),ie=l(e,"P",{});var ts=r(ie);Ap=a(ts,"This is very similar to what we had before, with one exception: the pipeline also gave us information about the "),Zn=l(ts,"CODE",{});var Hf=r(Zn);Xp=a(Hf,"start"),Hf.forEach(t),Hp=a(ts," and "),ea=l(ts,"CODE",{});var Gf=r(ea);Gp=a(Gf,"end"),Gf.forEach(t),Np=a(ts," of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set "),sa=l(ts,"CODE",{});var Nf=r(sa);Mp=a(Nf,"return_offsets_mapping=True"),Nf.forEach(t),Wp=a(ts," when we apply the tokenizer to our inputs:"),ts.forEach(t),Do=h(e),b(qs.$$.fragment,e),Po=h(e),b(Rs.$$.fragment,e),So=h(e),pe=l(e,"P",{});var ns=r(pe);Lp=a(ns,"Each tuple is the span of text corresponding to each token, where "),ta=l(ns,"CODE",{});var Mf=r(ta);Up=a(Mf,"(0, 0)"),Mf.forEach(t),Yp=a(ns," is reserved for the special tokens. We saw before that the token at index 5 is "),na=l(ns,"CODE",{});var Wf=r(na);Vp=a(Wf,"##yl"),Wf.forEach(t),Qp=a(ns,", which has "),aa=l(ns,"CODE",{});var Lf=r(aa);Jp=a(Lf,"(12, 14)"),Lf.forEach(t),Kp=a(ns," as offsets here. If we grab the corresponding slice in our example:"),ns.forEach(t),Fo=h(e),b(Bs.$$.fragment,e),qo=h(e),es=l(e,"P",{});var ml=r(es);Zp=a(ml,"we get the proper span of text without the "),oa=l(ml,"CODE",{});var Uf=r(oa);ec=a(Uf,"##"),Uf.forEach(t),sc=a(ml,":"),ml.forEach(t),Ro=h(e),b(As.$$.fragment,e),Bo=h(e),xt=l(e,"P",{});var Yf=r(xt);tc=a(Yf,"Using this, we can now complete the previous results:"),Yf.forEach(t),Ao=h(e),b(Xs.$$.fragment,e),Xo=h(e),b(Hs.$$.fragment,e),Ho=h(e),wt=l(e,"P",{});var Vf=r(wt);nc=a(Vf,"This is the same as what we got from the first pipeline!"),Vf.forEach(t),Go=h(e),Se=l(e,"H3",{class:!0});var gl=r(Se);ss=l(gl,"A",{id:!0,class:!0,href:!0});var Qf=r(ss);la=l(Qf,"SPAN",{});var Jf=r(la);b(Gs.$$.fragment,Jf),Jf.forEach(t),Qf.forEach(t),ac=h(gl),ra=l(gl,"SPAN",{});var Kf=r(ra);oc=a(Kf,"Grouping entities"),Kf.forEach(t),gl.forEach(t),No=h(e),L=l(e,"P",{});var ae=r(L);lc=a(ae,"Using the offsets to determine the start and end keys for each entity is handy, but that information isn\u2019t strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens "),ia=l(ae,"CODE",{});var Zf=r(ia);rc=a(Zf,"Hu"),Zf.forEach(t),ic=a(ae,", "),pa=l(ae,"CODE",{});var ed=r(pa);pc=a(ed,"##gging"),ed.forEach(t),cc=a(ae,", and "),ca=l(ae,"CODE",{});var sd=r(ca);hc=a(sd,"Face"),sd.forEach(t),fc=a(ae,", we could make special rules that say the first two should be attached while removing the "),ha=l(ae,"CODE",{});var td=r(ha);dc=a(td,"##"),td.forEach(t),uc=a(ae,", and the "),fa=l(ae,"CODE",{});var nd=r(fa);mc=a(nd,"Face"),nd.forEach(t),gc=a(ae," should be added with a space since it does not begin with "),da=l(ae,"CODE",{});var ad=r(da);xc=a(ad,"##"),ad.forEach(t),wc=a(ae," \u2014 but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter)."),ae.forEach(t),Mo=h(e),J=l(e,"P",{});var he=r(J);bc=a(he,"With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens "),ua=l(he,"CODE",{});var od=r(ua);jc=a(od,"Hu"),od.forEach(t),kc=a(he,", "),ma=l(he,"CODE",{});var ld=r(ma);_c=a(ld,"##gging"),ld.forEach(t),yc=a(he,", and "),ga=l(he,"CODE",{});var rd=r(ga);$c=a(rd,"Face"),rd.forEach(t),vc=a(he,", we should start at character 33 (the beginning of "),xa=l(he,"CODE",{});var id=r(xa);Ec=a(id,"Hu"),id.forEach(t),Oc=a(he,") and end before character 45 (the end of "),wa=l(he,"CODE",{});var pd=r(wa);Ic=a(pd,"Face"),pd.forEach(t),Cc=a(he,"):"),he.forEach(t),Wo=h(e),b(Ns.$$.fragment,e),Lo=h(e),b(Ms.$$.fragment,e),Uo=h(e),K=l(e,"P",{});var fe=r(K);zc=a(fe,"To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with "),ba=l(fe,"CODE",{});var cd=r(ba);Tc=a(cd,"I-XXX"),cd.forEach(t),Dc=a(fe,", except for the first one, which can be labeled as "),ja=l(fe,"CODE",{});var hd=r(ja);Pc=a(hd,"B-XXX"),hd.forEach(t),Sc=a(fe," or "),ka=l(fe,"CODE",{});var fd=r(ka);Fc=a(fd,"I-XXX"),fd.forEach(t),qc=a(fe," (so, we stop grouping an entity when we get a "),_a=l(fe,"CODE",{});var dd=r(_a);Rc=a(dd,"O"),dd.forEach(t),Bc=a(fe,", a new type of entity, or a "),ya=l(fe,"CODE",{});var ud=r(ya);Ac=a(ud,"B-XXX"),ud.forEach(t),Xc=a(fe," that tells us an entity of the same type is starting):"),fe.forEach(t),Yo=h(e),b(Ws.$$.fragment,e),Vo=h(e),bt=l(e,"P",{});var md=r(bt);Hc=a(md,"And we get the same results as with our second pipeline!"),md.forEach(t),Qo=h(e),b(Ls.$$.fragment,e),Jo=h(e),jt=l(e,"P",{});var gd=r(jt);Gc=a(gd,"Another example of a task where these offsets are extremely useful is question answering. Diving into that pipeline, which we\u2019ll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the \u{1F917} Transformers library: dealing with overflowing tokens when we truncate an input to a given length."),gd.forEach(t),this.h()},h(){y(f,"name","hf:doc:metadata"),y(f,"content",JSON.stringify(Ad)),y(E,"id","fast-tokenizers-special-powers"),y(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(E,"href","#fast-tokenizers-special-powers"),y(P,"class","relative group"),y(re,"href","/course/chapter1"),y(Ks,"href","/course/chapter5/3"),y(Zs,"align","center"),y(et,"align","center"),y(St,"align","center"),y(st,"align","center"),y(tt,"align","center"),y(nt,"align","center"),y(at,"align","center"),y(ot,"align","center"),y(lt,"align","center"),y(Be,"id","batch-encoding"),y(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Be,"href","#batch-encoding"),y(ze,"class","relative group"),y(We,"id","inside-the-tokenclassification-pipeline"),y(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(We,"href","#inside-the-tokenclassification-pipeline"),y(Te,"class","relative group"),y(pt,"href","/course/chapter1"),y(ct,"href","/course/chapter2"),y(Le,"id","getting-the-base-results-with-the-pipeline"),y(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Le,"href","#getting-the-base-results-with-the-pipeline"),y(De,"class","relative group"),y(ys,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),y(ys,"rel","nofollow"),y(Je,"id","from-inputs-to-predictions"),y(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Je,"href","#from-inputs-to-predictions"),y(Pe,"class","relative group"),yd(Ps.src,Wc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.PNG")||y(Ps,"src",Wc),y(Ps,"alt","IOB1 vs IOB2 format"),y(Ps,"width","80%"),y(Ds,"class","flex justify-center"),y(ss,"id","grouping-entities"),y(ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(ss,"href","#grouping-entities"),y(Se,"class","relative group")},m(e,i){s(xd.head,f),p(e,x,i),j(d,e,i),p(e,_,i),p(e,P,i),s(P,E),s(E,F),j(A,F,null),s(P,N),s(P,S),s(S,Y),p(e,I,i),Us[B].m(e,i),p(e,T,i),p(e,O,i),s(O,X),s(O,q),s(q,V),s(O,H),s(O,Z),s(Z,u),s(O,v),s(O,de),s(de,Ee),s(O,oe),s(O,re),s(re,jl),s(O,kl),p(e,Ia,i),j(as,e,i),p(e,Ca,i),p(e,Fe,i),s(Fe,_l),s(Fe,Ks),s(Ks,yl),s(Fe,$l),p(e,za,i),p(e,qe,i),s(qe,Pt),s(Pt,Oe),s(Oe,Zs),s(Zs,vl),s(Oe,El),s(Oe,et),s(et,Ol),s(Oe,Il),s(Oe,St),s(qe,Cl),s(qe,os),s(os,Ie),s(Ie,st),s(st,Ft),s(Ft,zl),s(Ie,Tl),s(Ie,tt),s(tt,Dl),s(Ie,Pl),s(Ie,nt),s(nt,Sl),s(os,Fl),s(os,Ce),s(Ce,at),s(at,qt),s(qt,ql),s(Ce,Rl),s(Ce,ot),s(ot,Bl),s(Ce,Al),s(Ce,lt),s(lt,Xl),p(e,Ta,i),j(Re,e,i),p(e,Da,i),p(e,ze,i),s(ze,Be),s(Be,Rt),j(ls,Rt,null),s(ze,Hl),s(ze,Bt),s(Bt,Gl),p(e,Pa,i),j(rs,e,i),p(e,Sa,i),p(e,Ae,i),s(Ae,Nl),s(Ae,At),s(At,Ml),s(Ae,Wl),p(e,Fa,i),p(e,Xe,i),s(Xe,Ll),s(Xe,Xt),s(Xt,Ul),s(Xe,Yl),p(e,qa,i),p(e,rt,i),s(rt,Vl),p(e,Ra,i),j(is,e,i),p(e,Ba,i),p(e,He,i),s(He,Ql),s(He,Ht),s(Ht,Jl),s(He,Kl),p(e,Aa,i),j(ps,e,i),p(e,Xa,i),p(e,ee,i),s(ee,Zl),s(ee,Gt),s(Gt,er),s(ee,sr),s(ee,Nt),s(Nt,tr),s(ee,nr),s(ee,Mt),s(Mt,ar),s(ee,or),s(ee,Wt),s(Wt,lr),s(ee,rr),p(e,Ha,i),j(cs,e,i),p(e,Ga,i),j(hs,e,i),p(e,Na,i),p(e,Ge,i),s(Ge,ir),s(Ge,Lt),s(Lt,pr),s(Ge,cr),p(e,Ma,i),j(fs,e,i),p(e,Wa,i),j(ds,e,i),p(e,La,i),p(e,it,i),s(it,hr),p(e,Ua,i),j(us,e,i),p(e,Ya,i),j(ms,e,i),p(e,Va,i),p(e,je,i),s(je,fr),s(je,Ut),s(Ut,dr),s(je,ur),s(je,Yt),s(Yt,mr),s(je,gr),p(e,Qa,i),j(gs,e,i),p(e,Ja,i),j(xs,e,i),p(e,Ka,i),p(e,Q,i),s(Q,xr),s(Q,Vt),s(Vt,wr),s(Q,br),s(Q,Qt),s(Qt,jr),s(Q,kr),s(Q,Jt),s(Jt,_r),s(Q,yr),s(Q,Kt),s(Kt,$r),s(Q,vr),s(Q,Zt),s(Zt,Er),s(Q,Or),p(e,Za,i),j(Ne,e,i),p(e,eo,i),p(e,ke,i),s(ke,Ir),s(ke,en),s(en,Cr),s(ke,zr),s(ke,sn),s(sn,Tr),s(ke,Dr),p(e,so,i),p(e,M,i),s(M,Pr),s(M,tn),s(tn,Sr),s(M,Fr),s(M,nn),s(nn,qr),s(M,Rr),s(M,an),s(an,Br),s(M,Ar),s(M,on),s(on,Xr),s(M,Hr),s(M,ln),s(ln,Gr),s(M,Nr),s(M,rn),s(rn,Mr),s(M,Wr),p(e,to,i),j(ws,e,i),p(e,no,i),j(bs,e,i),p(e,ao,i),p(e,_e,i),s(_e,Lr),s(_e,pn),s(pn,Ur),s(_e,Yr),s(_e,cn),s(cn,Vr),s(_e,Qr),p(e,oo,i),j(Me,e,i),p(e,lo,i),p(e,Te,i),s(Te,We),s(We,hn),j(js,hn,null),s(Te,Jr),s(Te,ks),s(ks,Kr),s(ks,fn),s(fn,Zr),s(ks,ei),p(e,ro,i),p(e,se,i),s(se,si),s(se,pt),s(pt,ti),s(se,ni),s(se,dn),s(dn,ai),s(se,oi),s(se,ct),s(ct,li),s(se,ri),s(se,un),s(un,ii),s(se,pi),p(e,io,i),Ys[ue].m(e,i),p(e,ht,i),p(e,De,i),s(De,Le),s(Le,mn),j(_s,mn,null),s(De,ci),s(De,gn),s(gn,hi),p(e,po,i),p(e,Ue,i),s(Ue,fi),s(Ue,ys),s(ys,xn),s(xn,di),s(Ue,ui),p(e,co,i),j($s,e,i),p(e,ho,i),j(vs,e,i),p(e,fo,i),p(e,ft,i),s(ft,mi),p(e,uo,i),j(Es,e,i),p(e,mo,i),j(Os,e,i),p(e,go,i),p(e,W,i),s(W,gi),s(W,wn),s(wn,xi),s(W,wi),s(W,bn),s(bn,bi),s(W,ji),s(W,jn),s(jn,ki),s(W,_i),s(W,kn),s(kn,yi),s(W,$i),s(W,_n),s(_n,vi),s(W,Ei),s(W,yn),s(yn,Oi),s(W,Ii),p(e,xo,i),p(e,ye,i),s(ye,Ye),s(Ye,$n),s($n,Ci),s(Ye,zi),s(Ye,vn),s(vn,Ti),s(Ye,Di),s(ye,Pi),s(ye,dt),s(dt,En),s(En,Si),s(dt,Fi),s(ye,qi),s(ye,Ve),s(Ve,On),s(On,Ri),s(Ve,Bi),s(Ve,In),s(In,Ai),s(Ve,Xi),p(e,wo,i),p(e,Qe,i),s(Qe,Hi),s(Qe,Cn),s(Cn,Gi),s(Qe,Ni),p(e,bo,i),p(e,Pe,i),s(Pe,Je),s(Je,zn),j(Is,zn,null),s(Pe,Mi),s(Pe,Tn),s(Tn,Wi),p(e,jo,i),Vs[ge].m(e,i),p(e,ut,i),p(e,mt,i),s(mt,Li),p(e,ko,i),Qs[we].m(e,i),p(e,gt,i),j(Cs,e,i),p(e,_o,i),p(e,Ke,i),s(Ke,Ui),s(Ke,Dn),s(Dn,Yi),s(Ke,Vi),p(e,yo,i),j(zs,e,i),p(e,$o,i),j(Ts,e,i),p(e,vo,i),p(e,C,i),s(C,Qi),s(C,Pn),s(Pn,Ji),s(C,Ki),s(C,Sn),s(Sn,Zi),s(C,ep),s(C,Fn),s(Fn,sp),s(C,tp),s(C,qn),s(qn,np),s(C,ap),s(C,Rn),s(Rn,op),s(C,lp),s(C,Bn),s(Bn,rp),s(C,ip),s(C,An),s(An,pp),s(C,cp),s(C,Xn),s(Xn,hp),s(C,fp),s(C,Hn),s(Hn,dp),s(C,up),s(C,Gn),s(Gn,mp),s(C,gp),s(C,Nn),s(Nn,xp),s(C,wp),p(e,Eo,i),p(e,G,i),s(G,bp),s(G,Mn),s(Mn,jp),s(G,kp),s(G,Wn),s(Wn,_p),s(G,yp),s(G,Ln),s(Ln,$p),s(G,vp),s(G,Un),s(Un,Ep),s(G,Op),s(G,Yn),s(Yn,Ip),s(G,Cp),s(G,Vn),s(Vn,zp),s(G,Tp),s(G,Qn),s(Qn,Dp),s(G,Pp),s(G,Jn),s(Jn,Sp),s(G,Fp),p(e,Oo,i),p(e,Ds,i),s(Ds,Ps),p(e,Io,i),p(e,Ze,i),s(Ze,qp),s(Ze,Kn),s(Kn,Rp),s(Ze,Bp),p(e,Co,i),j(Ss,e,i),p(e,zo,i),j(Fs,e,i),p(e,To,i),p(e,ie,i),s(ie,Ap),s(ie,Zn),s(Zn,Xp),s(ie,Hp),s(ie,ea),s(ea,Gp),s(ie,Np),s(ie,sa),s(sa,Mp),s(ie,Wp),p(e,Do,i),j(qs,e,i),p(e,Po,i),j(Rs,e,i),p(e,So,i),p(e,pe,i),s(pe,Lp),s(pe,ta),s(ta,Up),s(pe,Yp),s(pe,na),s(na,Vp),s(pe,Qp),s(pe,aa),s(aa,Jp),s(pe,Kp),p(e,Fo,i),j(Bs,e,i),p(e,qo,i),p(e,es,i),s(es,Zp),s(es,oa),s(oa,ec),s(es,sc),p(e,Ro,i),j(As,e,i),p(e,Bo,i),p(e,xt,i),s(xt,tc),p(e,Ao,i),j(Xs,e,i),p(e,Xo,i),j(Hs,e,i),p(e,Ho,i),p(e,wt,i),s(wt,nc),p(e,Go,i),p(e,Se,i),s(Se,ss),s(ss,la),j(Gs,la,null),s(Se,ac),s(Se,ra),s(ra,oc),p(e,No,i),p(e,L,i),s(L,lc),s(L,ia),s(ia,rc),s(L,ic),s(L,pa),s(pa,pc),s(L,cc),s(L,ca),s(ca,hc),s(L,fc),s(L,ha),s(ha,dc),s(L,uc),s(L,fa),s(fa,mc),s(L,gc),s(L,da),s(da,xc),s(L,wc),p(e,Mo,i),p(e,J,i),s(J,bc),s(J,ua),s(ua,jc),s(J,kc),s(J,ma),s(ma,_c),s(J,yc),s(J,ga),s(ga,$c),s(J,vc),s(J,xa),s(xa,Ec),s(J,Oc),s(J,wa),s(wa,Ic),s(J,Cc),p(e,Wo,i),j(Ns,e,i),p(e,Lo,i),j(Ms,e,i),p(e,Uo,i),p(e,K,i),s(K,zc),s(K,ba),s(ba,Tc),s(K,Dc),s(K,ja),s(ja,Pc),s(K,Sc),s(K,ka),s(ka,Fc),s(K,qc),s(K,_a),s(_a,Rc),s(K,Bc),s(K,ya),s(ya,Ac),s(K,Xc),p(e,Yo,i),j(Ws,e,i),p(e,Vo,i),p(e,bt,i),s(bt,Hc),p(e,Qo,i),j(Ls,e,i),p(e,Jo,i),p(e,jt,i),s(jt,Gc),Ko=!0},p(e,[i]){const Js={};i&1&&(Js.fw=e[0]),d.$set(Js);let kt=B;B=Uc(e),B!==kt&&(wl(),m(Us[kt],1,1,()=>{Us[kt]=null}),xl(),z=Us[B],z||(z=Us[B]=Lc[B](e),z.c()),g(z,1),z.m(T.parentNode,T));const $a={};i&2&&($a.$$scope={dirty:i,ctx:e}),Re.$set($a);const va={};i&2&&(va.$$scope={dirty:i,ctx:e}),Ne.$set(va);const le={};i&2&&(le.$$scope={dirty:i,ctx:e}),Me.$set(le);let _t=ue;ue=Vc(e),ue!==_t&&(wl(),m(Ys[_t],1,1,()=>{Ys[_t]=null}),xl(),me=Ys[ue],me||(me=Ys[ue]=Yc[ue](e),me.c()),g(me,1),me.m(ht.parentNode,ht));let yt=ge;ge=Jc(e),ge!==yt&&(wl(),m(Vs[yt],1,1,()=>{Vs[yt]=null}),xl(),xe=Vs[ge],xe||(xe=Vs[ge]=Qc[ge](e),xe.c()),g(xe,1),xe.m(ut.parentNode,ut));let $t=we;we=Zc(e),we!==$t&&(wl(),m(Qs[$t],1,1,()=>{Qs[$t]=null}),xl(),be=Qs[we],be||(be=Qs[we]=Kc[we](e),be.c()),g(be,1),be.m(gt.parentNode,gt))},i(e){Ko||(g(d.$$.fragment,e),g(A.$$.fragment,e),g(z),g(as.$$.fragment,e),g(Re.$$.fragment,e),g(ls.$$.fragment,e),g(rs.$$.fragment,e),g(is.$$.fragment,e),g(ps.$$.fragment,e),g(cs.$$.fragment,e),g(hs.$$.fragment,e),g(fs.$$.fragment,e),g(ds.$$.fragment,e),g(us.$$.fragment,e),g(ms.$$.fragment,e),g(gs.$$.fragment,e),g(xs.$$.fragment,e),g(Ne.$$.fragment,e),g(ws.$$.fragment,e),g(bs.$$.fragment,e),g(Me.$$.fragment,e),g(js.$$.fragment,e),g(me),g(_s.$$.fragment,e),g($s.$$.fragment,e),g(vs.$$.fragment,e),g(Es.$$.fragment,e),g(Os.$$.fragment,e),g(Is.$$.fragment,e),g(xe),g(be),g(Cs.$$.fragment,e),g(zs.$$.fragment,e),g(Ts.$$.fragment,e),g(Ss.$$.fragment,e),g(Fs.$$.fragment,e),g(qs.$$.fragment,e),g(Rs.$$.fragment,e),g(Bs.$$.fragment,e),g(As.$$.fragment,e),g(Xs.$$.fragment,e),g(Hs.$$.fragment,e),g(Gs.$$.fragment,e),g(Ns.$$.fragment,e),g(Ms.$$.fragment,e),g(Ws.$$.fragment,e),g(Ls.$$.fragment,e),Ko=!0)},o(e){m(d.$$.fragment,e),m(A.$$.fragment,e),m(z),m(as.$$.fragment,e),m(Re.$$.fragment,e),m(ls.$$.fragment,e),m(rs.$$.fragment,e),m(is.$$.fragment,e),m(ps.$$.fragment,e),m(cs.$$.fragment,e),m(hs.$$.fragment,e),m(fs.$$.fragment,e),m(ds.$$.fragment,e),m(us.$$.fragment,e),m(ms.$$.fragment,e),m(gs.$$.fragment,e),m(xs.$$.fragment,e),m(Ne.$$.fragment,e),m(ws.$$.fragment,e),m(bs.$$.fragment,e),m(Me.$$.fragment,e),m(js.$$.fragment,e),m(me),m(_s.$$.fragment,e),m($s.$$.fragment,e),m(vs.$$.fragment,e),m(Es.$$.fragment,e),m(Os.$$.fragment,e),m(Is.$$.fragment,e),m(xe),m(be),m(Cs.$$.fragment,e),m(zs.$$.fragment,e),m(Ts.$$.fragment,e),m(Ss.$$.fragment,e),m(Fs.$$.fragment,e),m(qs.$$.fragment,e),m(Rs.$$.fragment,e),m(Bs.$$.fragment,e),m(As.$$.fragment,e),m(Xs.$$.fragment,e),m(Hs.$$.fragment,e),m(Gs.$$.fragment,e),m(Ns.$$.fragment,e),m(Ms.$$.fragment,e),m(Ws.$$.fragment,e),m(Ls.$$.fragment,e),Ko=!1},d(e){t(f),e&&t(x),k(d,e),e&&t(_),e&&t(P),k(A),e&&t(I),Us[B].d(e),e&&t(T),e&&t(O),e&&t(Ia),k(as,e),e&&t(Ca),e&&t(Fe),e&&t(za),e&&t(qe),e&&t(Ta),k(Re,e),e&&t(Da),e&&t(ze),k(ls),e&&t(Pa),k(rs,e),e&&t(Sa),e&&t(Ae),e&&t(Fa),e&&t(Xe),e&&t(qa),e&&t(rt),e&&t(Ra),k(is,e),e&&t(Ba),e&&t(He),e&&t(Aa),k(ps,e),e&&t(Xa),e&&t(ee),e&&t(Ha),k(cs,e),e&&t(Ga),k(hs,e),e&&t(Na),e&&t(Ge),e&&t(Ma),k(fs,e),e&&t(Wa),k(ds,e),e&&t(La),e&&t(it),e&&t(Ua),k(us,e),e&&t(Ya),k(ms,e),e&&t(Va),e&&t(je),e&&t(Qa),k(gs,e),e&&t(Ja),k(xs,e),e&&t(Ka),e&&t(Q),e&&t(Za),k(Ne,e),e&&t(eo),e&&t(ke),e&&t(so),e&&t(M),e&&t(to),k(ws,e),e&&t(no),k(bs,e),e&&t(ao),e&&t(_e),e&&t(oo),k(Me,e),e&&t(lo),e&&t(Te),k(js),e&&t(ro),e&&t(se),e&&t(io),Ys[ue].d(e),e&&t(ht),e&&t(De),k(_s),e&&t(po),e&&t(Ue),e&&t(co),k($s,e),e&&t(ho),k(vs,e),e&&t(fo),e&&t(ft),e&&t(uo),k(Es,e),e&&t(mo),k(Os,e),e&&t(go),e&&t(W),e&&t(xo),e&&t(ye),e&&t(wo),e&&t(Qe),e&&t(bo),e&&t(Pe),k(Is),e&&t(jo),Vs[ge].d(e),e&&t(ut),e&&t(mt),e&&t(ko),Qs[we].d(e),e&&t(gt),k(Cs,e),e&&t(_o),e&&t(Ke),e&&t(yo),k(zs,e),e&&t($o),k(Ts,e),e&&t(vo),e&&t(C),e&&t(Eo),e&&t(G),e&&t(Oo),e&&t(Ds),e&&t(Io),e&&t(Ze),e&&t(Co),k(Ss,e),e&&t(zo),k(Fs,e),e&&t(To),e&&t(ie),e&&t(Do),k(qs,e),e&&t(Po),k(Rs,e),e&&t(So),e&&t(pe),e&&t(Fo),k(Bs,e),e&&t(qo),e&&t(es),e&&t(Ro),k(As,e),e&&t(Bo),e&&t(xt),e&&t(Ao),k(Xs,e),e&&t(Xo),k(Hs,e),e&&t(Ho),e&&t(wt),e&&t(Go),e&&t(Se),k(Gs),e&&t(No),e&&t(L),e&&t(Mo),e&&t(J),e&&t(Wo),k(Ns,e),e&&t(Lo),k(Ms,e),e&&t(Uo),e&&t(K),e&&t(Yo),k(Ws,e),e&&t(Vo),e&&t(bt),e&&t(Qo),k(Ls,e),e&&t(Jo),e&&t(jt)}}}const Ad={local:"fast-tokenizers-special-powers",sections:[{local:"batch-encoding",title:"Batch encoding"},{local:"inside-the-tokenclassification-pipeline",sections:[{local:"getting-the-base-results-with-the-pipeline",title:"Getting the base results with the pipeline"},{local:"from-inputs-to-predictions",title:"From inputs to predictions"},{local:"grouping-entities",title:"Grouping entities"}],title:"Inside the `token-classification` pipeline"}],title:"Fast tokenizers' special powers"};function Xd(D,f,x){let d="pt";return $d(()=>{const _=new URL(document.location).searchParams;x(0,d=_.get("fw")||"pt")}),[d]}class Yd extends bd{constructor(f){super();jd(this,f,Xd,Bd,kd,{})}}export{Yd as default,Ad as metadata};
