import{S as ap,i as rp,s as ip,e as i,k as f,w as E,t as a,M as pp,c as p,d as t,m,x as T,a as c,h as r,b as j,N as tp,F as s,g as u,y as x,o as $,p as ce,q as k,B as A,v as cp,O as up,n as ue}from"../../chunks/vendor-e7c81d8a.js";import{T as sp}from"../../chunks/Tip-989931f5.js";import{Y as op}from"../../chunks/Youtube-365ea064.js";import{I as ms}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";import{C as M}from"../../chunks/CodeBlock-105940ae.js";import{D as lp}from"../../chunks/DocNotebookDropdown-928568b4.js";import{F as hp}from"../../chunks/FrameworkSwitch-287292d8.js";const{document:np}=up;function fp(y){let n,h;return n=new lp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"}]}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function mp(y){let n,h;return n=new lp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"}]}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function dp(y){let n;return{c(){n=a("This is the first section where the content is slightly different depending on whether you use PyTorch and TensorFlow. Toogle the switch on top of the title to select the platform you prefer!")},l(h){n=r(h,"This is the first section where the content is slightly different depending on whether you use PyTorch and TensorFlow. Toogle the switch on top of the title to select the platform you prefer!")},m(h,o){u(h,n,o)},d(h){h&&t(n)}}}function bp(y){let n,h;return n=new op({props:{id:"wVN12smEvqg"}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function _p(y){let n,h;return n=new op({props:{id:"1pedAIvTWXk"}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function wp(y){let n,h;return n=new M({props:{codee:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs),`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function $p(y){let n,h;return n=new M({props:{codee:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs),`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function kp(y){let n,h,o,d,_;return d=new M({props:{codee:`{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
},`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;, 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;
}`}}),{c(){n=i("p"),h=a("Here\u2019s what the results look like as TensorFlow tensors:"),o=f(),E(d.$$.fragment)},l(b){n=p(b,"P",{});var P=c(n);h=r(P,"Here\u2019s what the results look like as TensorFlow tensors:"),P.forEach(t),o=m(b),T(d.$$.fragment,b)},m(b,P){u(b,n,P),s(n,h),u(b,o,P),x(d,b,P),_=!0},i(b){_||(k(d.$$.fragment,b),_=!0)},o(b){$(d.$$.fragment,b),_=!1},d(b){b&&t(n),b&&t(o),A(d,b)}}}function vp(y){let n,h,o,d,_;return d=new M({props:{codee:`{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
},`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
    ]), 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
    ])
}`}}),{c(){n=i("p"),h=a("Here\u2019s what the results look like as PyTorch tensors:"),o=f(),E(d.$$.fragment)},l(b){n=p(b,"P",{});var P=c(n);h=r(P,"Here\u2019s what the results look like as PyTorch tensors:"),P.forEach(t),o=m(b),T(d.$$.fragment,b)},m(b,P){u(b,n,P),s(n,h),u(b,o,P),x(d,b,P),_=!0},i(b){_||(k(d.$$.fragment,b),_=!0)},o(b){$(d.$$.fragment,b),_=!1},d(b){b&&t(n),b&&t(o),A(d,b)}}}function yp(y){let n,h,o,d,_,b,P,S,I,g,C;return g=new M({props:{codee:`from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModel.from_pretrained(checkpoint)`}}),{c(){n=i("p"),h=a("We can download our pretrained model the same way we did with our tokenizer. \u{1F917} Transformers provides an "),o=i("code"),d=a("TFAutoModel"),_=a(" class which also has a "),b=i("code"),P=a("from_pretrained"),S=a(" method:"),I=f(),E(g.$$.fragment)},l(w){n=p(w,"P",{});var v=c(n);h=r(v,"We can download our pretrained model the same way we did with our tokenizer. \u{1F917} Transformers provides an "),o=p(v,"CODE",{});var q=c(o);d=r(q,"TFAutoModel"),q.forEach(t),_=r(v," class which also has a "),b=p(v,"CODE",{});var N=c(b);P=r(N,"from_pretrained"),N.forEach(t),S=r(v," method:"),v.forEach(t),I=m(w),T(g.$$.fragment,w)},m(w,v){u(w,n,v),s(n,h),s(n,o),s(o,d),s(n,_),s(n,b),s(b,P),s(n,S),u(w,I,v),x(g,w,v),C=!0},i(w){C||(k(g.$$.fragment,w),C=!0)},o(w){$(g.$$.fragment,w),C=!1},d(w){w&&t(n),w&&t(I),A(g,w)}}}function gp(y){let n,h,o,d,_,b,P,S,I,g,C;return g=new M({props:{codee:`from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModel.from_pretrained(checkpoint)`}}),{c(){n=i("p"),h=a("We can download our pretrained model the same way we did with our tokenizer. \u{1F917} Transformers provides an "),o=i("code"),d=a("AutoModel"),_=a(" class which also has a "),b=i("code"),P=a("from_pretrained()"),S=a(" method:"),I=f(),E(g.$$.fragment)},l(w){n=p(w,"P",{});var v=c(n);h=r(v,"We can download our pretrained model the same way we did with our tokenizer. \u{1F917} Transformers provides an "),o=p(v,"CODE",{});var q=c(o);d=r(q,"AutoModel"),q.forEach(t),_=r(v," class which also has a "),b=p(v,"CODE",{});var N=c(b);P=r(N,"from_pretrained()"),N.forEach(t),S=r(v," method:"),v.forEach(t),I=m(w),T(g.$$.fragment,w)},m(w,v){u(w,n,v),s(n,h),s(n,o),s(o,d),s(n,_),s(n,b),s(b,P),s(n,S),u(w,I,v),x(g,w,v),C=!0},i(w){C||(k(g.$$.fragment,w),C=!0)},o(w){$(g.$$.fragment,w),C=!1},d(w){w&&t(n),w&&t(I),A(g,w)}}}function jp(y){let n,h,o,d;return n=new M({props:{codee:`outputs = model(inputs)
print(outputs.last_hidden_state.shape),`,highlighted:`outputs = model(inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),o=new M({props:{codee:"(2, 16, 768),",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>)'}}),{c(){E(n.$$.fragment),h=f(),E(o.$$.fragment)},l(_){T(n.$$.fragment,_),h=m(_),T(o.$$.fragment,_)},m(_,b){x(n,_,b),u(_,h,b),x(o,_,b),d=!0},i(_){d||(k(n.$$.fragment,_),k(o.$$.fragment,_),d=!0)},o(_){$(n.$$.fragment,_),$(o.$$.fragment,_),d=!1},d(_){A(n,_),_&&t(h),A(o,_)}}}function Ep(y){let n,h,o,d;return n=new M({props:{codee:`outputs = model(**inputs)
print(outputs.last_hidden_state.shape),`,highlighted:`outputs = model(**inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),o=new M({props:{codee:"torch.Size([2, 16, 768]),",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>])'}}),{c(){E(n.$$.fragment),h=f(),E(o.$$.fragment)},l(_){T(n.$$.fragment,_),h=m(_),T(o.$$.fragment,_)},m(_,b){x(n,_,b),u(_,h,b),x(o,_,b),d=!0},i(_){d||(k(n.$$.fragment,_),k(o.$$.fragment,_),d=!0)},o(_){$(n.$$.fragment,_),$(o.$$.fragment,_),d=!1},d(_){A(n,_),_&&t(h),A(o,_)}}}function Tp(y){let n,h,o,d,_,b,P,S,I,g,C;return g=new M({props:{codee:`from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`}}),{c(){n=i("p"),h=a("For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the "),o=i("code"),d=a("TFAutoModel"),_=a(" class, but "),b=i("code"),P=a("TFAutoModelForSequenceClassification"),S=a(":"),I=f(),E(g.$$.fragment)},l(w){n=p(w,"P",{});var v=c(n);h=r(v,"For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the "),o=p(v,"CODE",{});var q=c(o);d=r(q,"TFAutoModel"),q.forEach(t),_=r(v," class, but "),b=p(v,"CODE",{});var N=c(b);P=r(N,"TFAutoModelForSequenceClassification"),N.forEach(t),S=r(v,":"),v.forEach(t),I=m(w),T(g.$$.fragment,w)},m(w,v){u(w,n,v),s(n,h),s(n,o),s(o,d),s(n,_),s(n,b),s(b,P),s(n,S),u(w,I,v),x(g,w,v),C=!0},i(w){C||(k(g.$$.fragment,w),C=!0)},o(w){$(g.$$.fragment,w),C=!1},d(w){w&&t(n),w&&t(I),A(g,w)}}}function xp(y){let n,h,o,d,_,b,P,S,I,g,C;return g=new M({props:{codee:`from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`}}),{c(){n=i("p"),h=a("For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the "),o=i("code"),d=a("AutoModel"),_=a(" class, but "),b=i("code"),P=a("AutoModelForSequenceClassification"),S=a(":"),I=f(),E(g.$$.fragment)},l(w){n=p(w,"P",{});var v=c(n);h=r(v,"For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the "),o=p(v,"CODE",{});var q=c(o);d=r(q,"AutoModel"),q.forEach(t),_=r(v," class, but "),b=p(v,"CODE",{});var N=c(b);P=r(N,"AutoModelForSequenceClassification"),N.forEach(t),S=r(v,":"),v.forEach(t),I=m(w),T(g.$$.fragment,w)},m(w,v){u(w,n,v),s(n,h),s(n,o),s(o,d),s(n,_),s(n,b),s(b,P),s(n,S),u(w,I,v),x(g,w,v),C=!0},i(w){C||(k(g.$$.fragment,w),C=!0)},o(w){$(g.$$.fragment,w),C=!1},d(w){w&&t(n),w&&t(I),A(g,w)}}}function Ap(y){let n,h;return n=new M({props:{codee:"(2, 2),",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)'}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Pp(y){let n,h;return n=new M({props:{codee:"torch.Size([2, 2]),",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Ip(y){let n,h;return n=new M({props:{codee:`<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>,`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
    array([[-<span class="hljs-number">1.5606991</span>,  <span class="hljs-number">1.6122842</span>],
           [ <span class="hljs-number">4.169231</span> , -<span class="hljs-number">3.3464472</span>]], dtype=float32)&gt;`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Sp(y){let n,h;return n=new M({props:{codee:`tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>),`,highlighted:`tensor([[-<span class="hljs-number">1.5607</span>,  <span class="hljs-number">1.6123</span>],
        [ <span class="hljs-number">4.1692</span>, -<span class="hljs-number">3.3464</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Mp(y){let n,h;return n=new M({props:{codee:`import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

predictions = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Cp(y){let n,h;return n=new M({props:{codee:`import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions),`,highlighted:`<span class="hljs-keyword">import</span> torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function qp(y){let n,h;return n=new M({props:{codee:`tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32),`,highlighted:`tf.Tensor(
[[<span class="hljs-number">4.01951671e-02</span> <span class="hljs-number">9.59804833e-01</span>]
 [<span class="hljs-number">9.9945587e-01</span> <span class="hljs-number">5.4418424e-04</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Fp(y){let n,h;return n=new M({props:{codee:`tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>),`,highlighted:`tensor([[<span class="hljs-number">4.0195e-02</span>, <span class="hljs-number">9.5980e-01</span>],
        [<span class="hljs-number">9.9946e-01</span>, <span class="hljs-number">5.4418e-04</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`}}),{c(){E(n.$$.fragment)},l(o){T(n.$$.fragment,o)},m(o,d){x(n,o,d),h=!0},i(o){h||(k(n.$$.fragment,o),h=!0)},o(o){$(n.$$.fragment,o),h=!1},d(o){A(n,o)}}}function Np(y){let n,h,o,d,_,b,P,S;return{c(){n=i("p"),h=a("\u270F\uFE0F "),o=i("strong"),d=a("Try it out!"),_=a(" Choose two (or more) texts of your own and run them through the "),b=i("code"),P=a("sentiment-analysis"),S=a(" pipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!")},l(I){n=p(I,"P",{});var g=c(n);h=r(g,"\u270F\uFE0F "),o=p(g,"STRONG",{});var C=c(o);d=r(C,"Try it out!"),C.forEach(t),_=r(g," Choose two (or more) texts of your own and run them through the "),b=p(g,"CODE",{});var w=c(b);P=r(w,"sentiment-analysis"),w.forEach(t),S=r(g," pipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!"),g.forEach(t)},m(I,g){u(I,n,g),s(n,h),s(n,o),s(o,d),s(n,_),s(n,b),s(b,P),s(n,S)},d(I){I&&t(n)}}}function Op(y){let n,h,o,d,_,b,P,S,I,g,C,w,v,q,N,ge,kn,W,B,kt,je,Ao,vt,Po,Io,vn,Be,yn,yt,So,gn,Ue,jn,Ee,Mo,gt,Co,qo,En,Te,ar,Tn,jt,Fo,xn,we,xe,ds,Re,No,bs,Oo,An,Ae,Do,_s,zo,Lo,Pn,he,Et,Ho,ws,Go,Vo,$s,Wo,Bo,ks,Uo,In,L,Ro,Ye,Yo,Qo,vs,Jo,Xo,ys,Ko,Zo,Sn,H,el,gs,tl,sl,js,nl,ol,Qe,ll,al,Mn,Je,Cn,Tt,rl,qn,Pe,il,Es,pl,cl,Fn,Ie,ul,Ts,hl,fl,Nn,U,R,xt,At,ml,On,Y,Q,Pt,D,dl,xs,bl,_l,As,wl,$l,Ps,kl,vl,Is,yl,gl,Dn,$e,Se,Ss,Xe,jl,Ms,El,zn,J,X,It,St,Tl,Ln,G,xl,Cs,Al,Pl,qs,Il,Sl,Fs,Ml,Cl,Hn,Mt,ql,Gn,fe,Fl,Ns,Nl,Ol,Ct,Dl,zl,Vn,ke,Me,Os,Ke,Ll,Ds,Hl,Wn,qt,Gl,Bn,me,Ft,zs,Vl,Wl,Bl,Nt,Ls,Ul,Rl,Yl,Ot,Hs,Ql,Jl,Un,Dt,Xl,Rn,zt,Kl,Yn,K,Z,Lt,V,Zl,Gs,ea,ta,Vs,sa,na,Ws,oa,la,Qn,ve,Ce,Bs,Ze,aa,Us,ra,Jn,Ht,ia,Xn,et,tt,rr,Kn,Gt,pa,Zn,Vt,ca,eo,Wt,ua,to,F,Bt,Rs,ha,fa,ma,Ys,Qs,da,ba,Js,Xs,_a,wa,Ks,Zs,$a,ka,en,tn,va,ya,sn,nn,ga,ja,on,ln,Ea,Ta,an,xa,so,ee,te,Ut,Rt,Aa,no,st,oo,se,ne,Yt,Qt,Pa,lo,ye,qe,rn,nt,Ia,pn,Sa,ao,Jt,Ma,ro,ot,io,oe,le,Xt,z,Ca,cn,qa,Fa,un,Na,Oa,hn,Da,za,lt,La,Ha,po,ae,re,Kt,ie,pe,Zt,de,Ga,fn,Va,Wa,mn,Ba,Ua,co,Fe,Ra,dn,Ya,Qa,uo,at,ho,rt,fo,es,Ja,mo,Ne,bn,Xa,Ka,_n,Za,bo,ts,er,_o,Oe,wo;o=new hp({props:{fw:y[0]}}),S=new ms({});const ir=[mp,fp],it=[];function pr(e,l){return e[0]==="pt"?0:1}v=pr(y),q=it[v]=ir[v](y),ge=new sp({props:{$$slots:{default:[dp]},$$scope:{ctx:y}}});const cr=[_p,bp],pt=[];function ur(e,l){return e[0]==="pt"?0:1}W=ur(y),B=pt[W]=cr[W](y),Be=new M({props:{codee:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)
classifier(
    [
        <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
        <span class="hljs-string">&quot;I hate this so much!&quot;</span>,
    ]
)`}}),Ue=new M({props:{codee:`[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}],`,highlighted:`[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>},
 {<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994558095932007</span>}]`}}),Re=new ms({}),Je=new M({props:{codee:`from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`}});const hr=[$p,wp],ct=[];function fr(e,l){return e[0]==="pt"?0:1}U=fr(y),R=ct[U]=hr[U](y);const mr=[vp,kp],ut=[];function dr(e,l){return e[0]==="pt"?0:1}Y=dr(y),Q=ut[Y]=mr[Y](y),Xe=new ms({});const br=[gp,yp],ht=[];function _r(e,l){return e[0]==="pt"?0:1}J=_r(y),X=ht[J]=br[J](y),Ke=new ms({});const wr=[Ep,jp],ft=[];function $r(e,l){return e[0]==="pt"?0:1}K=$r(y),Z=ft[K]=wr[K](y),Ze=new ms({});const kr=[xp,Tp],mt=[];function vr(e,l){return e[0]==="pt"?0:1}ee=vr(y),te=mt[ee]=kr[ee](y),st=new M({props:{codee:"print(outputs.logits.shape),",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits.shape)'}});const yr=[Pp,Ap],dt=[];function gr(e,l){return e[0]==="pt"?0:1}se=gr(y),ne=dt[se]=yr[se](y),nt=new ms({}),ot=new M({props:{codee:"print(outputs.logits),",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits)'}});const jr=[Sp,Ip],bt=[];function Er(e,l){return e[0]==="pt"?0:1}oe=Er(y),le=bt[oe]=jr[oe](y);const Tr=[Cp,Mp],_t=[];function xr(e,l){return e[0]==="pt"?0:1}ae=xr(y),re=_t[ae]=Tr[ae](y);const Ar=[Fp,qp],wt=[];function Pr(e,l){return e[0]==="pt"?0:1}return ie=Pr(y),pe=wt[ie]=Ar[ie](y),at=new M({props:{codee:"model.config.id2label,",highlighted:"model.config.id2label"}}),rt=new M({props:{codee:"{0: 'NEGATIVE', 1: 'POSITIVE'},",highlighted:'{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>}'}}),Oe=new sp({props:{$$slots:{default:[Np]},$$scope:{ctx:y}}}),{c(){n=i("meta"),h=f(),E(o.$$.fragment),d=f(),_=i("h1"),b=i("a"),P=i("span"),E(S.$$.fragment),I=f(),g=i("span"),C=a("Behind the pipeline"),w=f(),q.c(),N=f(),E(ge.$$.fragment),kn=f(),B.c(),kt=f(),je=i("p"),Ao=a("Let\u2019s start with a complete example, taking a look at what happened behind the scenes when we executed the following code in "),vt=i("a"),Po=a("Chapter 1"),Io=a(":"),vn=f(),E(Be.$$.fragment),yn=f(),yt=i("p"),So=a("and obtained:"),gn=f(),E(Ue.$$.fragment),jn=f(),Ee=i("p"),Mo=a("As we saw in "),gt=i("a"),Co=a("Chapter 1"),qo=a(", this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:"),En=f(),Te=i("img"),Tn=f(),jt=i("p"),Fo=a("Let\u2019s quickly go over each of these."),xn=f(),we=i("h2"),xe=i("a"),ds=i("span"),E(Re.$$.fragment),No=f(),bs=i("span"),Oo=a("Preprocessing with a tokenizer"),An=f(),Ae=i("p"),Do=a("Like other neural networks, Transformer models can\u2019t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a "),_s=i("em"),zo=a("tokenizer"),Lo=a(", which will be responsible for:"),Pn=f(),he=i("ul"),Et=i("li"),Ho=a("Splitting the input into words, subwords, or symbols (like punctuation) that are called "),ws=i("em"),Go=a("tokens"),Vo=f(),$s=i("li"),Wo=a("Mapping each token to an integer"),Bo=f(),ks=i("li"),Uo=a("Adding additional inputs that may be useful to the model"),In=f(),L=i("p"),Ro=a("All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the "),Ye=i("a"),Yo=a("Model Hub"),Qo=a(". To do this, we use the "),vs=i("code"),Jo=a("AutoTokenizer"),Xo=a(" class and its "),ys=i("code"),Ko=a("from_pretrained()"),Zo=a(" method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it (so it\u2019s only downloaded the first time you run the code below)."),Sn=f(),H=i("p"),el=a("Since the default checkpoint of the "),gs=i("code"),tl=a("sentiment-analysis"),sl=a(" pipeline is "),js=i("code"),nl=a("distilbert-base-uncased-finetuned-sst-2-english"),ol=a(" (you can see its model card "),Qe=i("a"),ll=a("here"),al=a("), we run the following:"),Mn=f(),E(Je.$$.fragment),Cn=f(),Tt=i("p"),rl=a("Once we have the tokenizer, we can directly pass our sentences to it and we\u2019ll get back a dictionary that\u2019s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."),qn=f(),Pe=i("p"),il=a("You can use \u{1F917} Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept "),Es=i("em"),pl=a("tensors"),cl=a(" as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It\u2019s effectively a tensor; other ML frameworks\u2019 tensors behave similarly, and are usually as simple to instantiate as NumPy arrays."),Fn=f(),Ie=i("p"),ul=a("To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the "),Ts=i("code"),hl=a("return_tensors"),fl=a(" argument:"),Nn=f(),R.c(),xt=f(),At=i("p"),ml=a("Don\u2019t worry about padding and truncation just yet; we\u2019ll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result)."),On=f(),Q.c(),Pt=f(),D=i("p"),dl=a("The output itself is a dictionary containing two keys, "),xs=i("code"),bl=a("input_ids"),_l=a(" and "),As=i("code"),wl=a("attention_mask"),$l=a(". "),Ps=i("code"),kl=a("input_ids"),vl=a(" contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. We\u2019ll explain what the "),Is=i("code"),yl=a("attention_mask"),gl=a(" is later in this chapter."),Dn=f(),$e=i("h2"),Se=i("a"),Ss=i("span"),E(Xe.$$.fragment),jl=f(),Ms=i("span"),El=a("Going through the model"),zn=f(),X.c(),It=f(),St=i("p"),Tl=a("In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it."),Ln=f(),G=i("p"),xl=a("This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call "),Cs=i("em"),Al=a("hidden states"),Pl=a(", also known as "),qs=i("em"),Il=a("features"),Sl=a(". For each model input, we\u2019ll retrieve a high-dimensional vector representing the "),Fs=i("strong"),Ml=a("contextual understanding of that input by the Transformer model"),Cl=a("."),Hn=f(),Mt=i("p"),ql=a("If this doesn\u2019t make sense, don\u2019t worry about it. We\u2019ll explain it all later."),Gn=f(),fe=i("p"),Fl=a("While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the "),Ns=i("em"),Nl=a("head"),Ol=a(". In "),Ct=i("a"),Dl=a("Chapter 1"),zl=a(", the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."),Vn=f(),ke=i("h3"),Me=i("a"),Os=i("span"),E(Ke.$$.fragment),Ll=f(),Ds=i("span"),Hl=a("A high-dimensional vector?"),Wn=f(),qt=i("p"),Gl=a("The vector output by the Transformer module is usually large. It generally has three dimensions:"),Bn=f(),me=i("ul"),Ft=i("li"),zs=i("strong"),Vl=a("Batch size"),Wl=a(": The number of sequences processed at a time (2 in our example)."),Bl=f(),Nt=i("li"),Ls=i("strong"),Ul=a("Sequence length"),Rl=a(": The length of the numerical representation of the sequence (16 in our example)."),Yl=f(),Ot=i("li"),Hs=i("strong"),Ql=a("Hidden size"),Jl=a(": The vector dimension of each model input."),Un=f(),Dt=i("p"),Xl=a("It is said to be \u201Chigh dimensional\u201D because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."),Rn=f(),zt=i("p"),Kl=a("We can see this if we feed the inputs we preprocessed to our model:"),Yn=f(),Z.c(),Lt=f(),V=i("p"),Zl=a("Note that the outputs of \u{1F917} Transformers models behave like "),Gs=i("code"),ea=a("namedtuple"),ta=a("s or dictionaries. You can access the elements by attributes (like we did) or by key ("),Vs=i("code"),sa=a('outputs["last_hidden_state"]'),na=a("), or even by index if you know exactly where the thing you are looking for is ("),Ws=i("code"),oa=a("outputs[0]"),la=a(")."),Qn=f(),ve=i("h3"),Ce=i("a"),Bs=i("span"),E(Ze.$$.fragment),aa=f(),Us=i("span"),ra=a("Model heads: Making sense out of numbers"),Jn=f(),Ht=i("p"),ia=a("The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:"),Xn=f(),et=i("div"),tt=i("img"),Kn=f(),Gt=i("p"),pa=a("The output of the Transformer model is sent directly to the model head to be processed."),Zn=f(),Vt=i("p"),ca=a("In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."),eo=f(),Wt=i("p"),ua=a("There are many different architectures available in \u{1F917} Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:"),to=f(),F=i("ul"),Bt=i("li"),Rs=i("code"),ha=a("*Model"),fa=a(" (retrieve the hidden states)"),ma=f(),Ys=i("li"),Qs=i("code"),da=a("*ForCausalLM"),ba=f(),Js=i("li"),Xs=i("code"),_a=a("*ForMaskedLM"),wa=f(),Ks=i("li"),Zs=i("code"),$a=a("*ForMultipleChoice"),ka=f(),en=i("li"),tn=i("code"),va=a("*ForQuestionAnswering"),ya=f(),sn=i("li"),nn=i("code"),ga=a("*ForSequenceClassification"),ja=f(),on=i("li"),ln=i("code"),Ea=a("*ForTokenClassification"),Ta=f(),an=i("li"),xa=a("and others \u{1F917}"),so=f(),te.c(),Ut=f(),Rt=i("p"),Aa=a("Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"),no=f(),E(st.$$.fragment),oo=f(),ne.c(),Yt=f(),Qt=i("p"),Pa=a("Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."),lo=f(),ye=i("h2"),qe=i("a"),rn=i("span"),E(nt.$$.fragment),Ia=f(),pn=i("span"),Sa=a("Postprocessing the output"),ao=f(),Jt=i("p"),Ma=a("The values we get as output from our model don\u2019t necessarily make sense by themselves. Let\u2019s take a look:"),ro=f(),E(ot.$$.fragment),io=f(),le.c(),Xt=f(),z=i("p"),Ca=a("Our model predicted "),cn=i("code"),qa=a("[-1.5607, 1.6123]"),Fa=a(" for the first sentence and "),un=i("code"),Na=a("[ 4.1692, -3.3464]"),Oa=a(" for the second one. Those are not probabilities but "),hn=i("em"),Da=a("logits"),za=a(", the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a "),lt=i("a"),La=a("SoftMax"),Ha=a(" layer (all \u{1F917} Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"),po=f(),re.c(),Kt=f(),pe.c(),Zt=f(),de=i("p"),Ga=a("Now we can see that the model predicted "),fn=i("code"),Va=a("[0.0402, 0.9598]"),Wa=a(" for the first sentence and "),mn=i("code"),Ba=a("[0.9995, 0.0005]"),Ua=a(" for the second one. These are recognizable probability scores."),co=f(),Fe=i("p"),Ra=a("To get the labels corresponding to each position, we can inspect the "),dn=i("code"),Ya=a("id2label"),Qa=a(" attribute of the model config (more on this in the next section):"),uo=f(),E(at.$$.fragment),ho=f(),E(rt.$$.fragment),fo=f(),es=i("p"),Ja=a("Now we can conclude that the model predicted the following:"),mo=f(),Ne=i("ul"),bn=i("li"),Xa=a("First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598"),Ka=f(),_n=i("li"),Za=a("Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"),bo=f(),ts=i("p"),er=a("We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let\u2019s take some time to dive deeper into each of those steps."),_o=f(),E(Oe.$$.fragment),this.h()},l(e){const l=pp('[data-svelte="svelte-1phssyn"]',np.head);n=p(l,"META",{name:!0,content:!0}),l.forEach(t),h=m(e),T(o.$$.fragment,e),d=m(e),_=p(e,"H1",{class:!0});var $t=c(_);b=p($t,"A",{id:!0,class:!0,href:!0});var ss=c(b);P=p(ss,"SPAN",{});var wn=c(P);T(S.$$.fragment,wn),wn.forEach(t),ss.forEach(t),I=m($t),g=p($t,"SPAN",{});var ns=c(g);C=r(ns,"Behind the pipeline"),ns.forEach(t),$t.forEach(t),w=m(e),q.l(e),N=m(e),T(ge.$$.fragment,e),kn=m(e),B.l(e),kt=m(e),je=p(e,"P",{});var De=c(je);Ao=r(De,"Let\u2019s start with a complete example, taking a look at what happened behind the scenes when we executed the following code in "),vt=p(De,"A",{href:!0});var os=c(vt);Po=r(os,"Chapter 1"),os.forEach(t),Io=r(De,":"),De.forEach(t),vn=m(e),T(Be.$$.fragment,e),yn=m(e),yt=p(e,"P",{});var ls=c(yt);So=r(ls,"and obtained:"),ls.forEach(t),gn=m(e),T(Ue.$$.fragment,e),jn=m(e),Ee=p(e,"P",{});var ze=c(Ee);Mo=r(ze,"As we saw in "),gt=p(ze,"A",{href:!0});var as=c(gt);Co=r(as,"Chapter 1"),as.forEach(t),qo=r(ze,", this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:"),ze.forEach(t),En=m(e),Te=p(e,"IMG",{src:!0,alt:!0,width:!0}),Tn=m(e),jt=p(e,"P",{});var rs=c(jt);Fo=r(rs,"Let\u2019s quickly go over each of these."),rs.forEach(t),xn=m(e),we=p(e,"H2",{class:!0});var Le=c(we);xe=p(Le,"A",{id:!0,class:!0,href:!0});var is=c(xe);ds=p(is,"SPAN",{});var ps=c(ds);T(Re.$$.fragment,ps),ps.forEach(t),is.forEach(t),No=m(Le),bs=p(Le,"SPAN",{});var $n=c(bs);Oo=r($n,"Preprocessing with a tokenizer"),$n.forEach(t),Le.forEach(t),An=m(e),Ae=p(e,"P",{});var $o=c(Ae);Do=r($o,"Like other neural networks, Transformer models can\u2019t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a "),_s=p($o,"EM",{});var Ir=c(_s);zo=r(Ir,"tokenizer"),Ir.forEach(t),Lo=r($o,", which will be responsible for:"),$o.forEach(t),Pn=m(e),he=p(e,"UL",{});var cs=c(he);Et=p(cs,"LI",{});var tr=c(Et);Ho=r(tr,"Splitting the input into words, subwords, or symbols (like punctuation) that are called "),ws=p(tr,"EM",{});var Sr=c(ws);Go=r(Sr,"tokens"),Sr.forEach(t),tr.forEach(t),Vo=m(cs),$s=p(cs,"LI",{});var Mr=c($s);Wo=r(Mr,"Mapping each token to an integer"),Mr.forEach(t),Bo=m(cs),ks=p(cs,"LI",{});var Cr=c(ks);Uo=r(Cr,"Adding additional inputs that may be useful to the model"),Cr.forEach(t),cs.forEach(t),In=m(e),L=p(e,"P",{});var He=c(L);Ro=r(He,"All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the "),Ye=p(He,"A",{href:!0,rel:!0});var qr=c(Ye);Yo=r(qr,"Model Hub"),qr.forEach(t),Qo=r(He,". To do this, we use the "),vs=p(He,"CODE",{});var Fr=c(vs);Jo=r(Fr,"AutoTokenizer"),Fr.forEach(t),Xo=r(He," class and its "),ys=p(He,"CODE",{});var Nr=c(ys);Ko=r(Nr,"from_pretrained()"),Nr.forEach(t),Zo=r(He," method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it (so it\u2019s only downloaded the first time you run the code below)."),He.forEach(t),Sn=m(e),H=p(e,"P",{});var Ge=c(H);el=r(Ge,"Since the default checkpoint of the "),gs=p(Ge,"CODE",{});var Or=c(gs);tl=r(Or,"sentiment-analysis"),Or.forEach(t),sl=r(Ge," pipeline is "),js=p(Ge,"CODE",{});var Dr=c(js);nl=r(Dr,"distilbert-base-uncased-finetuned-sst-2-english"),Dr.forEach(t),ol=r(Ge," (you can see its model card "),Qe=p(Ge,"A",{href:!0,rel:!0});var zr=c(Qe);ll=r(zr,"here"),zr.forEach(t),al=r(Ge,"), we run the following:"),Ge.forEach(t),Mn=m(e),T(Je.$$.fragment,e),Cn=m(e),Tt=p(e,"P",{});var Lr=c(Tt);rl=r(Lr,"Once we have the tokenizer, we can directly pass our sentences to it and we\u2019ll get back a dictionary that\u2019s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."),Lr.forEach(t),qn=m(e),Pe=p(e,"P",{});var ko=c(Pe);il=r(ko,"You can use \u{1F917} Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept "),Es=p(ko,"EM",{});var Hr=c(Es);pl=r(Hr,"tensors"),Hr.forEach(t),cl=r(ko," as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It\u2019s effectively a tensor; other ML frameworks\u2019 tensors behave similarly, and are usually as simple to instantiate as NumPy arrays."),ko.forEach(t),Fn=m(e),Ie=p(e,"P",{});var vo=c(Ie);ul=r(vo,"To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the "),Ts=p(vo,"CODE",{});var Gr=c(Ts);hl=r(Gr,"return_tensors"),Gr.forEach(t),fl=r(vo," argument:"),vo.forEach(t),Nn=m(e),R.l(e),xt=m(e),At=p(e,"P",{});var Vr=c(At);ml=r(Vr,"Don\u2019t worry about padding and truncation just yet; we\u2019ll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result)."),Vr.forEach(t),On=m(e),Q.l(e),Pt=m(e),D=p(e,"P",{});var be=c(D);dl=r(be,"The output itself is a dictionary containing two keys, "),xs=p(be,"CODE",{});var Wr=c(xs);bl=r(Wr,"input_ids"),Wr.forEach(t),_l=r(be," and "),As=p(be,"CODE",{});var Br=c(As);wl=r(Br,"attention_mask"),Br.forEach(t),$l=r(be,". "),Ps=p(be,"CODE",{});var Ur=c(Ps);kl=r(Ur,"input_ids"),Ur.forEach(t),vl=r(be," contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. We\u2019ll explain what the "),Is=p(be,"CODE",{});var Rr=c(Is);yl=r(Rr,"attention_mask"),Rr.forEach(t),gl=r(be," is later in this chapter."),be.forEach(t),Dn=m(e),$e=p(e,"H2",{class:!0});var yo=c($e);Se=p(yo,"A",{id:!0,class:!0,href:!0});var Yr=c(Se);Ss=p(Yr,"SPAN",{});var Qr=c(Ss);T(Xe.$$.fragment,Qr),Qr.forEach(t),Yr.forEach(t),jl=m(yo),Ms=p(yo,"SPAN",{});var Jr=c(Ms);El=r(Jr,"Going through the model"),Jr.forEach(t),yo.forEach(t),zn=m(e),X.l(e),It=m(e),St=p(e,"P",{});var Xr=c(St);Tl=r(Xr,"In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it."),Xr.forEach(t),Ln=m(e),G=p(e,"P",{});var Ve=c(G);xl=r(Ve,"This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call "),Cs=p(Ve,"EM",{});var Kr=c(Cs);Al=r(Kr,"hidden states"),Kr.forEach(t),Pl=r(Ve,", also known as "),qs=p(Ve,"EM",{});var Zr=c(qs);Il=r(Zr,"features"),Zr.forEach(t),Sl=r(Ve,". For each model input, we\u2019ll retrieve a high-dimensional vector representing the "),Fs=p(Ve,"STRONG",{});var ei=c(Fs);Ml=r(ei,"contextual understanding of that input by the Transformer model"),ei.forEach(t),Cl=r(Ve,"."),Ve.forEach(t),Hn=m(e),Mt=p(e,"P",{});var ti=c(Mt);ql=r(ti,"If this doesn\u2019t make sense, don\u2019t worry about it. We\u2019ll explain it all later."),ti.forEach(t),Gn=m(e),fe=p(e,"P",{});var us=c(fe);Fl=r(us,"While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the "),Ns=p(us,"EM",{});var si=c(Ns);Nl=r(si,"head"),si.forEach(t),Ol=r(us,". In "),Ct=p(us,"A",{href:!0});var ni=c(Ct);Dl=r(ni,"Chapter 1"),ni.forEach(t),zl=r(us,", the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."),us.forEach(t),Vn=m(e),ke=p(e,"H3",{class:!0});var go=c(ke);Me=p(go,"A",{id:!0,class:!0,href:!0});var oi=c(Me);Os=p(oi,"SPAN",{});var li=c(Os);T(Ke.$$.fragment,li),li.forEach(t),oi.forEach(t),Ll=m(go),Ds=p(go,"SPAN",{});var ai=c(Ds);Hl=r(ai,"A high-dimensional vector?"),ai.forEach(t),go.forEach(t),Wn=m(e),qt=p(e,"P",{});var ri=c(qt);Gl=r(ri,"The vector output by the Transformer module is usually large. It generally has three dimensions:"),ri.forEach(t),Bn=m(e),me=p(e,"UL",{});var hs=c(me);Ft=p(hs,"LI",{});var sr=c(Ft);zs=p(sr,"STRONG",{});var ii=c(zs);Vl=r(ii,"Batch size"),ii.forEach(t),Wl=r(sr,": The number of sequences processed at a time (2 in our example)."),sr.forEach(t),Bl=m(hs),Nt=p(hs,"LI",{});var nr=c(Nt);Ls=p(nr,"STRONG",{});var pi=c(Ls);Ul=r(pi,"Sequence length"),pi.forEach(t),Rl=r(nr,": The length of the numerical representation of the sequence (16 in our example)."),nr.forEach(t),Yl=m(hs),Ot=p(hs,"LI",{});var or=c(Ot);Hs=p(or,"STRONG",{});var ci=c(Hs);Ql=r(ci,"Hidden size"),ci.forEach(t),Jl=r(or,": The vector dimension of each model input."),or.forEach(t),hs.forEach(t),Un=m(e),Dt=p(e,"P",{});var ui=c(Dt);Xl=r(ui,"It is said to be \u201Chigh dimensional\u201D because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."),ui.forEach(t),Rn=m(e),zt=p(e,"P",{});var hi=c(zt);Kl=r(hi,"We can see this if we feed the inputs we preprocessed to our model:"),hi.forEach(t),Yn=m(e),Z.l(e),Lt=m(e),V=p(e,"P",{});var We=c(V);Zl=r(We,"Note that the outputs of \u{1F917} Transformers models behave like "),Gs=p(We,"CODE",{});var fi=c(Gs);ea=r(fi,"namedtuple"),fi.forEach(t),ta=r(We,"s or dictionaries. You can access the elements by attributes (like we did) or by key ("),Vs=p(We,"CODE",{});var mi=c(Vs);sa=r(mi,'outputs["last_hidden_state"]'),mi.forEach(t),na=r(We,"), or even by index if you know exactly where the thing you are looking for is ("),Ws=p(We,"CODE",{});var di=c(Ws);oa=r(di,"outputs[0]"),di.forEach(t),la=r(We,")."),We.forEach(t),Qn=m(e),ve=p(e,"H3",{class:!0});var jo=c(ve);Ce=p(jo,"A",{id:!0,class:!0,href:!0});var bi=c(Ce);Bs=p(bi,"SPAN",{});var _i=c(Bs);T(Ze.$$.fragment,_i),_i.forEach(t),bi.forEach(t),aa=m(jo),Us=p(jo,"SPAN",{});var wi=c(Us);ra=r(wi,"Model heads: Making sense out of numbers"),wi.forEach(t),jo.forEach(t),Jn=m(e),Ht=p(e,"P",{});var $i=c(Ht);ia=r($i,"The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:"),$i.forEach(t),Xn=m(e),et=p(e,"DIV",{class:!0});var ki=c(et);tt=p(ki,"IMG",{src:!0,alt:!0,width:!0}),ki.forEach(t),Kn=m(e),Gt=p(e,"P",{});var vi=c(Gt);pa=r(vi,"The output of the Transformer model is sent directly to the model head to be processed."),vi.forEach(t),Zn=m(e),Vt=p(e,"P",{});var yi=c(Vt);ca=r(yi,"In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."),yi.forEach(t),eo=m(e),Wt=p(e,"P",{});var gi=c(Wt);ua=r(gi,"There are many different architectures available in \u{1F917} Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:"),gi.forEach(t),to=m(e),F=p(e,"UL",{});var O=c(F);Bt=p(O,"LI",{});var lr=c(Bt);Rs=p(lr,"CODE",{});var ji=c(Rs);ha=r(ji,"*Model"),ji.forEach(t),fa=r(lr," (retrieve the hidden states)"),lr.forEach(t),ma=m(O),Ys=p(O,"LI",{});var Ei=c(Ys);Qs=p(Ei,"CODE",{});var Ti=c(Qs);da=r(Ti,"*ForCausalLM"),Ti.forEach(t),Ei.forEach(t),ba=m(O),Js=p(O,"LI",{});var xi=c(Js);Xs=p(xi,"CODE",{});var Ai=c(Xs);_a=r(Ai,"*ForMaskedLM"),Ai.forEach(t),xi.forEach(t),wa=m(O),Ks=p(O,"LI",{});var Pi=c(Ks);Zs=p(Pi,"CODE",{});var Ii=c(Zs);$a=r(Ii,"*ForMultipleChoice"),Ii.forEach(t),Pi.forEach(t),ka=m(O),en=p(O,"LI",{});var Si=c(en);tn=p(Si,"CODE",{});var Mi=c(tn);va=r(Mi,"*ForQuestionAnswering"),Mi.forEach(t),Si.forEach(t),ya=m(O),sn=p(O,"LI",{});var Ci=c(sn);nn=p(Ci,"CODE",{});var qi=c(nn);ga=r(qi,"*ForSequenceClassification"),qi.forEach(t),Ci.forEach(t),ja=m(O),on=p(O,"LI",{});var Fi=c(on);ln=p(Fi,"CODE",{});var Ni=c(ln);Ea=r(Ni,"*ForTokenClassification"),Ni.forEach(t),Fi.forEach(t),Ta=m(O),an=p(O,"LI",{});var Oi=c(an);xa=r(Oi,"and others \u{1F917}"),Oi.forEach(t),O.forEach(t),so=m(e),te.l(e),Ut=m(e),Rt=p(e,"P",{});var Di=c(Rt);Aa=r(Di,"Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"),Di.forEach(t),no=m(e),T(st.$$.fragment,e),oo=m(e),ne.l(e),Yt=m(e),Qt=p(e,"P",{});var zi=c(Qt);Pa=r(zi,"Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."),zi.forEach(t),lo=m(e),ye=p(e,"H2",{class:!0});var Eo=c(ye);qe=p(Eo,"A",{id:!0,class:!0,href:!0});var Li=c(qe);rn=p(Li,"SPAN",{});var Hi=c(rn);T(nt.$$.fragment,Hi),Hi.forEach(t),Li.forEach(t),Ia=m(Eo),pn=p(Eo,"SPAN",{});var Gi=c(pn);Sa=r(Gi,"Postprocessing the output"),Gi.forEach(t),Eo.forEach(t),ao=m(e),Jt=p(e,"P",{});var Vi=c(Jt);Ma=r(Vi,"The values we get as output from our model don\u2019t necessarily make sense by themselves. Let\u2019s take a look:"),Vi.forEach(t),ro=m(e),T(ot.$$.fragment,e),io=m(e),le.l(e),Xt=m(e),z=p(e,"P",{});var _e=c(z);Ca=r(_e,"Our model predicted "),cn=p(_e,"CODE",{});var Wi=c(cn);qa=r(Wi,"[-1.5607, 1.6123]"),Wi.forEach(t),Fa=r(_e," for the first sentence and "),un=p(_e,"CODE",{});var Bi=c(un);Na=r(Bi,"[ 4.1692, -3.3464]"),Bi.forEach(t),Oa=r(_e," for the second one. Those are not probabilities but "),hn=p(_e,"EM",{});var Ui=c(hn);Da=r(Ui,"logits"),Ui.forEach(t),za=r(_e,", the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a "),lt=p(_e,"A",{href:!0,rel:!0});var Ri=c(lt);La=r(Ri,"SoftMax"),Ri.forEach(t),Ha=r(_e," layer (all \u{1F917} Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"),_e.forEach(t),po=m(e),re.l(e),Kt=m(e),pe.l(e),Zt=m(e),de=p(e,"P",{});var fs=c(de);Ga=r(fs,"Now we can see that the model predicted "),fn=p(fs,"CODE",{});var Yi=c(fn);Va=r(Yi,"[0.0402, 0.9598]"),Yi.forEach(t),Wa=r(fs," for the first sentence and "),mn=p(fs,"CODE",{});var Qi=c(mn);Ba=r(Qi,"[0.9995, 0.0005]"),Qi.forEach(t),Ua=r(fs," for the second one. These are recognizable probability scores."),fs.forEach(t),co=m(e),Fe=p(e,"P",{});var To=c(Fe);Ra=r(To,"To get the labels corresponding to each position, we can inspect the "),dn=p(To,"CODE",{});var Ji=c(dn);Ya=r(Ji,"id2label"),Ji.forEach(t),Qa=r(To," attribute of the model config (more on this in the next section):"),To.forEach(t),uo=m(e),T(at.$$.fragment,e),ho=m(e),T(rt.$$.fragment,e),fo=m(e),es=p(e,"P",{});var Xi=c(es);Ja=r(Xi,"Now we can conclude that the model predicted the following:"),Xi.forEach(t),mo=m(e),Ne=p(e,"UL",{});var xo=c(Ne);bn=p(xo,"LI",{});var Ki=c(bn);Xa=r(Ki,"First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598"),Ki.forEach(t),Ka=m(xo),_n=p(xo,"LI",{});var Zi=c(_n);Za=r(Zi,"Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"),Zi.forEach(t),xo.forEach(t),bo=m(e),ts=p(e,"P",{});var ep=c(ts);er=r(ep,"We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let\u2019s take some time to dive deeper into each of those steps."),ep.forEach(t),_o=m(e),T(Oe.$$.fragment,e),this.h()},h(){j(n,"name","hf:doc:metadata"),j(n,"content",JSON.stringify(Dp)),j(b,"id","behind-the-pipeline"),j(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(b,"href","#behind-the-pipeline"),j(_,"class","relative group"),j(vt,"href","/course/chapter1"),j(gt,"href","/course/chapter1"),tp(Te.src,ar="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.png")||j(Te,"src",ar),j(Te,"alt","The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."),j(Te,"width","100%"),j(xe,"id","preprocessing-with-a-tokenizer"),j(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(xe,"href","#preprocessing-with-a-tokenizer"),j(we,"class","relative group"),j(Ye,"href","https://huggingface.co/models"),j(Ye,"rel","nofollow"),j(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),j(Qe,"rel","nofollow"),j(Se,"id","going-through-the-model"),j(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Se,"href","#going-through-the-model"),j($e,"class","relative group"),j(Ct,"href","/course/chapter1"),j(Me,"id","a-highdimensional-vector"),j(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Me,"href","#a-highdimensional-vector"),j(ke,"class","relative group"),j(Ce,"id","model-heads-making-sense-out-of-numbers"),j(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Ce,"href","#model-heads-making-sense-out-of-numbers"),j(ve,"class","relative group"),tp(tt.src,rr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.png")||j(tt,"src",rr),j(tt,"alt","A Transformer network alongside its head."),j(tt,"width","80%"),j(et,"class","flex justify-center"),j(qe,"id","postprocessing-the-output"),j(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(qe,"href","#postprocessing-the-output"),j(ye,"class","relative group"),j(lt,"href","https://en.wikipedia.org/wiki/Softmax_function"),j(lt,"rel","nofollow")},m(e,l){s(np.head,n),u(e,h,l),x(o,e,l),u(e,d,l),u(e,_,l),s(_,b),s(b,P),x(S,P,null),s(_,I),s(_,g),s(g,C),u(e,w,l),it[v].m(e,l),u(e,N,l),x(ge,e,l),u(e,kn,l),pt[W].m(e,l),u(e,kt,l),u(e,je,l),s(je,Ao),s(je,vt),s(vt,Po),s(je,Io),u(e,vn,l),x(Be,e,l),u(e,yn,l),u(e,yt,l),s(yt,So),u(e,gn,l),x(Ue,e,l),u(e,jn,l),u(e,Ee,l),s(Ee,Mo),s(Ee,gt),s(gt,Co),s(Ee,qo),u(e,En,l),u(e,Te,l),u(e,Tn,l),u(e,jt,l),s(jt,Fo),u(e,xn,l),u(e,we,l),s(we,xe),s(xe,ds),x(Re,ds,null),s(we,No),s(we,bs),s(bs,Oo),u(e,An,l),u(e,Ae,l),s(Ae,Do),s(Ae,_s),s(_s,zo),s(Ae,Lo),u(e,Pn,l),u(e,he,l),s(he,Et),s(Et,Ho),s(Et,ws),s(ws,Go),s(he,Vo),s(he,$s),s($s,Wo),s(he,Bo),s(he,ks),s(ks,Uo),u(e,In,l),u(e,L,l),s(L,Ro),s(L,Ye),s(Ye,Yo),s(L,Qo),s(L,vs),s(vs,Jo),s(L,Xo),s(L,ys),s(ys,Ko),s(L,Zo),u(e,Sn,l),u(e,H,l),s(H,el),s(H,gs),s(gs,tl),s(H,sl),s(H,js),s(js,nl),s(H,ol),s(H,Qe),s(Qe,ll),s(H,al),u(e,Mn,l),x(Je,e,l),u(e,Cn,l),u(e,Tt,l),s(Tt,rl),u(e,qn,l),u(e,Pe,l),s(Pe,il),s(Pe,Es),s(Es,pl),s(Pe,cl),u(e,Fn,l),u(e,Ie,l),s(Ie,ul),s(Ie,Ts),s(Ts,hl),s(Ie,fl),u(e,Nn,l),ct[U].m(e,l),u(e,xt,l),u(e,At,l),s(At,ml),u(e,On,l),ut[Y].m(e,l),u(e,Pt,l),u(e,D,l),s(D,dl),s(D,xs),s(xs,bl),s(D,_l),s(D,As),s(As,wl),s(D,$l),s(D,Ps),s(Ps,kl),s(D,vl),s(D,Is),s(Is,yl),s(D,gl),u(e,Dn,l),u(e,$e,l),s($e,Se),s(Se,Ss),x(Xe,Ss,null),s($e,jl),s($e,Ms),s(Ms,El),u(e,zn,l),ht[J].m(e,l),u(e,It,l),u(e,St,l),s(St,Tl),u(e,Ln,l),u(e,G,l),s(G,xl),s(G,Cs),s(Cs,Al),s(G,Pl),s(G,qs),s(qs,Il),s(G,Sl),s(G,Fs),s(Fs,Ml),s(G,Cl),u(e,Hn,l),u(e,Mt,l),s(Mt,ql),u(e,Gn,l),u(e,fe,l),s(fe,Fl),s(fe,Ns),s(Ns,Nl),s(fe,Ol),s(fe,Ct),s(Ct,Dl),s(fe,zl),u(e,Vn,l),u(e,ke,l),s(ke,Me),s(Me,Os),x(Ke,Os,null),s(ke,Ll),s(ke,Ds),s(Ds,Hl),u(e,Wn,l),u(e,qt,l),s(qt,Gl),u(e,Bn,l),u(e,me,l),s(me,Ft),s(Ft,zs),s(zs,Vl),s(Ft,Wl),s(me,Bl),s(me,Nt),s(Nt,Ls),s(Ls,Ul),s(Nt,Rl),s(me,Yl),s(me,Ot),s(Ot,Hs),s(Hs,Ql),s(Ot,Jl),u(e,Un,l),u(e,Dt,l),s(Dt,Xl),u(e,Rn,l),u(e,zt,l),s(zt,Kl),u(e,Yn,l),ft[K].m(e,l),u(e,Lt,l),u(e,V,l),s(V,Zl),s(V,Gs),s(Gs,ea),s(V,ta),s(V,Vs),s(Vs,sa),s(V,na),s(V,Ws),s(Ws,oa),s(V,la),u(e,Qn,l),u(e,ve,l),s(ve,Ce),s(Ce,Bs),x(Ze,Bs,null),s(ve,aa),s(ve,Us),s(Us,ra),u(e,Jn,l),u(e,Ht,l),s(Ht,ia),u(e,Xn,l),u(e,et,l),s(et,tt),u(e,Kn,l),u(e,Gt,l),s(Gt,pa),u(e,Zn,l),u(e,Vt,l),s(Vt,ca),u(e,eo,l),u(e,Wt,l),s(Wt,ua),u(e,to,l),u(e,F,l),s(F,Bt),s(Bt,Rs),s(Rs,ha),s(Bt,fa),s(F,ma),s(F,Ys),s(Ys,Qs),s(Qs,da),s(F,ba),s(F,Js),s(Js,Xs),s(Xs,_a),s(F,wa),s(F,Ks),s(Ks,Zs),s(Zs,$a),s(F,ka),s(F,en),s(en,tn),s(tn,va),s(F,ya),s(F,sn),s(sn,nn),s(nn,ga),s(F,ja),s(F,on),s(on,ln),s(ln,Ea),s(F,Ta),s(F,an),s(an,xa),u(e,so,l),mt[ee].m(e,l),u(e,Ut,l),u(e,Rt,l),s(Rt,Aa),u(e,no,l),x(st,e,l),u(e,oo,l),dt[se].m(e,l),u(e,Yt,l),u(e,Qt,l),s(Qt,Pa),u(e,lo,l),u(e,ye,l),s(ye,qe),s(qe,rn),x(nt,rn,null),s(ye,Ia),s(ye,pn),s(pn,Sa),u(e,ao,l),u(e,Jt,l),s(Jt,Ma),u(e,ro,l),x(ot,e,l),u(e,io,l),bt[oe].m(e,l),u(e,Xt,l),u(e,z,l),s(z,Ca),s(z,cn),s(cn,qa),s(z,Fa),s(z,un),s(un,Na),s(z,Oa),s(z,hn),s(hn,Da),s(z,za),s(z,lt),s(lt,La),s(z,Ha),u(e,po,l),_t[ae].m(e,l),u(e,Kt,l),wt[ie].m(e,l),u(e,Zt,l),u(e,de,l),s(de,Ga),s(de,fn),s(fn,Va),s(de,Wa),s(de,mn),s(mn,Ba),s(de,Ua),u(e,co,l),u(e,Fe,l),s(Fe,Ra),s(Fe,dn),s(dn,Ya),s(Fe,Qa),u(e,uo,l),x(at,e,l),u(e,ho,l),x(rt,e,l),u(e,fo,l),u(e,es,l),s(es,Ja),u(e,mo,l),u(e,Ne,l),s(Ne,bn),s(bn,Xa),s(Ne,Ka),s(Ne,_n),s(_n,Za),u(e,bo,l),u(e,ts,l),s(ts,er),u(e,_o,l),x(Oe,e,l),wo=!0},p(e,[l]){const $t={};l&1&&($t.fw=e[0]),o.$set($t);let ss=v;v=pr(e),v!==ss&&(ue(),$(it[ss],1,1,()=>{it[ss]=null}),ce(),q=it[v],q||(q=it[v]=ir[v](e),q.c()),k(q,1),q.m(N.parentNode,N));const wn={};l&2&&(wn.$$scope={dirty:l,ctx:e}),ge.$set(wn);let ns=W;W=ur(e),W!==ns&&(ue(),$(pt[ns],1,1,()=>{pt[ns]=null}),ce(),B=pt[W],B||(B=pt[W]=cr[W](e),B.c()),k(B,1),B.m(kt.parentNode,kt));let De=U;U=fr(e),U!==De&&(ue(),$(ct[De],1,1,()=>{ct[De]=null}),ce(),R=ct[U],R||(R=ct[U]=hr[U](e),R.c()),k(R,1),R.m(xt.parentNode,xt));let os=Y;Y=dr(e),Y!==os&&(ue(),$(ut[os],1,1,()=>{ut[os]=null}),ce(),Q=ut[Y],Q||(Q=ut[Y]=mr[Y](e),Q.c()),k(Q,1),Q.m(Pt.parentNode,Pt));let ls=J;J=_r(e),J!==ls&&(ue(),$(ht[ls],1,1,()=>{ht[ls]=null}),ce(),X=ht[J],X||(X=ht[J]=br[J](e),X.c()),k(X,1),X.m(It.parentNode,It));let ze=K;K=$r(e),K!==ze&&(ue(),$(ft[ze],1,1,()=>{ft[ze]=null}),ce(),Z=ft[K],Z||(Z=ft[K]=wr[K](e),Z.c()),k(Z,1),Z.m(Lt.parentNode,Lt));let as=ee;ee=vr(e),ee!==as&&(ue(),$(mt[as],1,1,()=>{mt[as]=null}),ce(),te=mt[ee],te||(te=mt[ee]=kr[ee](e),te.c()),k(te,1),te.m(Ut.parentNode,Ut));let rs=se;se=gr(e),se!==rs&&(ue(),$(dt[rs],1,1,()=>{dt[rs]=null}),ce(),ne=dt[se],ne||(ne=dt[se]=yr[se](e),ne.c()),k(ne,1),ne.m(Yt.parentNode,Yt));let Le=oe;oe=Er(e),oe!==Le&&(ue(),$(bt[Le],1,1,()=>{bt[Le]=null}),ce(),le=bt[oe],le||(le=bt[oe]=jr[oe](e),le.c()),k(le,1),le.m(Xt.parentNode,Xt));let is=ae;ae=xr(e),ae!==is&&(ue(),$(_t[is],1,1,()=>{_t[is]=null}),ce(),re=_t[ae],re||(re=_t[ae]=Tr[ae](e),re.c()),k(re,1),re.m(Kt.parentNode,Kt));let ps=ie;ie=Pr(e),ie!==ps&&(ue(),$(wt[ps],1,1,()=>{wt[ps]=null}),ce(),pe=wt[ie],pe||(pe=wt[ie]=Ar[ie](e),pe.c()),k(pe,1),pe.m(Zt.parentNode,Zt));const $n={};l&2&&($n.$$scope={dirty:l,ctx:e}),Oe.$set($n)},i(e){wo||(k(o.$$.fragment,e),k(S.$$.fragment,e),k(q),k(ge.$$.fragment,e),k(B),k(Be.$$.fragment,e),k(Ue.$$.fragment,e),k(Re.$$.fragment,e),k(Je.$$.fragment,e),k(R),k(Q),k(Xe.$$.fragment,e),k(X),k(Ke.$$.fragment,e),k(Z),k(Ze.$$.fragment,e),k(te),k(st.$$.fragment,e),k(ne),k(nt.$$.fragment,e),k(ot.$$.fragment,e),k(le),k(re),k(pe),k(at.$$.fragment,e),k(rt.$$.fragment,e),k(Oe.$$.fragment,e),wo=!0)},o(e){$(o.$$.fragment,e),$(S.$$.fragment,e),$(q),$(ge.$$.fragment,e),$(B),$(Be.$$.fragment,e),$(Ue.$$.fragment,e),$(Re.$$.fragment,e),$(Je.$$.fragment,e),$(R),$(Q),$(Xe.$$.fragment,e),$(X),$(Ke.$$.fragment,e),$(Z),$(Ze.$$.fragment,e),$(te),$(st.$$.fragment,e),$(ne),$(nt.$$.fragment,e),$(ot.$$.fragment,e),$(le),$(re),$(pe),$(at.$$.fragment,e),$(rt.$$.fragment,e),$(Oe.$$.fragment,e),wo=!1},d(e){t(n),e&&t(h),A(o,e),e&&t(d),e&&t(_),A(S),e&&t(w),it[v].d(e),e&&t(N),A(ge,e),e&&t(kn),pt[W].d(e),e&&t(kt),e&&t(je),e&&t(vn),A(Be,e),e&&t(yn),e&&t(yt),e&&t(gn),A(Ue,e),e&&t(jn),e&&t(Ee),e&&t(En),e&&t(Te),e&&t(Tn),e&&t(jt),e&&t(xn),e&&t(we),A(Re),e&&t(An),e&&t(Ae),e&&t(Pn),e&&t(he),e&&t(In),e&&t(L),e&&t(Sn),e&&t(H),e&&t(Mn),A(Je,e),e&&t(Cn),e&&t(Tt),e&&t(qn),e&&t(Pe),e&&t(Fn),e&&t(Ie),e&&t(Nn),ct[U].d(e),e&&t(xt),e&&t(At),e&&t(On),ut[Y].d(e),e&&t(Pt),e&&t(D),e&&t(Dn),e&&t($e),A(Xe),e&&t(zn),ht[J].d(e),e&&t(It),e&&t(St),e&&t(Ln),e&&t(G),e&&t(Hn),e&&t(Mt),e&&t(Gn),e&&t(fe),e&&t(Vn),e&&t(ke),A(Ke),e&&t(Wn),e&&t(qt),e&&t(Bn),e&&t(me),e&&t(Un),e&&t(Dt),e&&t(Rn),e&&t(zt),e&&t(Yn),ft[K].d(e),e&&t(Lt),e&&t(V),e&&t(Qn),e&&t(ve),A(Ze),e&&t(Jn),e&&t(Ht),e&&t(Xn),e&&t(et),e&&t(Kn),e&&t(Gt),e&&t(Zn),e&&t(Vt),e&&t(eo),e&&t(Wt),e&&t(to),e&&t(F),e&&t(so),mt[ee].d(e),e&&t(Ut),e&&t(Rt),e&&t(no),A(st,e),e&&t(oo),dt[se].d(e),e&&t(Yt),e&&t(Qt),e&&t(lo),e&&t(ye),A(nt),e&&t(ao),e&&t(Jt),e&&t(ro),A(ot,e),e&&t(io),bt[oe].d(e),e&&t(Xt),e&&t(z),e&&t(po),_t[ae].d(e),e&&t(Kt),wt[ie].d(e),e&&t(Zt),e&&t(de),e&&t(co),e&&t(Fe),e&&t(uo),A(at,e),e&&t(ho),A(rt,e),e&&t(fo),e&&t(es),e&&t(mo),e&&t(Ne),e&&t(bo),e&&t(ts),e&&t(_o),A(Oe,e)}}}const Dp={local:"behind-the-pipeline",sections:[{local:"preprocessing-with-a-tokenizer",title:"Preprocessing with a tokenizer"},{local:"going-through-the-model",sections:[{local:"a-highdimensional-vector",title:"A high-dimensional vector?"},{local:"model-heads-making-sense-out-of-numbers",title:"Model heads: Making sense out of numbers"}],title:"Going through the model"},{local:"postprocessing-the-output",title:"Postprocessing the output"}],title:"Behind the pipeline"};function zp(y,n,h){let o="pt";return cp(()=>{const d=new URL(document.location).searchParams;h(0,o=d.get("fw")||"pt")}),[o]}class Rp extends ap{constructor(n){super();rp(this,n,zp,Op,ip,{})}}export{Rp as default,Dp as metadata};
