import{S as en,i as tn,s as an,e as n,k as p,w as A,t as o,l as Qr,M as on,c as i,d as t,m as u,x as D,a as l,h as s,b as S,F as e,g as h,y as O,o as j,p as hs,q as C,B as F,v as sn,n as ds}from"../../chunks/vendor-1e8b365d.js";import{T as Ao}from"../../chunks/Tip-62b14c6e.js";import{Y as Ho}from"../../chunks/Youtube-c2a8cc39.js";import{I as ro}from"../../chunks/IconCopyLink-483c28ba.js";import{C as X}from"../../chunks/CodeBlock-e5764662.js";import{D as Zr}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as rn}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function nn(G){let c,_;return c=new Zr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"}]}}),{c(){A(c.$$.fragment)},l(d){D(c.$$.fragment,d)},m(d,y){O(c,d,y),_=!0},i(d){_||(C(c.$$.fragment,d),_=!0)},o(d){j(c.$$.fragment,d),_=!1},d(d){F(c,d)}}}function ln(G){let c,_;return c=new Zr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"}]}}),{c(){A(c.$$.fragment)},l(d){D(c.$$.fragment,d)},m(d,y){O(c,d,y),_=!0},i(d){_||(C(c.$$.fragment,d),_=!0)},o(d){j(c.$$.fragment,d),_=!1},d(d){F(c,d)}}}function cn(G){let c,_,d,y,b,m,k,E;return y=new ro({}),{c(){c=n("h2"),_=n("a"),d=n("span"),A(y.$$.fragment),b=p(),m=n("span"),k=o("with Keras"),this.h()},l(v){c=i(v,"H2",{class:!0});var $=l(c);_=i($,"A",{id:!0,class:!0,href:!0});var P=l(_);d=i(P,"SPAN",{});var T=l(d);D(y.$$.fragment,T),T.forEach(t),P.forEach(t),b=u($),m=i($,"SPAN",{});var q=l(m);k=s(q,"with Keras"),q.forEach(t),$.forEach(t),this.h()},h(){S(_,"id","with-keras"),S(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(_,"href","#with-keras"),S(c,"class","relative group")},m(v,$){h(v,c,$),e(c,_),e(_,d),O(y,d,null),e(c,b),e(c,m),e(m,k),E=!0},i(v){E||(C(y.$$.fragment,v),E=!0)},o(v){j(y.$$.fragment,v),E=!1},d(v){v&&t(c),F(y)}}}function hn(G){let c,_,d,y,b,m,k,E;return y=new ro({}),{c(){c=n("h2"),_=n("a"),d=n("span"),A(y.$$.fragment),b=p(),m=n("span"),k=o("with the Trainer API"),this.h()},l(v){c=i(v,"H2",{class:!0});var $=l(c);_=i($,"A",{id:!0,class:!0,href:!0});var P=l(_);d=i(P,"SPAN",{});var T=l(d);D(y.$$.fragment,T),T.forEach(t),P.forEach(t),b=u($),m=i($,"SPAN",{});var q=l(m);k=s(q,"with the Trainer API"),q.forEach(t),$.forEach(t),this.h()},h(){S(_,"id","with-the-trainer-api"),S(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(_,"href","#with-the-trainer-api"),S(c,"class","relative group")},m(v,$){h(v,c,$),e(c,_),e(_,d),O(y,d,null),e(c,b),e(c,m),e(m,k),E=!0},i(v){E||(C(y.$$.fragment,v),E=!0)},o(v){j(y.$$.fragment,v),E=!1},d(v){v&&t(c),F(y)}}}function dn(G){let c,_,d,y,b,m,k,E,v,$,P,T,q,N;return q=new X({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){c=n("p"),_=o("Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to train the model. Note, however, that the "),d=n("code"),y=o("model.fit()"),b=o(" command will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on "),m=n("a"),k=o("Google Colab"),E=o("."),v=p(),$=n("p"),P=o("The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:"),T=p(),A(q.$$.fragment),this.h()},l(z){c=i(z,"P",{});var I=l(c);_=s(I,"Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to train the model. Note, however, that the "),d=i(I,"CODE",{});var W=l(d);y=s(W,"model.fit()"),W.forEach(t),b=s(I," command will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on "),m=i(I,"A",{href:!0,rel:!0});var Y=l(m);k=s(Y,"Google Colab"),Y.forEach(t),E=s(I,"."),I.forEach(t),v=u(z),$=i(z,"P",{});var K=l($);P=s(K,"The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:"),K.forEach(t),T=u(z),D(q.$$.fragment,z),this.h()},h(){S(m,"href","https://colab.research.google.com/"),S(m,"rel","nofollow")},m(z,I){h(z,c,I),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E),h(z,v,I),h(z,$,I),e($,P),h(z,T,I),O(q,z,I),N=!0},i(z){N||(C(q.$$.fragment,z),N=!0)},o(z){j(q.$$.fragment,z),N=!1},d(z){z&&t(c),z&&t(v),z&&t($),z&&t(T),F(q,z)}}}function pn(G){let c,_,d,y,b,m,k,E,v,$,P,T,q,N,z,I,W,Y,K,L,Q,H;return c=new Ho({props:{id:"nvBXf7s7vTI"}}),Q=new X({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){A(c.$$.fragment),_=p(),d=n("p"),y=o("\u{1F917} Transformers provides a "),b=n("code"),m=o("Trainer"),k=o(" class to help you fine-tune any of the pretrained models it provides on your dataset. Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to define the "),E=n("code"),v=o("Trainer"),$=o(". The hardest part is likely to be preparing the environment to run "),P=n("code"),T=o("Trainer.train()"),q=o(", as it will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on "),N=n("a"),z=o("Google Colab"),I=o("."),W=p(),Y=n("p"),K=o("The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:"),L=p(),A(Q.$$.fragment),this.h()},l(x){D(c.$$.fragment,x),_=u(x),d=i(x,"P",{});var M=l(d);y=s(M,"\u{1F917} Transformers provides a "),b=i(M,"CODE",{});var re=l(b);m=s(re,"Trainer"),re.forEach(t),k=s(M," class to help you fine-tune any of the pretrained models it provides on your dataset. Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to define the "),E=i(M,"CODE",{});var de=l(E);v=s(de,"Trainer"),de.forEach(t),$=s(M,". The hardest part is likely to be preparing the environment to run "),P=i(M,"CODE",{});var pe=l(P);T=s(pe,"Trainer.train()"),pe.forEach(t),q=s(M,", as it will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on "),N=i(M,"A",{href:!0,rel:!0});var R=l(N);z=s(R,"Google Colab"),R.forEach(t),I=s(M,"."),M.forEach(t),W=u(x),Y=i(x,"P",{});var be=l(Y);K=s(be,"The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:"),be.forEach(t),L=u(x),D(Q.$$.fragment,x),this.h()},h(){S(N,"href","https://colab.research.google.com/"),S(N,"rel","nofollow")},m(x,M){O(c,x,M),h(x,_,M),h(x,d,M),e(d,y),e(d,b),e(b,m),e(d,k),e(d,E),e(E,v),e(d,$),e(d,P),e(P,T),e(d,q),e(d,N),e(N,z),e(d,I),h(x,W,M),h(x,Y,M),e(Y,K),h(x,L,M),O(Q,x,M),H=!0},i(x){H||(C(c.$$.fragment,x),C(Q.$$.fragment,x),H=!0)},o(x){j(c.$$.fragment,x),j(Q.$$.fragment,x),H=!1},d(x){F(c,x),x&&t(_),x&&t(d),x&&t(W),x&&t(Y),x&&t(L),F(Q,x)}}}function un(G){let c,_,d,y,b,m,k,E,v,$,P,T,q,N,z,I,W,Y,K,L,Q,H,x,M,re,de,pe,R,be,J,ne,ze,ke,Z,Ee,Ie,xe,_e,ge,ot,ie,Me,ve,oe,Ae,f,U,le,De,Te,ce,ae,ye,ee,Le,jt,Aa,aa,te,Da,Ct,Pt,Oa,qt,zt,Fa,xt,At,Sa,Dt,oa,Oe,Ot,Ft,Na,sa,We,ra,st,St,Fe,na,rt,nt,Nt,je,he,ia,Ue,la,it,_t,Ge,Re,lt,ct,Ia,ht,Ma,ca,Ke,ha,Se,Wa,Ce,Ua,It,Mt,Ga,Wt,da,dt,Ut,ue,Be,pt,ut,Ka,gt,ft,Gt,V,pa,fe,Ha,Kt,Ht,La,Lt,Rt,Ra,Bt,Yt,Ba,ua,He,fa,mt,Jt,se,ma,Pe,wa,B,Vt,Xt,Ya,Qt,Zt,Ja,_a,Ye,Va,ga;return y=new Ho({props:{id:"rnTGBy2ax1c"}}),v=new Ho({props:{id:"AUozVp78dhk"}}),L=new X({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)`}}),xe=new Ao({props:{$$slots:{default:[mn]},$$scope:{ctx:G}}}),ge=new X({props:{code:`from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span> SparseCategoricalCrossentropy

model.<span class="hljs-built_in">compile</span>(
    optimizer=<span class="hljs-string">&quot;adam&quot;</span>,
    loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),
    metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)`}}),ie=new Ao({props:{warning:!0,$$slots:{default:[wn]},$$scope:{ctx:G}}}),f=new ro({}),ce=new Ho({props:{id:"cpzq6ESSM5c"}}),We=new X({props:{code:`from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers.schedules <span class="hljs-keyword">import</span> PolynomialDecay

batch_size = <span class="hljs-number">8</span>
num_epochs = <span class="hljs-number">3</span>
<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=<span class="hljs-number">5e-5</span>, end_learning_rate=<span class="hljs-number">0.0</span>, decay_steps=num_train_steps
)
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

opt = Adam(learning_rate=lr_scheduler)`}}),st=new Ao({props:{$$slots:{default:[_n]},$$scope:{ctx:G}}}),nt=new X({props:{code:`import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>)
model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])`}}),Ue=new X({props:{code:"model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)",highlighted:'model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=<span class="hljs-number">3</span>)'}}),it=new Ao({props:{$$slots:{default:[gn]},$$scope:{ctx:G}}}),ct=new ro({}),Ke=new Ho({props:{id:"nx10eh4CoOs"}}),dt=new X({props:{code:'preds = model.predict(tf_validation_dataset)["logits"]',highlighted:'preds = model.predict(tf_validation_dataset)[<span class="hljs-string">&quot;logits&quot;</span>]'}}),ft=new X({props:{code:`class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)`,highlighted:`class_preds = np.argmax(preds, axis=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(preds.shape, class_preds.shape)`}}),V=new X({props:{code:"(408, 2) (408,)",highlighted:'(<span class="hljs-number">408</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">408</span>,)'}}),He=new X({props:{code:`from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
metric.compute(predictions=class_preds, references=raw_datasets[<span class="hljs-string">&quot;validation&quot;</span>][<span class="hljs-string">&quot;label&quot;</span>])`}}),mt=new X({props:{code:"{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.8578431372549019</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.8996539792387542</span>}'}}),{c(){c=n("p"),_=o("TensorFlow models imported from \u{1F917} Transformers are already Keras models. Here is a short introduction to Keras."),d=p(),A(y.$$.fragment),b=p(),m=n("p"),k=o("That means that once we have our data, very little work is required to begin training on it."),E=p(),A(v.$$.fragment),$=p(),P=n("p"),T=o("As in the "),q=n("a"),N=o("previous chapter"),z=o(", we will use the "),I=n("code"),W=o("TFAutoModelForSequenceClassification"),Y=o(" class, with two labels:"),K=p(),A(L.$$.fragment),Q=p(),H=n("p"),x=o("You will notice that unlike in "),M=n("a"),re=o("Chapter 2"),de=o(", you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been inserted instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."),pe=p(),R=n("p"),be=o("To fine-tune the model on our dataset, we just have to "),J=n("code"),ne=o("compile()"),ze=o(" our model and then pass our data to the "),ke=n("code"),Z=o("fit()"),Ee=o(" method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch."),Ie=p(),A(xe.$$.fragment),_e=p(),A(ge.$$.fragment),ot=p(),A(ie.$$.fragment),Me=p(),ve=n("h3"),oe=n("a"),Ae=n("span"),A(f.$$.fragment),U=p(),le=n("span"),De=o("Improving training performance"),Te=p(),A(ce.$$.fragment),ae=p(),ye=n("p"),ee=o(`If you try the above code, it certainly runs, but you\u2019ll find that the loss declines only slowly or sporadically. The primary cause
is the `),Le=n("em"),jt=o("learning rate"),Aa=o(`. As with the loss, when we pass Keras the name of an optimizer as a string, Keras initializes
that optimizer with default values for all parameters, including learning rate. From long experience, though, we know
that transformer models benefit from a much lower learning rate than the default for Adam, which is 1e-3, also written
as 10 to the power of -3, or 0.001. 5e-5 (0.00005), which is some twenty times lower, is a much better starting point.`),aa=p(),te=n("p"),Da=o(`In addition to lowering the learning rate, we have a second trick up our sleeve: We can slowly reduce the learning rate
over the course of training. In the literature, you will sometimes see this referred to as `),Ct=n("em"),Pt=o("decaying"),Oa=o(" or "),qt=n("em"),zt=o("annealing"),Fa=o(`
the learning rate. In Keras, the best way to do this is to use a `),xt=n("em"),At=o("learning rate scheduler"),Sa=o(`. A good one to use is
`),Dt=n("code"),oa=o("PolynomialDecay"),Oe=o(` \u2014 despite the name, with default settings it simply linearly decays the learning rate from the initial
value to the final value over the course of training, which is exactly what we want. In order to use a scheduler correctly,
though, we need to tell it how long training is going to be. We compute that as `),Ot=n("code"),Ft=o("num_train_steps"),Na=o(" below."),sa=p(),A(We.$$.fragment),ra=p(),A(st.$$.fragment),St=p(),Fe=n("p"),na=o("Now we have our all-new optimizer, and we can try training with it. First, let\u2019s reload the model, to reset the changes to the weights from the training run we just did, and then we can compile it with the new optimizer:"),rt=p(),A(nt.$$.fragment),Nt=p(),je=n("p"),he=o("Now, we fit again:"),ia=p(),A(Ue.$$.fragment),la=p(),A(it.$$.fragment),_t=p(),Ge=n("h3"),Re=n("a"),lt=n("span"),A(ct.$$.fragment),Ia=p(),ht=n("span"),Ma=o("Model predictions"),ca=p(),A(Ke.$$.fragment),ha=p(),Se=n("p"),Wa=o("Training and watching the loss go down is all very nice, but what if we want to actually get outputs from the trained model, either to compute some metrics, or to use the model in production? To do that, we can just use the "),Ce=n("code"),Ua=o("predict()"),It=o(" method. This will return the "),Mt=n("em"),Ga=o("logits"),Wt=o(" from the output head of the model, one per class."),da=p(),A(dt.$$.fragment),Ut=p(),ue=n("p"),Be=o("We can convert these logits into the model\u2019s class predictions by using "),pt=n("code"),ut=o("argmax"),Ka=o(" to find the highest logit, which corresponds to the most likely class:"),gt=p(),A(ft.$$.fragment),Gt=p(),A(V.$$.fragment),pa=p(),fe=n("p"),Ha=o("Now, let\u2019s use those "),Kt=n("code"),Ht=o("preds"),La=o(" to compute some metrics! We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the "),Lt=n("code"),Rt=o("load_metric()"),Ra=o(" function. The object returned has a "),Bt=n("code"),Yt=o("compute()"),Ba=o(" method we can use to do the metric calculation:"),ua=p(),A(He.$$.fragment),fa=p(),A(mt.$$.fragment),Jt=p(),se=n("p"),ma=o("The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the "),Pe=n("a"),wa=o("BERT paper"),B=o(" reported an F1 score of 88.9 for the base model. That was the "),Vt=n("code"),Xt=o("uncased"),Ya=o(" model while we are currently using the "),Qt=n("code"),Zt=o("cased"),Ja=o(" model, which explains the better result."),_a=p(),Ye=n("p"),Va=o("This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in Chapter 7. If you would like to hone your skills on the Keras API, try to fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2."),this.h()},l(a){c=i(a,"P",{});var g=l(c);_=s(g,"TensorFlow models imported from \u{1F917} Transformers are already Keras models. Here is a short introduction to Keras."),g.forEach(t),d=u(a),D(y.$$.fragment,a),b=u(a),m=i(a,"P",{});var no=l(m);k=s(no,"That means that once we have our data, very little work is required to begin training on it."),no.forEach(t),E=u(a),D(v.$$.fragment,a),$=u(a),P=i(a,"P",{});var Je=l(P);T=s(Je,"As in the "),q=i(Je,"A",{href:!0});var io=l(q);N=s(io,"previous chapter"),io.forEach(t),z=s(Je,", we will use the "),I=i(Je,"CODE",{});var lo=l(I);W=s(lo,"TFAutoModelForSequenceClassification"),lo.forEach(t),Y=s(Je," class, with two labels:"),Je.forEach(t),K=u(a),D(L.$$.fragment,a),Q=u(a),H=i(a,"P",{});var vt=l(H);x=s(vt,"You will notice that unlike in "),M=i(vt,"A",{href:!0});var co=l(M);re=s(co,"Chapter 2"),co.forEach(t),de=s(vt,", you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been inserted instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."),vt.forEach(t),pe=u(a),R=i(a,"P",{});var yt=l(R);be=s(yt,"To fine-tune the model on our dataset, we just have to "),J=i(yt,"CODE",{});var va=l(J);ne=s(va,"compile()"),va.forEach(t),ze=s(yt," our model and then pass our data to the "),ke=i(yt,"CODE",{});var ho=l(ke);Z=s(ho,"fit()"),ho.forEach(t),Ee=s(yt," method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch."),yt.forEach(t),Ie=u(a),D(xe.$$.fragment,a),_e=u(a),D(ge.$$.fragment,a),ot=u(a),D(ie.$$.fragment,a),Me=u(a),ve=i(a,"H3",{class:!0});var ya=l(ve);oe=i(ya,"A",{id:!0,class:!0,href:!0});var $a=l(oe);Ae=i($a,"SPAN",{});var po=l(Ae);D(f.$$.fragment,po),po.forEach(t),$a.forEach(t),U=u(ya),le=i(ya,"SPAN",{});var uo=l(le);De=s(uo,"Improving training performance"),uo.forEach(t),ya.forEach(t),Te=u(a),D(ce.$$.fragment,a),ae=u(a),ye=i(a,"P",{});var ea=l(ye);ee=s(ea,`If you try the above code, it certainly runs, but you\u2019ll find that the loss declines only slowly or sporadically. The primary cause
is the `),Le=i(ea,"EM",{});var qe=l(Le);jt=s(qe,"learning rate"),qe.forEach(t),Aa=s(ea,`. As with the loss, when we pass Keras the name of an optimizer as a string, Keras initializes
that optimizer with default values for all parameters, including learning rate. From long experience, though, we know
that transformer models benefit from a much lower learning rate than the default for Adam, which is 1e-3, also written
as 10 to the power of -3, or 0.001. 5e-5 (0.00005), which is some twenty times lower, is a much better starting point.`),ea.forEach(t),aa=u(a),te=i(a,"P",{});var $e=l(te);Da=s($e,`In addition to lowering the learning rate, we have a second trick up our sleeve: We can slowly reduce the learning rate
over the course of training. In the literature, you will sometimes see this referred to as `),Ct=i($e,"EM",{});var ba=l(Ct);Pt=s(ba,"decaying"),ba.forEach(t),Oa=s($e," or "),qt=i($e,"EM",{});var fo=l(qt);zt=s(fo,"annealing"),fo.forEach(t),Fa=s($e,`
the learning rate. In Keras, the best way to do this is to use a `),xt=i($e,"EM",{});var mo=l(xt);At=s(mo,"learning rate scheduler"),mo.forEach(t),Sa=s($e,`. A good one to use is
`),Dt=i($e,"CODE",{});var ka=l(Dt);oa=s(ka,"PolynomialDecay"),ka.forEach(t),Oe=s($e,` \u2014 despite the name, with default settings it simply linearly decays the learning rate from the initial
value to the final value over the course of training, which is exactly what we want. In order to use a scheduler correctly,
though, we need to tell it how long training is going to be. We compute that as `),Ot=i($e,"CODE",{});var wo=l(Ot);Ft=s(wo,"num_train_steps"),wo.forEach(t),Na=s($e," below."),$e.forEach(t),sa=u(a),D(We.$$.fragment,a),ra=u(a),D(st.$$.fragment,a),St=u(a),Fe=i(a,"P",{});var _o=l(Fe);na=s(_o,"Now we have our all-new optimizer, and we can try training with it. First, let\u2019s reload the model, to reset the changes to the weights from the training run we just did, and then we can compile it with the new optimizer:"),_o.forEach(t),rt=u(a),D(nt.$$.fragment,a),Nt=u(a),je=i(a,"P",{});var ta=l(je);he=s(ta,"Now, we fit again:"),ta.forEach(t),ia=u(a),D(Ue.$$.fragment,a),la=u(a),D(it.$$.fragment,a),_t=u(a),Ge=i(a,"H3",{class:!0});var Ea=l(Ge);Re=i(Ea,"A",{id:!0,class:!0,href:!0});var go=l(Re);lt=i(go,"SPAN",{});var Xa=l(lt);D(ct.$$.fragment,Xa),Xa.forEach(t),go.forEach(t),Ia=u(Ea),ht=i(Ea,"SPAN",{});var $t=l(ht);Ma=s($t,"Model predictions"),$t.forEach(t),Ea.forEach(t),ca=u(a),D(Ke.$$.fragment,a),ha=u(a),Se=i(a,"P",{});var wt=l(Se);Wa=s(wt,"Training and watching the loss go down is all very nice, but what if we want to actually get outputs from the trained model, either to compute some metrics, or to use the model in production? To do that, we can just use the "),Ce=i(wt,"CODE",{});var we=l(Ce);Ua=s(we,"predict()"),we.forEach(t),It=s(wt," method. This will return the "),Mt=i(wt,"EM",{});var vo=l(Mt);Ga=s(vo,"logits"),vo.forEach(t),Wt=s(wt," from the output head of the model, one per class."),wt.forEach(t),da=u(a),D(dt.$$.fragment,a),Ut=u(a),ue=i(a,"P",{});var bt=l(ue);Be=s(bt,"We can convert these logits into the model\u2019s class predictions by using "),pt=i(bt,"CODE",{});var yo=l(pt);ut=s(yo,"argmax"),yo.forEach(t),Ka=s(bt," to find the highest logit, which corresponds to the most likely class:"),bt.forEach(t),gt=u(a),D(ft.$$.fragment,a),Gt=u(a),D(V.$$.fragment,a),pa=u(a),fe=i(a,"P",{});var Ve=l(fe);Ha=s(Ve,"Now, let\u2019s use those "),Kt=i(Ve,"CODE",{});var Ta=l(Kt);Ht=s(Ta,"preds"),Ta.forEach(t),La=s(Ve," to compute some metrics! We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the "),Lt=i(Ve,"CODE",{});var $o=l(Lt);Rt=s($o,"load_metric()"),$o.forEach(t),Ra=s(Ve," function. The object returned has a "),Bt=i(Ve,"CODE",{});var bo=l(Bt);Yt=s(bo,"compute()"),bo.forEach(t),Ba=s(Ve," method we can use to do the metric calculation:"),Ve.forEach(t),ua=u(a),D(He.$$.fragment,a),fa=u(a),D(mt.$$.fragment,a),Jt=u(a),se=i(a,"P",{});var Ne=l(se);ma=s(Ne,"The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the "),Pe=i(Ne,"A",{href:!0,rel:!0});var ko=l(Pe);wa=s(ko,"BERT paper"),ko.forEach(t),B=s(Ne," reported an F1 score of 88.9 for the base model. That was the "),Vt=i(Ne,"CODE",{});var Eo=l(Vt);Xt=s(Eo,"uncased"),Eo.forEach(t),Ya=s(Ne," model while we are currently using the "),Qt=i(Ne,"CODE",{});var ja=l(Qt);Zt=s(ja,"cased"),ja.forEach(t),Ja=s(Ne," model, which explains the better result."),Ne.forEach(t),_a=u(a),Ye=i(a,"P",{});var To=l(Ye);Va=s(To,"This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in Chapter 7. If you would like to hone your skills on the Keras API, try to fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2."),To.forEach(t),this.h()},h(){S(q,"href","/course/chapter2"),S(M,"href","/course/chapter2"),S(oe,"id","improving-training-performance"),S(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(oe,"href","#improving-training-performance"),S(ve,"class","relative group"),S(Re,"id","model-predictions"),S(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(Re,"href","#model-predictions"),S(Ge,"class","relative group"),S(Pe,"href","https://arxiv.org/pdf/1810.04805.pdf"),S(Pe,"rel","nofollow")},m(a,g){h(a,c,g),e(c,_),h(a,d,g),O(y,a,g),h(a,b,g),h(a,m,g),e(m,k),h(a,E,g),O(v,a,g),h(a,$,g),h(a,P,g),e(P,T),e(P,q),e(q,N),e(P,z),e(P,I),e(I,W),e(P,Y),h(a,K,g),O(L,a,g),h(a,Q,g),h(a,H,g),e(H,x),e(H,M),e(M,re),e(H,de),h(a,pe,g),h(a,R,g),e(R,be),e(R,J),e(J,ne),e(R,ze),e(R,ke),e(ke,Z),e(R,Ee),h(a,Ie,g),O(xe,a,g),h(a,_e,g),O(ge,a,g),h(a,ot,g),O(ie,a,g),h(a,Me,g),h(a,ve,g),e(ve,oe),e(oe,Ae),O(f,Ae,null),e(ve,U),e(ve,le),e(le,De),h(a,Te,g),O(ce,a,g),h(a,ae,g),h(a,ye,g),e(ye,ee),e(ye,Le),e(Le,jt),e(ye,Aa),h(a,aa,g),h(a,te,g),e(te,Da),e(te,Ct),e(Ct,Pt),e(te,Oa),e(te,qt),e(qt,zt),e(te,Fa),e(te,xt),e(xt,At),e(te,Sa),e(te,Dt),e(Dt,oa),e(te,Oe),e(te,Ot),e(Ot,Ft),e(te,Na),h(a,sa,g),O(We,a,g),h(a,ra,g),O(st,a,g),h(a,St,g),h(a,Fe,g),e(Fe,na),h(a,rt,g),O(nt,a,g),h(a,Nt,g),h(a,je,g),e(je,he),h(a,ia,g),O(Ue,a,g),h(a,la,g),O(it,a,g),h(a,_t,g),h(a,Ge,g),e(Ge,Re),e(Re,lt),O(ct,lt,null),e(Ge,Ia),e(Ge,ht),e(ht,Ma),h(a,ca,g),O(Ke,a,g),h(a,ha,g),h(a,Se,g),e(Se,Wa),e(Se,Ce),e(Ce,Ua),e(Se,It),e(Se,Mt),e(Mt,Ga),e(Se,Wt),h(a,da,g),O(dt,a,g),h(a,Ut,g),h(a,ue,g),e(ue,Be),e(ue,pt),e(pt,ut),e(ue,Ka),h(a,gt,g),O(ft,a,g),h(a,Gt,g),O(V,a,g),h(a,pa,g),h(a,fe,g),e(fe,Ha),e(fe,Kt),e(Kt,Ht),e(fe,La),e(fe,Lt),e(Lt,Rt),e(fe,Ra),e(fe,Bt),e(Bt,Yt),e(fe,Ba),h(a,ua,g),O(He,a,g),h(a,fa,g),O(mt,a,g),h(a,Jt,g),h(a,se,g),e(se,ma),e(se,Pe),e(Pe,wa),e(se,B),e(se,Vt),e(Vt,Xt),e(se,Ya),e(se,Qt),e(Qt,Zt),e(se,Ja),h(a,_a,g),h(a,Ye,g),e(Ye,Va),ga=!0},i(a){ga||(C(y.$$.fragment,a),C(v.$$.fragment,a),C(L.$$.fragment,a),C(xe.$$.fragment,a),C(ge.$$.fragment,a),C(ie.$$.fragment,a),C(f.$$.fragment,a),C(ce.$$.fragment,a),C(We.$$.fragment,a),C(st.$$.fragment,a),C(nt.$$.fragment,a),C(Ue.$$.fragment,a),C(it.$$.fragment,a),C(ct.$$.fragment,a),C(Ke.$$.fragment,a),C(dt.$$.fragment,a),C(ft.$$.fragment,a),C(V.$$.fragment,a),C(He.$$.fragment,a),C(mt.$$.fragment,a),ga=!0)},o(a){j(y.$$.fragment,a),j(v.$$.fragment,a),j(L.$$.fragment,a),j(xe.$$.fragment,a),j(ge.$$.fragment,a),j(ie.$$.fragment,a),j(f.$$.fragment,a),j(ce.$$.fragment,a),j(We.$$.fragment,a),j(st.$$.fragment,a),j(nt.$$.fragment,a),j(Ue.$$.fragment,a),j(it.$$.fragment,a),j(ct.$$.fragment,a),j(Ke.$$.fragment,a),j(dt.$$.fragment,a),j(ft.$$.fragment,a),j(V.$$.fragment,a),j(He.$$.fragment,a),j(mt.$$.fragment,a),ga=!1},d(a){a&&t(c),a&&t(d),F(y,a),a&&t(b),a&&t(m),a&&t(E),F(v,a),a&&t($),a&&t(P),a&&t(K),F(L,a),a&&t(Q),a&&t(H),a&&t(pe),a&&t(R),a&&t(Ie),F(xe,a),a&&t(_e),F(ge,a),a&&t(ot),F(ie,a),a&&t(Me),a&&t(ve),F(f),a&&t(Te),F(ce,a),a&&t(ae),a&&t(ye),a&&t(aa),a&&t(te),a&&t(sa),F(We,a),a&&t(ra),F(st,a),a&&t(St),a&&t(Fe),a&&t(rt),F(nt,a),a&&t(Nt),a&&t(je),a&&t(ia),F(Ue,a),a&&t(la),F(it,a),a&&t(_t),a&&t(Ge),F(ct),a&&t(ca),F(Ke,a),a&&t(ha),a&&t(Se),a&&t(da),F(dt,a),a&&t(Ut),a&&t(ue),a&&t(gt),F(ft,a),a&&t(Gt),F(V,a),a&&t(pa),a&&t(fe),a&&t(ua),F(He,a),a&&t(fa),F(mt,a),a&&t(Jt),a&&t(se),a&&t(_a),a&&t(Ye)}}}function fn(G){let c,_,d,y,b,m,k,E,v,$,P,T,q,N,z,I,W,Y,K,L,Q,H,x,M,re,de,pe,R,be,J,ne,ze,ke,Z,Ee,Ie,xe,_e,ge,ot,ie,Me,ve,oe,Ae,f,U,le,De,Te,ce,ae,ye,ee,Le,jt,Aa,aa,te,Da,Ct,Pt,Oa,qt,zt,Fa,xt,At,Sa,Dt,oa,Oe,Ot,Ft,Na,sa,We,ra,st,St,Fe,na,rt,nt,Nt,je,he,ia,Ue,la,it,_t,Ge,Re,lt,ct,Ia,ht,Ma,ca,Ke,ha,Se,Wa,Ce,Ua,It,Mt,Ga,Wt,da,dt,Ut,ue,Be,pt,ut,Ka,gt,ft,Gt,V,pa,fe,Ha,Kt,Ht,La,Lt,Rt,Ra,Bt,Yt,Ba,ua,He,fa,mt,Jt,se,ma,Pe,wa,B,Vt,Xt,Ya,Qt,Zt,Ja,_a,Ye,Va,ga,a,g,no,Je,io,lo,vt,co,yt,va,ho,ya,$a,po,uo,ea,qe,$e,ba,fo,mo,ka,wo,_o,ta,Ea,go,Xa,$t,wt,we,vo,bt,yo,Ve,Ta,$o,bo,Ne,ko,Eo,ja,To,ps,Lo,Qa,Ro,Za,Bo,Xe,us,eo,fs,ms,Do,ws,_s,Oo,gs,vs,Yo,Ca,ys,Fo,$s,bs,Jo,to,Vo,kt,ks,So,Es,Ts,No,js,Cs,Xo,ao,Qo,Qe,Ps,Io,qs,zs,Mo,xs,As,Wo,Ds,Os,Zo,oo,es,jo,Fs,ts,Et,Ss,Uo,Ns,Is,Go,Ms,Ws,as,Pa,Us,Ko,Gs,Ks,os,so,ss;return q=new X({props:{code:`from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

training_args = TrainingArguments(<span class="hljs-string">&quot;test-trainer&quot;</span>)`}}),z=new Ao({props:{$$slots:{default:[vn]},$$scope:{ctx:G}}}),de=new X({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)`}}),ae=new X({props:{code:`from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),Fe=new X({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),ut=new ro({}),se=new X({props:{code:`predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)`,highlighted:`predictions = trainer.predict(tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>])
<span class="hljs-built_in">print</span>(predictions.predictions.shape, predictions.label_ids.shape)`}}),Pe=new X({props:{code:"(408, 2) (408,)",highlighted:'(<span class="hljs-number">408</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">408</span>,)'}}),$t=new X({props:{code:`import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

preds = np.argmax(predictions.predictions, axis=-<span class="hljs-number">1</span>)`}}),Qa=new X({props:{code:`from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
metric.compute(predictions=preds, references=predictions.label_ids)`}}),Za=new X({props:{code:"{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.8578431372549019</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.8996539792387542</span>}'}}),to=new X({props:{code:`def compute_metrics(eval_preds):
    metric = load_metric("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):
    metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)`}}),ao=new X({props:{code:`training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,highlighted:`training_args = TrainingArguments(<span class="hljs-string">&quot;test-trainer&quot;</span>, evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`}}),oo=new X({props:{code:"trainer.train()",highlighted:'trainer.trai<span class="hljs-meta">n</span>()'}}),so=new Ao({props:{$$slots:{default:[yn]},$$scope:{ctx:G}}}),{c(){c=n("p"),_=o("The first step before we can define our "),d=n("code"),y=o("Trainer"),b=o(" is to define a "),m=n("code"),k=o("TrainingArguments"),E=o(" class that will contain all the hyperparameters the "),v=n("code"),$=o("Trainer"),P=o(" will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning."),T=p(),A(q.$$.fragment),N=p(),A(z.$$.fragment),I=p(),W=n("p"),Y=o("The second step is to define our model. As in the "),K=n("a"),L=o("previous chapter"),Q=o(", we will use the "),H=n("code"),x=o("AutoModelForSequenceClassification"),M=o(" class, with two labels:"),re=p(),A(de.$$.fragment),pe=p(),R=n("p"),be=o("You will notice that unlike in "),J=n("a"),ne=o("Chapter 2"),ze=o(", you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."),ke=p(),Z=n("p"),Ee=o("Once we have our model, we can define a "),Ie=n("code"),xe=o("Trainer"),_e=o(" by passing it all the objects constructed up to now \u2014 the "),ge=n("code"),ot=o("model"),ie=o(", the "),Me=n("code"),ve=o("training_args"),oe=o(", the training and validation datasets, our "),Ae=n("code"),f=o("data_collator"),U=o(", and our "),le=n("code"),De=o("tokenizer"),Te=o(":"),ce=p(),A(ae.$$.fragment),ye=p(),ee=n("p"),Le=o("Note that when you pass the "),jt=n("code"),Aa=o("tokenizer"),aa=o(" as we did here, the default "),te=n("code"),Da=o("data_collator"),Ct=o(" used by the "),Pt=n("code"),Oa=o("Trainer"),qt=o(" will be a "),zt=n("code"),Fa=o("DataCollatorWithPadding"),xt=o(" as defined previously, so you can skip the line "),At=n("code"),Sa=o("data_collator=data_collator"),Dt=o(" in this call. It was still important to show you this part of the processing in section 2!"),oa=p(),Oe=n("p"),Ot=o("To fine-tune the model on our dataset, we just have to call the "),Ft=n("code"),Na=o("train()"),sa=o(" method of our "),We=n("code"),ra=o("Trainer"),st=o(":"),St=p(),A(Fe.$$.fragment),na=p(),rt=n("p"),nt=o("This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won\u2019t, however, tell you how well (or badly) your model is performing. This is because:"),Nt=p(),je=n("ol"),he=n("li"),ia=o("We didn\u2019t tell the "),Ue=n("code"),la=o("Trainer"),it=o(" to evaluate during training by setting "),_t=n("code"),Ge=o("evaluation_strategy"),Re=o(" to either "),lt=n("code"),ct=o('"steps"'),Ia=o(" (evaluate every "),ht=n("code"),Ma=o("eval_steps"),ca=o(") or "),Ke=n("code"),ha=o('"epoch"'),Se=o(" (evaluate at the end of each epoch)."),Wa=p(),Ce=n("li"),Ua=o("We didn\u2019t provide the "),It=n("code"),Mt=o("Trainer"),Ga=o(" with a "),Wt=n("code"),da=o("compute_metrics()"),dt=o(" function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number)."),Ut=p(),ue=n("h3"),Be=n("a"),pt=n("span"),A(ut.$$.fragment),Ka=p(),gt=n("span"),ft=o("Evaluation"),Gt=p(),V=n("p"),pa=o("Let\u2019s see how we can build a useful "),fe=n("code"),Ha=o("compute_metrics()"),Kt=o(" function and use it the next time we train. The function must take an "),Ht=n("code"),La=o("EvalPrediction"),Lt=o(" object (which is a named tuple with a "),Rt=n("code"),Ra=o("predictions"),Bt=o(" field and a "),Yt=n("code"),Ba=o("label_ids"),ua=o(" field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the "),He=n("code"),fa=o("Trainer.predict()"),mt=o(" command:"),Jt=p(),A(se.$$.fragment),ma=p(),A(Pe.$$.fragment),wa=p(),B=n("p"),Vt=o("The output of the "),Xt=n("code"),Ya=o("predict()"),Qt=o(" method is another named tuple with three fields: "),Zt=n("code"),Ja=o("predictions"),_a=o(", "),Ye=n("code"),Va=o("label_ids"),ga=o(", and "),a=n("code"),g=o("metrics"),no=o(". The "),Je=n("code"),io=o("metrics"),lo=o(" field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our "),vt=n("code"),co=o("compute_metrics()"),yt=o(" function and pass it to the "),va=n("code"),ho=o("Trainer"),ya=o(", that field will also contain the metrics returned by "),$a=n("code"),po=o("compute_metrics()"),uo=o("."),ea=p(),qe=n("p"),$e=o("As you can see, "),ba=n("code"),fo=o("predictions"),mo=o(" is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to "),ka=n("code"),wo=o("predict()"),_o=o(" (as you saw in the "),ta=n("a"),Ea=o("previous chapter"),go=o(", all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"),Xa=p(),A($t.$$.fragment),wt=p(),we=n("p"),vo=o("We can now compare those "),bt=n("code"),yo=o("preds"),Ve=o(" to the labels. To build our "),Ta=n("code"),$o=o("compute_metric()"),bo=o(" function, we will rely on the metrics from the \u{1F917} Datasets library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the "),Ne=n("code"),ko=o("load_metric()"),Eo=o(" function. The object returned has a "),ja=n("code"),To=o("compute()"),ps=o(" method we can use to do the metric calculation:"),Lo=p(),A(Qa.$$.fragment),Ro=p(),A(Za.$$.fragment),Bo=p(),Xe=n("p"),us=o("The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the "),eo=n("a"),fs=o("BERT paper"),ms=o(" reported an F1 score of 88.9 for the base model. That was the "),Do=n("code"),ws=o("uncased"),_s=o(" model while we are currently using the "),Oo=n("code"),gs=o("cased"),vs=o(" model, which explains the better result."),Yo=p(),Ca=n("p"),ys=o("Wrapping everything together, we get our "),Fo=n("code"),$s=o("compute_metrics()"),bs=o(" function:"),Jo=p(),A(to.$$.fragment),Vo=p(),kt=n("p"),ks=o("And to see it used in action to report metrics at the end of each epoch, here is how we define a new "),So=n("code"),Es=o("Trainer"),Ts=o(" with this "),No=n("code"),js=o("compute_metrics()"),Cs=o(" function:"),Xo=p(),A(ao.$$.fragment),Qo=p(),Qe=n("p"),Ps=o("Note that we create a new "),Io=n("code"),qs=o("TrainingArguments"),zs=o(" with its "),Mo=n("code"),xs=o("evaluation_strategy"),As=o(" set to "),Wo=n("code"),Ds=o('"epoch"'),Os=o(" and a new model \u2014 otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:"),Zo=p(),A(oo.$$.fragment),es=p(),jo=n("p"),Fs=o("This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark."),ts=p(),Et=n("p"),Ss=o("The "),Uo=n("code"),Ns=o("Trainer"),Is=o(" will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use "),Go=n("code"),Ms=o("fp16 = True"),Ws=o(" in your training arguments). We will go over everything it supports in Chapter 10."),as=p(),Pa=n("p"),Us=o("This concludes the introduction to fine-tuning using the "),Ko=n("code"),Gs=o("Trainer"),Ks=o(" API. An example of doing this for most common NLP tasks will be given in Chapter 7, but for now let\u2019s look at how to do the same thing in pure PyTorch."),os=p(),A(so.$$.fragment),this.h()},l(r){c=i(r,"P",{});var w=l(c);_=s(w,"The first step before we can define our "),d=i(w,"CODE",{});var Hs=l(d);y=s(Hs,"Trainer"),Hs.forEach(t),b=s(w," is to define a "),m=i(w,"CODE",{});var Ls=l(m);k=s(Ls,"TrainingArguments"),Ls.forEach(t),E=s(w," class that will contain all the hyperparameters the "),v=i(w,"CODE",{});var Rs=l(v);$=s(Rs,"Trainer"),Rs.forEach(t),P=s(w," will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning."),w.forEach(t),T=u(r),D(q.$$.fragment,r),N=u(r),D(z.$$.fragment,r),I=u(r),W=i(r,"P",{});var Co=l(W);Y=s(Co,"The second step is to define our model. As in the "),K=i(Co,"A",{href:!0});var Bs=l(K);L=s(Bs,"previous chapter"),Bs.forEach(t),Q=s(Co,", we will use the "),H=i(Co,"CODE",{});var Ys=l(H);x=s(Ys,"AutoModelForSequenceClassification"),Ys.forEach(t),M=s(Co," class, with two labels:"),Co.forEach(t),re=u(r),D(de.$$.fragment,r),pe=u(r),R=i(r,"P",{});var rs=l(R);be=s(rs,"You will notice that unlike in "),J=i(rs,"A",{href:!0});var Js=l(J);ne=s(Js,"Chapter 2"),Js.forEach(t),ze=s(rs,", you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."),rs.forEach(t),ke=u(r),Z=i(r,"P",{});var Ze=l(Z);Ee=s(Ze,"Once we have our model, we can define a "),Ie=i(Ze,"CODE",{});var Vs=l(Ie);xe=s(Vs,"Trainer"),Vs.forEach(t),_e=s(Ze," by passing it all the objects constructed up to now \u2014 the "),ge=i(Ze,"CODE",{});var Xs=l(ge);ot=s(Xs,"model"),Xs.forEach(t),ie=s(Ze,", the "),Me=i(Ze,"CODE",{});var Qs=l(Me);ve=s(Qs,"training_args"),Qs.forEach(t),oe=s(Ze,", the training and validation datasets, our "),Ae=i(Ze,"CODE",{});var Zs=l(Ae);f=s(Zs,"data_collator"),Zs.forEach(t),U=s(Ze,", and our "),le=i(Ze,"CODE",{});var er=l(le);De=s(er,"tokenizer"),er.forEach(t),Te=s(Ze,":"),Ze.forEach(t),ce=u(r),D(ae.$$.fragment,r),ye=u(r),ee=i(r,"P",{});var et=l(ee);Le=s(et,"Note that when you pass the "),jt=i(et,"CODE",{});var tr=l(jt);Aa=s(tr,"tokenizer"),tr.forEach(t),aa=s(et," as we did here, the default "),te=i(et,"CODE",{});var ar=l(te);Da=s(ar,"data_collator"),ar.forEach(t),Ct=s(et," used by the "),Pt=i(et,"CODE",{});var or=l(Pt);Oa=s(or,"Trainer"),or.forEach(t),qt=s(et," will be a "),zt=i(et,"CODE",{});var sr=l(zt);Fa=s(sr,"DataCollatorWithPadding"),sr.forEach(t),xt=s(et," as defined previously, so you can skip the line "),At=i(et,"CODE",{});var rr=l(At);Sa=s(rr,"data_collator=data_collator"),rr.forEach(t),Dt=s(et," in this call. It was still important to show you this part of the processing in section 2!"),et.forEach(t),oa=u(r),Oe=i(r,"P",{});var Po=l(Oe);Ot=s(Po,"To fine-tune the model on our dataset, we just have to call the "),Ft=i(Po,"CODE",{});var nr=l(Ft);Na=s(nr,"train()"),nr.forEach(t),sa=s(Po," method of our "),We=i(Po,"CODE",{});var ir=l(We);ra=s(ir,"Trainer"),ir.forEach(t),st=s(Po,":"),Po.forEach(t),St=u(r),D(Fe.$$.fragment,r),na=u(r),rt=i(r,"P",{});var lr=l(rt);nt=s(lr,"This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won\u2019t, however, tell you how well (or badly) your model is performing. This is because:"),lr.forEach(t),Nt=u(r),je=i(r,"OL",{});var ns=l(je);he=i(ns,"LI",{});var tt=l(he);ia=s(tt,"We didn\u2019t tell the "),Ue=i(tt,"CODE",{});var cr=l(Ue);la=s(cr,"Trainer"),cr.forEach(t),it=s(tt," to evaluate during training by setting "),_t=i(tt,"CODE",{});var hr=l(_t);Ge=s(hr,"evaluation_strategy"),hr.forEach(t),Re=s(tt," to either "),lt=i(tt,"CODE",{});var dr=l(lt);ct=s(dr,'"steps"'),dr.forEach(t),Ia=s(tt," (evaluate every "),ht=i(tt,"CODE",{});var pr=l(ht);Ma=s(pr,"eval_steps"),pr.forEach(t),ca=s(tt,") or "),Ke=i(tt,"CODE",{});var ur=l(Ke);ha=s(ur,'"epoch"'),ur.forEach(t),Se=s(tt," (evaluate at the end of each epoch)."),tt.forEach(t),Wa=u(ns),Ce=i(ns,"LI",{});var qo=l(Ce);Ua=s(qo,"We didn\u2019t provide the "),It=i(qo,"CODE",{});var fr=l(It);Mt=s(fr,"Trainer"),fr.forEach(t),Ga=s(qo," with a "),Wt=i(qo,"CODE",{});var mr=l(Wt);da=s(mr,"compute_metrics()"),mr.forEach(t),dt=s(qo," function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number)."),qo.forEach(t),ns.forEach(t),Ut=u(r),ue=i(r,"H3",{class:!0});var is=l(ue);Be=i(is,"A",{id:!0,class:!0,href:!0});var wr=l(Be);pt=i(wr,"SPAN",{});var _r=l(pt);D(ut.$$.fragment,_r),_r.forEach(t),wr.forEach(t),Ka=u(is),gt=i(is,"SPAN",{});var gr=l(gt);ft=s(gr,"Evaluation"),gr.forEach(t),is.forEach(t),Gt=u(r),V=i(r,"P",{});var at=l(V);pa=s(at,"Let\u2019s see how we can build a useful "),fe=i(at,"CODE",{});var vr=l(fe);Ha=s(vr,"compute_metrics()"),vr.forEach(t),Kt=s(at," function and use it the next time we train. The function must take an "),Ht=i(at,"CODE",{});var yr=l(Ht);La=s(yr,"EvalPrediction"),yr.forEach(t),Lt=s(at," object (which is a named tuple with a "),Rt=i(at,"CODE",{});var $r=l(Rt);Ra=s($r,"predictions"),$r.forEach(t),Bt=s(at," field and a "),Yt=i(at,"CODE",{});var br=l(Yt);Ba=s(br,"label_ids"),br.forEach(t),ua=s(at," field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the "),He=i(at,"CODE",{});var kr=l(He);fa=s(kr,"Trainer.predict()"),kr.forEach(t),mt=s(at," command:"),at.forEach(t),Jt=u(r),D(se.$$.fragment,r),ma=u(r),D(Pe.$$.fragment,r),wa=u(r),B=i(r,"P",{});var me=l(B);Vt=s(me,"The output of the "),Xt=i(me,"CODE",{});var Er=l(Xt);Ya=s(Er,"predict()"),Er.forEach(t),Qt=s(me," method is another named tuple with three fields: "),Zt=i(me,"CODE",{});var Tr=l(Zt);Ja=s(Tr,"predictions"),Tr.forEach(t),_a=s(me,", "),Ye=i(me,"CODE",{});var jr=l(Ye);Va=s(jr,"label_ids"),jr.forEach(t),ga=s(me,", and "),a=i(me,"CODE",{});var Cr=l(a);g=s(Cr,"metrics"),Cr.forEach(t),no=s(me,". The "),Je=i(me,"CODE",{});var Pr=l(Je);io=s(Pr,"metrics"),Pr.forEach(t),lo=s(me," field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our "),vt=i(me,"CODE",{});var qr=l(vt);co=s(qr,"compute_metrics()"),qr.forEach(t),yt=s(me," function and pass it to the "),va=i(me,"CODE",{});var zr=l(va);ho=s(zr,"Trainer"),zr.forEach(t),ya=s(me,", that field will also contain the metrics returned by "),$a=i(me,"CODE",{});var xr=l($a);po=s(xr,"compute_metrics()"),xr.forEach(t),uo=s(me,"."),me.forEach(t),ea=u(r),qe=i(r,"P",{});var qa=l(qe);$e=s(qa,"As you can see, "),ba=i(qa,"CODE",{});var Ar=l(ba);fo=s(Ar,"predictions"),Ar.forEach(t),mo=s(qa," is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to "),ka=i(qa,"CODE",{});var Dr=l(ka);wo=s(Dr,"predict()"),Dr.forEach(t),_o=s(qa," (as you saw in the "),ta=i(qa,"A",{href:!0});var Or=l(ta);Ea=s(Or,"previous chapter"),Or.forEach(t),go=s(qa,", all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"),qa.forEach(t),Xa=u(r),D($t.$$.fragment,r),wt=u(r),we=i(r,"P",{});var Tt=l(we);vo=s(Tt,"We can now compare those "),bt=i(Tt,"CODE",{});var Fr=l(bt);yo=s(Fr,"preds"),Fr.forEach(t),Ve=s(Tt," to the labels. To build our "),Ta=i(Tt,"CODE",{});var Sr=l(Ta);$o=s(Sr,"compute_metric()"),Sr.forEach(t),bo=s(Tt," function, we will rely on the metrics from the \u{1F917} Datasets library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the "),Ne=i(Tt,"CODE",{});var Nr=l(Ne);ko=s(Nr,"load_metric()"),Nr.forEach(t),Eo=s(Tt," function. The object returned has a "),ja=i(Tt,"CODE",{});var Ir=l(ja);To=s(Ir,"compute()"),Ir.forEach(t),ps=s(Tt," method we can use to do the metric calculation:"),Tt.forEach(t),Lo=u(r),D(Qa.$$.fragment,r),Ro=u(r),D(Za.$$.fragment,r),Bo=u(r),Xe=i(r,"P",{});var za=l(Xe);us=s(za,"The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the "),eo=i(za,"A",{href:!0,rel:!0});var Mr=l(eo);fs=s(Mr,"BERT paper"),Mr.forEach(t),ms=s(za," reported an F1 score of 88.9 for the base model. That was the "),Do=i(za,"CODE",{});var Wr=l(Do);ws=s(Wr,"uncased"),Wr.forEach(t),_s=s(za," model while we are currently using the "),Oo=i(za,"CODE",{});var Ur=l(Oo);gs=s(Ur,"cased"),Ur.forEach(t),vs=s(za," model, which explains the better result."),za.forEach(t),Yo=u(r),Ca=i(r,"P",{});var ls=l(Ca);ys=s(ls,"Wrapping everything together, we get our "),Fo=i(ls,"CODE",{});var Gr=l(Fo);$s=s(Gr,"compute_metrics()"),Gr.forEach(t),bs=s(ls," function:"),ls.forEach(t),Jo=u(r),D(to.$$.fragment,r),Vo=u(r),kt=i(r,"P",{});var zo=l(kt);ks=s(zo,"And to see it used in action to report metrics at the end of each epoch, here is how we define a new "),So=i(zo,"CODE",{});var Kr=l(So);Es=s(Kr,"Trainer"),Kr.forEach(t),Ts=s(zo," with this "),No=i(zo,"CODE",{});var Hr=l(No);js=s(Hr,"compute_metrics()"),Hr.forEach(t),Cs=s(zo," function:"),zo.forEach(t),Xo=u(r),D(ao.$$.fragment,r),Qo=u(r),Qe=i(r,"P",{});var xa=l(Qe);Ps=s(xa,"Note that we create a new "),Io=i(xa,"CODE",{});var Lr=l(Io);qs=s(Lr,"TrainingArguments"),Lr.forEach(t),zs=s(xa," with its "),Mo=i(xa,"CODE",{});var Rr=l(Mo);xs=s(Rr,"evaluation_strategy"),Rr.forEach(t),As=s(xa," set to "),Wo=i(xa,"CODE",{});var Br=l(Wo);Ds=s(Br,'"epoch"'),Br.forEach(t),Os=s(xa," and a new model \u2014 otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:"),xa.forEach(t),Zo=u(r),D(oo.$$.fragment,r),es=u(r),jo=i(r,"P",{});var Yr=l(jo);Fs=s(Yr,"This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark."),Yr.forEach(t),ts=u(r),Et=i(r,"P",{});var xo=l(Et);Ss=s(xo,"The "),Uo=i(xo,"CODE",{});var Jr=l(Uo);Ns=s(Jr,"Trainer"),Jr.forEach(t),Is=s(xo," will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use "),Go=i(xo,"CODE",{});var Vr=l(Go);Ms=s(Vr,"fp16 = True"),Vr.forEach(t),Ws=s(xo," in your training arguments). We will go over everything it supports in Chapter 10."),xo.forEach(t),as=u(r),Pa=i(r,"P",{});var cs=l(Pa);Us=s(cs,"This concludes the introduction to fine-tuning using the "),Ko=i(cs,"CODE",{});var Xr=l(Ko);Gs=s(Xr,"Trainer"),Xr.forEach(t),Ks=s(cs," API. An example of doing this for most common NLP tasks will be given in Chapter 7, but for now let\u2019s look at how to do the same thing in pure PyTorch."),cs.forEach(t),os=u(r),D(so.$$.fragment,r),this.h()},h(){S(K,"href","/course/chapter2"),S(J,"href","/course/chapter2"),S(Be,"id","evaluation"),S(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(Be,"href","#evaluation"),S(ue,"class","relative group"),S(ta,"href","/course/chapter2"),S(eo,"href","https://arxiv.org/pdf/1810.04805.pdf"),S(eo,"rel","nofollow")},m(r,w){h(r,c,w),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E),e(c,v),e(v,$),e(c,P),h(r,T,w),O(q,r,w),h(r,N,w),O(z,r,w),h(r,I,w),h(r,W,w),e(W,Y),e(W,K),e(K,L),e(W,Q),e(W,H),e(H,x),e(W,M),h(r,re,w),O(de,r,w),h(r,pe,w),h(r,R,w),e(R,be),e(R,J),e(J,ne),e(R,ze),h(r,ke,w),h(r,Z,w),e(Z,Ee),e(Z,Ie),e(Ie,xe),e(Z,_e),e(Z,ge),e(ge,ot),e(Z,ie),e(Z,Me),e(Me,ve),e(Z,oe),e(Z,Ae),e(Ae,f),e(Z,U),e(Z,le),e(le,De),e(Z,Te),h(r,ce,w),O(ae,r,w),h(r,ye,w),h(r,ee,w),e(ee,Le),e(ee,jt),e(jt,Aa),e(ee,aa),e(ee,te),e(te,Da),e(ee,Ct),e(ee,Pt),e(Pt,Oa),e(ee,qt),e(ee,zt),e(zt,Fa),e(ee,xt),e(ee,At),e(At,Sa),e(ee,Dt),h(r,oa,w),h(r,Oe,w),e(Oe,Ot),e(Oe,Ft),e(Ft,Na),e(Oe,sa),e(Oe,We),e(We,ra),e(Oe,st),h(r,St,w),O(Fe,r,w),h(r,na,w),h(r,rt,w),e(rt,nt),h(r,Nt,w),h(r,je,w),e(je,he),e(he,ia),e(he,Ue),e(Ue,la),e(he,it),e(he,_t),e(_t,Ge),e(he,Re),e(he,lt),e(lt,ct),e(he,Ia),e(he,ht),e(ht,Ma),e(he,ca),e(he,Ke),e(Ke,ha),e(he,Se),e(je,Wa),e(je,Ce),e(Ce,Ua),e(Ce,It),e(It,Mt),e(Ce,Ga),e(Ce,Wt),e(Wt,da),e(Ce,dt),h(r,Ut,w),h(r,ue,w),e(ue,Be),e(Be,pt),O(ut,pt,null),e(ue,Ka),e(ue,gt),e(gt,ft),h(r,Gt,w),h(r,V,w),e(V,pa),e(V,fe),e(fe,Ha),e(V,Kt),e(V,Ht),e(Ht,La),e(V,Lt),e(V,Rt),e(Rt,Ra),e(V,Bt),e(V,Yt),e(Yt,Ba),e(V,ua),e(V,He),e(He,fa),e(V,mt),h(r,Jt,w),O(se,r,w),h(r,ma,w),O(Pe,r,w),h(r,wa,w),h(r,B,w),e(B,Vt),e(B,Xt),e(Xt,Ya),e(B,Qt),e(B,Zt),e(Zt,Ja),e(B,_a),e(B,Ye),e(Ye,Va),e(B,ga),e(B,a),e(a,g),e(B,no),e(B,Je),e(Je,io),e(B,lo),e(B,vt),e(vt,co),e(B,yt),e(B,va),e(va,ho),e(B,ya),e(B,$a),e($a,po),e(B,uo),h(r,ea,w),h(r,qe,w),e(qe,$e),e(qe,ba),e(ba,fo),e(qe,mo),e(qe,ka),e(ka,wo),e(qe,_o),e(qe,ta),e(ta,Ea),e(qe,go),h(r,Xa,w),O($t,r,w),h(r,wt,w),h(r,we,w),e(we,vo),e(we,bt),e(bt,yo),e(we,Ve),e(we,Ta),e(Ta,$o),e(we,bo),e(we,Ne),e(Ne,ko),e(we,Eo),e(we,ja),e(ja,To),e(we,ps),h(r,Lo,w),O(Qa,r,w),h(r,Ro,w),O(Za,r,w),h(r,Bo,w),h(r,Xe,w),e(Xe,us),e(Xe,eo),e(eo,fs),e(Xe,ms),e(Xe,Do),e(Do,ws),e(Xe,_s),e(Xe,Oo),e(Oo,gs),e(Xe,vs),h(r,Yo,w),h(r,Ca,w),e(Ca,ys),e(Ca,Fo),e(Fo,$s),e(Ca,bs),h(r,Jo,w),O(to,r,w),h(r,Vo,w),h(r,kt,w),e(kt,ks),e(kt,So),e(So,Es),e(kt,Ts),e(kt,No),e(No,js),e(kt,Cs),h(r,Xo,w),O(ao,r,w),h(r,Qo,w),h(r,Qe,w),e(Qe,Ps),e(Qe,Io),e(Io,qs),e(Qe,zs),e(Qe,Mo),e(Mo,xs),e(Qe,As),e(Qe,Wo),e(Wo,Ds),e(Qe,Os),h(r,Zo,w),O(oo,r,w),h(r,es,w),h(r,jo,w),e(jo,Fs),h(r,ts,w),h(r,Et,w),e(Et,Ss),e(Et,Uo),e(Uo,Ns),e(Et,Is),e(Et,Go),e(Go,Ms),e(Et,Ws),h(r,as,w),h(r,Pa,w),e(Pa,Us),e(Pa,Ko),e(Ko,Gs),e(Pa,Ks),h(r,os,w),O(so,r,w),ss=!0},i(r){ss||(C(q.$$.fragment,r),C(z.$$.fragment,r),C(de.$$.fragment,r),C(ae.$$.fragment,r),C(Fe.$$.fragment,r),C(ut.$$.fragment,r),C(se.$$.fragment,r),C(Pe.$$.fragment,r),C($t.$$.fragment,r),C(Qa.$$.fragment,r),C(Za.$$.fragment,r),C(to.$$.fragment,r),C(ao.$$.fragment,r),C(oo.$$.fragment,r),C(so.$$.fragment,r),ss=!0)},o(r){j(q.$$.fragment,r),j(z.$$.fragment,r),j(de.$$.fragment,r),j(ae.$$.fragment,r),j(Fe.$$.fragment,r),j(ut.$$.fragment,r),j(se.$$.fragment,r),j(Pe.$$.fragment,r),j($t.$$.fragment,r),j(Qa.$$.fragment,r),j(Za.$$.fragment,r),j(to.$$.fragment,r),j(ao.$$.fragment,r),j(oo.$$.fragment,r),j(so.$$.fragment,r),ss=!1},d(r){r&&t(c),r&&t(T),F(q,r),r&&t(N),F(z,r),r&&t(I),r&&t(W),r&&t(re),F(de,r),r&&t(pe),r&&t(R),r&&t(ke),r&&t(Z),r&&t(ce),F(ae,r),r&&t(ye),r&&t(ee),r&&t(oa),r&&t(Oe),r&&t(St),F(Fe,r),r&&t(na),r&&t(rt),r&&t(Nt),r&&t(je),r&&t(Ut),r&&t(ue),F(ut),r&&t(Gt),r&&t(V),r&&t(Jt),F(se,r),r&&t(ma),F(Pe,r),r&&t(wa),r&&t(B),r&&t(ea),r&&t(qe),r&&t(Xa),F($t,r),r&&t(wt),r&&t(we),r&&t(Lo),F(Qa,r),r&&t(Ro),F(Za,r),r&&t(Bo),r&&t(Xe),r&&t(Yo),r&&t(Ca),r&&t(Jo),F(to,r),r&&t(Vo),r&&t(kt),r&&t(Xo),F(ao,r),r&&t(Qo),r&&t(Qe),r&&t(Zo),F(oo,r),r&&t(es),r&&t(jo),r&&t(ts),r&&t(Et),r&&t(as),r&&t(Pa),r&&t(os),F(so,r)}}}function mn(G){let c,_,d,y,b;return{c(){c=n("p"),_=o("Note that \u{1F917} Transformers models have a special ability that most Keras models don\u2019t - they can automatically use an appropriate loss which they compute internally. They will use this loss by default if you don\u2019t set a loss argument in "),d=n("code"),y=o("compile()"),b=o(". Note that to use the internal loss you\u2019ll need to pass your labels as part of the input, not as a separate label, which is the normal way to use labels with Keras models. You\u2019ll see examples of this in Part 2 of the course, where defining the correct loss function can be tricky. For sequence classification, however, a standard Keras loss function works fine, so that\u2019s what we\u2019ll use here.")},l(m){c=i(m,"P",{});var k=l(c);_=s(k,"Note that \u{1F917} Transformers models have a special ability that most Keras models don\u2019t - they can automatically use an appropriate loss which they compute internally. They will use this loss by default if you don\u2019t set a loss argument in "),d=i(k,"CODE",{});var E=l(d);y=s(E,"compile()"),E.forEach(t),b=s(k,". Note that to use the internal loss you\u2019ll need to pass your labels as part of the input, not as a separate label, which is the normal way to use labels with Keras models. You\u2019ll see examples of this in Part 2 of the course, where defining the correct loss function can be tricky. For sequence classification, however, a standard Keras loss function works fine, so that\u2019s what we\u2019ll use here."),k.forEach(t)},m(m,k){h(m,c,k),e(c,_),e(c,d),e(d,y),e(c,b)},d(m){m&&t(c)}}}function wn(G){let c,_,d,y,b,m,k,E;return{c(){c=n("p"),_=o("Note a very common pitfall here \u2014 you "),d=n("em"),y=o("can"),b=o(" just pass the name of the loss as a string to Keras, but by default Keras will assume that you have already applied a softmax to your outputs. Many models, however, output the values right before the softmax is applied, which are also known as the "),m=n("em"),k=o("logits"),E=o(". We need to tell the loss function that that\u2019s what our model does, and the only way to do that is to call it directly, rather than by name with a string.")},l(v){c=i(v,"P",{});var $=l(c);_=s($,"Note a very common pitfall here \u2014 you "),d=i($,"EM",{});var P=l(d);y=s(P,"can"),P.forEach(t),b=s($," just pass the name of the loss as a string to Keras, but by default Keras will assume that you have already applied a softmax to your outputs. Many models, however, output the values right before the softmax is applied, which are also known as the "),m=i($,"EM",{});var T=l(m);k=s(T,"logits"),T.forEach(t),E=s($,". We need to tell the loss function that that\u2019s what our model does, and the only way to do that is to call it directly, rather than by name with a string."),$.forEach(t)},m(v,$){h(v,c,$),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E)},d(v){v&&t(c)}}}function _n(G){let c,_,d,y,b,m,k,E;return{c(){c=n("p"),_=o("The \u{1F917} Transformers library also has a "),d=n("code"),y=o("create_optimizer()"),b=o(" function that will create an "),m=n("code"),k=o("AdamW"),E=o(" optimizer with learning rate decay. This is a convenient shortcut that you\u2019ll see in detail in future sections of the course.")},l(v){c=i(v,"P",{});var $=l(c);_=s($,"The \u{1F917} Transformers library also has a "),d=i($,"CODE",{});var P=l(d);y=s(P,"create_optimizer()"),P.forEach(t),b=s($," function that will create an "),m=i($,"CODE",{});var T=l(m);k=s(T,"AdamW"),T.forEach(t),E=s($," optimizer with learning rate decay. This is a convenient shortcut that you\u2019ll see in detail in future sections of the course."),$.forEach(t)},m(v,$){h(v,c,$),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E)},d(v){v&&t(c)}}}function gn(G){let c,_,d,y,b,m,k,E,v,$;return{c(){c=n("p"),_=o("\u{1F4A1} If you want to automatically upload your model to the Hub during training, you can pass along a "),d=n("code"),y=o("PushToHubCallback"),b=o(" in the "),m=n("code"),k=o("model.fit()"),E=o(" method. We will learn more about this in "),v=n("a"),$=o("Chapter 4"),this.h()},l(P){c=i(P,"P",{});var T=l(c);_=s(T,"\u{1F4A1} If you want to automatically upload your model to the Hub during training, you can pass along a "),d=i(T,"CODE",{});var q=l(d);y=s(q,"PushToHubCallback"),q.forEach(t),b=s(T," in the "),m=i(T,"CODE",{});var N=l(m);k=s(N,"model.fit()"),N.forEach(t),E=s(T," method. We will learn more about this in "),v=i(T,"A",{href:!0});var z=l(v);$=s(z,"Chapter 4"),z.forEach(t),T.forEach(t),this.h()},h(){S(v,"href","/course/chapter4/3")},m(P,T){h(P,c,T),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E),e(c,v),e(v,$)},d(P){P&&t(c)}}}function vn(G){let c,_,d,y,b,m,k,E,v,$;return{c(){c=n("p"),_=o("\u{1F4A1} If you want to automatically upload your model to the Hub during training, pass along "),d=n("code"),y=o("push_to_hub=True"),b=o(" in the "),m=n("code"),k=o("TrainingArguments"),E=o(". We will learn more about this in "),v=n("a"),$=o("Chapter 4"),this.h()},l(P){c=i(P,"P",{});var T=l(c);_=s(T,"\u{1F4A1} If you want to automatically upload your model to the Hub during training, pass along "),d=i(T,"CODE",{});var q=l(d);y=s(q,"push_to_hub=True"),q.forEach(t),b=s(T," in the "),m=i(T,"CODE",{});var N=l(m);k=s(N,"TrainingArguments"),N.forEach(t),E=s(T,". We will learn more about this in "),v=i(T,"A",{href:!0});var z=l(v);$=s(z,"Chapter 4"),z.forEach(t),T.forEach(t),this.h()},h(){S(v,"href","/course/chapter4/3")},m(P,T){h(P,c,T),e(c,_),e(c,d),e(d,y),e(c,b),e(c,m),e(m,k),e(c,E),e(c,v),e(v,$)},d(P){P&&t(c)}}}function yn(G){let c,_,d,y,b;return{c(){c=n("p"),_=o("\u270F\uFE0F "),d=n("strong"),y=o("Try it out!"),b=o(" Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.")},l(m){c=i(m,"P",{});var k=l(c);_=s(k,"\u270F\uFE0F "),d=i(k,"STRONG",{});var E=l(d);y=s(E,"Try it out!"),E.forEach(t),b=s(k," Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2."),k.forEach(t)},m(m,k){h(m,c,k),e(c,_),e(c,d),e(d,y),e(c,b)},d(m){m&&t(c)}}}function $n(G){let c,_,d,y,b,m,k,E,v,$,P,T,q,N,z,I,W,Y,K,L,Q,H,x,M,re,de,pe,R,be,J,ne,ze,ke;d=new rn({props:{fw:G[0]}}),E=new ro({});const Z=[ln,nn],Ee=[];function Ie(f,U){return f[0]==="pt"?0:1}q=Ie(G),N=Ee[q]=Z[q](G);const xe=[hn,cn],_e=[];function ge(f,U){return f[0]==="pt"?0:1}I=ge(G),W=_e[I]=xe[I](G);const ot=[pn,dn],ie=[];function Me(f,U){return f[0]==="pt"?0:1}K=Me(G),L=ie[K]=ot[K](G),re=new ro({});const ve=[fn,un],oe=[];function Ae(f,U){return f[0]==="pt"?0:1}return J=Ae(G),ne=oe[J]=ve[J](G),{c(){c=n("meta"),_=p(),A(d.$$.fragment),y=p(),b=n("h1"),m=n("a"),k=n("span"),A(E.$$.fragment),v=p(),$=n("span"),P=o("Fine-tuning a model"),T=p(),N.c(),z=p(),W.c(),Y=p(),L.c(),Q=p(),H=n("h3"),x=n("a"),M=n("span"),A(re.$$.fragment),de=p(),pe=n("span"),R=o("Training"),be=p(),ne.c(),ze=Qr(),this.h()},l(f){const U=on('[data-svelte="svelte-1phssyn"]',document.head);c=i(U,"META",{name:!0,content:!0}),U.forEach(t),_=u(f),D(d.$$.fragment,f),y=u(f),b=i(f,"H1",{class:!0});var le=l(b);m=i(le,"A",{id:!0,class:!0,href:!0});var De=l(m);k=i(De,"SPAN",{});var Te=l(k);D(E.$$.fragment,Te),Te.forEach(t),De.forEach(t),v=u(le),$=i(le,"SPAN",{});var ce=l($);P=s(ce,"Fine-tuning a model"),ce.forEach(t),le.forEach(t),T=u(f),N.l(f),z=u(f),W.l(f),Y=u(f),L.l(f),Q=u(f),H=i(f,"H3",{class:!0});var ae=l(H);x=i(ae,"A",{id:!0,class:!0,href:!0});var ye=l(x);M=i(ye,"SPAN",{});var ee=l(M);D(re.$$.fragment,ee),ee.forEach(t),ye.forEach(t),de=u(ae),pe=i(ae,"SPAN",{});var Le=l(pe);R=s(Le,"Training"),Le.forEach(t),ae.forEach(t),be=u(f),ne.l(f),ze=Qr(),this.h()},h(){S(c,"name","hf:doc:metadata"),S(c,"content",JSON.stringify(bn)),S(m,"id","finetuning-a-model"),S(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(m,"href","#finetuning-a-model"),S(b,"class","relative group"),S(x,"id","training"),S(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),S(x,"href","#training"),S(H,"class","relative group")},m(f,U){e(document.head,c),h(f,_,U),O(d,f,U),h(f,y,U),h(f,b,U),e(b,m),e(m,k),O(E,k,null),e(b,v),e(b,$),e($,P),h(f,T,U),Ee[q].m(f,U),h(f,z,U),_e[I].m(f,U),h(f,Y,U),ie[K].m(f,U),h(f,Q,U),h(f,H,U),e(H,x),e(x,M),O(re,M,null),e(H,de),e(H,pe),e(pe,R),h(f,be,U),oe[J].m(f,U),h(f,ze,U),ke=!0},p(f,[U]){const le={};U&1&&(le.fw=f[0]),d.$set(le);let De=q;q=Ie(f),q!==De&&(ds(),j(Ee[De],1,1,()=>{Ee[De]=null}),hs(),N=Ee[q],N||(N=Ee[q]=Z[q](f),N.c()),C(N,1),N.m(z.parentNode,z));let Te=I;I=ge(f),I!==Te&&(ds(),j(_e[Te],1,1,()=>{_e[Te]=null}),hs(),W=_e[I],W||(W=_e[I]=xe[I](f),W.c()),C(W,1),W.m(Y.parentNode,Y));let ce=K;K=Me(f),K!==ce&&(ds(),j(ie[ce],1,1,()=>{ie[ce]=null}),hs(),L=ie[K],L||(L=ie[K]=ot[K](f),L.c()),C(L,1),L.m(Q.parentNode,Q));let ae=J;J=Ae(f),J!==ae&&(ds(),j(oe[ae],1,1,()=>{oe[ae]=null}),hs(),ne=oe[J],ne||(ne=oe[J]=ve[J](f),ne.c()),C(ne,1),ne.m(ze.parentNode,ze))},i(f){ke||(C(d.$$.fragment,f),C(E.$$.fragment,f),C(N),C(W),C(L),C(re.$$.fragment,f),C(ne),ke=!0)},o(f){j(d.$$.fragment,f),j(E.$$.fragment,f),j(N),j(W),j(L),j(re.$$.fragment,f),j(ne),ke=!1},d(f){t(c),f&&t(_),F(d,f),f&&t(y),f&&t(b),F(E),f&&t(T),Ee[q].d(f),f&&t(z),_e[I].d(f),f&&t(Y),ie[K].d(f),f&&t(Q),f&&t(H),F(re),f&&t(be),oe[J].d(f),f&&t(ze)}}}const bn={local:"finetuning-a-model",sections:[{local:"training",title:"Training"},{local:"evaluation",title:"Evaluation"},{local:"improving-training-performance",title:"Improving training performance"},{local:"model-predictions",title:"Model predictions"}],title:"Fine-tuning a model"};function kn(G,c,_){let d="pt";return sn(()=>{const y=new URLSearchParams(window.location.search);_(0,d=y.get("fw")||"pt")}),[d]}class xn extends en{constructor(c){super();tn(this,c,kn,$n,an,{})}}export{xn as default,bn as metadata};
