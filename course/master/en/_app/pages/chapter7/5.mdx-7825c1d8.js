import{S as vg,i as yg,s as kg,e as o,t as a,k as h,w as v,c as r,a as l,h as n,d as t,m as c,x as y,b as f,g as i,F as s,y as k,q as g,o as _,B as $,U as ug,M as $g,V as dg,N as Xp,p as Sr,v as jg,n as Ar}from"../../chunks/vendor-1e8b365d.js";import{T as ln}from"../../chunks/Tip-62b14c6e.js";import{Y as ku}from"../../chunks/Youtube-c2a8cc39.js";import{I as Vs}from"../../chunks/IconCopyLink-483c28ba.js";import{C as z}from"../../chunks/CodeBlock-e5764662.js";import{D as _g}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as xg}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Eg(K){let u,j;return u=new _g({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"}]}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),j=!0},i(d){j||(g(u.$$.fragment,d),j=!0)},o(d){_(u.$$.fragment,d),j=!1},d(d){$(u,d)}}}function Tg(K){let u,j;return u=new _g({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"}]}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),j=!0},i(d){j||(g(u.$$.fragment,d),j=!0)},o(d){_(u.$$.fragment,d),j=!1},d(d){$(u,d)}}}function qg(K){let u,j,d,E,C,x,A,P,T,S,I;return{c(){u=o("p"),j=a("\u270F\uFE0F "),d=o("strong"),E=a("Try it out!"),C=a(" Change the random seed in the "),x=o("code"),A=a("Dataset.shuffle()"),P=a(" command to explore other reviews in the corpus. If you\u2019re a Spanish speaker, take a look at some of the reviews in "),T=o("code"),S=a("spanish_dataset"),I=a(" to see if the titles also seem like reasonable summaries.")},l(D){u=r(D,"P",{});var O=l(u);j=n(O,"\u270F\uFE0F "),d=r(O,"STRONG",{});var N=l(d);E=n(N,"Try it out!"),N.forEach(t),C=n(O," Change the random seed in the "),x=r(O,"CODE",{});var J=l(x);A=n(J,"Dataset.shuffle()"),J.forEach(t),P=n(O," command to explore other reviews in the corpus. If you\u2019re a Spanish speaker, take a look at some of the reviews in "),T=r(O,"CODE",{});var X=l(T);S=n(X,"spanish_dataset"),X.forEach(t),I=n(O," to see if the titles also seem like reasonable summaries."),O.forEach(t)},m(D,O){i(D,u,O),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I)},d(D){D&&t(u)}}}function zg(K){let u,j,d,E,C,x,A,P;return{c(){u=o("p"),j=a("\u270F\uFE0F "),d=o("strong"),E=a("Try it out!"),C=a(" Once you\u2019ve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you\u2019ll need to prepend "),x=o("code"),A=a("summarize:"),P=a(" to the input examples in the preprocessing steps below.")},l(T){u=r(T,"P",{});var S=l(u);j=n(S,"\u270F\uFE0F "),d=r(S,"STRONG",{});var I=l(d);E=n(I,"Try it out!"),I.forEach(t),C=n(S," Once you\u2019ve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you\u2019ll need to prepend "),x=r(S,"CODE",{});var D=l(x);A=n(D,"summarize:"),D.forEach(t),P=n(S," to the input examples in the preprocessing steps below."),S.forEach(t)},m(T,S){i(T,u,S),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function Dg(K){let u,j;return{c(){u=o("p"),j=a("\u{1F4A1} In the early stages of your NLP projects, a good practice is to train a class of \u201Csmall\u201D models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!")},l(d){u=r(d,"P",{});var E=l(u);j=n(E,"\u{1F4A1} In the early stages of your NLP projects, a good practice is to train a class of \u201Csmall\u201D models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!"),E.forEach(t)},m(d,E){i(d,u,E),s(u,j)},d(d){d&&t(u)}}}function Sg(K){let u,j,d,E,C,x,A,P,T,S,I;return{c(){u=o("p"),j=a("\u{1F4A1} You may have noticed that we used "),d=o("code"),E=a("batched=True"),C=a(" in our "),x=o("code"),A=a("Dataset.map()"),P=a(" function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in \u{1F917} Transformers. Where possible, try using "),T=o("code"),S=a("batched=True"),I=a(" to get the most out of your preprocessing!")},l(D){u=r(D,"P",{});var O=l(u);j=n(O,"\u{1F4A1} You may have noticed that we used "),d=r(O,"CODE",{});var N=l(d);E=n(N,"batched=True"),N.forEach(t),C=n(O," in our "),x=r(O,"CODE",{});var J=l(x);A=n(J,"Dataset.map()"),J.forEach(t),P=n(O," function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in \u{1F917} Transformers. Where possible, try using "),T=r(O,"CODE",{});var X=l(T);S=n(X,"batched=True"),X.forEach(t),I=n(O," to get the most out of your preprocessing!"),O.forEach(t)},m(D,O){i(D,u,O),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I)},d(D){D&&t(u)}}}function Ag(K){let u,j,d,E,C,x,A,P;return{c(){u=o("p"),j=a("\u{1F64B} Don\u2019t worry if this is the first time you\u2019ve heard of precision and recall \u2014 we\u2019ll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out the "),d=o("code"),E=a("scikit-learn"),C=h(),x=o("a"),A=a("guides"),P=a("."),this.h()},l(T){u=r(T,"P",{});var S=l(u);j=n(S,"\u{1F64B} Don\u2019t worry if this is the first time you\u2019ve heard of precision and recall \u2014 we\u2019ll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out the "),d=r(S,"CODE",{});var I=l(d);E=n(I,"scikit-learn"),I.forEach(t),C=c(S),x=r(S,"A",{href:!0,rel:!0});var D=l(x);A=n(D,"guides"),D.forEach(t),P=n(S,"."),S.forEach(t),this.h()},h(){f(x,"href","https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"),f(x,"rel","nofollow")},m(T,S){i(T,u,S),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function Og(K){let u,j,d,E,C,x,A,P;return{c(){u=o("p"),j=a("\u270F\uFE0F "),d=o("strong"),E=a("Try it out!"),C=a(" Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for the "),x=o("code"),A=a("rouge2"),P=a(" metric.")},l(T){u=r(T,"P",{});var S=l(u);j=n(S,"\u270F\uFE0F "),d=r(S,"STRONG",{});var I=l(d);E=n(I,"Try it out!"),I.forEach(t),C=n(S," Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for the "),x=r(S,"CODE",{});var D=l(x);A=n(D,"rouge2"),D.forEach(t),P=n(S," metric."),S.forEach(t)},m(T,S){i(T,u,S),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function Cg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z;return E=new Vs({}),W=new z({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){u=o("h2"),j=o("a"),d=o("span"),v(E.$$.fragment),C=h(),x=o("span"),A=a("Fine-tuning mT5 with Keras"),P=h(),T=o("p"),S=a("Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),I=o("code"),D=a("mt5-small"),O=a(" checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),N=o("code"),J=a("AutoModelForSeq2SeqLM"),X=a(" class, which will automatically download and cache the weights:"),Q=h(),v(W.$$.fragment),this.h()},l(L){u=r(L,"H2",{class:!0});var U=l(u);j=r(U,"A",{id:!0,class:!0,href:!0});var ee=l(j);d=r(ee,"SPAN",{});var F=l(d);y(E.$$.fragment,F),F.forEach(t),ee.forEach(t),C=c(U),x=r(U,"SPAN",{});var B=l(x);A=n(B,"Fine-tuning mT5 with Keras"),B.forEach(t),U.forEach(t),P=c(L),T=r(L,"P",{});var te=l(T);S=n(te,"Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),I=r(te,"CODE",{});var M=l(I);D=n(M,"mt5-small"),M.forEach(t),O=n(te," checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),N=r(te,"CODE",{});var ie=l(N);J=n(ie,"AutoModelForSeq2SeqLM"),ie.forEach(t),X=n(te," class, which will automatically download and cache the weights:"),te.forEach(t),Q=c(L),y(W.$$.fragment,L),this.h()},h(){f(j,"id","finetuning-mt5-with-keras"),f(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(j,"href","#finetuning-mt5-with-keras"),f(u,"class","relative group")},m(L,U){i(L,u,U),s(u,j),s(j,d),k(E,d,null),s(u,C),s(u,x),s(x,A),i(L,P,U),i(L,T,U),s(T,S),s(T,I),s(I,D),s(T,O),s(T,N),s(N,J),s(T,X),i(L,Q,U),k(W,L,U),Z=!0},i(L){Z||(g(E.$$.fragment,L),g(W.$$.fragment,L),Z=!0)},o(L){_(E.$$.fragment,L),_(W.$$.fragment,L),Z=!1},d(L){L&&t(u),$(E),L&&t(P),L&&t(T),L&&t(Q),$(W,L)}}}function Pg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee;return E=new Vs({}),U=new z({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){u=o("h2"),j=o("a"),d=o("span"),v(E.$$.fragment),C=h(),x=o("span"),A=a("Fine-tuning mT5 with the "),P=o("code"),T=a("Trainer"),S=a(" API"),I=h(),D=o("p"),O=a("Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),N=o("code"),J=a("mt5-small"),X=a(" checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),Q=o("code"),W=a("AutoModelForSeq2SeqLM"),Z=a(" class, which will automatically download and cache the weights:"),L=h(),v(U.$$.fragment),this.h()},l(F){u=r(F,"H2",{class:!0});var B=l(u);j=r(B,"A",{id:!0,class:!0,href:!0});var te=l(j);d=r(te,"SPAN",{});var M=l(d);y(E.$$.fragment,M),M.forEach(t),te.forEach(t),C=c(B),x=r(B,"SPAN",{});var ie=l(x);A=n(ie,"Fine-tuning mT5 with the "),P=r(ie,"CODE",{});var Y=l(P);T=n(Y,"Trainer"),Y.forEach(t),S=n(ie," API"),ie.forEach(t),B.forEach(t),I=c(F),D=r(F,"P",{});var ae=l(D);O=n(ae,"Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),N=r(ae,"CODE",{});var de=l(N);J=n(de,"mt5-small"),de.forEach(t),X=n(ae," checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),Q=r(ae,"CODE",{});var we=l(Q);W=n(we,"AutoModelForSeq2SeqLM"),we.forEach(t),Z=n(ae," class, which will automatically download and cache the weights:"),ae.forEach(t),L=c(F),y(U.$$.fragment,F),this.h()},h(){f(j,"id","finetuning-mt5-with-the-trainer-api"),f(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(j,"href","#finetuning-mt5-with-the-trainer-api"),f(u,"class","relative group")},m(F,B){i(F,u,B),s(u,j),s(j,d),k(E,d,null),s(u,C),s(u,x),s(x,A),s(x,P),s(P,T),s(x,S),i(F,I,B),i(F,D,B),s(D,O),s(D,N),s(N,J),s(D,X),s(D,Q),s(Q,W),s(D,Z),i(F,L,B),k(U,F,B),ee=!0},i(F){ee||(g(E.$$.fragment,F),g(U.$$.fragment,F),ee=!0)},o(F){_(E.$$.fragment,F),_(U.$$.fragment,F),ee=!1},d(F){F&&t(u),$(E),F&&t(I),F&&t(D),F&&t(L),$(U,F)}}}function Lg(K){let u,j,d,E,C;return{c(){u=o("p"),j=a("\u{1F4A1} If you\u2019re wondering why you don\u2019t see any warnings about fine-tuning the model on a downstream task, that\u2019s because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in "),d=o("a"),E=a("Chapter 3"),C=a(", where the head of the pretrained model was replaced with a randomly initialized network."),this.h()},l(x){u=r(x,"P",{});var A=l(u);j=n(A,"\u{1F4A1} If you\u2019re wondering why you don\u2019t see any warnings about fine-tuning the model on a downstream task, that\u2019s because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in "),d=r(A,"A",{href:!0});var P=l(d);E=n(P,"Chapter 3"),P.forEach(t),C=n(A,", where the head of the pretrained model was replaced with a randomly initialized network."),A.forEach(t),this.h()},h(){f(d,"href","/course/chapter3")},m(x,A){i(x,u,A),s(u,j),s(u,d),s(d,E),s(u,C)},d(x){x&&t(u)}}}function fg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee,F,B,te,M,ie,Y,ae,de,we,Te,V,fe,me,oe,be,ke,se,_e,he,ge,w,R,ve,b,H,qe,pe,$e,Ce,re,ze,Ze,Ae,ne,le,Ie,ws,bs,vs,ys,zs,He,rs,es,ls,Ue,Ks,is,Oe,ks,Ds,Re,ps;return S=new z({props:{code:`from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments

batch_size = <span class="hljs-number">8</span>
num_train_epochs = <span class="hljs-number">8</span>
<span class="hljs-comment"># Show the training loss with every epoch</span>
logging_steps = <span class="hljs-built_in">len</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">5.6e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=<span class="hljs-number">0.01</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    num_train_epochs=num_train_epochs,
    predict_with_generate=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
    push_to_hub=<span class="hljs-literal">True</span>,
)`}}),Re=new z({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-comment"># Decode generated summaries into text</span>
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    <span class="hljs-comment"># Decode reference summaries into text</span>
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    <span class="hljs-comment"># Compute ROUGE scores</span>
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Extract the median scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-keyword">return</span> {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),{c(){u=o("p"),j=a("We\u2019ll need to generate summaries in order to compute ROUGE scores during training. Fortunately, \u{1F917} Transformers provides dedicated "),d=o("code"),E=a("Seq2SeqTrainingArguments"),C=a(" and "),x=o("code"),A=a("Seq2SeqTrainer"),P=a(" classes that can do this for us automatically! To see how this works, let\u2019s first define the hyperparameters and other arguments for our experiments:"),T=h(),v(S.$$.fragment),I=h(),D=o("p"),O=a("Here, the "),N=o("code"),J=a("predict_with_generate"),X=a(" argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in "),Q=o("a"),W=a("Chapter 1"),Z=a(", the decoder performs inference by predicting tokens one by one, and this is implemented by the model\u2019s "),L=o("code"),U=a("generate()"),ee=a(" method. Setting "),F=o("code"),B=a("predict_with_generate=True"),te=a(" tells the "),M=o("code"),ie=a("Seq2SeqTrainer"),Y=a(" to use that method for evaluation. We\u2019ve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we\u2019ve set the "),ae=o("code"),de=a("save_total_limit"),we=a(" option to only save up to 3 checkpoints during training \u2014 this is because even the \u201Csmall\u201D version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save."),Te=h(),V=o("p"),fe=a("The "),me=o("code"),oe=a("push_to_hub=True"),be=a(" argument will allow us to push the model to the Hub after training; you\u2019ll find the repository under your user profile in the location defined by "),ke=o("code"),se=a("output_dir"),_e=a(". Note that you can specify the name of the repository you want to push to with the "),he=o("code"),ge=a("hub_model_id"),w=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),R=o("a"),ve=o("code"),b=a("huggingface-course"),H=a(" organization"),qe=a(", we added "),pe=o("code"),$e=a('hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),Ce=a(" to "),re=o("code"),ze=a("Seq2SeqTrainingArguments"),Ze=a("."),Ae=h(),ne=o("p"),le=a("The next thing we need to do is provide the trainer with a "),Ie=o("code"),ws=a("compute_metrics()"),bs=a(" function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling "),vs=o("code"),ys=a("rouge_score.compute()"),zs=a(" on the model\u2019s predictions, since we need to "),He=o("em"),rs=a("decode"),es=a(" the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the "),ls=o("code"),Ue=a("sent_tokenize()"),Ks=a(" function from "),is=o("code"),Oe=a("nltk"),ks=a(" to separate the summary sentences with newlines:"),Ds=h(),v(Re.$$.fragment),this.h()},l(G){u=r(G,"P",{});var ce=l(u);j=n(ce,"We\u2019ll need to generate summaries in order to compute ROUGE scores during training. Fortunately, \u{1F917} Transformers provides dedicated "),d=r(ce,"CODE",{});var Me=l(d);E=n(Me,"Seq2SeqTrainingArguments"),Me.forEach(t),C=n(ce," and "),x=r(ce,"CODE",{});var Ys=l(x);A=n(Ys,"Seq2SeqTrainer"),Ys.forEach(t),P=n(ce," classes that can do this for us automatically! To see how this works, let\u2019s first define the hyperparameters and other arguments for our experiments:"),ce.forEach(t),T=c(G),y(S.$$.fragment,G),I=c(G),D=r(G,"P",{});var ue=l(D);O=n(ue,"Here, the "),N=r(ue,"CODE",{});var ms=l(N);J=n(ms,"predict_with_generate"),ms.forEach(t),X=n(ue," argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in "),Q=r(ue,"A",{href:!0});var ss=l(Q);W=n(ss,"Chapter 1"),ss.forEach(t),Z=n(ue,", the decoder performs inference by predicting tokens one by one, and this is implemented by the model\u2019s "),L=r(ue,"CODE",{});var Ss=l(L);U=n(Ss,"generate()"),Ss.forEach(t),ee=n(ue," method. Setting "),F=r(ue,"CODE",{});var De=l(F);B=n(De,"predict_with_generate=True"),De.forEach(t),te=n(ue," tells the "),M=r(ue,"CODE",{});var Js=l(M);ie=n(Js,"Seq2SeqTrainer"),Js.forEach(t),Y=n(ue," to use that method for evaluation. We\u2019ve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we\u2019ve set the "),ae=r(ue,"CODE",{});var ts=l(ae);de=n(ts,"save_total_limit"),ts.forEach(t),we=n(ue," option to only save up to 3 checkpoints during training \u2014 this is because even the \u201Csmall\u201D version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save."),ue.forEach(t),Te=c(G),V=r(G,"P",{});var je=l(V);fe=n(je,"The "),me=r(je,"CODE",{});var Xs=l(me);oe=n(Xs,"push_to_hub=True"),Xs.forEach(t),be=n(je," argument will allow us to push the model to the Hub after training; you\u2019ll find the repository under your user profile in the location defined by "),ke=r(je,"CODE",{});var We=l(ke);se=n(We,"output_dir"),We.forEach(t),_e=n(je,". Note that you can specify the name of the repository you want to push to with the "),he=r(je,"CODE",{});var Qs=l(he);ge=n(Qs,"hub_model_id"),Qs.forEach(t),w=n(je," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),R=r(je,"A",{href:!0,rel:!0});var Fe=l(R);ve=r(Fe,"CODE",{});var Zs=l(ve);b=n(Zs,"huggingface-course"),Zs.forEach(t),H=n(Fe," organization"),Fe.forEach(t),qe=n(je,", we added "),pe=r(je,"CODE",{});var Be=l(pe);$e=n(Be,'hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),Be.forEach(t),Ce=n(je," to "),re=r(je,"CODE",{});var et=l(re);ze=n(et,"Seq2SeqTrainingArguments"),et.forEach(t),Ze=n(je,"."),je.forEach(t),Ae=c(G),ne=r(G,"P",{});var ye=l(ne);le=n(ye,"The next thing we need to do is provide the trainer with a "),Ie=r(ye,"CODE",{});var $s=l(Ie);ws=n($s,"compute_metrics()"),$s.forEach(t),bs=n(ye," function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling "),vs=r(ye,"CODE",{});var Se=l(vs);ys=n(Se,"rouge_score.compute()"),Se.forEach(t),zs=n(ye," on the model\u2019s predictions, since we need to "),He=r(ye,"EM",{});var vt=l(He);rs=n(vt,"decode"),vt.forEach(t),es=n(ye," the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the "),ls=r(ye,"CODE",{});var js=l(ls);Ue=n(js,"sent_tokenize()"),js.forEach(t),Ks=n(ye," function from "),is=r(ye,"CODE",{});var xs=l(is);Oe=n(xs,"nltk"),xs.forEach(t),ks=n(ye," to separate the summary sentences with newlines:"),ye.forEach(t),Ds=c(G),y(Re.$$.fragment,G),this.h()},h(){f(Q,"href","/course/chapter1"),f(R,"href","https://huggingface.co/huggingface-course"),f(R,"rel","nofollow")},m(G,ce){i(G,u,ce),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),i(G,T,ce),k(S,G,ce),i(G,I,ce),i(G,D,ce),s(D,O),s(D,N),s(N,J),s(D,X),s(D,Q),s(Q,W),s(D,Z),s(D,L),s(L,U),s(D,ee),s(D,F),s(F,B),s(D,te),s(D,M),s(M,ie),s(D,Y),s(D,ae),s(ae,de),s(D,we),i(G,Te,ce),i(G,V,ce),s(V,fe),s(V,me),s(me,oe),s(V,be),s(V,ke),s(ke,se),s(V,_e),s(V,he),s(he,ge),s(V,w),s(V,R),s(R,ve),s(ve,b),s(R,H),s(V,qe),s(V,pe),s(pe,$e),s(V,Ce),s(V,re),s(re,ze),s(V,Ze),i(G,Ae,ce),i(G,ne,ce),s(ne,le),s(ne,Ie),s(Ie,ws),s(ne,bs),s(ne,vs),s(vs,ys),s(ne,zs),s(ne,He),s(He,rs),s(ne,es),s(ne,ls),s(ls,Ue),s(ne,Ks),s(ne,is),s(is,Oe),s(ne,ks),i(G,Ds,ce),k(Re,G,ce),ps=!0},i(G){ps||(g(S.$$.fragment,G),g(Re.$$.fragment,G),ps=!0)},o(G){_(S.$$.fragment,G),_(Re.$$.fragment,G),ps=!1},d(G){G&&t(u),G&&t(T),$(S,G),G&&t(I),G&&t(D),G&&t(Te),G&&t(V),G&&t(Ae),G&&t(ne),G&&t(Ds),$(Re,G)}}}function Ig(K){let u,j;return u=new z({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),j=!0},i(d){j||(g(u.$$.fragment,d),j=!0)},o(d){_(u.$$.fragment,d),j=!1},d(d){$(u,d)}}}function Rg(K){let u,j;return u=new z({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),j=!0},i(d){j||(g(u.$$.fragment,d),j=!0)},o(d){_(u.$$.fragment,d),j=!1},d(d){$(u,d)}}}function Fg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee,F,B,te,M,ie,Y,ae,de,we,Te,V,fe,me,oe,be,ke,se,_e,he,ge;return O=new z({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">8</span>,
)
tf_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
)`}}),W=new z({props:{code:`from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_train_epochs = <span class="hljs-number">8</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5.6e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),M=new z({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>, tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=<span class="hljs-number">8</span>
)`}}),fe=new z({props:{code:`from tqdm import tqdm
import numpy as np

all_preds = []
all_labels = []
for batch in tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = batch["labels"].numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`,highlighted:`<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

all_preds = []
all_labels = []
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`}}),se=new z({props:{code:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}`,highlighted:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
)
result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
{k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),he=new z({props:{code:"{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}",highlighted:'{&#x27;rouge1&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rouge2&#x27;: <span class="hljs-number">25.4386</span>, &#x27;rougeL&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rougeLsum&#x27;: <span class="hljs-number">31.4815</span>}'}}),{c(){u=o("p"),j=a("We\u2019re almost ready to train! We just need to convert our datasets to "),d=o("code"),E=a("tf.data.Dataset"),C=a("s using the data collator we defined above, and then "),x=o("code"),A=a("compile()"),P=a(" and "),T=o("code"),S=a("fit()"),I=a(" the model. First, the datasets:"),D=h(),v(O.$$.fragment),N=h(),J=o("p"),X=a("Now, we define our training hyperparameters and compile:"),Q=h(),v(W.$$.fragment),Z=h(),L=o("p"),U=a("And finally, we fit the model. We use a "),ee=o("code"),F=a("PushToHubCallback"),B=a(" to save the model to the Hub after each epoch, which will allow us to use it for inference later:"),te=h(),v(M.$$.fragment),ie=h(),Y=o("p"),ae=a("We got some loss values during training, but really we\u2019d like to see the ROUGE metrics we computed earlier. To get those metrics, we\u2019ll need to generate outputs from the model and convert them to strings. Let\u2019s build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to"),de=o("code"),we=a("!pip install tqdm"),Te=a("):"),V=h(),v(fe.$$.fragment),me=h(),oe=o("p"),be=a("Once we have our lists of label and prediction strings, computing the ROUGE score is easy:"),ke=h(),v(se.$$.fragment),_e=h(),v(he.$$.fragment)},l(w){u=r(w,"P",{});var R=l(u);j=n(R,"We\u2019re almost ready to train! We just need to convert our datasets to "),d=r(R,"CODE",{});var ve=l(d);E=n(ve,"tf.data.Dataset"),ve.forEach(t),C=n(R,"s using the data collator we defined above, and then "),x=r(R,"CODE",{});var b=l(x);A=n(b,"compile()"),b.forEach(t),P=n(R," and "),T=r(R,"CODE",{});var H=l(T);S=n(H,"fit()"),H.forEach(t),I=n(R," the model. First, the datasets:"),R.forEach(t),D=c(w),y(O.$$.fragment,w),N=c(w),J=r(w,"P",{});var qe=l(J);X=n(qe,"Now, we define our training hyperparameters and compile:"),qe.forEach(t),Q=c(w),y(W.$$.fragment,w),Z=c(w),L=r(w,"P",{});var pe=l(L);U=n(pe,"And finally, we fit the model. We use a "),ee=r(pe,"CODE",{});var $e=l(ee);F=n($e,"PushToHubCallback"),$e.forEach(t),B=n(pe," to save the model to the Hub after each epoch, which will allow us to use it for inference later:"),pe.forEach(t),te=c(w),y(M.$$.fragment,w),ie=c(w),Y=r(w,"P",{});var Ce=l(Y);ae=n(Ce,"We got some loss values during training, but really we\u2019d like to see the ROUGE metrics we computed earlier. To get those metrics, we\u2019ll need to generate outputs from the model and convert them to strings. Let\u2019s build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to"),de=r(Ce,"CODE",{});var re=l(de);we=n(re,"!pip install tqdm"),re.forEach(t),Te=n(Ce,"):"),Ce.forEach(t),V=c(w),y(fe.$$.fragment,w),me=c(w),oe=r(w,"P",{});var ze=l(oe);be=n(ze,"Once we have our lists of label and prediction strings, computing the ROUGE score is easy:"),ze.forEach(t),ke=c(w),y(se.$$.fragment,w),_e=c(w),y(he.$$.fragment,w)},m(w,R){i(w,u,R),s(u,j),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I),i(w,D,R),k(O,w,R),i(w,N,R),i(w,J,R),s(J,X),i(w,Q,R),k(W,w,R),i(w,Z,R),i(w,L,R),s(L,U),s(L,ee),s(ee,F),s(L,B),i(w,te,R),k(M,w,R),i(w,ie,R),i(w,Y,R),s(Y,ae),s(Y,de),s(de,we),s(Y,Te),i(w,V,R),k(fe,w,R),i(w,me,R),i(w,oe,R),s(oe,be),i(w,ke,R),k(se,w,R),i(w,_e,R),k(he,w,R),ge=!0},i(w){ge||(g(O.$$.fragment,w),g(W.$$.fragment,w),g(M.$$.fragment,w),g(fe.$$.fragment,w),g(se.$$.fragment,w),g(he.$$.fragment,w),ge=!0)},o(w){_(O.$$.fragment,w),_(W.$$.fragment,w),_(M.$$.fragment,w),_(fe.$$.fragment,w),_(se.$$.fragment,w),_(he.$$.fragment,w),ge=!1},d(w){w&&t(u),w&&t(D),$(O,w),w&&t(N),w&&t(J),w&&t(Q),$(W,w),w&&t(Z),w&&t(L),w&&t(te),$(M,w),w&&t(ie),w&&t(Y),w&&t(V),$(fe,w),w&&t(me),w&&t(oe),w&&t(ke),$(se,w),w&&t(_e),$(he,w)}}}function Ng(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee,F,B,te,M,ie,Y,ae,de,we,Te,V,fe,me,oe,be,ke,se,_e,he,ge,w,R,ve;return E=new z({props:{code:`from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`}}),T=new z({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),Q=new z({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),Z=new z({props:{code:`{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}`,highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">3.028524398803711</span>,
 <span class="hljs-string">&#x27;eval_rouge1&#x27;</span>: <span class="hljs-number">16.9728</span>,
 <span class="hljs-string">&#x27;eval_rouge2&#x27;</span>: <span class="hljs-number">8.2969</span>,
 <span class="hljs-string">&#x27;eval_rougeL&#x27;</span>: <span class="hljs-number">16.8366</span>,
 <span class="hljs-string">&#x27;eval_rougeLsum&#x27;</span>: <span class="hljs-number">16.851</span>,
 <span class="hljs-string">&#x27;eval_gen_len&#x27;</span>: <span class="hljs-number">10.1597</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">6.1054</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">38.982</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">4.914</span>}`}}),B=new z({props:{code:'trainer.push_to_hub(commit_message="Training complete", tags="summarization")',highlighted:'trainer.push_to_hub(<span class="hljs-attribute">commit_message</span>=<span class="hljs-string">&quot;Training complete&quot;</span>, <span class="hljs-attribute">tags</span>=<span class="hljs-string">&quot;summarization&quot;</span>)'}}),M=new z({props:{code:"'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0&#x27;</span>'}}),{c(){u=o("p"),j=a("We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments:"),d=h(),v(E.$$.fragment),C=h(),x=o("p"),A=a("and launch our training run:"),P=h(),v(T.$$.fragment),S=h(),I=o("p"),D=a("During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running "),O=o("code"),N=a("Trainer.evaluate()"),J=a(":"),X=h(),v(Q.$$.fragment),W=h(),v(Z.$$.fragment),L=h(),U=o("p"),ee=a("From the scores we can see that our model has handily outperformed our lead-3 baseline \u2014 nice! The final thing to do is push the model weights to the Hub, as follows:"),F=h(),v(B.$$.fragment),te=h(),v(M.$$.fragment),ie=h(),Y=o("p"),ae=a("This will save the checkpoint and configuration files to "),de=o("code"),we=a("output_dir"),Te=a(", before uploading all the files to the Hub. By specifying the "),V=o("code"),fe=a("tags"),me=a(" argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the "),oe=o("a"),be=a("\u{1F917} Hub documentation"),ke=a("). The output from "),se=o("code"),_e=a("trainer.push_to_hub()"),he=a(" is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository!"),ge=h(),w=o("p"),R=a("To wrap up this section, let\u2019s take a look at how we can also fine-tune mT5 using the low-level features provided by \u{1F917} Accelerate."),this.h()},l(b){u=r(b,"P",{});var H=l(u);j=n(H,"We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments:"),H.forEach(t),d=c(b),y(E.$$.fragment,b),C=c(b),x=r(b,"P",{});var qe=l(x);A=n(qe,"and launch our training run:"),qe.forEach(t),P=c(b),y(T.$$.fragment,b),S=c(b),I=r(b,"P",{});var pe=l(I);D=n(pe,"During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running "),O=r(pe,"CODE",{});var $e=l(O);N=n($e,"Trainer.evaluate()"),$e.forEach(t),J=n(pe,":"),pe.forEach(t),X=c(b),y(Q.$$.fragment,b),W=c(b),y(Z.$$.fragment,b),L=c(b),U=r(b,"P",{});var Ce=l(U);ee=n(Ce,"From the scores we can see that our model has handily outperformed our lead-3 baseline \u2014 nice! The final thing to do is push the model weights to the Hub, as follows:"),Ce.forEach(t),F=c(b),y(B.$$.fragment,b),te=c(b),y(M.$$.fragment,b),ie=c(b),Y=r(b,"P",{});var re=l(Y);ae=n(re,"This will save the checkpoint and configuration files to "),de=r(re,"CODE",{});var ze=l(de);we=n(ze,"output_dir"),ze.forEach(t),Te=n(re,", before uploading all the files to the Hub. By specifying the "),V=r(re,"CODE",{});var Ze=l(V);fe=n(Ze,"tags"),Ze.forEach(t),me=n(re," argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the "),oe=r(re,"A",{href:!0,rel:!0});var Ae=l(oe);be=n(Ae,"\u{1F917} Hub documentation"),Ae.forEach(t),ke=n(re,"). The output from "),se=r(re,"CODE",{});var ne=l(se);_e=n(ne,"trainer.push_to_hub()"),ne.forEach(t),he=n(re," is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository!"),re.forEach(t),ge=c(b),w=r(b,"P",{});var le=l(w);R=n(le,"To wrap up this section, let\u2019s take a look at how we can also fine-tune mT5 using the low-level features provided by \u{1F917} Accelerate."),le.forEach(t),this.h()},h(){f(oe,"href","https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined"),f(oe,"rel","nofollow")},m(b,H){i(b,u,H),s(u,j),i(b,d,H),k(E,b,H),i(b,C,H),i(b,x,H),s(x,A),i(b,P,H),k(T,b,H),i(b,S,H),i(b,I,H),s(I,D),s(I,O),s(O,N),s(I,J),i(b,X,H),k(Q,b,H),i(b,W,H),k(Z,b,H),i(b,L,H),i(b,U,H),s(U,ee),i(b,F,H),k(B,b,H),i(b,te,H),k(M,b,H),i(b,ie,H),i(b,Y,H),s(Y,ae),s(Y,de),s(de,we),s(Y,Te),s(Y,V),s(V,fe),s(Y,me),s(Y,oe),s(oe,be),s(Y,ke),s(Y,se),s(se,_e),s(Y,he),i(b,ge,H),i(b,w,H),s(w,R),ve=!0},i(b){ve||(g(E.$$.fragment,b),g(T.$$.fragment,b),g(Q.$$.fragment,b),g(Z.$$.fragment,b),g(B.$$.fragment,b),g(M.$$.fragment,b),ve=!0)},o(b){_(E.$$.fragment,b),_(T.$$.fragment,b),_(Q.$$.fragment,b),_(Z.$$.fragment,b),_(B.$$.fragment,b),_(M.$$.fragment,b),ve=!1},d(b){b&&t(u),b&&t(d),$(E,b),b&&t(C),b&&t(x),b&&t(P),$(T,b),b&&t(S),b&&t(I),b&&t(X),$(Q,b),b&&t(W),$(Z,b),b&&t(L),b&&t(U),b&&t(F),$(B,b),b&&t(te),$(M,b),b&&t(ie),b&&t(Y),b&&t(ge),b&&t(w)}}}function gg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee,F,B,te,M,ie,Y,ae,de,we,Te,V,fe,me,oe,be,ke,se,_e,he,ge,w,R,ve,b,H,qe,pe,$e,Ce,re,ze,Ze,Ae,ne,le,Ie,ws,bs,vs,ys,zs,He,rs,es,ls,Ue,Ks,is,Oe,ks,Ds,Re,ps,G,ce,Me,Ys,ue,ms,ss,Ss,De,Js,ts,je,Xs,We,Qs,Fe,Zs,Be,et,ye,$s,Se,vt,js,xs,pn,yt,st,fa,hs,ga,kt,Ft,_a,tt,Nt,Ve,wa,xe,mn,Gt,ba,Es,cs,Ht,at,Ut,Mt,hn,$t,jt,cn,xt,Ke,nt,va,Ts,ya,Et,un,Tt,ot,ka,qs,$a,Ye,as,Wt,Bt,dn,Vt,Kt,fn,Yt,Jt,gn,ja,As,rt,lt,Xt,us,xa,Os,ds,Je,yo,Qt,it,ko,Zt;return E=new Vs({}),U=new Vs({}),me=new z({props:{code:'tokenized_datasets.set_format("torch")',highlighted:'tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)'}}),w=new z({props:{code:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",highlighted:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"}}),qe=new z({props:{code:`from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

batch_size = <span class="hljs-number">8</span>
train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=batch_size
)`}}),ne=new z({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`}}),He=new z({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),es=new ln({props:{$$slots:{default:[Gg]},$$scope:{ctx:K}}}),De=new z({props:{code:`from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">10</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),We=new z({props:{code:`def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [label.strip() <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(pred)) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(label)) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-keyword">return</span> preds, labels`}}),st=new z({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;test-bert-finetuned-squad-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),hs=new z({props:{code:"'lewtun/mt5-finetuned-amazon-en-es-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/mt5-finetuned-amazon-en-es-accelerate&#x27;</span>'}}),tt=new z({props:{code:`from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = <span class="hljs-string">&quot;results-mt5-finetuned-squad-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),at=new Vs({}),us=new z({props:{code:`from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Training</span>
    model.train()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
                attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )
            labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

            <span class="hljs-comment"># If we did not pad to max length, we need to pad the labels too</span>
            labels = accelerator.pad_across_processes(
                batch[<span class="hljs-string">&quot;labels&quot;</span>], dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
            labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(generated_tokens, <span class="hljs-built_in">tuple</span>):
                generated_tokens = generated_tokens[<span class="hljs-number">0</span>]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    <span class="hljs-comment"># Compute metrics</span>
    result = rouge_score.compute()
    <span class="hljs-comment"># Extract the median ROUGE scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch}</span>:&quot;</span>, result)

    <span class="hljs-comment"># Save and upload</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),Os=new z({props:{code:`Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}`,highlighted:`Epoch <span class="hljs-number">0</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">5.6351</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">1.1625</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">5.4866</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">5.5005</span>}
Epoch <span class="hljs-number">1</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">9.8646</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.4106</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">9.9439</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">9.9306</span>}
Epoch <span class="hljs-number">2</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.0872</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.3273</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.0508</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">10.9468</span>}
Epoch <span class="hljs-number">3</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.8587</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">4.8167</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.7986</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">11.7518</span>}
Epoch <span class="hljs-number">4</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9842</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.5887</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.7546</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.7029</span>}
Epoch <span class="hljs-number">5</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.4628</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.4598</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.312</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.2913</span>}
Epoch <span class="hljs-number">6</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9131</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.8914</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.6896</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.5701</span>}
Epoch <span class="hljs-number">7</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.3079</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.2994</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.1536</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.1194</span>}
Epoch <span class="hljs-number">8</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.96</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.5998</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.9123</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.7744</span>}
Epoch <span class="hljs-number">9</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">14.1192</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">7.0059</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">14.1172</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.9509</span>}`}}),{c(){u=o("h2"),j=o("a"),d=o("span"),v(E.$$.fragment),C=h(),x=o("span"),A=a("Fine-tuning mT5 with \u{1F917} Accelerate"),P=h(),T=o("p"),S=a("Fine-tuning our model with \u{1F917} Accelerate is very similar to the text classification example we encountered in "),I=o("a"),D=a("Chapter 3"),O=a(". The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the "),N=o("code"),J=a("Seq2SeqTrainer"),X=a(" took care of the generation for us). Let\u2019s take a look how we can implement these two requirements within \u{1F917} Accelerate!"),Q=h(),W=o("h3"),Z=o("a"),L=o("span"),v(U.$$.fragment),ee=h(),F=o("span"),B=a("Preparing everything for training"),te=h(),M=o("p"),ie=a("The first thing we need to do is create a "),Y=o("code"),ae=a("DataLoader"),de=a(" for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to "),we=o("code"),Te=a('"torch"'),V=a(" in our datasets:"),fe=h(),v(me.$$.fragment),oe=h(),be=o("p"),ke=a("Now that we\u2019ve got datasets consisting of just tensors, the next thing to do is instantiate the "),se=o("code"),_e=a("DataCollatorForSeq2Seq"),he=a(" again. For this we need to provide a fresh version of the model, so let\u2019s load it again from our cache:"),ge=h(),v(w.$$.fragment),R=h(),ve=o("p"),b=a("We can then instantiate the data collator and use this to define our dataloaders:"),H=h(),v(qe.$$.fragment),pe=h(),$e=o("p"),Ce=a("The next thing to do is define the optimizer we want to use. As in our other examples, we\u2019ll use "),re=o("code"),ze=a("AdamW"),Ze=a(", which works well for most problems:"),Ae=h(),v(ne.$$.fragment),le=h(),Ie=o("p"),ws=a("Finally, we feed our model, optimizer, and dataloaders to the "),bs=o("code"),vs=a("accelerator.prepare()"),ys=a(" method:"),zs=h(),v(He.$$.fragment),rs=h(),v(es.$$.fragment),ls=h(),Ue=o("p"),Ks=a("Now that we\u2019ve prepared our objects, there are three remaining things to do:"),is=h(),Oe=o("ul"),ks=o("li"),Ds=a("Define the learning rate schedule."),Re=h(),ps=o("li"),G=a("Implement a function to post-process the summaries for evaluation."),ce=h(),Me=o("li"),Ys=a("Create a repository on the Hub that we can push our model to."),ue=h(),ms=o("p"),ss=a("For the learning rate schedule, we\u2019ll use the standard linear one from previous sections:"),Ss=h(),v(De.$$.fragment),Js=h(),ts=o("p"),je=a("For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code:"),Xs=h(),v(We.$$.fragment),Qs=h(),Fe=o("p"),Zs=a("This should look familiar to you if you recall how we defined the "),Be=o("code"),et=a("compute_metrics()"),ye=a(" function of the "),$s=o("code"),Se=a("Seq2SeqTrainer"),vt=a("."),js=h(),xs=o("p"),pn=a("Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled \u{1F917} Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:"),yt=h(),v(st.$$.fragment),fa=h(),v(hs.$$.fragment),ga=h(),kt=o("p"),Ft=a("Now we can use this repository name to clone a local version to our results directory that will store the training artifacts:"),_a=h(),v(tt.$$.fragment),Nt=h(),Ve=o("p"),wa=a("This will allow us to push the artifacts back to the Hub by calling the "),xe=o("code"),mn=a("repo.push_to_hub()"),Gt=a(" method during training! Let\u2019s now wrap up our analysis by writing out the training loop."),ba=h(),Es=o("h3"),cs=o("a"),Ht=o("span"),v(at.$$.fragment),Ut=h(),Mt=o("span"),hn=a("Training loop"),$t=h(),jt=o("p"),cn=a("The training loop for summarization is quite similar to the other \u{1F917} Accelerate examples that we\u2019ve encountered and is roughly split into four main steps:"),xt=h(),Ke=o("ol"),nt=o("li"),va=a("Train the model by iterating over all the examples in "),Ts=o("code"),ya=a("train_dataloader"),Et=a(" for each epoch."),un=h(),Tt=o("li"),ot=a("Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text."),ka=h(),qs=o("li"),$a=a("Compute the ROUGE scores using the same techniques we saw earlier."),Ye=h(),as=o("li"),Wt=a("Save the checkpoints and push everything to the Hub. Here we rely on the nifty "),Bt=o("code"),dn=a("blocking=False"),Vt=a(" argument of the "),Kt=o("code"),fn=a("Repository"),Yt=a(" object so that we can push the checkpoints per epoch "),Jt=o("em"),gn=a("asynchronously"),ja=a(". This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model!"),As=h(),rt=o("p"),lt=a("These steps can be seen in the following block of code:"),Xt=h(),v(us.$$.fragment),xa=h(),v(Os.$$.fragment),ds=h(),Je=o("p"),yo=a("And that\u2019s it! Once you run this, you\u2019ll have a model and results that are pretty similar to the ones we obtained with the "),Qt=o("code"),it=a("Trainer"),ko=a("."),this.h()},l(m){u=r(m,"H2",{class:!0});var q=l(u);j=r(q,"A",{id:!0,class:!0,href:!0});var Ea=l(j);d=r(Ea,"SPAN",{});var $o=l(d);y(E.$$.fragment,$o),$o.forEach(t),Ea.forEach(t),C=c(q),x=r(q,"SPAN",{});var jo=l(x);A=n(jo,"Fine-tuning mT5 with \u{1F917} Accelerate"),jo.forEach(t),q.forEach(t),P=c(m),T=r(m,"P",{});var pt=l(T);S=n(pt,"Fine-tuning our model with \u{1F917} Accelerate is very similar to the text classification example we encountered in "),I=r(pt,"A",{href:!0});var qt=l(I);D=n(qt,"Chapter 3"),qt.forEach(t),O=n(pt,". The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the "),N=r(pt,"CODE",{});var _n=l(N);J=n(_n,"Seq2SeqTrainer"),_n.forEach(t),X=n(pt," took care of the generation for us). Let\u2019s take a look how we can implement these two requirements within \u{1F917} Accelerate!"),pt.forEach(t),Q=c(m),W=r(m,"H3",{class:!0});var mt=l(W);Z=r(mt,"A",{id:!0,class:!0,href:!0});var xo=l(Z);L=r(xo,"SPAN",{});var wn=l(L);y(U.$$.fragment,wn),wn.forEach(t),xo.forEach(t),ee=c(mt),F=r(mt,"SPAN",{});var Cs=l(F);B=n(Cs,"Preparing everything for training"),Cs.forEach(t),mt.forEach(t),te=c(m),M=r(m,"P",{});var ns=l(M);ie=n(ns,"The first thing we need to do is create a "),Y=r(ns,"CODE",{});var Ta=l(Y);ae=n(Ta,"DataLoader"),Ta.forEach(t),de=n(ns," for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to "),we=r(ns,"CODE",{});var zt=l(we);Te=n(zt,'"torch"'),zt.forEach(t),V=n(ns," in our datasets:"),ns.forEach(t),fe=c(m),y(me.$$.fragment,m),oe=c(m),be=r(m,"P",{});var qa=l(be);ke=n(qa,"Now that we\u2019ve got datasets consisting of just tensors, the next thing to do is instantiate the "),se=r(qa,"CODE",{});var za=l(se);_e=n(za,"DataCollatorForSeq2Seq"),za.forEach(t),he=n(qa," again. For this we need to provide a fresh version of the model, so let\u2019s load it again from our cache:"),qa.forEach(t),ge=c(m),y(w.$$.fragment,m),R=c(m),ve=r(m,"P",{});var Eo=l(ve);b=n(Eo,"We can then instantiate the data collator and use this to define our dataloaders:"),Eo.forEach(t),H=c(m),y(qe.$$.fragment,m),pe=c(m),$e=r(m,"P",{});var ea=l($e);Ce=n(ea,"The next thing to do is define the optimizer we want to use. As in our other examples, we\u2019ll use "),re=r(ea,"CODE",{});var ht=l(re);ze=n(ht,"AdamW"),ht.forEach(t),Ze=n(ea,", which works well for most problems:"),ea.forEach(t),Ae=c(m),y(ne.$$.fragment,m),le=c(m),Ie=r(m,"P",{});var Da=l(Ie);ws=n(Da,"Finally, we feed our model, optimizer, and dataloaders to the "),bs=r(Da,"CODE",{});var sa=l(bs);vs=n(sa,"accelerator.prepare()"),sa.forEach(t),ys=n(Da," method:"),Da.forEach(t),zs=c(m),y(He.$$.fragment,m),rs=c(m),y(es.$$.fragment,m),ls=c(m),Ue=r(m,"P",{});var To=l(Ue);Ks=n(To,"Now that we\u2019ve prepared our objects, there are three remaining things to do:"),To.forEach(t),is=c(m),Oe=r(m,"UL",{});var Dt=l(Oe);ks=r(Dt,"LI",{});var bn=l(ks);Ds=n(bn,"Define the learning rate schedule."),bn.forEach(t),Re=c(Dt),ps=r(Dt,"LI",{});var ct=l(ps);G=n(ct,"Implement a function to post-process the summaries for evaluation."),ct.forEach(t),ce=c(Dt),Me=r(Dt,"LI",{});var Sa=l(Me);Ys=n(Sa,"Create a repository on the Hub that we can push our model to."),Sa.forEach(t),Dt.forEach(t),ue=c(m),ms=r(m,"P",{});var Ps=l(ms);ss=n(Ps,"For the learning rate schedule, we\u2019ll use the standard linear one from previous sections:"),Ps.forEach(t),Ss=c(m),y(De.$$.fragment,m),Js=c(m),ts=r(m,"P",{});var ta=l(ts);je=n(ta,"For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code:"),ta.forEach(t),Xs=c(m),y(We.$$.fragment,m),Qs=c(m),Fe=r(m,"P",{});var St=l(Fe);Zs=n(St,"This should look familiar to you if you recall how we defined the "),Be=r(St,"CODE",{});var qo=l(Be);et=n(qo,"compute_metrics()"),qo.forEach(t),ye=n(St," function of the "),$s=r(St,"CODE",{});var Aa=l($s);Se=n(Aa,"Seq2SeqTrainer"),Aa.forEach(t),vt=n(St,"."),St.forEach(t),js=c(m),xs=r(m,"P",{});var zo=l(xs);pn=n(zo,"Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled \u{1F917} Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:"),zo.forEach(t),yt=c(m),y(st.$$.fragment,m),fa=c(m),y(hs.$$.fragment,m),ga=c(m),kt=r(m,"P",{});var Do=l(kt);Ft=n(Do,"Now we can use this repository name to clone a local version to our results directory that will store the training artifacts:"),Do.forEach(t),_a=c(m),y(tt.$$.fragment,m),Nt=c(m),Ve=r(m,"P",{});var ut=l(Ve);wa=n(ut,"This will allow us to push the artifacts back to the Hub by calling the "),xe=r(ut,"CODE",{});var So=l(xe);mn=n(So,"repo.push_to_hub()"),So.forEach(t),Gt=n(ut," method during training! Let\u2019s now wrap up our analysis by writing out the training loop."),ut.forEach(t),ba=c(m),Es=r(m,"H3",{class:!0});var Oa=l(Es);cs=r(Oa,"A",{id:!0,class:!0,href:!0});var Ne=l(cs);Ht=r(Ne,"SPAN",{});var Ls=l(Ht);y(at.$$.fragment,Ls),Ls.forEach(t),Ne.forEach(t),Ut=c(Oa),Mt=r(Oa,"SPAN",{});var aa=l(Mt);hn=n(aa,"Training loop"),aa.forEach(t),Oa.forEach(t),$t=c(m),jt=r(m,"P",{});var At=l(jt);cn=n(At,"The training loop for summarization is quite similar to the other \u{1F917} Accelerate examples that we\u2019ve encountered and is roughly split into four main steps:"),At.forEach(t),xt=c(m),Ke=r(m,"OL",{});var Is=l(Ke);nt=r(Is,"LI",{});var Ca=l(nt);va=n(Ca,"Train the model by iterating over all the examples in "),Ts=r(Ca,"CODE",{});var Pa=l(Ts);ya=n(Pa,"train_dataloader"),Pa.forEach(t),Et=n(Ca," for each epoch."),Ca.forEach(t),un=c(Is),Tt=r(Is,"LI",{});var Ao=l(Tt);ot=n(Ao,"Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text."),Ao.forEach(t),ka=c(Is),qs=r(Is,"LI",{});var Oo=l(qs);$a=n(Oo,"Compute the ROUGE scores using the same techniques we saw earlier."),Oo.forEach(t),Ye=c(Is),as=r(Is,"LI",{});var os=l(as);Wt=n(os,"Save the checkpoints and push everything to the Hub. Here we rely on the nifty "),Bt=r(os,"CODE",{});var Co=l(Bt);dn=n(Co,"blocking=False"),Co.forEach(t),Vt=n(os," argument of the "),Kt=r(os,"CODE",{});var Po=l(Kt);fn=n(Po,"Repository"),Po.forEach(t),Yt=n(os," object so that we can push the checkpoints per epoch "),Jt=r(os,"EM",{});var Rs=l(Jt);gn=n(Rs,"asynchronously"),Rs.forEach(t),ja=n(os,". This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model!"),os.forEach(t),Is.forEach(t),As=c(m),rt=r(m,"P",{});var na=l(rt);lt=n(na,"These steps can be seen in the following block of code:"),na.forEach(t),Xt=c(m),y(us.$$.fragment,m),xa=c(m),y(Os.$$.fragment,m),ds=c(m),Je=r(m,"P",{});var Fs=l(Je);yo=n(Fs,"And that\u2019s it! Once you run this, you\u2019ll have a model and results that are pretty similar to the ones we obtained with the "),Qt=r(Fs,"CODE",{});var Lo=l(Qt);it=n(Lo,"Trainer"),Lo.forEach(t),ko=n(Fs,"."),Fs.forEach(t),this.h()},h(){f(j,"id","finetuning-mt5-with-accelerate"),f(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(j,"href","#finetuning-mt5-with-accelerate"),f(u,"class","relative group"),f(I,"href","/course/chapter3"),f(Z,"id","preparing-everything-for-training"),f(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Z,"href","#preparing-everything-for-training"),f(W,"class","relative group"),f(cs,"id","training-loop"),f(cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(cs,"href","#training-loop"),f(Es,"class","relative group")},m(m,q){i(m,u,q),s(u,j),s(j,d),k(E,d,null),s(u,C),s(u,x),s(x,A),i(m,P,q),i(m,T,q),s(T,S),s(T,I),s(I,D),s(T,O),s(T,N),s(N,J),s(T,X),i(m,Q,q),i(m,W,q),s(W,Z),s(Z,L),k(U,L,null),s(W,ee),s(W,F),s(F,B),i(m,te,q),i(m,M,q),s(M,ie),s(M,Y),s(Y,ae),s(M,de),s(M,we),s(we,Te),s(M,V),i(m,fe,q),k(me,m,q),i(m,oe,q),i(m,be,q),s(be,ke),s(be,se),s(se,_e),s(be,he),i(m,ge,q),k(w,m,q),i(m,R,q),i(m,ve,q),s(ve,b),i(m,H,q),k(qe,m,q),i(m,pe,q),i(m,$e,q),s($e,Ce),s($e,re),s(re,ze),s($e,Ze),i(m,Ae,q),k(ne,m,q),i(m,le,q),i(m,Ie,q),s(Ie,ws),s(Ie,bs),s(bs,vs),s(Ie,ys),i(m,zs,q),k(He,m,q),i(m,rs,q),k(es,m,q),i(m,ls,q),i(m,Ue,q),s(Ue,Ks),i(m,is,q),i(m,Oe,q),s(Oe,ks),s(ks,Ds),s(Oe,Re),s(Oe,ps),s(ps,G),s(Oe,ce),s(Oe,Me),s(Me,Ys),i(m,ue,q),i(m,ms,q),s(ms,ss),i(m,Ss,q),k(De,m,q),i(m,Js,q),i(m,ts,q),s(ts,je),i(m,Xs,q),k(We,m,q),i(m,Qs,q),i(m,Fe,q),s(Fe,Zs),s(Fe,Be),s(Be,et),s(Fe,ye),s(Fe,$s),s($s,Se),s(Fe,vt),i(m,js,q),i(m,xs,q),s(xs,pn),i(m,yt,q),k(st,m,q),i(m,fa,q),k(hs,m,q),i(m,ga,q),i(m,kt,q),s(kt,Ft),i(m,_a,q),k(tt,m,q),i(m,Nt,q),i(m,Ve,q),s(Ve,wa),s(Ve,xe),s(xe,mn),s(Ve,Gt),i(m,ba,q),i(m,Es,q),s(Es,cs),s(cs,Ht),k(at,Ht,null),s(Es,Ut),s(Es,Mt),s(Mt,hn),i(m,$t,q),i(m,jt,q),s(jt,cn),i(m,xt,q),i(m,Ke,q),s(Ke,nt),s(nt,va),s(nt,Ts),s(Ts,ya),s(nt,Et),s(Ke,un),s(Ke,Tt),s(Tt,ot),s(Ke,ka),s(Ke,qs),s(qs,$a),s(Ke,Ye),s(Ke,as),s(as,Wt),s(as,Bt),s(Bt,dn),s(as,Vt),s(as,Kt),s(Kt,fn),s(as,Yt),s(as,Jt),s(Jt,gn),s(as,ja),i(m,As,q),i(m,rt,q),s(rt,lt),i(m,Xt,q),k(us,m,q),i(m,xa,q),k(Os,m,q),i(m,ds,q),i(m,Je,q),s(Je,yo),s(Je,Qt),s(Qt,it),s(Je,ko),Zt=!0},i(m){Zt||(g(E.$$.fragment,m),g(U.$$.fragment,m),g(me.$$.fragment,m),g(w.$$.fragment,m),g(qe.$$.fragment,m),g(ne.$$.fragment,m),g(He.$$.fragment,m),g(es.$$.fragment,m),g(De.$$.fragment,m),g(We.$$.fragment,m),g(st.$$.fragment,m),g(hs.$$.fragment,m),g(tt.$$.fragment,m),g(at.$$.fragment,m),g(us.$$.fragment,m),g(Os.$$.fragment,m),Zt=!0)},o(m){_(E.$$.fragment,m),_(U.$$.fragment,m),_(me.$$.fragment,m),_(w.$$.fragment,m),_(qe.$$.fragment,m),_(ne.$$.fragment,m),_(He.$$.fragment,m),_(es.$$.fragment,m),_(De.$$.fragment,m),_(We.$$.fragment,m),_(st.$$.fragment,m),_(hs.$$.fragment,m),_(tt.$$.fragment,m),_(at.$$.fragment,m),_(us.$$.fragment,m),_(Os.$$.fragment,m),Zt=!1},d(m){m&&t(u),$(E),m&&t(P),m&&t(T),m&&t(Q),m&&t(W),$(U),m&&t(te),m&&t(M),m&&t(fe),$(me,m),m&&t(oe),m&&t(be),m&&t(ge),$(w,m),m&&t(R),m&&t(ve),m&&t(H),$(qe,m),m&&t(pe),m&&t($e),m&&t(Ae),$(ne,m),m&&t(le),m&&t(Ie),m&&t(zs),$(He,m),m&&t(rs),$(es,m),m&&t(ls),m&&t(Ue),m&&t(is),m&&t(Oe),m&&t(ue),m&&t(ms),m&&t(Ss),$(De,m),m&&t(Js),m&&t(ts),m&&t(Xs),$(We,m),m&&t(Qs),m&&t(Fe),m&&t(js),m&&t(xs),m&&t(yt),$(st,m),m&&t(fa),$(hs,m),m&&t(ga),m&&t(kt),m&&t(_a),$(tt,m),m&&t(Nt),m&&t(Ve),m&&t(ba),m&&t(Es),$(at),m&&t($t),m&&t(jt),m&&t(xt),m&&t(Ke),m&&t(As),m&&t(rt),m&&t(Xt),$(us,m),m&&t(xa),$(Os,m),m&&t(ds),m&&t(Je)}}}function Gg(K){let u,j,d,E,C;return{c(){u=o("p"),j=a("\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code above into a dedicated training function. See "),d=o("a"),E=a("Chapter 3"),C=a(" for more details."),this.h()},l(x){u=r(x,"P",{});var A=l(u);j=n(A,"\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code above into a dedicated training function. See "),d=r(A,"A",{href:!0});var P=l(d);E=n(P,"Chapter 3"),P.forEach(t),C=n(A," for more details."),A.forEach(t),this.h()},h(){f(d,"href","/course/chapter3")},m(x,A){i(x,u,A),s(u,j),s(u,d),s(d,E),s(u,C)},d(x){x&&t(u)}}}function Hg(K){let u,j,d,E,C,x,A,P,T,S,I,D,O,N,J,X,Q,W,Z,L,U,ee,F,B,te,M,ie,Y,ae,de,we,Te,V,fe,me,oe,be,ke,se,_e,he,ge,w,R,ve,b,H,qe,pe,$e,Ce,re,ze,Ze,Ae,ne,le,Ie,ws,bs,vs,ys,zs,He,rs,es,ls,Ue,Ks,is,Oe,ks,Ds,Re,ps,G,ce,Me,Ys,ue,ms,ss,Ss,De,Js,ts,je,Xs,We,Qs,Fe,Zs,Be,et,ye,$s,Se,vt,js,xs,pn,yt,st,fa,hs,ga,kt,Ft,_a,tt,Nt,Ve,wa,xe,mn,Gt,ba,Es,cs,Ht,at,Ut,Mt,hn,$t,jt,cn,xt,Ke,nt,va,Ts,ya,Et,un,Tt,ot,ka,qs,$a,Ye,as,Wt,Bt,dn,Vt,Kt,fn,Yt,Jt,gn,ja,As,rt,lt,Xt,us,xa,Os,ds,Je,yo,Qt,it,ko,Zt,m,q,Ea,$o,jo,pt,qt,_n,mt,xo,wn,Cs,ns,Ta,zt,qa,za,Eo,ea,ht,Da,sa,To,Dt,bn,ct,Sa,Ps,ta,St,qo,Aa,zo,Do,ut,So,Oa,Ne,Ls,aa,At,Is,Ca,Pa,Ao,Oo,os,Co,Po,Rs,na,Fs,Lo,Qp,Or,Zp,em,Io,sm,tm,oa,Ro,vn,am,nm,yn,om,Cr,rm,lm,im,Fo,pm,mm,ra,No,kn,hm,cm,Pr,um,dm,Go,fm,gm,la,Ho,$n,_m,wm,Lr,bm,vm,Uo,ym,km,ia,Mo,jn,$m,jm,Ir,xm,Em,Wo,Tm,Bl,Bo,qm,Vl,La,zm,Rr,Dm,Sm,Kl,pa,xn,$u,Am,En,ju,Yl,Vo,Om,Jl,Ia,Xl,ma,Ra,Fr,Tn,Cm,Nr,Pm,Ql,qn,Zl,Fa,Lm,Gr,Im,Rm,ei,zn,si,Na,ti,Ko,Fm,ai,Dn,ni,Sn,oi,fs,Nm,Hr,Gm,Hm,Ur,Um,Mm,Yo,Wm,Bm,Mr,Vm,Km,ri,An,li,On,ii,Ns,Ym,Wr,Jm,Xm,Br,Qm,Zm,Jo,eh,sh,pi,Ga,th,Vr,ah,nh,mi,Cn,hi,gs,oh,Kr,rh,lh,Yr,ih,ph,Jr,mh,hh,Xr,ch,uh,ci,Ot,dh,Qr,fh,gh,Zr,_h,wh,ui,Pn,di,Xo,bh,fi,Ha,gi,ha,Ua,el,Ln,vh,sl,yh,_i,In,wi,Qo,kh,bi,Ma,$h,Rn,jh,xh,vi,Fn,yi,Ct,Eh,tl,Th,qh,al,zh,Dh,ki,Wa,$i,Nn,Sh,ji,wg='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>\u2009</mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Recall} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, reference\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Recall</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">reference</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',xi,Gn,Ah,Ei,bg='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>\u2009</mtext><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Precision} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, generated\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm">Precision</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">generated</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',Ti,Ba,Oh,nl,Ch,Ph,qi,Hn,zi,Zo,Lh,Di,Un,Si,Va,Ih,ol,Rh,Fh,Ai,Mn,Oi,Wn,Ci,Xe,Nh,rl,Gh,Hh,ll,Uh,Mh,il,Wh,Bh,pl,Vh,Kh,ml,Yh,Jh,Pi,Bn,Li,Vn,Ii,Qe,Xh,hl,Qh,Zh,cl,ec,sc,ul,tc,ac,dl,nc,oc,fl,rc,lc,Ri,Ka,Fi,er,ic,Ni,ca,Ya,gl,Kn,pc,_l,mc,Gi,Gs,hc,wl,cc,uc,bl,dc,fc,vl,gc,_c,Hi,Yn,Ui,sr,wc,Mi,Jn,Wi,Ja,bc,yl,vc,yc,Bi,Xn,Vi,Qn,Ki,tr,kc,Yi,Zn,Ji,ar,$c,Xi,eo,Qi,so,Zi,Xa,jc,kl,xc,Ec,ep,dt,ft,nr,Qa,sp,or,Tc,tp,to,ap,rr,qc,np,ao,op,lr,Za,zc,ir,Dc,Sc,rp,Hs,Ac,$l,Oc,Cc,jl,Pc,Lc,xl,Ic,Rc,lp,gt,_t,pr,mr,Fc,ip,no,pp,Pt,Nc,El,Gc,Hc,Tl,Uc,Mc,mp,oo,hp,ro,cp,Ee,Wc,ql,Bc,Vc,zl,Kc,Yc,Dl,Jc,Xc,Sl,Qc,Zc,Al,eu,su,Ol,tu,au,Cl,nu,ou,Pl,ru,lu,up,wt,bt,hr,cr,ua,en,Ll,lo,iu,Il,pu,dp,sn,mu,Rl,hu,cu,fp,io,gp,ur,uu,_p,po,wp,dr,du,bp,mo,vp,ho,yp,tn,fu,Fl,gu,_u,kp,co,$p,uo,jp,fr,wu,xp,gr,bu,Ep;d=new xg({props:{fw:K[0]}}),P=new Vs({});const xu=[Tg,Eg],fo=[];function Eu(e,p){return e[0]==="pt"?0:1}O=Eu(K),N=fo[O]=xu[O](K),ee=new ku({props:{id:"yHnr5Dk2zCI"}}),ge=new Vs({}),ze=new z({props:{code:`from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

spanish_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;es&quot;</span>)
english_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>)
english_dataset`}}),Ae=new z({props:{code:`DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">200000</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
})`}}),Me=new z({props:{code:`def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_samples</span>(<span class="hljs-params">dataset, num_samples=<span class="hljs-number">3</span>, seed=<span class="hljs-number">42</span></span>):
    sample = dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=seed).select(<span class="hljs-built_in">range</span>(num_samples))
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> sample:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt; Title: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_title&#x27;</span>]}</span>&#x27;&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt; Review: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_body&#x27;</span>]}</span>&#x27;&quot;</span>)


show_samples(english_dataset)`}}),ue=new z({props:{code:`'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Worked in front position, not rear&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: meh&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Can\\&#x27;t beat these for the money&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Bought this for handling miscellaneous aircraft parts and hanger &quot;stuff&quot; that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\&#x27;t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\&#x27;s heavy duty enough to hold metal parts, but being made of plastic it\\&#x27;s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\&#x27;t beat it. Best one of these I\\&#x27;ve bought to date-- and I\\&#x27;ve been using some version of these for over forty years.&#x27;</span>`}}),ss=new ln({props:{$$slots:{default:[qg]},$$scope:{ctx:K}}}),Be=new z({props:{code:`english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]`,highlighted:`english_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
english_df = english_dataset[<span class="hljs-string">&quot;train&quot;</span>][:]
<span class="hljs-comment"># Show counts for top 20 products</span>
english_df[<span class="hljs-string">&quot;product_category&quot;</span>].value_counts()[:<span class="hljs-number">20</span>]`}}),ye=new z({props:{code:`home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64`,highlighted:`home                      <span class="hljs-number">17679</span>
apparel                   <span class="hljs-number">15951</span>
wireless                  <span class="hljs-number">15717</span>
other                     <span class="hljs-number">13418</span>
beauty                    <span class="hljs-number">12091</span>
drugstore                 <span class="hljs-number">11730</span>
kitchen                   <span class="hljs-number">10382</span>
toy                        <span class="hljs-number">8745</span>
sports                     <span class="hljs-number">8277</span>
automotive                 <span class="hljs-number">7506</span>
lawn_and_garden            <span class="hljs-number">7327</span>
home_improvement           <span class="hljs-number">7136</span>
pet_products               <span class="hljs-number">7082</span>
digital_ebook_purchase     <span class="hljs-number">6749</span>
pc                         <span class="hljs-number">6401</span>
electronics                <span class="hljs-number">6186</span>
office_product             <span class="hljs-number">5521</span>
shoes                      <span class="hljs-number">5197</span>
grocery                    <span class="hljs-number">4730</span>
book                       <span class="hljs-number">3756</span>
Name: product_category, dtype: int64`}}),Ve=new z({props:{code:`def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_books</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> (
        example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;book&quot;</span>
        <span class="hljs-keyword">or</span> example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;digital_ebook_purchase&quot;</span>
    )`}}),Ts=new z({props:{code:"english_dataset.reset_format()",highlighted:"english_dataset.reset_format()"}}),ot=new z({props:{code:`spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)`,highlighted:`spanish_books = spanish_dataset.<span class="hljs-built_in">filter</span>(filter_books)
english_books = english_dataset.<span class="hljs-built_in">filter</span>(filter_books)
show_samples(english_books)`}}),qs=new z({props:{code:`'>> Title: I\\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: I\\&#x27;m dissapointed.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I guess I had higher expectations for this book from the reviews. I really thought I\\&#x27;d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\&#x27;m dissapointed.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Good art, good price, poor design&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\&#x27;s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Helpful&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.&#x27;</span>`}}),As=new z({props:{code:`from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

<span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># Peek at a few examples</span>
show_samples(books_dataset)`}}),lt=new z({props:{code:`'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DA\xD1ADO'
'>> Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Easy to follow!!!!&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: PARCIALMENTE DA\xD1ADO&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: no lo he podido descargar&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: igual que el anterior&#x27;</span>`}}),qt=new z({props:{code:'books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)',highlighted:'books_dataset = books_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;review_title&quot;</span>].split()) &gt; <span class="hljs-number">2</span>)'}}),zt=new Vs({}),Ia=new ln({props:{$$slots:{default:[zg]},$$scope:{ctx:K}}}),Tn=new Vs({}),qn=new ku({props:{id:"1m7BerpSq8A"}}),zn=new z({props:{code:`from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;google/mt5-small&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}}),Na=new ln({props:{$$slots:{default:[Dg]},$$scope:{ctx:K}}}),Dn=new z({props:{code:`inputs = tokenizer("I loved reading the Hunger Games!")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;I loved reading the Hunger Games!&quot;</span>)
inputs`}}),Sn=new z({props:{code:"{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">336</span>, <span class="hljs-number">259</span>, <span class="hljs-number">28387</span>, <span class="hljs-number">11807</span>, <span class="hljs-number">287</span>, <span class="hljs-number">62893</span>, <span class="hljs-number">295</span>, <span class="hljs-number">12507</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}'}}),An=new z({props:{code:"tokenizer.convert_ids_to_tokens(inputs.input_ids)",highlighted:"tokenizer.convert_ids_to_tokens(inputs.input_ids)"}}),On=new z({props:{code:"['\u2581I', '\u2581', 'loved', '\u2581reading', '\u2581the', '\u2581Hung', 'er', '\u2581Games', '</s>']",highlighted:'[<span class="hljs-string">&#x27;\u2581I&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;loved&#x27;</span>, <span class="hljs-string">&#x27;\u2581reading&#x27;</span>, <span class="hljs-string">&#x27;\u2581the&#x27;</span>, <span class="hljs-string">&#x27;\u2581Hung&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;\u2581Games&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]'}}),Cn=new z({props:{code:`max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"], max_length=max_input_length, truncation=True
    )
    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["review_title"], max_length=max_target_length, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`max_input_length = <span class="hljs-number">512</span>
max_target_length = <span class="hljs-number">30</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    model_inputs = tokenizer(
        examples[<span class="hljs-string">&quot;review_body&quot;</span>], max_length=max_input_length, truncation=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Set up the tokenizer for targets</span>
    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples[<span class="hljs-string">&quot;review_title&quot;</span>], max_length=max_target_length, truncation=<span class="hljs-literal">True</span>
        )

    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
    <span class="hljs-keyword">return</span> model_inputs`}}),Pn=new z({props:{code:"tokenized_datasets = books_dataset.map(preprocess_function, batched=True)",highlighted:'tokenized_datasets = books_dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),Ha=new ln({props:{$$slots:{default:[Sg]},$$scope:{ctx:K}}}),Ln=new Vs({}),In=new ku({props:{id:"TMshhnrEXlg"}}),Fn=new z({props:{code:`generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"`,highlighted:`generated_summary = <span class="hljs-string">&quot;I absolutely loved reading the Hunger Games&quot;</span>
reference_summary = <span class="hljs-string">&quot;I loved reading the Hunger Games&quot;</span>`}}),Wa=new ln({props:{$$slots:{default:[Ag]},$$scope:{ctx:K}}}),Hn=new z({props:{code:"!pip install rouge_score",highlighted:"!pip install rouge_score"}}),Un=new z({props:{code:`from datasets import load_metric

rouge_score = load_metric("rouge")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

rouge_score = load_metric(<span class="hljs-string">&quot;rouge&quot;</span>)`}}),Mn=new z({props:{code:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`,highlighted:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`}}),Wn=new z({props:{code:`{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}`,highlighted:`{<span class="hljs-string">&#x27;rouge1&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rouge2&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), mid=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), high=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>)),
 <span class="hljs-string">&#x27;rougeL&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>))}`}}),Bn=new z({props:{code:'scores["rouge1"].mid',highlighted:'scores[<span class="hljs-string">&quot;rouge1&quot;</span>].mid'}}),Vn=new z({props:{code:"Score(precision=0.86, recall=1.0, fmeasure=0.92)",highlighted:'Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)'}}),Ka=new ln({props:{$$slots:{default:[Og]},$$scope:{ctx:K}}}),Kn=new Vs({}),Yn=new z({props:{code:"!pip install nltk",highlighted:"!pip install nltk"}}),Jn=new z({props:{code:`import nltk

nltk.download("punkt")`,highlighted:`<span class="hljs-keyword">import</span> nltk

nltk.download(<span class="hljs-string">&quot;punkt&quot;</span>)`}}),Xn=new z({props:{code:`from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))`,highlighted:`<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> sent_tokenize


<span class="hljs-keyword">def</span> <span class="hljs-title function_">three_sentence_summary</span>(<span class="hljs-params">text</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(text)[:<span class="hljs-number">3</span>])


<span class="hljs-built_in">print</span>(three_sentence_summary(books_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;review_body&quot;</span>]))`}}),Qn=new z({props:{code:`'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'`,highlighted:`<span class="hljs-string">&#x27;I grew up reading Koontz, and years ago, I stopped,convinced i had &quot;outgrown&quot; him.&#x27;</span>
<span class="hljs-string">&#x27;Still,when a friend was looking for something suspenseful too read, I suggested Koontz.&#x27;</span>
<span class="hljs-string">&#x27;She found Strangers.&#x27;</span>`}}),Zn=new z({props:{code:`def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_baseline</span>(<span class="hljs-params">dataset, metric</span>):
    summaries = [three_sentence_summary(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">&quot;review_body&quot;</span>]]
    <span class="hljs-keyword">return</span> metric.compute(predictions=summaries, references=dataset[<span class="hljs-string">&quot;review_title&quot;</span>])`}}),eo=new z({props:{code:`import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

score = evaluate_baseline(books_dataset[<span class="hljs-string">&quot;validation&quot;</span>], rouge_score)
rouge_names = [<span class="hljs-string">&quot;rouge1&quot;</span>, <span class="hljs-string">&quot;rouge2&quot;</span>, <span class="hljs-string">&quot;rougeL&quot;</span>, <span class="hljs-string">&quot;rougeLsum&quot;</span>]
rouge_dict = <span class="hljs-built_in">dict</span>((rn, <span class="hljs-built_in">round</span>(score[rn].mid.fmeasure * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> rn <span class="hljs-keyword">in</span> rouge_names)
rouge_dict`}}),so=new z({props:{code:"{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}",highlighted:'{<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">16.74</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">8.83</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">15.6</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">15.96</span>}'}});const Tu=[Pg,Cg],go=[];function qu(e,p){return e[0]==="pt"?0:1}dt=qu(K),ft=go[dt]=Tu[dt](K),Qa=new ln({props:{$$slots:{default:[Lg]},$$scope:{ctx:K}}}),to=new z({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),ao=new z({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});let Pe=K[0]==="pt"&&fg();const zu=[Rg,Ig],_o=[];function Du(e,p){return e[0]==="pt"?0:1}gt=Du(K),_t=_o[gt]=zu[gt](K),no=new z({props:{code:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)`,highlighted:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset[<span class="hljs-string">&quot;train&quot;</span>].column_names
)`}}),oo=new z({props:{code:`features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)`,highlighted:`features = [tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
data_collator(features)`}}),ro=new z({props:{code:`{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">1494</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">8622</span>,    <span class="hljs-number">390</span>,    <span class="hljs-number">259</span>,    <span class="hljs-number">262</span>,   <span class="hljs-number">2316</span>,   <span class="hljs-number">3435</span>,    <span class="hljs-number">955</span>,
            <span class="hljs-number">772</span>,    <span class="hljs-number">281</span>,    <span class="hljs-number">772</span>,   <span class="hljs-number">1617</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">305</span>,  <span class="hljs-number">14701</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">1385</span>,
           <span class="hljs-number">3031</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">24146</span>,    <span class="hljs-number">332</span>,   <span class="hljs-number">1037</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">43906</span>,    <span class="hljs-number">305</span>,    <span class="hljs-number">336</span>,
            <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>],
        [   <span class="hljs-number">259</span>,  <span class="hljs-number">27531</span>,  <span class="hljs-number">13483</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">7505</span>,    <span class="hljs-number">260</span>, <span class="hljs-number">112240</span>,  <span class="hljs-number">15192</span>,    <span class="hljs-number">305</span>,
          <span class="hljs-number">53198</span>,    <span class="hljs-number">276</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">74060</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">260</span>,    <span class="hljs-number">459</span>,  <span class="hljs-number">25640</span>,    <span class="hljs-number">776</span>,
           <span class="hljs-number">2119</span>,    <span class="hljs-number">336</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">2220</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">18896</span>,    <span class="hljs-number">288</span>,   <span class="hljs-number">4906</span>,    <span class="hljs-number">288</span>,
           <span class="hljs-number">1037</span>,   <span class="hljs-number">3931</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">7083</span>, <span class="hljs-number">101476</span>,   <span class="hljs-number">1143</span>,    <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;labels&#x27;</span>: tensor([[ <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>,  -<span class="hljs-number">100</span>],
        [  <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>,     <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;decoder_input_ids&#x27;</span>: tensor([[    <span class="hljs-number">0</span>,  <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>],
        [    <span class="hljs-number">0</span>,   <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>]])}`}});const Su=[Ng,Fg],wo=[];function Au(e,p){return e[0]==="pt"?0:1}wt=Au(K),bt=wo[wt]=Su[wt](K);let Le=K[0]==="pt"&&gg(K);return lo=new Vs({}),io=new z({props:{code:`from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

hub_model_id = <span class="hljs-string">&quot;huggingface-course/mt5-small-finetuned-amazon-en-es&quot;</span>
summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>, model=hub_model_id)`}}),po=new z({props:{code:`def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\\n'>>> Title: {title}'")
    print(f"\\n'>>> Summary: {summary}'")`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">idx</span>):
    review = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>]
    title = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_title&quot;</span>]
    summary = summarizer(books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>])[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;summary_text&quot;</span>]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{review}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Title: <span class="hljs-subst">{title}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Summary: <span class="hljs-subst">{summary}</span>&#x27;&quot;</span>)`}}),mo=new z({props:{code:"print_summary(100)",highlighted:'print_summary(<span class="hljs-number">100</span>)'}}),ho=new z({props:{code:`'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Not impressed at all... buy something else&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Nothing special at all about this product&#x27;</span>`}}),co=new z({props:{code:"print_summary(0)",highlighted:'print_summary(<span class="hljs-number">0</span>)'}}),uo=new z({props:{code:`'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Buena literatura para adolescentes&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Muy facil de leer&#x27;</span>`}}),{c(){u=o("meta"),j=h(),v(d.$$.fragment),E=h(),C=o("h1"),x=o("a"),A=o("span"),v(P.$$.fragment),T=h(),S=o("span"),I=a("Summarization"),D=h(),N.c(),J=h(),X=o("p"),Q=a("In this section we\u2019ll take a look at how Transformer models can be used to condense long documents into summaries, a task known as "),W=o("em"),Z=a("text summarization"),L=a(". This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail."),U=h(),v(ee.$$.fragment),F=h(),B=o("p"),te=a("Although there already exist various fine-tuned models for summarization on the "),M=o("a"),ie=a("Hugging Face Hub"),Y=a(", almost all of these are only suitable for English documents. So, to add a twist in this section, we\u2019ll train a bilingual model for English and Spanish. By the end of this section, you\u2019ll have a "),ae=o("a"),de=a("model"),we=a(" that can summarize customer reviews like the one shown here:"),Te=h(),V=o("div"),fe=o("div"),me=h(),oe=o("p"),be=a("As we\u2019ll see, these summaries are concise because they\u2019re learned from the titles that customers provide in their product reviews. Let\u2019s start by putting together a suitable bilingual corpus for this task."),ke=h(),se=o("h2"),_e=o("a"),he=o("span"),v(ge.$$.fragment),w=h(),R=o("span"),ve=a("Preparing a multilingual corpus"),b=h(),H=o("p"),qe=a("We\u2019ll use the "),pe=o("a"),$e=a("Multilingual Amazon Reviews Corpus"),Ce=a(" to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let\u2019s download the English and Spanish subsets from the Hugging Face Hub:"),re=h(),v(ze.$$.fragment),Ze=h(),v(Ae.$$.fragment),ne=h(),le=o("p"),Ie=a("As you can see, for each language there are 200,000 reviews for the "),ws=o("code"),bs=a("train"),vs=a(" split, and 5,000 reviews for each of the "),ys=o("code"),zs=a("validation"),He=a(" and "),rs=o("code"),es=a("test"),ls=a(" splits. The review information we are interested in is contained in the "),Ue=o("code"),Ks=a("review_body"),is=a(" and "),Oe=o("code"),ks=a("review_title"),Ds=a(" columns. Let\u2019s take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in "),Re=o("a"),ps=a("Chapter 5"),G=a(":"),ce=h(),v(Me.$$.fragment),Ys=h(),v(ue.$$.fragment),ms=h(),v(ss.$$.fragment),Ss=h(),De=o("p"),Js=a("This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the \u201Cmeh\u201D title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we\u2019ll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let\u2019s convert "),ts=o("code"),je=a("english_dataset"),Xs=a(" to a "),We=o("code"),Qs=a("pandas.DataFrame"),Fe=a(" and compute the number of reviews per product category:"),Zs=h(),v(Be.$$.fragment),et=h(),v(ye.$$.fragment),$s=h(),Se=o("p"),vt=a("The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let\u2019s focus on summarizing book reviews \u2014 after all, this is what the company was founded on! We can see two product categories that fit the bill ("),js=o("code"),xs=a("book"),pn=a(" and "),yt=o("code"),st=a("digital_ebook_purchase"),fa=a("), so let\u2019s filter the datasets in both languages for just these products. As we saw in "),hs=o("a"),ga=a("Chapter 5"),kt=a(", the "),Ft=o("code"),_a=a("Dataset.filter()"),tt=a(" function allows us to slice a dataset very efficiently, so we can define a simple function to do this:"),Nt=h(),v(Ve.$$.fragment),wa=h(),xe=o("p"),mn=a("Now when we apply this function to "),Gt=o("code"),ba=a("english_dataset"),Es=a(" and "),cs=o("code"),Ht=a("spanish_dataset"),at=a(", the result will contain just those rows involving the book categories. Before applying the filter, let\u2019s switch the format of "),Ut=o("code"),Mt=a("english_dataset"),hn=a(" from "),$t=o("code"),jt=a('"pandas"'),cn=a(" back to "),xt=o("code"),Ke=a('"arrow"'),nt=a(":"),va=h(),v(Ts.$$.fragment),ya=h(),Et=o("p"),un=a("We can then apply the filter function, and as a sanity check let\u2019s inspect a sample of reviews to see if they are indeed about books:"),Tt=h(),v(ot.$$.fragment),ka=h(),v(qs.$$.fragment),$a=h(),Ye=o("p"),as=a("Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single "),Wt=o("code"),Bt=a("DatasetDict"),dn=a(" object. \u{1F917} Datasets provides a handy "),Vt=o("code"),Kt=a("concatenate_datasets()"),fn=a(" function that (as the name suggests) will stack two "),Yt=o("code"),Jt=a("Dataset"),gn=a(" objects on top of each other. So, to create our bilingual dataset, we\u2019ll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn\u2019t overfit to a single language:"),ja=h(),v(As.$$.fragment),rt=h(),v(lt.$$.fragment),Xt=h(),us=o("p"),xa=a("This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words:"),Os=h(),ds=o("div"),Je=o("img"),Qt=h(),it=o("img"),Zt=h(),m=o("p"),q=a("To deal with this, we\u2019ll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we\u2019re dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty "),Ea=o("code"),$o=a("Dataset.filter()"),jo=a(" method as follows:"),pt=h(),v(qt.$$.fragment),_n=h(),mt=o("p"),xo=a("Now that we\u2019ve prepared our corpus, let\u2019s take a look at a few possible Transformer models that one might fine-tune on it!"),wn=h(),Cs=o("h2"),ns=o("a"),Ta=o("span"),v(zt.$$.fragment),qa=h(),za=o("span"),Eo=a("Models for text summarization"),ea=h(),ht=o("p"),Da=a("If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we\u2019d like to \u201Ctranslate\u201D into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in "),sa=o("a"),To=a("Chapter 1"),Dt=a(", although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization."),bn=h(),ct=o("table"),Sa=o("thead"),Ps=o("tr"),ta=o("th"),St=a("Transformer model"),qo=h(),Aa=o("th"),zo=a("Description"),Do=h(),ut=o("th"),So=a("Multilingual?"),Oa=h(),Ne=o("tbody"),Ls=o("tr"),aa=o("td"),At=o("a"),Is=a("GPT-2"),Ca=h(),Pa=o("td"),Ao=a("Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending \u201CTL;DR\u201D at the end of the input text."),Oo=h(),os=o("td"),Co=a("\u274C"),Po=h(),Rs=o("tr"),na=o("td"),Fs=o("a"),Lo=a("PEGASUS"),Qp=h(),Or=o("td"),Zp=a("Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks."),em=h(),Io=o("td"),sm=a("\u274C"),tm=h(),oa=o("tr"),Ro=o("td"),vn=o("a"),am=a("T5"),nm=h(),yn=o("td"),om=a("A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is "),Cr=o("code"),rm=a("summarize: ARTICLE"),lm=a("."),im=h(),Fo=o("td"),pm=a("\u274C"),mm=h(),ra=o("tr"),No=o("td"),kn=o("a"),hm=a("mT5"),cm=h(),Pr=o("td"),um=a("A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages."),dm=h(),Go=o("td"),fm=a("\u2705"),gm=h(),la=o("tr"),Ho=o("td"),$n=o("a"),_m=a("BART"),wm=h(),Lr=o("td"),bm=a("A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2."),vm=h(),Uo=o("td"),ym=a("\u274C"),km=h(),ia=o("tr"),Mo=o("td"),jn=o("a"),$m=a("mBART-50"),jm=h(),Ir=o("td"),xm=a("A multilingual version of BART, pretrained on 50 languages."),Em=h(),Wo=o("td"),Tm=a("\u2705"),Bl=h(),Bo=o("p"),qm=a("As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a \u201Chigh-resource\u201D language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!"),Vl=h(),La=o("p"),zm=a("We\u2019ll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like "),Rr=o("code"),Dm=a("summarize:"),Sm=a(" which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!"),Kl=h(),pa=o("div"),xn=o("img"),Am=h(),En=o("img"),Yl=h(),Vo=o("p"),Om=a("mT5 doesn\u2019t use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we\u2019ve picked a model, let\u2019s take a look at preparing our data for training."),Jl=h(),v(Ia.$$.fragment),Xl=h(),ma=o("h2"),Ra=o("a"),Fr=o("span"),v(Tn.$$.fragment),Cm=h(),Nr=o("span"),Pm=a("Preprocessing the data"),Ql=h(),v(qn.$$.fragment),Zl=h(),Fa=o("p"),Lm=a("Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We\u2019ll use "),Gr=o("code"),Im=a("mt5-small"),Rm=a(" as our checkpoint so we can fine-tune the model in a reasonable amount of time:"),ei=h(),v(zn.$$.fragment),si=h(),v(Na.$$.fragment),ti=h(),Ko=o("p"),Fm=a("Let\u2019s test out the mT5 tokenizer on a small example:"),ai=h(),v(Dn.$$.fragment),ni=h(),v(Sn.$$.fragment),oi=h(),fs=o("p"),Nm=a("Here we can see the familiar "),Hr=o("code"),Gm=a("input_ids"),Hm=a(" and "),Ur=o("code"),Um=a("attention_mask"),Mm=a(" that we encountered in our first fine-tuning experiments back in "),Yo=o("a"),Wm=a("Chapter 3"),Bm=a(". Let\u2019s decode these input IDs with the tokenizer\u2019s "),Mr=o("code"),Vm=a("convert_ids_to_tokens()"),Km=a(" function to see what kind of tokenizer we\u2019re dealing with:"),ri=h(),v(An.$$.fragment),li=h(),v(On.$$.fragment),ii=h(),Ns=o("p"),Ym=a("The special Unicode character "),Wr=o("code"),Jm=a("\u2581"),Xm=a(" and end-of-sequence token "),Br=o("code"),Qm=a("</s>"),Zm=a(" indicate that we\u2019re dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in "),Jo=o("a"),eh=a("Chapter 6"),sh=a(". Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters."),pi=h(),Ga=o("p"),th=a("To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model\u2019s maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don\u2019t pass excessively long inputs to our model. The tokenizers in \u{1F917} Transformers provide a nifty "),Vr=o("code"),ah=a("as_target_tokenizer()"),nh=a(" function that allows you to tokenize the labels in parallel to the inputs. This is typically done using a context manager inside a preprocessing function that first encodes the inputs, and then encodes the labels as a separate column. Here is an example of such a function for mT5:"),mi=h(),v(Cn.$$.fragment),hi=h(),gs=o("p"),oh=a("Let\u2019s walk through this code to understand what\u2019s happening. The first thing we\u2019ve done is define values for "),Kr=o("code"),rh=a("max_input_length"),lh=a(" and "),Yr=o("code"),ih=a("max_target_length"),ph=a(", which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we\u2019ve scaled these values accordingly. Then, in the  "),Jr=o("code"),mh=a("preprocess_function()"),hh=a(" itself we can see the reviews are first tokenized, followed by the titles with "),Xr=o("code"),ch=a("as_target_tokenizer()"),uh=a("."),ci=h(),Ot=o("p"),dh=a("With "),Qr=o("code"),fh=a("preprocess_function()"),gh=a(", it is then a simple matter to tokenize the whole corpus using the handy "),Zr=o("code"),_h=a("Dataset.map()"),wh=a(" function we\u2019ve used extensively throughout this course:"),ui=h(),v(Pn.$$.fragment),di=h(),Xo=o("p"),bh=a("Now that the corpus has been preprocessed, let\u2019s take a look at some metrics that are commonly used for summarization. As we\u2019ll see, there is no silver bullet when it comes to measuring the quality of machine-generated text."),fi=h(),v(Ha.$$.fragment),gi=h(),ha=o("h2"),Ua=o("a"),el=o("span"),v(Ln.$$.fragment),vh=h(),sl=o("span"),yh=a("Metrics for text summarization"),_i=h(),v(In.$$.fragment),wi=h(),Qo=o("p"),kh=a("In comparison to most of the other tasks we\u2019ve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like \u201CI loved reading the Hunger Games\u201D, there are multiple valid summaries, like \u201CI loved the Hunger Games\u201D or \u201CHunger Games is a great read\u201D. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution \u2014 even humans would fare poorly under such a metric, because we all have our own writing style."),bi=h(),Ma=o("p"),$h=a("For summarization, one of the most commonly used metrics is the "),Rn=o("a"),jh=a("ROUGE score"),xh=a(" (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries:"),vi=h(),v(Fn.$$.fragment),yi=h(),Ct=o("p"),Eh=a("One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the "),tl=o("em"),Th=a("precision"),qh=a(" and "),al=o("em"),zh=a("recall"),Dh=a(" scores for the overlap."),ki=h(),v(Wa.$$.fragment),$i=h(),Nn=o("p"),Sh=a(`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),ji=new ug,xi=h(),Gn=o("p"),Ah=a(`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been \u201CI really really loved reading the Hunger Games all night\u201D. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),Ei=new ug,Ti=h(),Ba=o("p"),Oh=a("Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in \u{1F917} Datasets by first installing the "),nl=o("code"),Ch=a("rouge_score"),Ph=a(" package:"),qi=h(),v(Hn.$$.fragment),zi=h(),Zo=o("p"),Lh=a("and then loading the ROUGE metric as follows:"),Di=h(),v(Un.$$.fragment),Si=h(),Va=o("p"),Ih=a("Then we can use the "),ol=o("code"),Rh=a("rouge_score.compute()"),Fh=a(" function to calculate all the metrics at once:"),Ai=h(),v(Mn.$$.fragment),Oi=h(),v(Wn.$$.fragment),Ci=h(),Xe=o("p"),Nh=a("Whoa, there\u2019s a lot of information in that output \u2014 what does it all mean? First, \u{1F917} Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the "),rl=o("code"),Gh=a("low"),Hh=a(", "),ll=o("code"),Uh=a("mid"),Mh=a(", and "),il=o("code"),Wh=a("high"),Bh=a(" attributes you can see here. Moreover, \u{1F917} Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The "),pl=o("code"),Vh=a("rouge1"),Kh=a(" variant is the overlap of unigrams \u2014 this is just a fancy way of saying the overlap of words and is exactly the metric we\u2019ve discussed above. To verify this, let\u2019s pull out the "),ml=o("code"),Yh=a("mid"),Jh=a(" value of our scores:"),Pi=h(),v(Bn.$$.fragment),Li=h(),v(Vn.$$.fragment),Ii=h(),Qe=o("p"),Xh=a("Great, the precision and recall numbers match up! Now what about those other ROUGE scores? "),hl=o("code"),Qh=a("rouge2"),Zh=a(" measures the overlap between bigrams (think the overlap of pairs of words), while "),cl=o("code"),ec=a("rougeL"),sc=a(" and "),ul=o("code"),tc=a("rougeLsum"),ac=a(" measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The \u201Csum\u201D in "),dl=o("code"),nc=a("rougeLsum"),oc=a(" refers to the fact that this metric is computed over a whole summary, while "),fl=o("code"),rc=a("rougeL"),lc=a(" is computed as the average over individual sentences."),Ri=h(),v(Ka.$$.fragment),Fi=h(),er=o("p"),ic=a("We\u2019ll use these ROUGE scores to track the performance of our model, but before doing that let\u2019s do something every good NLP practitioner should do: create a strong, yet simple baseline!"),Ni=h(),ca=o("h3"),Ya=o("a"),gl=o("span"),v(Kn.$$.fragment),pc=h(),_l=o("span"),mc=a("Creating a strong baseline"),Gi=h(),Gs=o("p"),hc=a("A common baseline for text summarization is to simply take the first three sentences of an article, often called the "),wl=o("em"),cc=a("lead-3"),uc=a(" baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like \u201CU.S.\u201D or \u201CU.N.\u201D \u2014 so instead we\u2019ll use the "),bl=o("code"),dc=a("nltk"),fc=a(" library, which includes a better algorithm to handle these cases. You can install the package using "),vl=o("code"),gc=a("pip"),_c=a(" as follows:"),Hi=h(),v(Yn.$$.fragment),Ui=h(),sr=o("p"),wc=a("and then download the punctuation rules:"),Mi=h(),v(Jn.$$.fragment),Wi=h(),Ja=o("p"),bc=a("Next, we import the sentence tokenizer from "),yl=o("code"),vc=a("nltk"),yc=a(" and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let\u2019s also include this and test it on a training example:"),Bi=h(),v(Xn.$$.fragment),Vi=h(),v(Qn.$$.fragment),Ki=h(),tr=o("p"),kc=a("This seems to work, so let\u2019s now implement a function that extracts these \u201Csummaries\u201D from a dataset and computes the ROUGE scores for the baseline:"),Yi=h(),v(Zn.$$.fragment),Ji=h(),ar=o("p"),$c=a("We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas:"),Xi=h(),v(eo.$$.fragment),Qi=h(),v(so.$$.fragment),Zi=h(),Xa=o("p"),jc=a("We can see that the "),kl=o("code"),xc=a("rouge2"),Ec=a(" score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let\u2019s turn our attention toward fine-tuning mT5!"),ep=h(),ft.c(),nr=h(),v(Qa.$$.fragment),sp=h(),or=o("p"),Tc=a("The next thing we need to do is log in to the Hugging Face Hub. If you\u2019re running this code in a notebook, you can do so with the following utility function:"),tp=h(),v(to.$$.fragment),ap=h(),rr=o("p"),qc=a("which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:"),np=h(),v(ao.$$.fragment),op=h(),Pe&&Pe.c(),lr=h(),Za=o("p"),zc=a("Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like "),ir=o("a"),Dc=a("causal language modeling"),Sc=a("."),rp=h(),Hs=o("p"),Ac=a("Luckily, \u{1F917} Transformers provides a "),$l=o("code"),Oc=a("DataCollatorForSeq2Seq"),Cc=a(" collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the "),jl=o("code"),Pc=a("tokenizer"),Lc=a(" and "),xl=o("code"),Ic=a("model"),Rc=a(":"),lp=h(),_t.c(),pr=h(),mr=o("p"),Fc=a("Let\u2019s see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won\u2019t know how to pad these elements:"),ip=h(),v(no.$$.fragment),pp=h(),Pt=o("p"),Nc=a("Since the collator expects a list of "),El=o("code"),Gc=a("dict"),Hc=a("s, where each "),Tl=o("code"),Uc=a("dict"),Mc=a(" represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator:"),mp=h(),v(oo.$$.fragment),hp=h(),v(ro.$$.fragment),cp=h(),Ee=o("p"),Wc=a("The main thing to notice here is that the first example is longer than the second one, so the "),ql=o("code"),Bc=a("input_ids"),Vc=a(" and "),zl=o("code"),Kc=a("attention_mask"),Yc=a(" of the second example have been padded on the right with a "),Dl=o("code"),Jc=a("[PAD]"),Xc=a(" token (whose ID is "),Sl=o("code"),Qc=a("0"),Zc=a("). Similarly, we can see that the "),Al=o("code"),eu=a("labels"),su=a(" have been padded with "),Ol=o("code"),tu=a("-100"),au=a("s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new "),Cl=o("code"),nu=a("decoder_input_ids"),ou=a(" which has shifted the labels to the right by inserting a "),Pl=o("code"),ru=a("[PAD]"),lu=a(" token in the first entry."),up=h(),bt.c(),hr=h(),Le&&Le.c(),cr=h(),ua=o("h2"),en=o("a"),Ll=o("span"),v(lo.$$.fragment),iu=h(),Il=o("span"),pu=a("Using your fine-tuned model"),dp=h(),sn=o("p"),mu=a("Once you\u2019ve pushed the model to the Hub, you can play with it either via the inference widget or with a "),Rl=o("code"),hu=a("pipeline"),cu=a(" object, as follows:"),fp=h(),v(io.$$.fragment),gp=h(),ur=o("p"),uu=a("We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let\u2019s implement a simple function to show the review, title, and generated summary together:"),_p=h(),v(po.$$.fragment),wp=h(),dr=o("p"),du=a("Let\u2019s take a look at one of the English examples we get:"),bp=h(),v(mo.$$.fragment),vp=h(),v(ho.$$.fragment),yp=h(),tn=o("p"),fu=a("This is not too bad! We can see that our model has actually been able to perform "),Fl=o("em"),gu=a("abstractive"),_u=a(" summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:"),kp=h(),v(co.$$.fragment),$p=h(),v(uo.$$.fragment),jp=h(),fr=o("p"),wu=a("The summary translates into \u201CVery easy to read\u201D in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it\u2019s like to deal with a multilingual corpus!"),xp=h(),gr=o("p"),bu=a("Next, we\u2019ll turn our attention to a slightly more complex task: training a language model from scratch."),this.h()},l(e){const p=$g('[data-svelte="svelte-1phssyn"]',document.head);u=r(p,"META",{name:!0,content:!0}),p.forEach(t),j=c(e),y(d.$$.fragment,e),E=c(e),C=r(e,"H1",{class:!0});var bo=l(C);x=r(bo,"A",{id:!0,class:!0,href:!0});var _r=l(x);A=r(_r,"SPAN",{});var Nl=l(A);y(P.$$.fragment,Nl),Nl.forEach(t),_r.forEach(t),T=c(bo),S=r(bo,"SPAN",{});var Gl=l(S);I=n(Gl,"Summarization"),Gl.forEach(t),bo.forEach(t),D=c(e),N.l(e),J=c(e),X=r(e,"P",{});var vo=l(X);Q=n(vo,"In this section we\u2019ll take a look at how Transformer models can be used to condense long documents into summaries, a task known as "),W=r(vo,"EM",{});var Hl=l(W);Z=n(Hl,"text summarization"),Hl.forEach(t),L=n(vo,". This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail."),vo.forEach(t),U=c(e),y(ee.$$.fragment,e),F=c(e),B=r(e,"P",{});var da=l(B);te=n(da,"Although there already exist various fine-tuned models for summarization on the "),M=r(da,"A",{href:!0,rel:!0});var Ul=l(M);ie=n(Ul,"Hugging Face Hub"),Ul.forEach(t),Y=n(da,", almost all of these are only suitable for English documents. So, to add a twist in this section, we\u2019ll train a bilingual model for English and Spanish. By the end of this section, you\u2019ll have a "),ae=r(da,"A",{href:!0,rel:!0});var wr=l(ae);de=n(wr,"model"),wr.forEach(t),we=n(da," that can summarize customer reviews like the one shown here:"),da.forEach(t),Te=c(e),V=r(e,"DIV",{class:!0});var Ml=l(V);fe=r(Ml,"DIV",{class:!0});var Wl=l(fe);Wl.forEach(t),Ml.forEach(t),me=c(e),oe=r(e,"P",{});var br=l(oe);be=n(br,"As we\u2019ll see, these summaries are concise because they\u2019re learned from the titles that customers provide in their product reviews. Let\u2019s start by putting together a suitable bilingual corpus for this task."),br.forEach(t),ke=c(e),se=r(e,"H2",{class:!0});var Tp=l(se);_e=r(Tp,"A",{id:!0,class:!0,href:!0});var Ou=l(_e);he=r(Ou,"SPAN",{});var Cu=l(he);y(ge.$$.fragment,Cu),Cu.forEach(t),Ou.forEach(t),w=c(Tp),R=r(Tp,"SPAN",{});var Pu=l(R);ve=n(Pu,"Preparing a multilingual corpus"),Pu.forEach(t),Tp.forEach(t),b=c(e),H=r(e,"P",{});var qp=l(H);qe=n(qp,"We\u2019ll use the "),pe=r(qp,"A",{href:!0,rel:!0});var Lu=l(pe);$e=n(Lu,"Multilingual Amazon Reviews Corpus"),Lu.forEach(t),Ce=n(qp," to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let\u2019s download the English and Spanish subsets from the Hugging Face Hub:"),qp.forEach(t),re=c(e),y(ze.$$.fragment,e),Ze=c(e),y(Ae.$$.fragment,e),ne=c(e),le=r(e,"P",{});var _s=l(le);Ie=n(_s,"As you can see, for each language there are 200,000 reviews for the "),ws=r(_s,"CODE",{});var Iu=l(ws);bs=n(Iu,"train"),Iu.forEach(t),vs=n(_s," split, and 5,000 reviews for each of the "),ys=r(_s,"CODE",{});var Ru=l(ys);zs=n(Ru,"validation"),Ru.forEach(t),He=n(_s," and "),rs=r(_s,"CODE",{});var Fu=l(rs);es=n(Fu,"test"),Fu.forEach(t),ls=n(_s," splits. The review information we are interested in is contained in the "),Ue=r(_s,"CODE",{});var Nu=l(Ue);Ks=n(Nu,"review_body"),Nu.forEach(t),is=n(_s," and "),Oe=r(_s,"CODE",{});var Gu=l(Oe);ks=n(Gu,"review_title"),Gu.forEach(t),Ds=n(_s," columns. Let\u2019s take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in "),Re=r(_s,"A",{href:!0});var Hu=l(Re);ps=n(Hu,"Chapter 5"),Hu.forEach(t),G=n(_s,":"),_s.forEach(t),ce=c(e),y(Me.$$.fragment,e),Ys=c(e),y(ue.$$.fragment,e),ms=c(e),y(ss.$$.fragment,e),Ss=c(e),De=r(e,"P",{});var vr=l(De);Js=n(vr,"This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the \u201Cmeh\u201D title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we\u2019ll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let\u2019s convert "),ts=r(vr,"CODE",{});var Uu=l(ts);je=n(Uu,"english_dataset"),Uu.forEach(t),Xs=n(vr," to a "),We=r(vr,"CODE",{});var Mu=l(We);Qs=n(Mu,"pandas.DataFrame"),Mu.forEach(t),Fe=n(vr," and compute the number of reviews per product category:"),vr.forEach(t),Zs=c(e),y(Be.$$.fragment,e),et=c(e),y(ye.$$.fragment,e),$s=c(e),Se=r(e,"P",{});var Lt=l(Se);vt=n(Lt,"The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let\u2019s focus on summarizing book reviews \u2014 after all, this is what the company was founded on! We can see two product categories that fit the bill ("),js=r(Lt,"CODE",{});var Wu=l(js);xs=n(Wu,"book"),Wu.forEach(t),pn=n(Lt," and "),yt=r(Lt,"CODE",{});var Bu=l(yt);st=n(Bu,"digital_ebook_purchase"),Bu.forEach(t),fa=n(Lt,"), so let\u2019s filter the datasets in both languages for just these products. As we saw in "),hs=r(Lt,"A",{href:!0});var Vu=l(hs);ga=n(Vu,"Chapter 5"),Vu.forEach(t),kt=n(Lt,", the "),Ft=r(Lt,"CODE",{});var Ku=l(Ft);_a=n(Ku,"Dataset.filter()"),Ku.forEach(t),tt=n(Lt," function allows us to slice a dataset very efficiently, so we can define a simple function to do this:"),Lt.forEach(t),Nt=c(e),y(Ve.$$.fragment,e),wa=c(e),xe=r(e,"P",{});var Us=l(xe);mn=n(Us,"Now when we apply this function to "),Gt=r(Us,"CODE",{});var Yu=l(Gt);ba=n(Yu,"english_dataset"),Yu.forEach(t),Es=n(Us," and "),cs=r(Us,"CODE",{});var Ju=l(cs);Ht=n(Ju,"spanish_dataset"),Ju.forEach(t),at=n(Us,", the result will contain just those rows involving the book categories. Before applying the filter, let\u2019s switch the format of "),Ut=r(Us,"CODE",{});var Xu=l(Ut);Mt=n(Xu,"english_dataset"),Xu.forEach(t),hn=n(Us," from "),$t=r(Us,"CODE",{});var Qu=l($t);jt=n(Qu,'"pandas"'),Qu.forEach(t),cn=n(Us," back to "),xt=r(Us,"CODE",{});var Zu=l(xt);Ke=n(Zu,'"arrow"'),Zu.forEach(t),nt=n(Us,":"),Us.forEach(t),va=c(e),y(Ts.$$.fragment,e),ya=c(e),Et=r(e,"P",{});var ed=l(Et);un=n(ed,"We can then apply the filter function, and as a sanity check let\u2019s inspect a sample of reviews to see if they are indeed about books:"),ed.forEach(t),Tt=c(e),y(ot.$$.fragment,e),ka=c(e),y(qs.$$.fragment,e),$a=c(e),Ye=r(e,"P",{});var an=l(Ye);as=n(an,"Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single "),Wt=r(an,"CODE",{});var sd=l(Wt);Bt=n(sd,"DatasetDict"),sd.forEach(t),dn=n(an," object. \u{1F917} Datasets provides a handy "),Vt=r(an,"CODE",{});var td=l(Vt);Kt=n(td,"concatenate_datasets()"),td.forEach(t),fn=n(an," function that (as the name suggests) will stack two "),Yt=r(an,"CODE",{});var ad=l(Yt);Jt=n(ad,"Dataset"),ad.forEach(t),gn=n(an," objects on top of each other. So, to create our bilingual dataset, we\u2019ll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn\u2019t overfit to a single language:"),an.forEach(t),ja=c(e),y(As.$$.fragment,e),rt=c(e),y(lt.$$.fragment,e),Xt=c(e),us=r(e,"P",{});var nd=l(us);xa=n(nd,"This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words:"),nd.forEach(t),Os=c(e),ds=r(e,"DIV",{class:!0});var zp=l(ds);Je=r(zp,"IMG",{class:!0,src:!0,alt:!0}),Qt=c(zp),it=r(zp,"IMG",{class:!0,src:!0,alt:!0}),zp.forEach(t),Zt=c(e),m=r(e,"P",{});var Dp=l(m);q=n(Dp,"To deal with this, we\u2019ll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we\u2019re dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty "),Ea=r(Dp,"CODE",{});var od=l(Ea);$o=n(od,"Dataset.filter()"),od.forEach(t),jo=n(Dp," method as follows:"),Dp.forEach(t),pt=c(e),y(qt.$$.fragment,e),_n=c(e),mt=r(e,"P",{});var rd=l(mt);xo=n(rd,"Now that we\u2019ve prepared our corpus, let\u2019s take a look at a few possible Transformer models that one might fine-tune on it!"),rd.forEach(t),wn=c(e),Cs=r(e,"H2",{class:!0});var Sp=l(Cs);ns=r(Sp,"A",{id:!0,class:!0,href:!0});var ld=l(ns);Ta=r(ld,"SPAN",{});var id=l(Ta);y(zt.$$.fragment,id),id.forEach(t),ld.forEach(t),qa=c(Sp),za=r(Sp,"SPAN",{});var pd=l(za);Eo=n(pd,"Models for text summarization"),pd.forEach(t),Sp.forEach(t),ea=c(e),ht=r(e,"P",{});var Ap=l(ht);Da=n(Ap,"If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we\u2019d like to \u201Ctranslate\u201D into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in "),sa=r(Ap,"A",{href:!0});var md=l(sa);To=n(md,"Chapter 1"),md.forEach(t),Dt=n(Ap,", although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization."),Ap.forEach(t),bn=c(e),ct=r(e,"TABLE",{});var Op=l(ct);Sa=r(Op,"THEAD",{});var hd=l(Sa);Ps=r(hd,"TR",{});var yr=l(Ps);ta=r(yr,"TH",{align:!0});var cd=l(ta);St=n(cd,"Transformer model"),cd.forEach(t),qo=c(yr),Aa=r(yr,"TH",{});var ud=l(Aa);zo=n(ud,"Description"),ud.forEach(t),Do=c(yr),ut=r(yr,"TH",{align:!0});var dd=l(ut);So=n(dd,"Multilingual?"),dd.forEach(t),yr.forEach(t),hd.forEach(t),Oa=c(Op),Ne=r(Op,"TBODY",{});var Ms=l(Ne);Ls=r(Ms,"TR",{});var kr=l(Ls);aa=r(kr,"TD",{align:!0});var fd=l(aa);At=r(fd,"A",{href:!0,rel:!0});var gd=l(At);Is=n(gd,"GPT-2"),gd.forEach(t),fd.forEach(t),Ca=c(kr),Pa=r(kr,"TD",{});var _d=l(Pa);Ao=n(_d,"Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending \u201CTL;DR\u201D at the end of the input text."),_d.forEach(t),Oo=c(kr),os=r(kr,"TD",{align:!0});var wd=l(os);Co=n(wd,"\u274C"),wd.forEach(t),kr.forEach(t),Po=c(Ms),Rs=r(Ms,"TR",{});var $r=l(Rs);na=r($r,"TD",{align:!0});var bd=l(na);Fs=r(bd,"A",{href:!0,rel:!0});var vd=l(Fs);Lo=n(vd,"PEGASUS"),vd.forEach(t),bd.forEach(t),Qp=c($r),Or=r($r,"TD",{});var yd=l(Or);Zp=n(yd,"Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks."),yd.forEach(t),em=c($r),Io=r($r,"TD",{align:!0});var kd=l(Io);sm=n(kd,"\u274C"),kd.forEach(t),$r.forEach(t),tm=c(Ms),oa=r(Ms,"TR",{});var jr=l(oa);Ro=r(jr,"TD",{align:!0});var $d=l(Ro);vn=r($d,"A",{href:!0,rel:!0});var jd=l(vn);am=n(jd,"T5"),jd.forEach(t),$d.forEach(t),nm=c(jr),yn=r(jr,"TD",{});var Cp=l(yn);om=n(Cp,"A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is "),Cr=r(Cp,"CODE",{});var xd=l(Cr);rm=n(xd,"summarize: ARTICLE"),xd.forEach(t),lm=n(Cp,"."),Cp.forEach(t),im=c(jr),Fo=r(jr,"TD",{align:!0});var Ed=l(Fo);pm=n(Ed,"\u274C"),Ed.forEach(t),jr.forEach(t),mm=c(Ms),ra=r(Ms,"TR",{});var xr=l(ra);No=r(xr,"TD",{align:!0});var Td=l(No);kn=r(Td,"A",{href:!0,rel:!0});var qd=l(kn);hm=n(qd,"mT5"),qd.forEach(t),Td.forEach(t),cm=c(xr),Pr=r(xr,"TD",{});var zd=l(Pr);um=n(zd,"A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages."),zd.forEach(t),dm=c(xr),Go=r(xr,"TD",{align:!0});var Dd=l(Go);fm=n(Dd,"\u2705"),Dd.forEach(t),xr.forEach(t),gm=c(Ms),la=r(Ms,"TR",{});var Er=l(la);Ho=r(Er,"TD",{align:!0});var Sd=l(Ho);$n=r(Sd,"A",{href:!0,rel:!0});var Ad=l($n);_m=n(Ad,"BART"),Ad.forEach(t),Sd.forEach(t),wm=c(Er),Lr=r(Er,"TD",{});var Od=l(Lr);bm=n(Od,"A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2."),Od.forEach(t),vm=c(Er),Uo=r(Er,"TD",{align:!0});var Cd=l(Uo);ym=n(Cd,"\u274C"),Cd.forEach(t),Er.forEach(t),km=c(Ms),ia=r(Ms,"TR",{});var Tr=l(ia);Mo=r(Tr,"TD",{align:!0});var Pd=l(Mo);jn=r(Pd,"A",{href:!0,rel:!0});var Ld=l(jn);$m=n(Ld,"mBART-50"),Ld.forEach(t),Pd.forEach(t),jm=c(Tr),Ir=r(Tr,"TD",{});var Id=l(Ir);xm=n(Id,"A multilingual version of BART, pretrained on 50 languages."),Id.forEach(t),Em=c(Tr),Wo=r(Tr,"TD",{align:!0});var Rd=l(Wo);Tm=n(Rd,"\u2705"),Rd.forEach(t),Tr.forEach(t),Ms.forEach(t),Op.forEach(t),Bl=c(e),Bo=r(e,"P",{});var Fd=l(Bo);qm=n(Fd,"As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a \u201Chigh-resource\u201D language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!"),Fd.forEach(t),Vl=c(e),La=r(e,"P",{});var Pp=l(La);zm=n(Pp,"We\u2019ll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like "),Rr=r(Pp,"CODE",{});var Nd=l(Rr);Dm=n(Nd,"summarize:"),Nd.forEach(t),Sm=n(Pp," which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!"),Pp.forEach(t),Kl=c(e),pa=r(e,"DIV",{class:!0});var Lp=l(pa);xn=r(Lp,"IMG",{class:!0,src:!0,alt:!0}),Am=c(Lp),En=r(Lp,"IMG",{class:!0,src:!0,alt:!0}),Lp.forEach(t),Yl=c(e),Vo=r(e,"P",{});var Gd=l(Vo);Om=n(Gd,"mT5 doesn\u2019t use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we\u2019ve picked a model, let\u2019s take a look at preparing our data for training."),Gd.forEach(t),Jl=c(e),y(Ia.$$.fragment,e),Xl=c(e),ma=r(e,"H2",{class:!0});var Ip=l(ma);Ra=r(Ip,"A",{id:!0,class:!0,href:!0});var Hd=l(Ra);Fr=r(Hd,"SPAN",{});var Ud=l(Fr);y(Tn.$$.fragment,Ud),Ud.forEach(t),Hd.forEach(t),Cm=c(Ip),Nr=r(Ip,"SPAN",{});var Md=l(Nr);Pm=n(Md,"Preprocessing the data"),Md.forEach(t),Ip.forEach(t),Ql=c(e),y(qn.$$.fragment,e),Zl=c(e),Fa=r(e,"P",{});var Rp=l(Fa);Lm=n(Rp,"Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We\u2019ll use "),Gr=r(Rp,"CODE",{});var Wd=l(Gr);Im=n(Wd,"mt5-small"),Wd.forEach(t),Rm=n(Rp," as our checkpoint so we can fine-tune the model in a reasonable amount of time:"),Rp.forEach(t),ei=c(e),y(zn.$$.fragment,e),si=c(e),y(Na.$$.fragment,e),ti=c(e),Ko=r(e,"P",{});var Bd=l(Ko);Fm=n(Bd,"Let\u2019s test out the mT5 tokenizer on a small example:"),Bd.forEach(t),ai=c(e),y(Dn.$$.fragment,e),ni=c(e),y(Sn.$$.fragment,e),oi=c(e),fs=r(e,"P",{});var It=l(fs);Nm=n(It,"Here we can see the familiar "),Hr=r(It,"CODE",{});var Vd=l(Hr);Gm=n(Vd,"input_ids"),Vd.forEach(t),Hm=n(It," and "),Ur=r(It,"CODE",{});var Kd=l(Ur);Um=n(Kd,"attention_mask"),Kd.forEach(t),Mm=n(It," that we encountered in our first fine-tuning experiments back in "),Yo=r(It,"A",{href:!0});var Yd=l(Yo);Wm=n(Yd,"Chapter 3"),Yd.forEach(t),Bm=n(It,". Let\u2019s decode these input IDs with the tokenizer\u2019s "),Mr=r(It,"CODE",{});var Jd=l(Mr);Vm=n(Jd,"convert_ids_to_tokens()"),Jd.forEach(t),Km=n(It," function to see what kind of tokenizer we\u2019re dealing with:"),It.forEach(t),ri=c(e),y(An.$$.fragment,e),li=c(e),y(On.$$.fragment,e),ii=c(e),Ns=r(e,"P",{});var nn=l(Ns);Ym=n(nn,"The special Unicode character "),Wr=r(nn,"CODE",{});var Xd=l(Wr);Jm=n(Xd,"\u2581"),Xd.forEach(t),Xm=n(nn," and end-of-sequence token "),Br=r(nn,"CODE",{});var Qd=l(Br);Qm=n(Qd,"</s>"),Qd.forEach(t),Zm=n(nn," indicate that we\u2019re dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in "),Jo=r(nn,"A",{href:!0});var Zd=l(Jo);eh=n(Zd,"Chapter 6"),Zd.forEach(t),sh=n(nn,". Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters."),nn.forEach(t),pi=c(e),Ga=r(e,"P",{});var Fp=l(Ga);th=n(Fp,"To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model\u2019s maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don\u2019t pass excessively long inputs to our model. The tokenizers in \u{1F917} Transformers provide a nifty "),Vr=r(Fp,"CODE",{});var ef=l(Vr);ah=n(ef,"as_target_tokenizer()"),ef.forEach(t),nh=n(Fp," function that allows you to tokenize the labels in parallel to the inputs. This is typically done using a context manager inside a preprocessing function that first encodes the inputs, and then encodes the labels as a separate column. Here is an example of such a function for mT5:"),Fp.forEach(t),mi=c(e),y(Cn.$$.fragment,e),hi=c(e),gs=r(e,"P",{});var Rt=l(gs);oh=n(Rt,"Let\u2019s walk through this code to understand what\u2019s happening. The first thing we\u2019ve done is define values for "),Kr=r(Rt,"CODE",{});var sf=l(Kr);rh=n(sf,"max_input_length"),sf.forEach(t),lh=n(Rt," and "),Yr=r(Rt,"CODE",{});var tf=l(Yr);ih=n(tf,"max_target_length"),tf.forEach(t),ph=n(Rt,", which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we\u2019ve scaled these values accordingly. Then, in the  "),Jr=r(Rt,"CODE",{});var af=l(Jr);mh=n(af,"preprocess_function()"),af.forEach(t),hh=n(Rt," itself we can see the reviews are first tokenized, followed by the titles with "),Xr=r(Rt,"CODE",{});var nf=l(Xr);ch=n(nf,"as_target_tokenizer()"),nf.forEach(t),uh=n(Rt,"."),Rt.forEach(t),ci=c(e),Ot=r(e,"P",{});var qr=l(Ot);dh=n(qr,"With "),Qr=r(qr,"CODE",{});var of=l(Qr);fh=n(of,"preprocess_function()"),of.forEach(t),gh=n(qr,", it is then a simple matter to tokenize the whole corpus using the handy "),Zr=r(qr,"CODE",{});var rf=l(Zr);_h=n(rf,"Dataset.map()"),rf.forEach(t),wh=n(qr," function we\u2019ve used extensively throughout this course:"),qr.forEach(t),ui=c(e),y(Pn.$$.fragment,e),di=c(e),Xo=r(e,"P",{});var lf=l(Xo);bh=n(lf,"Now that the corpus has been preprocessed, let\u2019s take a look at some metrics that are commonly used for summarization. As we\u2019ll see, there is no silver bullet when it comes to measuring the quality of machine-generated text."),lf.forEach(t),fi=c(e),y(Ha.$$.fragment,e),gi=c(e),ha=r(e,"H2",{class:!0});var Np=l(ha);Ua=r(Np,"A",{id:!0,class:!0,href:!0});var pf=l(Ua);el=r(pf,"SPAN",{});var mf=l(el);y(Ln.$$.fragment,mf),mf.forEach(t),pf.forEach(t),vh=c(Np),sl=r(Np,"SPAN",{});var hf=l(sl);yh=n(hf,"Metrics for text summarization"),hf.forEach(t),Np.forEach(t),_i=c(e),y(In.$$.fragment,e),wi=c(e),Qo=r(e,"P",{});var cf=l(Qo);kh=n(cf,"In comparison to most of the other tasks we\u2019ve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like \u201CI loved reading the Hunger Games\u201D, there are multiple valid summaries, like \u201CI loved the Hunger Games\u201D or \u201CHunger Games is a great read\u201D. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution \u2014 even humans would fare poorly under such a metric, because we all have our own writing style."),cf.forEach(t),bi=c(e),Ma=r(e,"P",{});var Gp=l(Ma);$h=n(Gp,"For summarization, one of the most commonly used metrics is the "),Rn=r(Gp,"A",{href:!0,rel:!0});var uf=l(Rn);jh=n(uf,"ROUGE score"),uf.forEach(t),xh=n(Gp," (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries:"),Gp.forEach(t),vi=c(e),y(Fn.$$.fragment,e),yi=c(e),Ct=r(e,"P",{});var zr=l(Ct);Eh=n(zr,"One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the "),tl=r(zr,"EM",{});var df=l(tl);Th=n(df,"precision"),df.forEach(t),qh=n(zr," and "),al=r(zr,"EM",{});var ff=l(al);zh=n(ff,"recall"),ff.forEach(t),Dh=n(zr," scores for the overlap."),zr.forEach(t),ki=c(e),y(Wa.$$.fragment,e),$i=c(e),Nn=r(e,"P",{});var vu=l(Nn);Sh=n(vu,`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),ji=dg(vu),vu.forEach(t),xi=c(e),Gn=r(e,"P",{});var yu=l(Gn);Ah=n(yu,`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been \u201CI really really loved reading the Hunger Games all night\u201D. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),Ei=dg(yu),yu.forEach(t),Ti=c(e),Ba=r(e,"P",{});var Hp=l(Ba);Oh=n(Hp,"Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in \u{1F917} Datasets by first installing the "),nl=r(Hp,"CODE",{});var gf=l(nl);Ch=n(gf,"rouge_score"),gf.forEach(t),Ph=n(Hp," package:"),Hp.forEach(t),qi=c(e),y(Hn.$$.fragment,e),zi=c(e),Zo=r(e,"P",{});var _f=l(Zo);Lh=n(_f,"and then loading the ROUGE metric as follows:"),_f.forEach(t),Di=c(e),y(Un.$$.fragment,e),Si=c(e),Va=r(e,"P",{});var Up=l(Va);Ih=n(Up,"Then we can use the "),ol=r(Up,"CODE",{});var wf=l(ol);Rh=n(wf,"rouge_score.compute()"),wf.forEach(t),Fh=n(Up," function to calculate all the metrics at once:"),Up.forEach(t),Ai=c(e),y(Mn.$$.fragment,e),Oi=c(e),y(Wn.$$.fragment,e),Ci=c(e),Xe=r(e,"P",{});var Ws=l(Xe);Nh=n(Ws,"Whoa, there\u2019s a lot of information in that output \u2014 what does it all mean? First, \u{1F917} Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the "),rl=r(Ws,"CODE",{});var bf=l(rl);Gh=n(bf,"low"),bf.forEach(t),Hh=n(Ws,", "),ll=r(Ws,"CODE",{});var vf=l(ll);Uh=n(vf,"mid"),vf.forEach(t),Mh=n(Ws,", and "),il=r(Ws,"CODE",{});var yf=l(il);Wh=n(yf,"high"),yf.forEach(t),Bh=n(Ws," attributes you can see here. Moreover, \u{1F917} Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The "),pl=r(Ws,"CODE",{});var kf=l(pl);Vh=n(kf,"rouge1"),kf.forEach(t),Kh=n(Ws," variant is the overlap of unigrams \u2014 this is just a fancy way of saying the overlap of words and is exactly the metric we\u2019ve discussed above. To verify this, let\u2019s pull out the "),ml=r(Ws,"CODE",{});var $f=l(ml);Yh=n($f,"mid"),$f.forEach(t),Jh=n(Ws," value of our scores:"),Ws.forEach(t),Pi=c(e),y(Bn.$$.fragment,e),Li=c(e),y(Vn.$$.fragment,e),Ii=c(e),Qe=r(e,"P",{});var Bs=l(Qe);Xh=n(Bs,"Great, the precision and recall numbers match up! Now what about those other ROUGE scores? "),hl=r(Bs,"CODE",{});var jf=l(hl);Qh=n(jf,"rouge2"),jf.forEach(t),Zh=n(Bs," measures the overlap between bigrams (think the overlap of pairs of words), while "),cl=r(Bs,"CODE",{});var xf=l(cl);ec=n(xf,"rougeL"),xf.forEach(t),sc=n(Bs," and "),ul=r(Bs,"CODE",{});var Ef=l(ul);tc=n(Ef,"rougeLsum"),Ef.forEach(t),ac=n(Bs," measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The \u201Csum\u201D in "),dl=r(Bs,"CODE",{});var Tf=l(dl);nc=n(Tf,"rougeLsum"),Tf.forEach(t),oc=n(Bs," refers to the fact that this metric is computed over a whole summary, while "),fl=r(Bs,"CODE",{});var qf=l(fl);rc=n(qf,"rougeL"),qf.forEach(t),lc=n(Bs," is computed as the average over individual sentences."),Bs.forEach(t),Ri=c(e),y(Ka.$$.fragment,e),Fi=c(e),er=r(e,"P",{});var zf=l(er);ic=n(zf,"We\u2019ll use these ROUGE scores to track the performance of our model, but before doing that let\u2019s do something every good NLP practitioner should do: create a strong, yet simple baseline!"),zf.forEach(t),Ni=c(e),ca=r(e,"H3",{class:!0});var Mp=l(ca);Ya=r(Mp,"A",{id:!0,class:!0,href:!0});var Df=l(Ya);gl=r(Df,"SPAN",{});var Sf=l(gl);y(Kn.$$.fragment,Sf),Sf.forEach(t),Df.forEach(t),pc=c(Mp),_l=r(Mp,"SPAN",{});var Af=l(_l);mc=n(Af,"Creating a strong baseline"),Af.forEach(t),Mp.forEach(t),Gi=c(e),Gs=r(e,"P",{});var on=l(Gs);hc=n(on,"A common baseline for text summarization is to simply take the first three sentences of an article, often called the "),wl=r(on,"EM",{});var Of=l(wl);cc=n(Of,"lead-3"),Of.forEach(t),uc=n(on," baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like \u201CU.S.\u201D or \u201CU.N.\u201D \u2014 so instead we\u2019ll use the "),bl=r(on,"CODE",{});var Cf=l(bl);dc=n(Cf,"nltk"),Cf.forEach(t),fc=n(on," library, which includes a better algorithm to handle these cases. You can install the package using "),vl=r(on,"CODE",{});var Pf=l(vl);gc=n(Pf,"pip"),Pf.forEach(t),_c=n(on," as follows:"),on.forEach(t),Hi=c(e),y(Yn.$$.fragment,e),Ui=c(e),sr=r(e,"P",{});var Lf=l(sr);wc=n(Lf,"and then download the punctuation rules:"),Lf.forEach(t),Mi=c(e),y(Jn.$$.fragment,e),Wi=c(e),Ja=r(e,"P",{});var Wp=l(Ja);bc=n(Wp,"Next, we import the sentence tokenizer from "),yl=r(Wp,"CODE",{});var If=l(yl);vc=n(If,"nltk"),If.forEach(t),yc=n(Wp," and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let\u2019s also include this and test it on a training example:"),Wp.forEach(t),Bi=c(e),y(Xn.$$.fragment,e),Vi=c(e),y(Qn.$$.fragment,e),Ki=c(e),tr=r(e,"P",{});var Rf=l(tr);kc=n(Rf,"This seems to work, so let\u2019s now implement a function that extracts these \u201Csummaries\u201D from a dataset and computes the ROUGE scores for the baseline:"),Rf.forEach(t),Yi=c(e),y(Zn.$$.fragment,e),Ji=c(e),ar=r(e,"P",{});var Ff=l(ar);$c=n(Ff,"We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas:"),Ff.forEach(t),Xi=c(e),y(eo.$$.fragment,e),Qi=c(e),y(so.$$.fragment,e),Zi=c(e),Xa=r(e,"P",{});var Bp=l(Xa);jc=n(Bp,"We can see that the "),kl=r(Bp,"CODE",{});var Nf=l(kl);xc=n(Nf,"rouge2"),Nf.forEach(t),Ec=n(Bp," score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let\u2019s turn our attention toward fine-tuning mT5!"),Bp.forEach(t),ep=c(e),ft.l(e),nr=c(e),y(Qa.$$.fragment,e),sp=c(e),or=r(e,"P",{});var Gf=l(or);Tc=n(Gf,"The next thing we need to do is log in to the Hugging Face Hub. If you\u2019re running this code in a notebook, you can do so with the following utility function:"),Gf.forEach(t),tp=c(e),y(to.$$.fragment,e),ap=c(e),rr=r(e,"P",{});var Hf=l(rr);qc=n(Hf,"which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:"),Hf.forEach(t),np=c(e),y(ao.$$.fragment,e),op=c(e),Pe&&Pe.l(e),lr=c(e),Za=r(e,"P",{});var Vp=l(Za);zc=n(Vp,"Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like "),ir=r(Vp,"A",{href:!0});var Uf=l(ir);Dc=n(Uf,"causal language modeling"),Uf.forEach(t),Sc=n(Vp,"."),Vp.forEach(t),rp=c(e),Hs=r(e,"P",{});var rn=l(Hs);Ac=n(rn,"Luckily, \u{1F917} Transformers provides a "),$l=r(rn,"CODE",{});var Mf=l($l);Oc=n(Mf,"DataCollatorForSeq2Seq"),Mf.forEach(t),Cc=n(rn," collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the "),jl=r(rn,"CODE",{});var Wf=l(jl);Pc=n(Wf,"tokenizer"),Wf.forEach(t),Lc=n(rn," and "),xl=r(rn,"CODE",{});var Bf=l(xl);Ic=n(Bf,"model"),Bf.forEach(t),Rc=n(rn,":"),rn.forEach(t),lp=c(e),_t.l(e),pr=c(e),mr=r(e,"P",{});var Vf=l(mr);Fc=n(Vf,"Let\u2019s see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won\u2019t know how to pad these elements:"),Vf.forEach(t),ip=c(e),y(no.$$.fragment,e),pp=c(e),Pt=r(e,"P",{});var Dr=l(Pt);Nc=n(Dr,"Since the collator expects a list of "),El=r(Dr,"CODE",{});var Kf=l(El);Gc=n(Kf,"dict"),Kf.forEach(t),Hc=n(Dr,"s, where each "),Tl=r(Dr,"CODE",{});var Yf=l(Tl);Uc=n(Yf,"dict"),Yf.forEach(t),Mc=n(Dr," represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator:"),Dr.forEach(t),mp=c(e),y(oo.$$.fragment,e),hp=c(e),y(ro.$$.fragment,e),cp=c(e),Ee=r(e,"P",{});var Ge=l(Ee);Wc=n(Ge,"The main thing to notice here is that the first example is longer than the second one, so the "),ql=r(Ge,"CODE",{});var Jf=l(ql);Bc=n(Jf,"input_ids"),Jf.forEach(t),Vc=n(Ge," and "),zl=r(Ge,"CODE",{});var Xf=l(zl);Kc=n(Xf,"attention_mask"),Xf.forEach(t),Yc=n(Ge," of the second example have been padded on the right with a "),Dl=r(Ge,"CODE",{});var Qf=l(Dl);Jc=n(Qf,"[PAD]"),Qf.forEach(t),Xc=n(Ge," token (whose ID is "),Sl=r(Ge,"CODE",{});var Zf=l(Sl);Qc=n(Zf,"0"),Zf.forEach(t),Zc=n(Ge,"). Similarly, we can see that the "),Al=r(Ge,"CODE",{});var eg=l(Al);eu=n(eg,"labels"),eg.forEach(t),su=n(Ge," have been padded with "),Ol=r(Ge,"CODE",{});var sg=l(Ol);tu=n(sg,"-100"),sg.forEach(t),au=n(Ge,"s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new "),Cl=r(Ge,"CODE",{});var tg=l(Cl);nu=n(tg,"decoder_input_ids"),tg.forEach(t),ou=n(Ge," which has shifted the labels to the right by inserting a "),Pl=r(Ge,"CODE",{});var ag=l(Pl);ru=n(ag,"[PAD]"),ag.forEach(t),lu=n(Ge," token in the first entry."),Ge.forEach(t),up=c(e),bt.l(e),hr=c(e),Le&&Le.l(e),cr=c(e),ua=r(e,"H2",{class:!0});var Kp=l(ua);en=r(Kp,"A",{id:!0,class:!0,href:!0});var ng=l(en);Ll=r(ng,"SPAN",{});var og=l(Ll);y(lo.$$.fragment,og),og.forEach(t),ng.forEach(t),iu=c(Kp),Il=r(Kp,"SPAN",{});var rg=l(Il);pu=n(rg,"Using your fine-tuned model"),rg.forEach(t),Kp.forEach(t),dp=c(e),sn=r(e,"P",{});var Yp=l(sn);mu=n(Yp,"Once you\u2019ve pushed the model to the Hub, you can play with it either via the inference widget or with a "),Rl=r(Yp,"CODE",{});var lg=l(Rl);hu=n(lg,"pipeline"),lg.forEach(t),cu=n(Yp," object, as follows:"),Yp.forEach(t),fp=c(e),y(io.$$.fragment,e),gp=c(e),ur=r(e,"P",{});var ig=l(ur);uu=n(ig,"We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let\u2019s implement a simple function to show the review, title, and generated summary together:"),ig.forEach(t),_p=c(e),y(po.$$.fragment,e),wp=c(e),dr=r(e,"P",{});var pg=l(dr);du=n(pg,"Let\u2019s take a look at one of the English examples we get:"),pg.forEach(t),bp=c(e),y(mo.$$.fragment,e),vp=c(e),y(ho.$$.fragment,e),yp=c(e),tn=r(e,"P",{});var Jp=l(tn);fu=n(Jp,"This is not too bad! We can see that our model has actually been able to perform "),Fl=r(Jp,"EM",{});var mg=l(Fl);gu=n(mg,"abstractive"),mg.forEach(t),_u=n(Jp," summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:"),Jp.forEach(t),kp=c(e),y(co.$$.fragment,e),$p=c(e),y(uo.$$.fragment,e),jp=c(e),fr=r(e,"P",{});var hg=l(fr);wu=n(hg,"The summary translates into \u201CVery easy to read\u201D in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it\u2019s like to deal with a multilingual corpus!"),hg.forEach(t),xp=c(e),gr=r(e,"P",{});var cg=l(gr);bu=n(cg,"Next, we\u2019ll turn our attention to a slightly more complex task: training a language model from scratch."),cg.forEach(t),this.h()},h(){f(u,"name","hf:doc:metadata"),f(u,"content",JSON.stringify(Ug)),f(x,"id","summarization"),f(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(x,"href","#summarization"),f(C,"class","relative group"),f(M,"href","https://huggingface.co/models?pipeline_tag=summarization&sort=downloads"),f(M,"rel","nofollow"),f(ae,"href","https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es"),f(ae,"rel","nofollow"),f(fe,"class","max-w-md"),f(V,"class","w-full flex justify-center"),f(_e,"id","preparing-a-multilingual-corpus"),f(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_e,"href","#preparing-a-multilingual-corpus"),f(se,"class","relative group"),f(pe,"href","https://huggingface.co/datasets/amazon_reviews_multi"),f(pe,"rel","nofollow"),f(Re,"href","/course/chapter5"),f(hs,"href","/course/chapter5"),f(Je,"class","block dark:hidden"),Xp(Je.src,yo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg")||f(Je,"src",yo),f(Je,"alt","Word count distributions for the review titles and texts."),f(it,"class","hidden dark:block"),Xp(it.src,ko="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg")||f(it,"src",ko),f(it,"alt","Word count distributions for the review titles and texts."),f(ds,"class","flex justify-center"),f(ns,"id","models-for-text-summarization"),f(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ns,"href","#models-for-text-summarization"),f(Cs,"class","relative group"),f(sa,"href","/course/chapter1"),f(ta,"align","center"),f(ut,"align","center"),f(At,"href","https://huggingface.co/gpt2-xl"),f(At,"rel","nofollow"),f(aa,"align","center"),f(os,"align","center"),f(Fs,"href","https://huggingface.co/google/pegasus-large"),f(Fs,"rel","nofollow"),f(na,"align","center"),f(Io,"align","center"),f(vn,"href","https://huggingface.co/t5-base"),f(vn,"rel","nofollow"),f(Ro,"align","center"),f(Fo,"align","center"),f(kn,"href","https://huggingface.co/google/mt5-base"),f(kn,"rel","nofollow"),f(No,"align","center"),f(Go,"align","center"),f($n,"href","https://huggingface.co/facebook/bart-base"),f($n,"rel","nofollow"),f(Ho,"align","center"),f(Uo,"align","center"),f(jn,"href","https://huggingface.co/facebook/mbart-large-50"),f(jn,"rel","nofollow"),f(Mo,"align","center"),f(Wo,"align","center"),f(xn,"class","block dark:hidden"),Xp(xn.src,$u="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg")||f(xn,"src",$u),f(xn,"alt","Different tasks performed by the T5 architecture."),f(En,"class","hidden dark:block"),Xp(En.src,ju="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg")||f(En,"src",ju),f(En,"alt","Different tasks performed by the T5 architecture."),f(pa,"class","flex justify-center"),f(Ra,"id","preprocessing-the-data"),f(Ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ra,"href","#preprocessing-the-data"),f(ma,"class","relative group"),f(Yo,"href","/course/chapter3"),f(Jo,"href","/course/chapter6"),f(Ua,"id","metrics-for-text-summarization"),f(Ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ua,"href","#metrics-for-text-summarization"),f(ha,"class","relative group"),f(Rn,"href","https://en.wikipedia.org/wiki/ROUGE_(metric)"),f(Rn,"rel","nofollow"),ji.a=null,Ei.a=null,f(Ya,"id","creating-a-strong-baseline"),f(Ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ya,"href","#creating-a-strong-baseline"),f(ca,"class","relative group"),f(ir,"href","/course/chapter7/6"),f(en,"id","using-your-finetuned-model"),f(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(en,"href","#using-your-finetuned-model"),f(ua,"class","relative group")},m(e,p){s(document.head,u),i(e,j,p),k(d,e,p),i(e,E,p),i(e,C,p),s(C,x),s(x,A),k(P,A,null),s(C,T),s(C,S),s(S,I),i(e,D,p),fo[O].m(e,p),i(e,J,p),i(e,X,p),s(X,Q),s(X,W),s(W,Z),s(X,L),i(e,U,p),k(ee,e,p),i(e,F,p),i(e,B,p),s(B,te),s(B,M),s(M,ie),s(B,Y),s(B,ae),s(ae,de),s(B,we),i(e,Te,p),i(e,V,p),s(V,fe),i(e,me,p),i(e,oe,p),s(oe,be),i(e,ke,p),i(e,se,p),s(se,_e),s(_e,he),k(ge,he,null),s(se,w),s(se,R),s(R,ve),i(e,b,p),i(e,H,p),s(H,qe),s(H,pe),s(pe,$e),s(H,Ce),i(e,re,p),k(ze,e,p),i(e,Ze,p),k(Ae,e,p),i(e,ne,p),i(e,le,p),s(le,Ie),s(le,ws),s(ws,bs),s(le,vs),s(le,ys),s(ys,zs),s(le,He),s(le,rs),s(rs,es),s(le,ls),s(le,Ue),s(Ue,Ks),s(le,is),s(le,Oe),s(Oe,ks),s(le,Ds),s(le,Re),s(Re,ps),s(le,G),i(e,ce,p),k(Me,e,p),i(e,Ys,p),k(ue,e,p),i(e,ms,p),k(ss,e,p),i(e,Ss,p),i(e,De,p),s(De,Js),s(De,ts),s(ts,je),s(De,Xs),s(De,We),s(We,Qs),s(De,Fe),i(e,Zs,p),k(Be,e,p),i(e,et,p),k(ye,e,p),i(e,$s,p),i(e,Se,p),s(Se,vt),s(Se,js),s(js,xs),s(Se,pn),s(Se,yt),s(yt,st),s(Se,fa),s(Se,hs),s(hs,ga),s(Se,kt),s(Se,Ft),s(Ft,_a),s(Se,tt),i(e,Nt,p),k(Ve,e,p),i(e,wa,p),i(e,xe,p),s(xe,mn),s(xe,Gt),s(Gt,ba),s(xe,Es),s(xe,cs),s(cs,Ht),s(xe,at),s(xe,Ut),s(Ut,Mt),s(xe,hn),s(xe,$t),s($t,jt),s(xe,cn),s(xe,xt),s(xt,Ke),s(xe,nt),i(e,va,p),k(Ts,e,p),i(e,ya,p),i(e,Et,p),s(Et,un),i(e,Tt,p),k(ot,e,p),i(e,ka,p),k(qs,e,p),i(e,$a,p),i(e,Ye,p),s(Ye,as),s(Ye,Wt),s(Wt,Bt),s(Ye,dn),s(Ye,Vt),s(Vt,Kt),s(Ye,fn),s(Ye,Yt),s(Yt,Jt),s(Ye,gn),i(e,ja,p),k(As,e,p),i(e,rt,p),k(lt,e,p),i(e,Xt,p),i(e,us,p),s(us,xa),i(e,Os,p),i(e,ds,p),s(ds,Je),s(ds,Qt),s(ds,it),i(e,Zt,p),i(e,m,p),s(m,q),s(m,Ea),s(Ea,$o),s(m,jo),i(e,pt,p),k(qt,e,p),i(e,_n,p),i(e,mt,p),s(mt,xo),i(e,wn,p),i(e,Cs,p),s(Cs,ns),s(ns,Ta),k(zt,Ta,null),s(Cs,qa),s(Cs,za),s(za,Eo),i(e,ea,p),i(e,ht,p),s(ht,Da),s(ht,sa),s(sa,To),s(ht,Dt),i(e,bn,p),i(e,ct,p),s(ct,Sa),s(Sa,Ps),s(Ps,ta),s(ta,St),s(Ps,qo),s(Ps,Aa),s(Aa,zo),s(Ps,Do),s(Ps,ut),s(ut,So),s(ct,Oa),s(ct,Ne),s(Ne,Ls),s(Ls,aa),s(aa,At),s(At,Is),s(Ls,Ca),s(Ls,Pa),s(Pa,Ao),s(Ls,Oo),s(Ls,os),s(os,Co),s(Ne,Po),s(Ne,Rs),s(Rs,na),s(na,Fs),s(Fs,Lo),s(Rs,Qp),s(Rs,Or),s(Or,Zp),s(Rs,em),s(Rs,Io),s(Io,sm),s(Ne,tm),s(Ne,oa),s(oa,Ro),s(Ro,vn),s(vn,am),s(oa,nm),s(oa,yn),s(yn,om),s(yn,Cr),s(Cr,rm),s(yn,lm),s(oa,im),s(oa,Fo),s(Fo,pm),s(Ne,mm),s(Ne,ra),s(ra,No),s(No,kn),s(kn,hm),s(ra,cm),s(ra,Pr),s(Pr,um),s(ra,dm),s(ra,Go),s(Go,fm),s(Ne,gm),s(Ne,la),s(la,Ho),s(Ho,$n),s($n,_m),s(la,wm),s(la,Lr),s(Lr,bm),s(la,vm),s(la,Uo),s(Uo,ym),s(Ne,km),s(Ne,ia),s(ia,Mo),s(Mo,jn),s(jn,$m),s(ia,jm),s(ia,Ir),s(Ir,xm),s(ia,Em),s(ia,Wo),s(Wo,Tm),i(e,Bl,p),i(e,Bo,p),s(Bo,qm),i(e,Vl,p),i(e,La,p),s(La,zm),s(La,Rr),s(Rr,Dm),s(La,Sm),i(e,Kl,p),i(e,pa,p),s(pa,xn),s(pa,Am),s(pa,En),i(e,Yl,p),i(e,Vo,p),s(Vo,Om),i(e,Jl,p),k(Ia,e,p),i(e,Xl,p),i(e,ma,p),s(ma,Ra),s(Ra,Fr),k(Tn,Fr,null),s(ma,Cm),s(ma,Nr),s(Nr,Pm),i(e,Ql,p),k(qn,e,p),i(e,Zl,p),i(e,Fa,p),s(Fa,Lm),s(Fa,Gr),s(Gr,Im),s(Fa,Rm),i(e,ei,p),k(zn,e,p),i(e,si,p),k(Na,e,p),i(e,ti,p),i(e,Ko,p),s(Ko,Fm),i(e,ai,p),k(Dn,e,p),i(e,ni,p),k(Sn,e,p),i(e,oi,p),i(e,fs,p),s(fs,Nm),s(fs,Hr),s(Hr,Gm),s(fs,Hm),s(fs,Ur),s(Ur,Um),s(fs,Mm),s(fs,Yo),s(Yo,Wm),s(fs,Bm),s(fs,Mr),s(Mr,Vm),s(fs,Km),i(e,ri,p),k(An,e,p),i(e,li,p),k(On,e,p),i(e,ii,p),i(e,Ns,p),s(Ns,Ym),s(Ns,Wr),s(Wr,Jm),s(Ns,Xm),s(Ns,Br),s(Br,Qm),s(Ns,Zm),s(Ns,Jo),s(Jo,eh),s(Ns,sh),i(e,pi,p),i(e,Ga,p),s(Ga,th),s(Ga,Vr),s(Vr,ah),s(Ga,nh),i(e,mi,p),k(Cn,e,p),i(e,hi,p),i(e,gs,p),s(gs,oh),s(gs,Kr),s(Kr,rh),s(gs,lh),s(gs,Yr),s(Yr,ih),s(gs,ph),s(gs,Jr),s(Jr,mh),s(gs,hh),s(gs,Xr),s(Xr,ch),s(gs,uh),i(e,ci,p),i(e,Ot,p),s(Ot,dh),s(Ot,Qr),s(Qr,fh),s(Ot,gh),s(Ot,Zr),s(Zr,_h),s(Ot,wh),i(e,ui,p),k(Pn,e,p),i(e,di,p),i(e,Xo,p),s(Xo,bh),i(e,fi,p),k(Ha,e,p),i(e,gi,p),i(e,ha,p),s(ha,Ua),s(Ua,el),k(Ln,el,null),s(ha,vh),s(ha,sl),s(sl,yh),i(e,_i,p),k(In,e,p),i(e,wi,p),i(e,Qo,p),s(Qo,kh),i(e,bi,p),i(e,Ma,p),s(Ma,$h),s(Ma,Rn),s(Rn,jh),s(Ma,xh),i(e,vi,p),k(Fn,e,p),i(e,yi,p),i(e,Ct,p),s(Ct,Eh),s(Ct,tl),s(tl,Th),s(Ct,qh),s(Ct,al),s(al,zh),s(Ct,Dh),i(e,ki,p),k(Wa,e,p),i(e,$i,p),i(e,Nn,p),s(Nn,Sh),ji.m(wg,Nn),i(e,xi,p),i(e,Gn,p),s(Gn,Ah),Ei.m(bg,Gn),i(e,Ti,p),i(e,Ba,p),s(Ba,Oh),s(Ba,nl),s(nl,Ch),s(Ba,Ph),i(e,qi,p),k(Hn,e,p),i(e,zi,p),i(e,Zo,p),s(Zo,Lh),i(e,Di,p),k(Un,e,p),i(e,Si,p),i(e,Va,p),s(Va,Ih),s(Va,ol),s(ol,Rh),s(Va,Fh),i(e,Ai,p),k(Mn,e,p),i(e,Oi,p),k(Wn,e,p),i(e,Ci,p),i(e,Xe,p),s(Xe,Nh),s(Xe,rl),s(rl,Gh),s(Xe,Hh),s(Xe,ll),s(ll,Uh),s(Xe,Mh),s(Xe,il),s(il,Wh),s(Xe,Bh),s(Xe,pl),s(pl,Vh),s(Xe,Kh),s(Xe,ml),s(ml,Yh),s(Xe,Jh),i(e,Pi,p),k(Bn,e,p),i(e,Li,p),k(Vn,e,p),i(e,Ii,p),i(e,Qe,p),s(Qe,Xh),s(Qe,hl),s(hl,Qh),s(Qe,Zh),s(Qe,cl),s(cl,ec),s(Qe,sc),s(Qe,ul),s(ul,tc),s(Qe,ac),s(Qe,dl),s(dl,nc),s(Qe,oc),s(Qe,fl),s(fl,rc),s(Qe,lc),i(e,Ri,p),k(Ka,e,p),i(e,Fi,p),i(e,er,p),s(er,ic),i(e,Ni,p),i(e,ca,p),s(ca,Ya),s(Ya,gl),k(Kn,gl,null),s(ca,pc),s(ca,_l),s(_l,mc),i(e,Gi,p),i(e,Gs,p),s(Gs,hc),s(Gs,wl),s(wl,cc),s(Gs,uc),s(Gs,bl),s(bl,dc),s(Gs,fc),s(Gs,vl),s(vl,gc),s(Gs,_c),i(e,Hi,p),k(Yn,e,p),i(e,Ui,p),i(e,sr,p),s(sr,wc),i(e,Mi,p),k(Jn,e,p),i(e,Wi,p),i(e,Ja,p),s(Ja,bc),s(Ja,yl),s(yl,vc),s(Ja,yc),i(e,Bi,p),k(Xn,e,p),i(e,Vi,p),k(Qn,e,p),i(e,Ki,p),i(e,tr,p),s(tr,kc),i(e,Yi,p),k(Zn,e,p),i(e,Ji,p),i(e,ar,p),s(ar,$c),i(e,Xi,p),k(eo,e,p),i(e,Qi,p),k(so,e,p),i(e,Zi,p),i(e,Xa,p),s(Xa,jc),s(Xa,kl),s(kl,xc),s(Xa,Ec),i(e,ep,p),go[dt].m(e,p),i(e,nr,p),k(Qa,e,p),i(e,sp,p),i(e,or,p),s(or,Tc),i(e,tp,p),k(to,e,p),i(e,ap,p),i(e,rr,p),s(rr,qc),i(e,np,p),k(ao,e,p),i(e,op,p),Pe&&Pe.m(e,p),i(e,lr,p),i(e,Za,p),s(Za,zc),s(Za,ir),s(ir,Dc),s(Za,Sc),i(e,rp,p),i(e,Hs,p),s(Hs,Ac),s(Hs,$l),s($l,Oc),s(Hs,Cc),s(Hs,jl),s(jl,Pc),s(Hs,Lc),s(Hs,xl),s(xl,Ic),s(Hs,Rc),i(e,lp,p),_o[gt].m(e,p),i(e,pr,p),i(e,mr,p),s(mr,Fc),i(e,ip,p),k(no,e,p),i(e,pp,p),i(e,Pt,p),s(Pt,Nc),s(Pt,El),s(El,Gc),s(Pt,Hc),s(Pt,Tl),s(Tl,Uc),s(Pt,Mc),i(e,mp,p),k(oo,e,p),i(e,hp,p),k(ro,e,p),i(e,cp,p),i(e,Ee,p),s(Ee,Wc),s(Ee,ql),s(ql,Bc),s(Ee,Vc),s(Ee,zl),s(zl,Kc),s(Ee,Yc),s(Ee,Dl),s(Dl,Jc),s(Ee,Xc),s(Ee,Sl),s(Sl,Qc),s(Ee,Zc),s(Ee,Al),s(Al,eu),s(Ee,su),s(Ee,Ol),s(Ol,tu),s(Ee,au),s(Ee,Cl),s(Cl,nu),s(Ee,ou),s(Ee,Pl),s(Pl,ru),s(Ee,lu),i(e,up,p),wo[wt].m(e,p),i(e,hr,p),Le&&Le.m(e,p),i(e,cr,p),i(e,ua,p),s(ua,en),s(en,Ll),k(lo,Ll,null),s(ua,iu),s(ua,Il),s(Il,pu),i(e,dp,p),i(e,sn,p),s(sn,mu),s(sn,Rl),s(Rl,hu),s(sn,cu),i(e,fp,p),k(io,e,p),i(e,gp,p),i(e,ur,p),s(ur,uu),i(e,_p,p),k(po,e,p),i(e,wp,p),i(e,dr,p),s(dr,du),i(e,bp,p),k(mo,e,p),i(e,vp,p),k(ho,e,p),i(e,yp,p),i(e,tn,p),s(tn,fu),s(tn,Fl),s(Fl,gu),s(tn,_u),i(e,kp,p),k(co,e,p),i(e,$p,p),k(uo,e,p),i(e,jp,p),i(e,fr,p),s(fr,wu),i(e,xp,p),i(e,gr,p),s(gr,bu),Ep=!0},p(e,[p]){const bo={};p&1&&(bo.fw=e[0]),d.$set(bo);let _r=O;O=Eu(e),O!==_r&&(Ar(),_(fo[_r],1,1,()=>{fo[_r]=null}),Sr(),N=fo[O],N||(N=fo[O]=xu[O](e),N.c()),g(N,1),N.m(J.parentNode,J));const Nl={};p&2&&(Nl.$$scope={dirty:p,ctx:e}),ss.$set(Nl);const Gl={};p&2&&(Gl.$$scope={dirty:p,ctx:e}),Ia.$set(Gl);const vo={};p&2&&(vo.$$scope={dirty:p,ctx:e}),Na.$set(vo);const Hl={};p&2&&(Hl.$$scope={dirty:p,ctx:e}),Ha.$set(Hl);const da={};p&2&&(da.$$scope={dirty:p,ctx:e}),Wa.$set(da);const Ul={};p&2&&(Ul.$$scope={dirty:p,ctx:e}),Ka.$set(Ul);let wr=dt;dt=qu(e),dt!==wr&&(Ar(),_(go[wr],1,1,()=>{go[wr]=null}),Sr(),ft=go[dt],ft||(ft=go[dt]=Tu[dt](e),ft.c()),g(ft,1),ft.m(nr.parentNode,nr));const Ml={};p&2&&(Ml.$$scope={dirty:p,ctx:e}),Qa.$set(Ml),e[0]==="pt"?Pe?p&1&&g(Pe,1):(Pe=fg(),Pe.c(),g(Pe,1),Pe.m(lr.parentNode,lr)):Pe&&(Ar(),_(Pe,1,1,()=>{Pe=null}),Sr());let Wl=gt;gt=Du(e),gt!==Wl&&(Ar(),_(_o[Wl],1,1,()=>{_o[Wl]=null}),Sr(),_t=_o[gt],_t||(_t=_o[gt]=zu[gt](e),_t.c()),g(_t,1),_t.m(pr.parentNode,pr));let br=wt;wt=Au(e),wt!==br&&(Ar(),_(wo[br],1,1,()=>{wo[br]=null}),Sr(),bt=wo[wt],bt||(bt=wo[wt]=Su[wt](e),bt.c()),g(bt,1),bt.m(hr.parentNode,hr)),e[0]==="pt"?Le?p&1&&g(Le,1):(Le=gg(e),Le.c(),g(Le,1),Le.m(cr.parentNode,cr)):Le&&(Ar(),_(Le,1,1,()=>{Le=null}),Sr())},i(e){Ep||(g(d.$$.fragment,e),g(P.$$.fragment,e),g(N),g(ee.$$.fragment,e),g(ge.$$.fragment,e),g(ze.$$.fragment,e),g(Ae.$$.fragment,e),g(Me.$$.fragment,e),g(ue.$$.fragment,e),g(ss.$$.fragment,e),g(Be.$$.fragment,e),g(ye.$$.fragment,e),g(Ve.$$.fragment,e),g(Ts.$$.fragment,e),g(ot.$$.fragment,e),g(qs.$$.fragment,e),g(As.$$.fragment,e),g(lt.$$.fragment,e),g(qt.$$.fragment,e),g(zt.$$.fragment,e),g(Ia.$$.fragment,e),g(Tn.$$.fragment,e),g(qn.$$.fragment,e),g(zn.$$.fragment,e),g(Na.$$.fragment,e),g(Dn.$$.fragment,e),g(Sn.$$.fragment,e),g(An.$$.fragment,e),g(On.$$.fragment,e),g(Cn.$$.fragment,e),g(Pn.$$.fragment,e),g(Ha.$$.fragment,e),g(Ln.$$.fragment,e),g(In.$$.fragment,e),g(Fn.$$.fragment,e),g(Wa.$$.fragment,e),g(Hn.$$.fragment,e),g(Un.$$.fragment,e),g(Mn.$$.fragment,e),g(Wn.$$.fragment,e),g(Bn.$$.fragment,e),g(Vn.$$.fragment,e),g(Ka.$$.fragment,e),g(Kn.$$.fragment,e),g(Yn.$$.fragment,e),g(Jn.$$.fragment,e),g(Xn.$$.fragment,e),g(Qn.$$.fragment,e),g(Zn.$$.fragment,e),g(eo.$$.fragment,e),g(so.$$.fragment,e),g(ft),g(Qa.$$.fragment,e),g(to.$$.fragment,e),g(ao.$$.fragment,e),g(Pe),g(_t),g(no.$$.fragment,e),g(oo.$$.fragment,e),g(ro.$$.fragment,e),g(bt),g(Le),g(lo.$$.fragment,e),g(io.$$.fragment,e),g(po.$$.fragment,e),g(mo.$$.fragment,e),g(ho.$$.fragment,e),g(co.$$.fragment,e),g(uo.$$.fragment,e),Ep=!0)},o(e){_(d.$$.fragment,e),_(P.$$.fragment,e),_(N),_(ee.$$.fragment,e),_(ge.$$.fragment,e),_(ze.$$.fragment,e),_(Ae.$$.fragment,e),_(Me.$$.fragment,e),_(ue.$$.fragment,e),_(ss.$$.fragment,e),_(Be.$$.fragment,e),_(ye.$$.fragment,e),_(Ve.$$.fragment,e),_(Ts.$$.fragment,e),_(ot.$$.fragment,e),_(qs.$$.fragment,e),_(As.$$.fragment,e),_(lt.$$.fragment,e),_(qt.$$.fragment,e),_(zt.$$.fragment,e),_(Ia.$$.fragment,e),_(Tn.$$.fragment,e),_(qn.$$.fragment,e),_(zn.$$.fragment,e),_(Na.$$.fragment,e),_(Dn.$$.fragment,e),_(Sn.$$.fragment,e),_(An.$$.fragment,e),_(On.$$.fragment,e),_(Cn.$$.fragment,e),_(Pn.$$.fragment,e),_(Ha.$$.fragment,e),_(Ln.$$.fragment,e),_(In.$$.fragment,e),_(Fn.$$.fragment,e),_(Wa.$$.fragment,e),_(Hn.$$.fragment,e),_(Un.$$.fragment,e),_(Mn.$$.fragment,e),_(Wn.$$.fragment,e),_(Bn.$$.fragment,e),_(Vn.$$.fragment,e),_(Ka.$$.fragment,e),_(Kn.$$.fragment,e),_(Yn.$$.fragment,e),_(Jn.$$.fragment,e),_(Xn.$$.fragment,e),_(Qn.$$.fragment,e),_(Zn.$$.fragment,e),_(eo.$$.fragment,e),_(so.$$.fragment,e),_(ft),_(Qa.$$.fragment,e),_(to.$$.fragment,e),_(ao.$$.fragment,e),_(Pe),_(_t),_(no.$$.fragment,e),_(oo.$$.fragment,e),_(ro.$$.fragment,e),_(bt),_(Le),_(lo.$$.fragment,e),_(io.$$.fragment,e),_(po.$$.fragment,e),_(mo.$$.fragment,e),_(ho.$$.fragment,e),_(co.$$.fragment,e),_(uo.$$.fragment,e),Ep=!1},d(e){t(u),e&&t(j),$(d,e),e&&t(E),e&&t(C),$(P),e&&t(D),fo[O].d(e),e&&t(J),e&&t(X),e&&t(U),$(ee,e),e&&t(F),e&&t(B),e&&t(Te),e&&t(V),e&&t(me),e&&t(oe),e&&t(ke),e&&t(se),$(ge),e&&t(b),e&&t(H),e&&t(re),$(ze,e),e&&t(Ze),$(Ae,e),e&&t(ne),e&&t(le),e&&t(ce),$(Me,e),e&&t(Ys),$(ue,e),e&&t(ms),$(ss,e),e&&t(Ss),e&&t(De),e&&t(Zs),$(Be,e),e&&t(et),$(ye,e),e&&t($s),e&&t(Se),e&&t(Nt),$(Ve,e),e&&t(wa),e&&t(xe),e&&t(va),$(Ts,e),e&&t(ya),e&&t(Et),e&&t(Tt),$(ot,e),e&&t(ka),$(qs,e),e&&t($a),e&&t(Ye),e&&t(ja),$(As,e),e&&t(rt),$(lt,e),e&&t(Xt),e&&t(us),e&&t(Os),e&&t(ds),e&&t(Zt),e&&t(m),e&&t(pt),$(qt,e),e&&t(_n),e&&t(mt),e&&t(wn),e&&t(Cs),$(zt),e&&t(ea),e&&t(ht),e&&t(bn),e&&t(ct),e&&t(Bl),e&&t(Bo),e&&t(Vl),e&&t(La),e&&t(Kl),e&&t(pa),e&&t(Yl),e&&t(Vo),e&&t(Jl),$(Ia,e),e&&t(Xl),e&&t(ma),$(Tn),e&&t(Ql),$(qn,e),e&&t(Zl),e&&t(Fa),e&&t(ei),$(zn,e),e&&t(si),$(Na,e),e&&t(ti),e&&t(Ko),e&&t(ai),$(Dn,e),e&&t(ni),$(Sn,e),e&&t(oi),e&&t(fs),e&&t(ri),$(An,e),e&&t(li),$(On,e),e&&t(ii),e&&t(Ns),e&&t(pi),e&&t(Ga),e&&t(mi),$(Cn,e),e&&t(hi),e&&t(gs),e&&t(ci),e&&t(Ot),e&&t(ui),$(Pn,e),e&&t(di),e&&t(Xo),e&&t(fi),$(Ha,e),e&&t(gi),e&&t(ha),$(Ln),e&&t(_i),$(In,e),e&&t(wi),e&&t(Qo),e&&t(bi),e&&t(Ma),e&&t(vi),$(Fn,e),e&&t(yi),e&&t(Ct),e&&t(ki),$(Wa,e),e&&t($i),e&&t(Nn),e&&t(xi),e&&t(Gn),e&&t(Ti),e&&t(Ba),e&&t(qi),$(Hn,e),e&&t(zi),e&&t(Zo),e&&t(Di),$(Un,e),e&&t(Si),e&&t(Va),e&&t(Ai),$(Mn,e),e&&t(Oi),$(Wn,e),e&&t(Ci),e&&t(Xe),e&&t(Pi),$(Bn,e),e&&t(Li),$(Vn,e),e&&t(Ii),e&&t(Qe),e&&t(Ri),$(Ka,e),e&&t(Fi),e&&t(er),e&&t(Ni),e&&t(ca),$(Kn),e&&t(Gi),e&&t(Gs),e&&t(Hi),$(Yn,e),e&&t(Ui),e&&t(sr),e&&t(Mi),$(Jn,e),e&&t(Wi),e&&t(Ja),e&&t(Bi),$(Xn,e),e&&t(Vi),$(Qn,e),e&&t(Ki),e&&t(tr),e&&t(Yi),$(Zn,e),e&&t(Ji),e&&t(ar),e&&t(Xi),$(eo,e),e&&t(Qi),$(so,e),e&&t(Zi),e&&t(Xa),e&&t(ep),go[dt].d(e),e&&t(nr),$(Qa,e),e&&t(sp),e&&t(or),e&&t(tp),$(to,e),e&&t(ap),e&&t(rr),e&&t(np),$(ao,e),e&&t(op),Pe&&Pe.d(e),e&&t(lr),e&&t(Za),e&&t(rp),e&&t(Hs),e&&t(lp),_o[gt].d(e),e&&t(pr),e&&t(mr),e&&t(ip),$(no,e),e&&t(pp),e&&t(Pt),e&&t(mp),$(oo,e),e&&t(hp),$(ro,e),e&&t(cp),e&&t(Ee),e&&t(up),wo[wt].d(e),e&&t(hr),Le&&Le.d(e),e&&t(cr),e&&t(ua),$(lo),e&&t(dp),e&&t(sn),e&&t(fp),$(io,e),e&&t(gp),e&&t(ur),e&&t(_p),$(po,e),e&&t(wp),e&&t(dr),e&&t(bp),$(mo,e),e&&t(vp),$(ho,e),e&&t(yp),e&&t(tn),e&&t(kp),$(co,e),e&&t($p),$(uo,e),e&&t(jp),e&&t(fr),e&&t(xp),e&&t(gr)}}}const Ug={local:"summarization",sections:[{local:"preparing-a-multilingual-corpus",title:"Preparing a multilingual corpus"},{local:"models-for-text-summarization",title:"Models for text summarization"},{local:"preprocessing-the-data",title:"Preprocessing the data"},{local:"metrics-for-text-summarization",sections:[{local:"creating-a-strong-baseline",title:"Creating a strong baseline"}],title:"Metrics for text summarization"},{local:"finetuning-mt5-with-the-trainer-api",title:"Fine-tuning mT5 with the `Trainer` API"},{local:"finetuning-mt5-with-keras",title:"Fine-tuning mT5 with Keras"},{local:"finetuning-mt5-with-accelerate",sections:[{local:"preparing-everything-for-training",title:"Preparing everything for training"},{local:"training-loop",title:"Training loop"}],title:"Fine-tuning mT5 with \u{1F917} Accelerate"},{local:"using-your-finetuned-model",title:"Using your fine-tuned model"}],title:"Summarization"};function Mg(K,u,j){let d="pt";return jg(()=>{const E=new URLSearchParams(window.location.search);j(0,d=E.get("fw")||"pt")}),[d]}class Qg extends vg{constructor(u){super();yg(this,u,Mg,Hg,kg,{})}}export{Qg as default,Ug as metadata};
