import{S as Ne,i as xe,s as Ae,e as n,k as u,w as be,t as s,M as Ue,c as r,d as t,m as c,a as i,x as Ee,h,b as P,F as o,g as f,y as Le,L as Fe,q as Pe,o as $e,B as Ie,v as He,O as Ke}from"../../chunks/vendor-e7c81d8a.js";import{Y as Me}from"../../chunks/Youtube-365ea064.js";import{I as Oe}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";const{document:Te}=Ke;function qe(he){let m,F,p,w,$,g,Y,I,G,H,k,W,K,b,J,M,y,O,E,j,q,l,T,z,Q,N,V,X,v,Z,x,ee,te,oe,A,ae,ne,_,re,U,ie,le,S,L,se,C;return g=new Oe({}),y=new Me({props:{id:"-RPeakdlHYo"}}),{c(){m=n("meta"),F=u(),p=n("h1"),w=n("a"),$=n("span"),be(g.$$.fragment),Y=u(),I=n("span"),G=s("Mastering NLP"),H=u(),k=n("p"),W=s("If you\u2019ve made it this far in the course, congratulations \u2014 you now have all the knowledge and tools you need to tackle (almost) any NLP task with \u{1F917} Transformers and the Hugging Face ecosystem!"),K=u(),b=n("p"),J=s("We have seen a lot of different data collators, so we made this little video to help you find which one to use for each task:"),M=u(),be(y.$$.fragment),O=u(),E=n("p"),j=s("After completing this lightning tour through the core NLP tasks, you should:"),q=u(),l=n("ul"),T=n("li"),z=s("Know which architectures (encoder, decoder, or encoder-decoder) are best suited for each task"),Q=u(),N=n("li"),V=s("Understand the difference between pretraining and fine-tuning a language model"),X=u(),v=n("li"),Z=s("Know how to train Transformer models using either the "),x=n("code"),ee=s("Trainer"),te=s(" API and distributed training features of \u{1F917} Accelerate or TensorFlow and Keras, depending on which track you\u2019ve been following"),oe=u(),A=n("li"),ae=s("Understand the meaning and limitations of metrics like ROUGE and BLEU for text generation tasks"),ne=u(),_=n("li"),re=s("Know how to interact with your fine-tuned models, both on the Hub and using the "),U=n("code"),ie=s("pipeline"),le=s(" from \u{1F917} Transformers"),S=u(),L=n("p"),se=s("Despite all this knowledge, there will come a time when you\u2019ll either encounter a difficult bug in your code or have a question about how to solve a particular NLP problem. Fortunately, the Hugging Face community is here to help you! In the final chapter of this part of the course, we\u2019ll explore how you can debug your Transformer models and ask for help effectively."),this.h()},l(e){const a=Ue('[data-svelte="svelte-1phssyn"]',Te.head);m=r(a,"META",{name:!0,content:!0}),a.forEach(t),F=c(e),p=r(e,"H1",{class:!0});var D=i(p);w=r(D,"A",{id:!0,class:!0,href:!0});var fe=i(w);$=r(fe,"SPAN",{});var ue=i($);Ee(g.$$.fragment,ue),ue.forEach(t),fe.forEach(t),Y=c(D),I=r(D,"SPAN",{});var ce=i(I);G=h(ce,"Mastering NLP"),ce.forEach(t),D.forEach(t),H=c(e),k=r(e,"P",{});var de=i(k);W=h(de,"If you\u2019ve made it this far in the course, congratulations \u2014 you now have all the knowledge and tools you need to tackle (almost) any NLP task with \u{1F917} Transformers and the Hugging Face ecosystem!"),de.forEach(t),K=c(e),b=r(e,"P",{});var me=i(b);J=h(me,"We have seen a lot of different data collators, so we made this little video to help you find which one to use for each task:"),me.forEach(t),M=c(e),Ee(y.$$.fragment,e),O=c(e),E=r(e,"P",{});var pe=i(E);j=h(pe,"After completing this lightning tour through the core NLP tasks, you should:"),pe.forEach(t),q=c(e),l=r(e,"UL",{});var d=i(l);T=r(d,"LI",{});var we=i(T);z=h(we,"Know which architectures (encoder, decoder, or encoder-decoder) are best suited for each task"),we.forEach(t),Q=c(d),N=r(d,"LI",{});var ge=i(N);V=h(ge,"Understand the difference between pretraining and fine-tuning a language model"),ge.forEach(t),X=c(d),v=r(d,"LI",{});var R=i(v);Z=h(R,"Know how to train Transformer models using either the "),x=r(R,"CODE",{});var ye=i(x);ee=h(ye,"Trainer"),ye.forEach(t),te=h(R," API and distributed training features of \u{1F917} Accelerate or TensorFlow and Keras, depending on which track you\u2019ve been following"),R.forEach(t),oe=c(d),A=r(d,"LI",{});var ve=i(A);ae=h(ve,"Understand the meaning and limitations of metrics like ROUGE and BLEU for text generation tasks"),ve.forEach(t),ne=c(d),_=r(d,"LI",{});var B=i(_);re=h(B,"Know how to interact with your fine-tuned models, both on the Hub and using the "),U=r(B,"CODE",{});var _e=i(U);ie=h(_e,"pipeline"),_e.forEach(t),le=h(B," from \u{1F917} Transformers"),B.forEach(t),d.forEach(t),S=c(e),L=r(e,"P",{});var ke=i(L);se=h(ke,"Despite all this knowledge, there will come a time when you\u2019ll either encounter a difficult bug in your code or have a question about how to solve a particular NLP problem. Fortunately, the Hugging Face community is here to help you! In the final chapter of this part of the course, we\u2019ll explore how you can debug your Transformer models and ask for help effectively."),ke.forEach(t),this.h()},h(){P(m,"name","hf:doc:metadata"),P(m,"content",JSON.stringify(Se)),P(w,"id","mastering-nlp"),P(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(w,"href","#mastering-nlp"),P(p,"class","relative group")},m(e,a){o(Te.head,m),f(e,F,a),f(e,p,a),o(p,w),o(w,$),Le(g,$,null),o(p,Y),o(p,I),o(I,G),f(e,H,a),f(e,k,a),o(k,W),f(e,K,a),f(e,b,a),o(b,J),f(e,M,a),Le(y,e,a),f(e,O,a),f(e,E,a),o(E,j),f(e,q,a),f(e,l,a),o(l,T),o(T,z),o(l,Q),o(l,N),o(N,V),o(l,X),o(l,v),o(v,Z),o(v,x),o(x,ee),o(v,te),o(l,oe),o(l,A),o(A,ae),o(l,ne),o(l,_),o(_,re),o(_,U),o(U,ie),o(_,le),f(e,S,a),f(e,L,a),o(L,se),C=!0},p:Fe,i(e){C||(Pe(g.$$.fragment,e),Pe(y.$$.fragment,e),C=!0)},o(e){$e(g.$$.fragment,e),$e(y.$$.fragment,e),C=!1},d(e){t(m),e&&t(F),e&&t(p),Ie(g),e&&t(H),e&&t(k),e&&t(K),e&&t(b),e&&t(M),Ie(y,e),e&&t(O),e&&t(E),e&&t(q),e&&t(l),e&&t(S),e&&t(L)}}}const Se={local:"mastering-nlp",title:"Mastering NLP"};function Ce(he){return He(()=>{new URL(document.location).searchParams.get("fw")}),[]}class Ye extends Ne{constructor(m){super();xe(this,m,Ce,qe,Ae,{})}}export{Ye as default,Se as metadata};
