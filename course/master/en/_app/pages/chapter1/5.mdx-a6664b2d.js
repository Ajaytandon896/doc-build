import{S as Se,i as Ce,s as Ne,e as o,k as f,w as ke,t as d,M as Ue,c as n,d as t,m,a as r,x as xe,h,b as l,F as a,g as c,y as Be,L as ze,q as qe,o as Pe,B as Ie,v as De,O as Je}from"../../chunks/vendor-e7c81d8a.js";import{Y as Oe}from"../../chunks/Youtube-365ea064.js";import{I as Ye}from"../../chunks/WidgetTextarea.svelte_svelte_type_style_lang-08e92eaf.js";const{document:Me}=Je;function je(fe){let p,N,v,_,x,g,Q,B,G,U,w,z,E,K,q,V,W,D,R,X,J,L,Z,O,k,ee,Y,i,P,y,te,ae,I,$,oe,ne,M,T,re,se,S,b,le,ie,C,A,ce,j;return g=new Ye({}),w=new Oe({props:{id:"MUqNwgPjJvQ"}}),{c(){p=o("meta"),N=f(),v=o("h1"),_=o("a"),x=o("span"),ke(g.$$.fragment),Q=f(),B=o("span"),G=d("Encoder models"),U=f(),ke(w.$$.fragment),z=f(),E=o("p"),K=d("Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),q=o("em"),V=d("auto-encoding models"),W=d("."),D=f(),R=o("p"),X=d("The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),J=f(),L=o("p"),Z=d("Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),O=f(),k=o("p"),ee=d("Representatives of this family of models include:"),Y=f(),i=o("ul"),P=o("li"),y=o("a"),te=d("ALBERT"),ae=f(),I=o("li"),$=o("a"),oe=d("BERT"),ne=f(),M=o("li"),T=o("a"),re=d("DistilBERT"),se=f(),S=o("li"),b=o("a"),le=d("ELECTRA"),ie=f(),C=o("li"),A=o("a"),ce=d("RoBERTa"),this.h()},l(e){const s=Ue('[data-svelte="svelte-1phssyn"]',Me.head);p=n(s,"META",{name:!0,content:!0}),s.forEach(t),N=m(e),v=n(e,"H1",{class:!0});var F=r(v);_=n(F,"A",{id:!0,class:!0,href:!0});var de=r(_);x=n(de,"SPAN",{});var me=r(x);xe(g.$$.fragment,me),me.forEach(t),de.forEach(t),Q=m(F),B=n(F,"SPAN",{});var he=r(B);G=h(he,"Encoder models"),he.forEach(t),F.forEach(t),U=m(e),xe(w.$$.fragment,e),z=m(e),E=n(e,"P",{});var H=r(E);K=h(H,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),q=n(H,"EM",{});var ue=r(q);V=h(ue,"auto-encoding models"),ue.forEach(t),W=h(H,"."),H.forEach(t),D=m(e),R=n(e,"P",{});var pe=r(R);X=h(pe,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),pe.forEach(t),J=m(e),L=n(e,"P",{});var ve=r(L);Z=h(ve,"Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),ve.forEach(t),O=m(e),k=n(e,"P",{});var _e=r(k);ee=h(_e,"Representatives of this family of models include:"),_e.forEach(t),Y=m(e),i=n(e,"UL",{});var u=r(i);P=n(u,"LI",{});var Ee=r(P);y=n(Ee,"A",{href:!0,rel:!0});var ge=r(y);te=h(ge,"ALBERT"),ge.forEach(t),Ee.forEach(t),ae=m(u),I=n(u,"LI",{});var we=r(I);$=n(we,"A",{href:!0,rel:!0});var ye=r($);oe=h(ye,"BERT"),ye.forEach(t),we.forEach(t),ne=m(u),M=n(u,"LI",{});var $e=r(M);T=n($e,"A",{href:!0,rel:!0});var Te=r(T);re=h(Te,"DistilBERT"),Te.forEach(t),$e.forEach(t),se=m(u),S=n(u,"LI",{});var be=r(S);b=n(be,"A",{href:!0,rel:!0});var Ae=r(b);le=h(Ae,"ELECTRA"),Ae.forEach(t),be.forEach(t),ie=m(u),C=n(u,"LI",{});var Re=r(C);A=n(Re,"A",{href:!0,rel:!0});var Le=r(A);ce=h(Le,"RoBERTa"),Le.forEach(t),Re.forEach(t),u.forEach(t),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(Fe)),l(_,"id","encoder-models"),l(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_,"href","#encoder-models"),l(v,"class","relative group"),l(y,"href","https://huggingface.co/transformers/model_doc/albert.html"),l(y,"rel","nofollow"),l($,"href","https://huggingface.co/transformers/model_doc/bert.html"),l($,"rel","nofollow"),l(T,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),l(T,"rel","nofollow"),l(b,"href","https://huggingface.co/transformers/model_doc/electra.html"),l(b,"rel","nofollow"),l(A,"href","https://huggingface.co/transformers/model_doc/roberta.html"),l(A,"rel","nofollow")},m(e,s){a(Me.head,p),c(e,N,s),c(e,v,s),a(v,_),a(_,x),Be(g,x,null),a(v,Q),a(v,B),a(B,G),c(e,U,s),Be(w,e,s),c(e,z,s),c(e,E,s),a(E,K),a(E,q),a(q,V),a(E,W),c(e,D,s),c(e,R,s),a(R,X),c(e,J,s),c(e,L,s),a(L,Z),c(e,O,s),c(e,k,s),a(k,ee),c(e,Y,s),c(e,i,s),a(i,P),a(P,y),a(y,te),a(i,ae),a(i,I),a(I,$),a($,oe),a(i,ne),a(i,M),a(M,T),a(T,re),a(i,se),a(i,S),a(S,b),a(b,le),a(i,ie),a(i,C),a(C,A),a(A,ce),j=!0},p:ze,i(e){j||(qe(g.$$.fragment,e),qe(w.$$.fragment,e),j=!0)},o(e){Pe(g.$$.fragment,e),Pe(w.$$.fragment,e),j=!1},d(e){t(p),e&&t(N),e&&t(v),Ie(g),e&&t(U),Ie(w,e),e&&t(z),e&&t(E),e&&t(D),e&&t(R),e&&t(J),e&&t(L),e&&t(O),e&&t(k),e&&t(Y),e&&t(i)}}}const Fe={local:"encoder-models",title:"Encoder models"};function He(fe){return De(()=>{new URL(document.location).searchParams.get("fw")}),[]}class Ve extends Se{constructor(p){super();Ce(this,p,He,je,Ne,{})}}export{Ve as default,Fe as metadata};
