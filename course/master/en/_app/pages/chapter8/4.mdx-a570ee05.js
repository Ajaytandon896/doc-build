import{S as dm,i as fm,s as bm,e as o,k as u,w,t as n,M as ym,c as i,d as s,m as c,x as j,a as p,h as r,b as $,F as a,g as l,y as _,o as y,p as bu,q as g,B as v,v as gm,n as yu}from"../../chunks/vendor-1e8b365d.js";import{T as Rt}from"../../chunks/Tip-62b14c6e.js";import{Y as cm}from"../../chunks/Youtube-c2a8cc39.js";import{I as ce}from"../../chunks/IconCopyLink-483c28ba.js";import{C as P}from"../../chunks/CodeBlock-e5764662.js";import{D as mm}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as wm}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function jm(S){let m,k;return m=new mm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"}]}}),{c(){w(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,E){_(m,d,E),k=!0},i(d){k||(g(m.$$.fragment,d),k=!0)},o(d){y(m.$$.fragment,d),k=!1},d(d){v(m,d)}}}function _m(S){let m,k;return m=new mm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"}]}}),{c(){w(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,E){_(m,d,E),k=!0},i(d){k||(g(m.$$.fragment,d),k=!0)},o(d){y(m.$$.fragment,d),k=!1},d(d){v(m,d)}}}function vm(S){let m,k,d,E,z,C,q,O;return{c(){m=o("p"),k=n("You\u2019ve written a beautiful script to train or fine-tune a model on a given task, dutifully following the advice from "),d=o("a"),E=n("Chapter 7"),z=n(". But when you launch the command "),C=o("code"),q=n("model.fit()"),O=n(", something horrible happens: you get an error \u{1F631}! Or worse, everything seems to be fine and the training runs without error, but the resulting model is crappy. In this section, we will show you what you can do to debug these kinds of issues."),this.h()},l(D){m=i(D,"P",{});var A=p(m);k=r(A,"You\u2019ve written a beautiful script to train or fine-tune a model on a given task, dutifully following the advice from "),d=i(A,"A",{href:!0});var W=p(d);E=r(W,"Chapter 7"),W.forEach(s),z=r(A,". But when you launch the command "),C=i(A,"CODE",{});var T=p(C);q=r(T,"model.fit()"),T.forEach(s),O=r(A,", something horrible happens: you get an error \u{1F631}! Or worse, everything seems to be fine and the training runs without error, but the resulting model is crappy. In this section, we will show you what you can do to debug these kinds of issues."),A.forEach(s),this.h()},h(){$(d,"href","/course/chapter7")},m(D,A){l(D,m,A),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O)},d(D){D&&s(m)}}}function km(S){let m,k,d,E,z,C,q,O;return{c(){m=o("p"),k=n("You\u2019ve written a beautiful script to train or fine-tune a model on a given task, dutifully following the advice from "),d=o("a"),E=n("Chapter 7"),z=n(". But when you launch the command "),C=o("code"),q=n("trainer.train()"),O=n(", something horrible happens: you get an error \u{1F631}! Or worse, everything seems to be fine and the training runs without error, but the resulting model is crappy. In this section, we will show you what you can do to debug these kinds of issues."),this.h()},l(D){m=i(D,"P",{});var A=p(m);k=r(A,"You\u2019ve written a beautiful script to train or fine-tune a model on a given task, dutifully following the advice from "),d=i(A,"A",{href:!0});var W=p(d);E=r(W,"Chapter 7"),W.forEach(s),z=r(A,". But when you launch the command "),C=i(A,"CODE",{});var T=p(C);q=r(T,"trainer.train()"),T.forEach(s),O=r(A,", something horrible happens: you get an error \u{1F631}! Or worse, everything seems to be fine and the training runs without error, but the resulting model is crappy. In this section, we will show you what you can do to debug these kinds of issues."),A.forEach(s),this.h()},h(){$(d,"href","/course/chapter7")},m(D,A){l(D,m,A),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O)},d(D){D&&s(m)}}}function $m(S){let m,k;return m=new cm({props:{id:"N9kO52itd0Q"}}),{c(){w(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,E){_(m,d,E),k=!0},i(d){k||(g(m.$$.fragment,d),k=!0)},o(d){y(m.$$.fragment,d),k=!1},d(d){v(m,d)}}}function Em(S){let m,k;return m=new cm({props:{id:"L-WSwUWde1U"}}),{c(){w(m.$$.fragment)},l(d){j(m.$$.fragment,d)},m(d,E){_(m,d,E),k=!0},i(d){k||(g(m.$$.fragment,d),k=!0)},o(d){y(m.$$.fragment,d),k=!1},d(d){v(m,d)}}}function Tm(S){let m,k,d,E,z,C,q,O,D,A,W;return{c(){m=o("p"),k=n("The problem when you encounter an error in "),d=o("code"),E=n("model.fit()"),z=n(" is that it could come from multiple sources, as training usually brings together a lot of things that you\u2019ve been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric."),C=u(),q=o("p"),O=n("The best way to debug an error that arises in "),D=o("code"),A=n("model.fit()"),W=n(" is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.")},l(T){m=i(T,"P",{});var x=p(m);k=r(x,"The problem when you encounter an error in "),d=i(x,"CODE",{});var L=p(d);E=r(L,"model.fit()"),L.forEach(s),z=r(x," is that it could come from multiple sources, as training usually brings together a lot of things that you\u2019ve been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric."),x.forEach(s),C=c(T),q=i(T,"P",{});var F=p(q);O=r(F,"The best way to debug an error that arises in "),D=i(F,"CODE",{});var I=p(D);A=r(I,"model.fit()"),I.forEach(s),W=r(F," is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve."),F.forEach(s)},m(T,x){l(T,m,x),a(m,k),a(m,d),a(d,E),a(m,z),l(T,C,x),l(T,q,x),a(q,O),a(q,D),a(D,A),a(q,W)},d(T){T&&s(m),T&&s(C),T&&s(q)}}}function Cm(S){let m,k,d,E,z,C,q,O,D,A,W,T,x,L;return{c(){m=o("p"),k=n("The problem when you encounter an error in "),d=o("code"),E=n("trainer.train()"),z=n(" is that it could come from multiple sources, as the "),C=o("code"),q=n("Trainer"),O=n(" usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric."),D=u(),A=o("p"),W=n("The best way to debug an error that arises in "),T=o("code"),x=n("trainer.train()"),L=n(" is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.")},l(F){m=i(F,"P",{});var I=p(m);k=r(I,"The problem when you encounter an error in "),d=i(I,"CODE",{});var R=p(d);E=r(R,"trainer.train()"),R.forEach(s),z=r(I," is that it could come from multiple sources, as the "),C=i(I,"CODE",{});var Y=p(C);q=r(Y,"Trainer"),Y.forEach(s),O=r(I," usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric."),I.forEach(s),D=c(F),A=i(F,"P",{});var Q=p(A);W=r(Q,"The best way to debug an error that arises in "),T=i(Q,"CODE",{});var J=p(T);x=r(J,"trainer.train()"),J.forEach(s),L=r(Q," is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve."),Q.forEach(s)},m(F,I){l(F,m,I),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O),l(F,D,I),l(F,A,I),a(A,W),a(A,T),a(T,x),a(A,L)},d(F){F&&s(m),F&&s(D),F&&s(A)}}}function qm(S){let m,k,d,E,z,C,q,O,D,A,W,T,x,L,F,I,R,Y,Q,J,Ie,V,ws,H,M,ne,ge,B,me,re,we,oe,fa,Us,je,ie,Je,Se,qe,Ls,X,ba,js,Kt,Ms,Xe,Ne,We,_e,Ze,ve,Fe,Vt,Qa,ke,_s,Qe,es,Jt,ya,$e,ss,as,K,vs,pe,et,Gs,ga,ks,Pe,wa,st,$s,ts,ee,at,ns,Xt,Es,de,Ee,Ts,Cs,Z,Zt,rs,Vn,qs,ja,Bs,_a,le,Jn,ls,os,Qt,Ue,Ae,is,ps,Hs,tt,ae,xe,b,N,hs,fe,us,Ys,va,cs,ms,Le,Rs,Ks,ka,ds,en,Ps,he,As,Xn,sn,De,an,Vs,Te,Js,Zn,$a,ue,nt,fs,rt,Me,lt,ll,Ei,wo,Ea,jo,tn,ol,U,_o,Ta,vo,Xs,il,pl,Ti,hl,ul,Ci,cl,ko,Qn,ml,$o,nn,er,sr,qi,ar,tr,Pi,nr,Ca,ot,rn,ln,Ai,on,xi,Eo,be,Di,dl,fl,zi,bl,yl,Oi,gl,wl,Ii,jl,_l,Si,To,it,Ge,rr,vl,Co,pn,lr,Zs,Ni,hn,Wi,Fi,or,un,qo,cn,mn,kl,xs,$l,se,El,Tl,Ui,Cl,ql,Li,Pl,Al,Mi,Po,pt,Be,dn,ir,pr,Gi,hr,fn,Ao,qa,xo,ur,Do,ht,ut,bn,yn,xl,ze,zo,ct,Bi,Oo,mt,Hi,Io,dt,ft,Ce,gn,cr,bt,Yi,wn,Ri,Ki,mr,jn,So,Pa,No,yt,Wo,Aa,Fo,dr,Uo,Qs,Ds,Dl,gt,xa,ea,zl,wt,sa,Ol,Il,Lo,fr,Sl,Mo,_n,Nl,He,Vi,Da,Ji,Xi,Wl,Ye,Zi,br,yr,Qi,gr,jt,ep,wr,vn,Go,_t,vt,kn,$n,Fl,bs,Bo,za,Ho,jr,Yo,kt,aa,ys,_r,ta,sp,vr,En,Ul,zs,ap,kr,Oa,$t,$r,Os,Ro,Et,tp,Ll,Oe,Ko,Tt,np,Vo,Is,Ct,Ml,qt,Tn,Er,Ia,Sa,Pt,Cn,Jo,na,rp,Tr,ra,lp,Cr,qn,Xo,At,op,Zo,Ss,qr,Gl,Qo,xt,Bl,Hl,ip,ei,Dt,la,Re,Yl,Na,pp,Rl,si,oa,te,Kl,Wa,hp,Vl,Jl,ai,zt,Xl,Zl,up,Ql,ti,Pr,ni,Ot,It,Pn,Fa,Ua,La,St,Ar,eo,ri,Ke,ia,cp,so,ao,mp,li,Ma,oi,An,xr,Dr,ii,xn,Nt,pi,Wt,dp,hi,Dn,zn,to,Ve,Ga,Ba,Ft,zr,no,ui,Ut,On,Or,ci,Ns,In,Ir,pa,fp,Sn,bp,yp,Sr,Nn,mi,Lt,Mt,Ha,Wn,Nr,gs,gp,Wr,Fr,wp,Ur,Fn,di;return m=new P({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)

train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=<span class="hljs-string">&quot;adam&quot;</span>)

model.fit(train_dataset)`}}),T=new P({props:{code:"ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']",highlighted:'ValueError: No gradients provided <span class="hljs-keyword">for</span> <span class="hljs-built_in">any</span> variable: [<span class="hljs-string">&#x27;tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>]'}}),J=new ce({}),_e=new P({props:{code:`for batch in train_dataset:
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>`}}),ss=new P({props:{code:`{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        ...,
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[ <span class="hljs-number">101</span>, <span class="hljs-number">2174</span>, <span class="hljs-number">1010</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3174</span>, <span class="hljs-number">2420</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">2044</span>, <span class="hljs-number">2048</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        ...,
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3398</span>, <span class="hljs-number">3398</span>, ..., <span class="hljs-number">2051</span>, <span class="hljs-number">2894</span>,  <span class="hljs-number">102</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">4124</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">2070</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>]])&gt;}`}}),Bs=new P({props:{code:'model.compile(optimizer="adam")',highlighted:'model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>)'}}),os=new Rt({props:{$$slots:{default:[Am]},$$scope:{ctx:S}}}),ae=new P({props:{code:"  246/24543 [..............................] - ETA: 15:52 - loss: nan",highlighted:'  <span class="hljs-number">246</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">52</span> - loss: nan'}}),Ks=new ce({}),Ea=new P({props:{code:"model(batch)",highlighted:"model(batch)"}}),tn=new P({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),un=new P({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`}}),xs=new P({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([<span class="hljs-number">0.6844486</span> ,        nan,        nan, <span class="hljs-number">0.67127866</span>, <span class="hljs-number">0.7068601</span> ,
              nan, <span class="hljs-number">0.69309855</span>,        nan, <span class="hljs-number">0.65531296</span>,        nan,
              nan,        nan, <span class="hljs-number">0.675402</span>  ,        nan,        nan,
       <span class="hljs-number">0.69831556</span>], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">0.04761693</span>, -<span class="hljs-number">0.06509043</span>],
       [-<span class="hljs-number">0.0481936</span> , -<span class="hljs-number">0.04556257</span>],
       [-<span class="hljs-number">0.0040929</span> , -<span class="hljs-number">0.05848458</span>],
       [-<span class="hljs-number">0.02417453</span>, -<span class="hljs-number">0.0684005</span> ],
       [-<span class="hljs-number">0.02517801</span>, -<span class="hljs-number">0.05241832</span>],
       [-<span class="hljs-number">0.04514256</span>, -<span class="hljs-number">0.0757378</span> ],
       [-<span class="hljs-number">0.02656011</span>, -<span class="hljs-number">0.02646275</span>],
       [ <span class="hljs-number">0.00766164</span>, -<span class="hljs-number">0.04350497</span>],
       [ <span class="hljs-number">0.02060014</span>, -<span class="hljs-number">0.05655622</span>],
       [-<span class="hljs-number">0.02615328</span>, -<span class="hljs-number">0.0447021</span> ],
       [-<span class="hljs-number">0.05119278</span>, -<span class="hljs-number">0.06928903</span>],
       [-<span class="hljs-number">0.02859691</span>, -<span class="hljs-number">0.04879177</span>],
       [-<span class="hljs-number">0.02210129</span>, -<span class="hljs-number">0.05791225</span>],
       [-<span class="hljs-number">0.02363213</span>, -<span class="hljs-number">0.05962167</span>],
       [-<span class="hljs-number">0.05352269</span>, -<span class="hljs-number">0.0481673</span> ],
       [-<span class="hljs-number">0.08141848</span>, -<span class="hljs-number">0.07110836</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),pt=new P({props:{code:`import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`}}),dn=new P({props:{code:"array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])",highlighted:'array([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>])'}}),fn=new P({props:{code:`input_ids = batch["input_ids"].numpy()
input_ids[indices]`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
input_ids[indices]`}}),qa=new P({props:{code:`array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])`,highlighted:`array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2032</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">16480</span>,  <span class="hljs-number">3917</span>,  <span class="hljs-number">2594</span>,  <span class="hljs-number">4135</span>,
        <span class="hljs-number">23212</span>,  <span class="hljs-number">3070</span>,  <span class="hljs-number">2214</span>, <span class="hljs-number">10170</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2012</span>,  <span class="hljs-number">4356</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">3183</span>,
         <span class="hljs-number">6838</span>, <span class="hljs-number">12953</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">6147</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2606</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">6838</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3294</span>,  <span class="hljs-number">6625</span>,  <span class="hljs-number">3773</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2214</span>,
         <span class="hljs-number">2158</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">6814</span>,  <span class="hljs-number">2016</span>,  <span class="hljs-number">2234</span>,  <span class="hljs-number">2461</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1998</span>, <span class="hljs-number">13322</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">2053</span>,  <span class="hljs-number">3382</span>,  <span class="hljs-number">2008</span>,
         <span class="hljs-number">2016</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2222</span>,  <span class="hljs-number">3046</span>,  <span class="hljs-number">8103</span>,  <span class="hljs-number">2075</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1012</span>,
          <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">3712</span>,  <span class="hljs-number">4634</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2057</span>,  <span class="hljs-number">8108</span>,
         <span class="hljs-number">2025</span>,  <span class="hljs-number">3404</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1012</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2616</span>, <span class="hljs-number">18449</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">1999</span>,
         <span class="hljs-number">1037</span>,  <span class="hljs-number">9666</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4100</span>,  <span class="hljs-number">8663</span>, <span class="hljs-number">11020</span>,  <span class="hljs-number">6313</span>,  <span class="hljs-number">2791</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2431</span>,  <span class="hljs-number">1011</span>,  <span class="hljs-number">4301</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">5177</span>,
         <span class="hljs-number">2110</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">3977</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">2832</span>,  <span class="hljs-number">2106</span>,  <span class="hljs-number">2025</span>,  <span class="hljs-number">2689</span>,  <span class="hljs-number">2104</span>,
         <span class="hljs-number">2122</span>,  <span class="hljs-number">6214</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">13090</span>,  <span class="hljs-number">5948</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2048</span>,
         <span class="hljs-number">2308</span>,  <span class="hljs-number">2006</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">5001</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2171</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">2170</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3564</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2277</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2195</span>,  <span class="hljs-number">4279</span>,  <span class="hljs-number">2191</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2181</span>,  <span class="hljs-number">2124</span>,  <span class="hljs-number">2004</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2069</span>,  <span class="hljs-number">2028</span>,
         <span class="hljs-number">2451</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2008</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2123</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1056</span>,  <span class="hljs-number">2113</span>,  <span class="hljs-number">2065</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">2428</span>, <span class="hljs-number">10654</span>,  <span class="hljs-number">7347</span>,  <span class="hljs-number">2030</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">7126</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,
         <span class="hljs-number">2291</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">5094</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,  <span class="hljs-number">2291</span>,  <span class="hljs-number">2035</span>,
         <span class="hljs-number">2105</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2029</span>,  <span class="hljs-number">3216</span>,  <span class="hljs-number">2019</span>,  <span class="hljs-number">2503</span>,  <span class="hljs-number">3444</span>,  <span class="hljs-number">1010</span>,
         <span class="hljs-number">6732</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2038</span>, <span class="hljs-number">19840</span>,  <span class="hljs-number">2098</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">9906</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2003</span>,  <span class="hljs-number">2770</span>,  <span class="hljs-number">2041</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4784</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">6732</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">9525</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">4569</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1996</span>, <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2162</span>,
         <span class="hljs-number">2252</span>,  <span class="hljs-number">5689</span>,  <span class="hljs-number">2013</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">7223</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">1996</span>,
        <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2252</span>,  <span class="hljs-number">3062</span>,  <span class="hljs-number">2000</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2598</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>, <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2049</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2025</span>,
        <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]])`}}),ut=new P({props:{code:`labels = batch['labels'].numpy()
labels[indices]`,highlighted:`labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].numpy()
labels[indices]`}}),yn=new P({props:{code:"array([2, 2, 2, 2, 2, 2, 2, 2, 2])",highlighted:'array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),ft=new P({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),gn=new P({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),jn=new P({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, <span class="hljs-attribute">num_labels</span>=3)
model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">&#x27;adam&#x27;</span>)
model.fit(train_dataset)`}}),Pa=new P({props:{code:"  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032",highlighted:'  <span class="hljs-number">869</span>/<span class="hljs-number">24543</span> [&gt;.............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">29</span> - loss: <span class="hljs-number">1.1032</span>'}}),gt=new ce({}),vn=new P({props:{code:`from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">5e-5</span>))`}}),_t=new Rt({props:{$$slots:{default:[xm]},$$scope:{ctx:S}}}),bs=new P({props:{code:"model.fit(train_dataset)",highlighted:"model.fit(train_dataset)"}}),za=new P({props:{code:"319/24543 [..............................] - ETA: 16:07 - loss: 0.9718",highlighted:'<span class="hljs-number">319</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">16</span>:07 - loss: <span class="hljs-number">0.9718</span>'}}),ta=new ce({}),Os=new ce({}),Tn=new Rt({props:{$$slots:{default:[Dm]},$$scope:{ctx:S}}}),Cn=new ce({}),Na=new ce({}),Ot=new P({props:{code:`input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
tokenizer.decode(input_ids[<span class="hljs-number">0</span>])`}}),La=new P({props:{code:`labels = batch["labels"].numpy()
label = labels[0]`,highlighted:`labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
label = labels[<span class="hljs-number">0</span>]`}}),Ft=new ce({}),Nn=new P({props:{code:`for batch in train_dataset:
    break

# Make sure you have run model.compile() and set your optimizer,
# and your loss/metrics if you're using them

model.fit(batch, epochs=20)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>

<span class="hljs-comment"># Make sure you have run model.compile() and set your optimizer,</span>
<span class="hljs-comment"># and your loss/metrics if you&#x27;re using them</span>

model.fit(batch, epochs=<span class="hljs-number">20</span>)`}}),Lt=new Rt({props:{$$slots:{default:[zm]},$$scope:{ctx:S}}}),Fn=new Rt({props:{warning:!0,$$slots:{default:[Om]},$$scope:{ctx:S}}}),{c(){w(m.$$.fragment),k=u(),d=o("p"),E=n("If you try to execute it, you might get some "),z=o("code"),C=n("VisibleDeprecationWarning"),q=n("s when doing the dataset conversion \u2014 this is a known UX issue we have, so please ignore it. If you\u2019re reading the course after, say, November 2021 and it\u2019s still happening, then send rage tweets at @carrigmat until he fixes it."),O=u(),D=o("p"),A=n("What\u2019s a more serious problem, though, is that we get an outright error. And it\u2019s really, terrifyingly long:"),W=u(),w(T.$$.fragment),x=u(),L=o("p"),F=n("What does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing; how do we even begin to debug something like that? When the error you get doesn\u2019t immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that everything looks right. And of course, the place to start is always to\u2026"),I=u(),R=o("h3"),Y=o("a"),Q=o("span"),w(J.$$.fragment),Ie=u(),V=o("span"),ws=n("Check your data"),H=u(),M=o("p"),ne=n("This goes without saying, but if your data is corrupted, Keras is not going to be able to fix it for you. So first things first, you need to have a look at what is inside your training set."),ge=u(),B=o("p"),me=n("Although it\u2019s tempting to look inside "),re=o("code"),we=n("raw_datasets"),oe=n(" and "),fa=o("code"),Us=n("tokenized_datasets"),je=n(", we highly recommend you go to the data right at the point where it\u2019s going to enter the model. That means reading an output from the "),ie=o("code"),Je=n("tf.data.Dataset"),Se=n(" you created with the "),qe=o("code"),Ls=n("to_tf_dataset()"),X=n(" function! So how do we do that? "),ba=o("code"),js=n("tf.data.Dataset"),Kt=n(" objects give us whole batches at a time and don\u2019t support indexing, so we can\u2019t just ask for "),Ms=o("code"),Xe=n("train_dataset[0]"),Ne=n(". We can, however, ask it politely for a batch:"),We=u(),w(_e.$$.fragment),Ze=u(),ve=o("p"),Fe=o("code"),Vt=n("break"),Qa=n(" ends the loop after one iteration, so this grabs the first batch that comes out of "),ke=o("code"),_s=n("train_dataset"),Qe=n(" and saves it as "),es=o("code"),Jt=n("batch"),ya=n(". Now, let\u2019s take a look at what\u2019s inside:"),$e=u(),w(ss.$$.fragment),as=u(),K=o("p"),vs=n("This looks right, doesn\u2019t it? We\u2019re passing the "),pe=o("code"),et=n("labels"),Gs=n(", "),ga=o("code"),ks=n("attention_mask"),Pe=n(", and "),wa=o("code"),st=n("input_ids"),$s=n(" to the model, which should be everything it needs to compute outputs and calculate the loss. So why don\u2019t we have a gradient? Look closer: we\u2019re passing a single dictionary as input, but a training batch is usually an input tensor or dictionary, plus a labels tensor. Our labels are just a key in our input dictionary."),ts=u(),ee=o("p"),at=n("Is this a problem? Not always, actually! But it\u2019s one of the most common issues you\u2019ll encounter when training Transformer models with TensorFlow. Our models can all compute loss internally, but to do that the labels need to be passed in the input dictionary. This is the loss that is used when we don\u2019t specify a loss value to "),ns=o("code"),Xt=n("compile()"),Es=n(". Keras, on the other hand, usually expects labels to be passed separately from the input dictionary, and loss computations will usually fail if you don\u2019t do that."),de=u(),Ee=o("p"),Ts=n("The problem has now become clearer: we passed a "),Cs=o("code"),Z=n("loss"),Zt=n(" argument, which means we\u2019re asking Keras to compute losses for us, but we passed our labels as inputs to the model, not as labels in the place Keras expects them! We need to choose one or the other: either we use the model\u2019s internal loss and keep the labels where they are, or we keep using Keras losses, but we move the labels to the place Keras expects them. For simplicity, let\u2019s take the first approach. Change the call to "),rs=o("code"),Vn=n("compile()"),qs=n(" to read:"),ja=u(),w(Bs.$$.fragment),_a=u(),le=o("p"),Jn=n("Now we\u2019ll use the model\u2019s internal loss, and this problem should be resolved!"),ls=u(),w(os.$$.fragment),Qt=u(),Ue=o("p"),Ae=n("Now, let\u2019s try training. We should get gradients now, so hopefully (ominous music plays here) we can just call "),is=o("code"),ps=n("model.fit()"),Hs=n(" and everything will work fine!"),tt=u(),w(ae.$$.fragment),xe=u(),b=o("p"),N=n("Oh no."),hs=u(),fe=o("p"),us=o("code"),Ys=n("nan"),va=n(" is not a very encouraging loss value. Still, we\u2019ve checked our data, and it looks pretty good. If that\u2019s not the problem, where can we go next? The obvious next step is to\u2026"),cs=u(),ms=o("h3"),Le=o("a"),Rs=o("span"),w(Ks.$$.fragment),ka=u(),ds=o("span"),en=n("Check your model"),Ps=u(),he=o("p"),As=o("code"),Xn=n("model.fit()"),sn=n(" is a really great convenience function in Keras, but it does a lot of things for you, and that can make it trickier to find exactly where a problem has occurred. If you\u2019re debugging your model, one strategy that can really help is to pass just a single batch to the model, and look at the outputs for that one batch in detail. Another really helpful tip if the model is throwing errors is to "),De=o("code"),an=n("compile()"),Vs=n(" the model with "),Te=o("code"),Js=n("run_eagerly=True"),Zn=n(". This will make it a lot slower, but it will make the error messages much more comprehensible, because they\u2019ll indicate exactly where in your model\u2019s code the problem occurred."),$a=u(),ue=o("p"),nt=n("For now, though, we don\u2019t need "),fs=o("code"),rt=n("run_eagerly"),Me=n(" just yet. Let\u2019s run the "),lt=o("code"),ll=n("batch"),Ei=n(" we got before through the model and see what the outputs look like:"),wo=u(),w(Ea.$$.fragment),jo=u(),w(tn.$$.fragment),ol=u(),U=o("p"),_o=n("Well, this is tricky. Everything is "),Ta=o("code"),vo=n("nan"),Xs=n("! But that\u2019s strange, isn\u2019t it? How would all our logits become "),il=o("code"),pl=n("nan"),Ti=n("? "),hl=o("code"),ul=n("nan"),Ci=n(" means \u201Cnot a number.\u201D "),cl=o("code"),ko=n("nan"),Qn=n(" values often occur when you perform a forbidden operation, such as division by zero. But one thing that\u2019s very important to know about "),ml=o("code"),$o=n("nan"),nn=n(" in machine learning is that this value tends to "),er=o("em"),sr=n("propagate"),qi=n(". If you multiply a number by "),ar=o("code"),tr=n("nan"),Pi=n(", the output is also "),nr=o("code"),Ca=n("nan"),ot=n(". And if you get a "),rn=o("code"),ln=n("nan"),Ai=n(" anywhere in your output, your loss, or your gradient, then it will rapidly spread throughout your whole model \u2014 because when that "),on=o("code"),xi=n("nan"),Eo=n(" value is propagated back through your network, you\u2019ll get "),be=o("code"),Di=n("nan"),dl=n(" gradients, and when weight updates are computed with those gradients, you\u2019ll get "),fl=o("code"),zi=n("nan"),bl=n(" weights, and those weights will compute even more "),yl=o("code"),Oi=n("nan"),gl=n(" outputs! Soon enough the whole network will just be one big block of "),wl=o("code"),Ii=n("nan"),jl=n("s. Once that happens, it\u2019s pretty hard to see where the problem started. How can we isolate where "),_l=o("code"),Si=n("nan"),To=n(" first crept in?"),it=u(),Ge=o("p"),rr=n("The answer is to try "),vl=o("em"),Co=n("reinitializing"),pn=n(" our model. Once we started training, we got a "),lr=o("code"),Zs=n("nan"),Ni=n(" somewhere and it quickly propagated through the whole model. So, let\u2019s load the model from a checkpoint and not do any weight updates, and see where we get a "),hn=o("code"),Wi=n("nan"),Fi=n(" value:"),or=u(),w(un.$$.fragment),qo=u(),cn=o("p"),mn=n("When we run that, we get:"),kl=u(),w(xs.$$.fragment),$l=u(),se=o("p"),El=o("em"),Tl=n("Now"),Ui=n(" we\u2019re getting somewhere! There are no "),Cl=o("code"),ql=n("nan"),Li=n(" values in our logits, which is reassuring. But we do see a few "),Pl=o("code"),Al=n("nan"),Mi=n(" values in our loss! Is there something about those samples in particular that\u2019s causing this problem? Let\u2019s see which ones they are (note that if you run this code yourself, you may get different indices because the dataset has been shuffled):"),Po=u(),w(pt.$$.fragment),Be=u(),w(dn.$$.fragment),ir=u(),pr=o("p"),Gi=n("Let\u2019s look at the samples these indices came from:"),hr=u(),w(fn.$$.fragment),Ao=u(),w(qa.$$.fragment),xo=u(),ur=o("p"),Do=n("Well, there\u2019s a lot in here, but nothing stands out as unusual. Let\u2019s look at the labels:"),ht=u(),w(ut.$$.fragment),bn=u(),w(yn.$$.fragment),xl=u(),ze=o("p"),zo=n("Ah! The "),ct=o("code"),Bi=n("nan"),Oo=n(" samples all have the same label, and it\u2019s label 2. This is a very strong hint. The fact that we\u2019re only getting a loss of "),mt=o("code"),Hi=n("nan"),Io=n(" when our label is 2 suggests that this is a very good time to check the number of labels in our model:"),dt=u(),w(ft.$$.fragment),Ce=u(),w(gn.$$.fragment),cr=u(),bt=o("p"),Yi=n("Now we see the problem: the model thinks there are only two classes, but the labels go up to 2, which means there are in fact three classes (because 0 is also a class). This is how we got a "),wn=o("code"),Ri=n("nan"),Ki=n(" \u2014 by trying to compute the loss for a nonexistent class! Let\u2019s try changing that and fitting the model again:"),mr=u(),w(jn.$$.fragment),So=u(),w(Pa.$$.fragment),No=u(),yt=o("p"),Wo=n("We\u2019re training! No more "),Aa=o("code"),Fo=n("nan"),dr=n("s, and our loss is declining\u2026 sort of. If you watch it for a while, you might start to get a bit impatient, because the loss value stays stubbornly high. Let\u2019s stop training here and try to think about what could be causing this problem. At this point, we\u2019re pretty sure both the data and the model are okay, but our model isn\u2019t learning well. What else is left? It\u2019s time to\u2026"),Uo=u(),Qs=o("h3"),Ds=o("a"),Dl=o("span"),w(gt.$$.fragment),xa=u(),ea=o("span"),zl=n("Check your hyperparameters"),wt=u(),sa=o("p"),Ol=n("If you look back at the code above, you might not be able to see any hyperparameters at all, except perhaps the "),Il=o("code"),Lo=n("batch_size"),fr=n(", and that doesn\u2019t seem like a likely culprit. Don\u2019t be fooled, though; there are always hyperparameters, and if you can\u2019t see them, it just means that you don\u2019t know what they\u2019re set to. In particular, remember a critical thing about Keras: if you set a loss, optimizer, or activation function with a string, "),Sl=o("em"),Mo=n("all of its arguments will be set to their default values"),_n=n(". This means that even though using strings for this is very convenient, you should be very careful when doing so, as it can easily hide critical things from you. (Anyone trying the optional challenge above should take careful note of this fact.)"),Nl=u(),He=o("p"),Vi=n("In this case, where have we set an argument with a string? We were setting the loss with a string initially, but we\u2019re not doing that anymore. We are, however, setting the optimizer with a string. Could that be hiding anything from us? Let\u2019s take a look at "),Da=o("a"),Ji=n("its arguments"),Xi=n("."),Wl=u(),Ye=o("p"),Zi=n("Does anything stand out here? That\u2019s right \u2014 the learning rate! When we just use the string "),br=o("code"),yr=n("'adam'"),Qi=n(", we\u2019re going to get the default learning rate, which is 0.001, or 1e-3. This is way too high for a Transformer model! In general, we recommend trying learning rates between 1e-5 and 1e-4 for your models; that\u2019s somewhere between 10X and 100X smaller than the value we\u2019re actually using here. That sounds like it might be a major problem, so let\u2019s try reducing it. To do that, we need to import the actual "),gr=o("code"),jt=n("optimizer"),ep=n(" object. While we\u2019re at it, let\u2019s reinitialize the model from the checkpoint, in case training with the high learning rate damaged its weights:"),wr=u(),w(vn.$$.fragment),Go=u(),w(_t.$$.fragment),vt=u(),kn=o("p"),$n=n("Now, we can try fitting the model with the new, improved learning rate:"),Fl=u(),w(bs.$$.fragment),Bo=u(),w(za.$$.fragment),Ho=u(),jr=o("p"),Yo=n("Now our loss is really going somewhere! Training finally looks like it\u2019s working. There\u2019s a lesson here: when your model is running but loss isn\u2019t declining, and you\u2019re sure your data is okay, it\u2019s a good idea to check hyperparameters like the learning rate and weight decay. Setting either of those too high is very likely to cause training to \u201Cstall\u201D at a high loss value."),kt=u(),aa=o("h2"),ys=o("a"),_r=o("span"),w(ta.$$.fragment),sp=u(),vr=o("span"),En=n("Other potential issues"),Ul=u(),zs=o("p"),ap=n("We\u2019ve covered the issues in the script above, but there are several other common errors you might face. Let\u2019s take a look at a (very incomplete) list."),kr=u(),Oa=o("h3"),$t=o("a"),$r=o("span"),w(Os.$$.fragment),Ro=u(),Et=o("span"),tp=n("Dealing with out-of-memory errors"),Ll=u(),Oe=o("p"),Ko=n("The telltale sign of running out of memory is an error like \u201COOM when allocating tensor\u201D \u2014 OOM is short for \u201Cout of memory.\u201D This is a very common hazard when dealing with large language models. If you encounter this, a good strategy is to halve your batch size and try again. Bear in mind, though, that some models are "),Tt=o("em"),np=n("very"),Vo=n(" large. For example, the full-size GPT-2 has 1.5B parameters, which means you\u2019ll need 6 GB of memory just to store the model, and another 6 GB for its gradients! Training the full GPT-2 model will usually require over 20 GB of VRAM no matter what batch size you use, which only a few GPUs have. More lightweight models like "),Is=o("code"),Ct=n("distilbert-base-cased"),Ml=n(" are much easier to run, and train much more quickly too."),qt=u(),w(Tn.$$.fragment),Er=u(),Ia=o("h3"),Sa=o("a"),Pt=o("span"),w(Cn.$$.fragment),Jo=u(),na=o("span"),rp=n("Hungry Hungry TensorFlow \u{1F99B}"),Tr=u(),ra=o("p"),lp=n("One particular quirk of TensorFlow that you should be aware of is that it allocates "),Cr=o("em"),qn=n("all"),Xo=n(" of your GPU memory to itself as soon as you load a model or do any training, and then it divides up that memory as required. This is different from the behavior of other frameworks, like PyTorch, which allocate memory as required with CUDA rather than doing it internally. One advantage of the TensorFlow approach is that it can often give useful errors when you run out of memory, and it can recover from that state without crashing the whole CUDA kernel. But there\u2019s also an important downside: if you run two TensorFlow processes at once, then "),At=o("strong"),op=n("you\u2019re going to have a bad time"),Zo=n("."),Ss=u(),qr=o("p"),Gl=n("If you\u2019re running on Colab you don\u2019t need to worry about this, but if you\u2019re running locally this is definitely something you should be careful about. In particular, be aware that closing a notebook tab does not necessarily shut that notebook down! You may need to select running notebooks (the ones with a green icon) and manually shut them down in the directory listing. Any running notebook that was using TensorFlow could still be holding on to a bunch of your GPU memory, and that means any new notebook you start may encounter some very odd issues."),Qo=u(),xt=o("p"),Bl=n("If you start getting errors about CUDA, BLAS, or cuBLAS in code that worked before, this is very often the culprit. You can use a command like "),Hl=o("code"),ip=n("nvidia-smi"),ei=n(" to check \u2014 when you shut down or restart your current notebook, is most of your memory free, or is it still in use? If it\u2019s still in use, something else is holding on to it!"),Dt=u(),la=o("h3"),Re=o("a"),Yl=o("span"),w(Na.$$.fragment),pp=u(),Rl=o("span"),si=n("Check your data (again!)"),oa=u(),te=o("p"),Kl=n("Your model will only learn something if it\u2019s actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are attributed randomly, it\u2019s very likely you won\u2019t get any model training on your dataset. One helpful tool here is "),Wa=o("code"),hp=n("tokenizer.decode()"),Vl=n(". This will turn "),Jl=o("code"),ai=n("input_ids"),zt=n(" back into strings, so you can view the data and see if your training data is teaching what you want it to teach. For example, after you get a "),Xl=o("code"),Zl=n("batch"),up=n(" from your "),Ql=o("code"),ti=n("tf.data.Dataset"),Pr=n(" like we did above, you can decode the first element like so:"),ni=u(),w(Ot.$$.fragment),It=u(),Pn=o("p"),Fa=n("Then you can compare it with the first label, like so:"),Ua=u(),w(La.$$.fragment),St=u(),Ar=o("p"),eo=n("Once you can view your data like this, you can ask yourself the following questions:"),ri=u(),Ke=o("ul"),ia=o("li"),cp=n("Is the decoded data understandable?"),so=u(),ao=o("li"),mp=n("Do you agree with the labels?"),li=u(),Ma=o("li"),oi=n("Is there one label that\u2019s more common than the others?"),An=u(),xr=o("li"),Dr=n("What should the loss/metric be if the model predicted a random answer/always the same answer?"),ii=u(),xn=o("p"),Nt=n("After looking at your data, go through a few of the model\u2019s predictions \u2014 if your model outputs tokens, try decoding them too! If the model is always predicting the same thing it might be because your dataset is biased toward one category (for classification problems), so techniques like oversampling rare classes might help. Alternatively, this can also be caused by training issues like bad hyperparameter settings."),pi=u(),Wt=o("p"),dp=n("If the loss/metric you get on your initial model before any training is very different from the loss/metric you would expect for random predictions, double-check the way your loss or metric is computed, as there is probably a bug there. If you are using several losses that you add at the end, make sure they are of the same scale."),hi=u(),Dn=o("p"),zn=n("When you are sure your data is perfect, you can see if the model is capable of training on it with one simple test."),to=u(),Ve=o("h3"),Ga=o("a"),Ba=o("span"),w(Ft.$$.fragment),zr=u(),no=o("span"),ui=n("Overfit your model on one batch"),Ut=u(),On=o("p"),Or=n("Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high."),ci=u(),Ns=o("p"),In=n("Doing this once you have defined your "),Ir=o("code"),pa=n("model"),fp=n(" is really easy; just grab a batch of training data, then treat that "),Sn=o("code"),bp=n("batch"),yp=n(" as your entire dataset, fitting on it for a large number of epochs:"),Sr=u(),w(Nn.$$.fragment),mi=u(),w(Lt.$$.fragment),Mt=u(),Ha=o("p"),Wn=n("The resulting model should have close-to-perfect results on the "),Nr=o("code"),gs=n("batch"),gp=n(", with a loss declining quickly toward 0 (or the minimum value for the loss you\u2019re using)."),Wr=u(),Fr=o("p"),wp=n("If you don\u2019t manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something."),Ur=u(),w(Fn.$$.fragment),this.h()},l(t){j(m.$$.fragment,t),k=c(t),d=i(t,"P",{});var f=p(d);E=r(f,"If you try to execute it, you might get some "),z=i(f,"CODE",{});var Vp=p(z);C=r(Vp,"VisibleDeprecationWarning"),Vp.forEach(s),q=r(f,"s when doing the dataset conversion \u2014 this is a known UX issue we have, so please ignore it. If you\u2019re reading the course after, say, November 2021 and it\u2019s still happening, then send rage tweets at @carrigmat until he fixes it."),f.forEach(s),O=c(t),D=i(t,"P",{});var jp=p(D);A=r(jp,"What\u2019s a more serious problem, though, is that we get an outright error. And it\u2019s really, terrifyingly long:"),jp.forEach(s),W=c(t),j(T.$$.fragment,t),x=c(t),L=i(t,"P",{});var Lr=p(L);F=r(Lr,"What does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing; how do we even begin to debug something like that? When the error you get doesn\u2019t immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that everything looks right. And of course, the place to start is always to\u2026"),Lr.forEach(s),I=c(t),R=i(t,"H3",{class:!0});var ro=p(R);Y=i(ro,"A",{id:!0,class:!0,href:!0});var Mr=p(Y);Q=i(Mr,"SPAN",{});var _p=p(Q);j(J.$$.fragment,_p),_p.forEach(s),Mr.forEach(s),Ie=c(ro),V=i(ro,"SPAN",{});var Un=p(V);ws=r(Un,"Check your data"),Un.forEach(s),ro.forEach(s),H=c(t),M=i(t,"P",{});var Jp=p(M);ne=r(Jp,"This goes without saying, but if your data is corrupted, Keras is not going to be able to fix it for you. So first things first, you need to have a look at what is inside your training set."),Jp.forEach(s),ge=c(t),B=i(t,"P",{});var ye=p(B);me=r(ye,"Although it\u2019s tempting to look inside "),re=i(ye,"CODE",{});var Xp=p(re);we=r(Xp,"raw_datasets"),Xp.forEach(s),oe=r(ye," and "),fa=i(ye,"CODE",{});var Zp=p(fa);Us=r(Zp,"tokenized_datasets"),Zp.forEach(s),je=r(ye,", we highly recommend you go to the data right at the point where it\u2019s going to enter the model. That means reading an output from the "),ie=i(ye,"CODE",{});var vp=p(ie);Je=r(vp,"tf.data.Dataset"),vp.forEach(s),Se=r(ye," you created with the "),qe=i(ye,"CODE",{});var Gr=p(qe);Ls=r(Gr,"to_tf_dataset()"),Gr.forEach(s),X=r(ye," function! So how do we do that? "),ba=i(ye,"CODE",{});var kp=p(ba);js=r(kp,"tf.data.Dataset"),kp.forEach(s),Kt=r(ye," objects give us whole batches at a time and don\u2019t support indexing, so we can\u2019t just ask for "),Ms=i(ye,"CODE",{});var Br=p(Ms);Xe=r(Br,"train_dataset[0]"),Br.forEach(s),Ne=r(ye,". We can, however, ask it politely for a batch:"),ye.forEach(s),We=c(t),j(_e.$$.fragment,t),Ze=c(t),ve=i(t,"P",{});var Gt=p(ve);Fe=i(Gt,"CODE",{});var lo=p(Fe);Vt=r(lo,"break"),lo.forEach(s),Qa=r(Gt," ends the loop after one iteration, so this grabs the first batch that comes out of "),ke=i(Gt,"CODE",{});var Qp=p(ke);_s=r(Qp,"train_dataset"),Qp.forEach(s),Qe=r(Gt," and saves it as "),es=i(Gt,"CODE",{});var $p=p(es);Jt=r($p,"batch"),$p.forEach(s),ya=r(Gt,". Now, let\u2019s take a look at what\u2019s inside:"),Gt.forEach(s),$e=c(t),j(ss.$$.fragment,t),as=c(t),K=i(t,"P",{});var Ws=p(K);vs=r(Ws,"This looks right, doesn\u2019t it? We\u2019re passing the "),pe=i(Ws,"CODE",{});var eh=p(pe);et=r(eh,"labels"),eh.forEach(s),Gs=r(Ws,", "),ga=i(Ws,"CODE",{});var Ep=p(ga);ks=r(Ep,"attention_mask"),Ep.forEach(s),Pe=r(Ws,", and "),wa=i(Ws,"CODE",{});var Hr=p(wa);st=r(Hr,"input_ids"),Hr.forEach(s),$s=r(Ws," to the model, which should be everything it needs to compute outputs and calculate the loss. So why don\u2019t we have a gradient? Look closer: we\u2019re passing a single dictionary as input, but a training batch is usually an input tensor or dictionary, plus a labels tensor. Our labels are just a key in our input dictionary."),Ws.forEach(s),ts=c(t),ee=i(t,"P",{});var oo=p(ee);at=r(oo,"Is this a problem? Not always, actually! But it\u2019s one of the most common issues you\u2019ll encounter when training Transformer models with TensorFlow. Our models can all compute loss internally, but to do that the labels need to be passed in the input dictionary. This is the loss that is used when we don\u2019t specify a loss value to "),ns=i(oo,"CODE",{});var io=p(ns);Xt=r(io,"compile()"),io.forEach(s),Es=r(oo,". Keras, on the other hand, usually expects labels to be passed separately from the input dictionary, and loss computations will usually fail if you don\u2019t do that."),oo.forEach(s),de=c(t),Ee=i(t,"P",{});var Yr=p(Ee);Ts=r(Yr,"The problem has now become clearer: we passed a "),Cs=i(Yr,"CODE",{});var Tp=p(Cs);Z=r(Tp,"loss"),Tp.forEach(s),Zt=r(Yr," argument, which means we\u2019re asking Keras to compute losses for us, but we passed our labels as inputs to the model, not as labels in the place Keras expects them! We need to choose one or the other: either we use the model\u2019s internal loss and keep the labels where they are, or we keep using Keras losses, but we move the labels to the place Keras expects them. For simplicity, let\u2019s take the first approach. Change the call to "),rs=i(Yr,"CODE",{});var Rr=p(rs);Vn=r(Rr,"compile()"),Rr.forEach(s),qs=r(Yr," to read:"),Yr.forEach(s),ja=c(t),j(Bs.$$.fragment,t),_a=c(t),le=i(t,"P",{});var Cp=p(le);Jn=r(Cp,"Now we\u2019ll use the model\u2019s internal loss, and this problem should be resolved!"),Cp.forEach(s),ls=c(t),j(os.$$.fragment,t),Qt=c(t),Ue=i(t,"P",{});var ha=p(Ue);Ae=r(ha,"Now, let\u2019s try training. We should get gradients now, so hopefully (ominous music plays here) we can just call "),is=i(ha,"CODE",{});var Ln=p(is);ps=r(Ln,"model.fit()"),Ln.forEach(s),Hs=r(ha," and everything will work fine!"),ha.forEach(s),tt=c(t),j(ae.$$.fragment,t),xe=c(t),b=i(t,"P",{});var fi=p(b);N=r(fi,"Oh no."),fi.forEach(s),hs=c(t),fe=i(t,"P",{});var Mn=p(fe);us=i(Mn,"CODE",{});var sh=p(us);Ys=r(sh,"nan"),sh.forEach(s),va=r(Mn," is not a very encouraging loss value. Still, we\u2019ve checked our data, and it looks pretty good. If that\u2019s not the problem, where can we go next? The obvious next step is to\u2026"),Mn.forEach(s),cs=c(t),ms=i(t,"H3",{class:!0});var Kr=p(ms);Le=i(Kr,"A",{id:!0,class:!0,href:!0});var ah=p(Le);Rs=i(ah,"SPAN",{});var qp=p(Rs);j(Ks.$$.fragment,qp),qp.forEach(s),ah.forEach(s),ka=c(Kr),ds=i(Kr,"SPAN",{});var po=p(ds);en=r(po,"Check your model"),po.forEach(s),Kr.forEach(s),Ps=c(t),he=i(t,"P",{});var Gn=p(he);As=i(Gn,"CODE",{});var Pp=p(As);Xn=r(Pp,"model.fit()"),Pp.forEach(s),sn=r(Gn," is a really great convenience function in Keras, but it does a lot of things for you, and that can make it trickier to find exactly where a problem has occurred. If you\u2019re debugging your model, one strategy that can really help is to pass just a single batch to the model, and look at the outputs for that one batch in detail. Another really helpful tip if the model is throwing errors is to "),De=i(Gn,"CODE",{});var Bt=p(De);an=r(Bt,"compile()"),Bt.forEach(s),Vs=r(Gn," the model with "),Te=i(Gn,"CODE",{});var Bn=p(Te);Js=r(Bn,"run_eagerly=True"),Bn.forEach(s),Zn=r(Gn,". This will make it a lot slower, but it will make the error messages much more comprehensible, because they\u2019ll indicate exactly where in your model\u2019s code the problem occurred."),Gn.forEach(s),$a=c(t),ue=i(t,"P",{});var Ht=p(ue);nt=r(Ht,"For now, though, we don\u2019t need "),fs=i(Ht,"CODE",{});var Vr=p(fs);rt=r(Vr,"run_eagerly"),Vr.forEach(s),Me=r(Ht," just yet. Let\u2019s run the "),lt=i(Ht,"CODE",{});var th=p(lt);ll=r(th,"batch"),th.forEach(s),Ei=r(Ht," we got before through the model and see what the outputs look like:"),Ht.forEach(s),wo=c(t),j(Ea.$$.fragment,t),jo=c(t),j(tn.$$.fragment,t),ol=c(t),U=i(t,"P",{});var G=p(U);_o=r(G,"Well, this is tricky. Everything is "),Ta=i(G,"CODE",{});var nh=p(Ta);vo=r(nh,"nan"),nh.forEach(s),Xs=r(G,"! But that\u2019s strange, isn\u2019t it? How would all our logits become "),il=i(G,"CODE",{});var Ap=p(il);pl=r(Ap,"nan"),Ap.forEach(s),Ti=r(G,"? "),hl=i(G,"CODE",{});var ho=p(hl);ul=r(ho,"nan"),ho.forEach(s),Ci=r(G," means \u201Cnot a number.\u201D "),cl=i(G,"CODE",{});var rh=p(cl);ko=r(rh,"nan"),rh.forEach(s),Qn=r(G," values often occur when you perform a forbidden operation, such as division by zero. But one thing that\u2019s very important to know about "),ml=i(G,"CODE",{});var xp=p(ml);$o=r(xp,"nan"),xp.forEach(s),nn=r(G," in machine learning is that this value tends to "),er=i(G,"EM",{});var Fs=p(er);sr=r(Fs,"propagate"),Fs.forEach(s),qi=r(G,". If you multiply a number by "),ar=i(G,"CODE",{});var bi=p(ar);tr=r(bi,"nan"),bi.forEach(s),Pi=r(G,", the output is also "),nr=i(G,"CODE",{});var lh=p(nr);Ca=r(lh,"nan"),lh.forEach(s),ot=r(G,". And if you get a "),rn=i(G,"CODE",{});var oh=p(rn);ln=r(oh,"nan"),oh.forEach(s),Ai=r(G," anywhere in your output, your loss, or your gradient, then it will rapidly spread throughout your whole model \u2014 because when that "),on=i(G,"CODE",{});var yi=p(on);xi=r(yi,"nan"),yi.forEach(s),Eo=r(G," value is propagated back through your network, you\u2019ll get "),be=i(G,"CODE",{});var ih=p(be);Di=r(ih,"nan"),ih.forEach(s),dl=r(G," gradients, and when weight updates are computed with those gradients, you\u2019ll get "),fl=i(G,"CODE",{});var ph=p(fl);zi=r(ph,"nan"),ph.forEach(s),bl=r(G," weights, and those weights will compute even more "),yl=i(G,"CODE",{});var gi=p(yl);Oi=r(gi,"nan"),gi.forEach(s),gl=r(G," outputs! Soon enough the whole network will just be one big block of "),wl=i(G,"CODE",{});var hh=p(wl);Ii=r(hh,"nan"),hh.forEach(s),jl=r(G,"s. Once that happens, it\u2019s pretty hard to see where the problem started. How can we isolate where "),_l=i(G,"CODE",{});var uh=p(_l);Si=r(uh,"nan"),uh.forEach(s),To=r(G," first crept in?"),G.forEach(s),it=c(t),Ge=i(t,"P",{});var ua=p(Ge);rr=r(ua,"The answer is to try "),vl=i(ua,"EM",{});var ch=p(vl);Co=r(ch,"reinitializing"),ch.forEach(s),pn=r(ua," our model. Once we started training, we got a "),lr=i(ua,"CODE",{});var Dp=p(lr);Zs=r(Dp,"nan"),Dp.forEach(s),Ni=r(ua," somewhere and it quickly propagated through the whole model. So, let\u2019s load the model from a checkpoint and not do any weight updates, and see where we get a "),hn=i(ua,"CODE",{});var Jr=p(hn);Wi=r(Jr,"nan"),Jr.forEach(s),Fi=r(ua," value:"),ua.forEach(s),or=c(t),j(un.$$.fragment,t),qo=c(t),cn=i(t,"P",{});var zp=p(cn);mn=r(zp,"When we run that, we get:"),zp.forEach(s),kl=c(t),j(xs.$$.fragment,t),$l=c(t),se=i(t,"P",{});var ca=p(se);El=i(ca,"EM",{});var mh=p(El);Tl=r(mh,"Now"),mh.forEach(s),Ui=r(ca," we\u2019re getting somewhere! There are no "),Cl=i(ca,"CODE",{});var Op=p(Cl);ql=r(Op,"nan"),Op.forEach(s),Li=r(ca," values in our logits, which is reassuring. But we do see a few "),Pl=i(ca,"CODE",{});var uo=p(Pl);Al=r(uo,"nan"),uo.forEach(s),Mi=r(ca," values in our loss! Is there something about those samples in particular that\u2019s causing this problem? Let\u2019s see which ones they are (note that if you run this code yourself, you may get different indices because the dataset has been shuffled):"),ca.forEach(s),Po=c(t),j(pt.$$.fragment,t),Be=c(t),j(dn.$$.fragment,t),ir=c(t),pr=i(t,"P",{});var dh=p(pr);Gi=r(dh,"Let\u2019s look at the samples these indices came from:"),dh.forEach(s),hr=c(t),j(fn.$$.fragment,t),Ao=c(t),j(qa.$$.fragment,t),xo=c(t),ur=i(t,"P",{});var Ip=p(ur);Do=r(Ip,"Well, there\u2019s a lot in here, but nothing stands out as unusual. Let\u2019s look at the labels:"),Ip.forEach(s),ht=c(t),j(ut.$$.fragment,t),bn=c(t),j(yn.$$.fragment,t),xl=c(t),ze=i(t,"P",{});var Ya=p(ze);zo=r(Ya,"Ah! The "),ct=i(Ya,"CODE",{});var fh=p(ct);Bi=r(fh,"nan"),fh.forEach(s),Oo=r(Ya," samples all have the same label, and it\u2019s label 2. This is a very strong hint. The fact that we\u2019re only getting a loss of "),mt=i(Ya,"CODE",{});var Sp=p(mt);Hi=r(Sp,"nan"),Sp.forEach(s),Io=r(Ya," when our label is 2 suggests that this is a very good time to check the number of labels in our model:"),Ya.forEach(s),dt=c(t),j(ft.$$.fragment,t),Ce=c(t),j(gn.$$.fragment,t),cr=c(t),bt=i(t,"P",{});var ma=p(bt);Yi=r(ma,"Now we see the problem: the model thinks there are only two classes, but the labels go up to 2, which means there are in fact three classes (because 0 is also a class). This is how we got a "),wn=i(ma,"CODE",{});var Hn=p(wn);Ri=r(Hn,"nan"),Hn.forEach(s),Ki=r(ma," \u2014 by trying to compute the loss for a nonexistent class! Let\u2019s try changing that and fitting the model again:"),ma.forEach(s),mr=c(t),j(jn.$$.fragment,t),So=c(t),j(Pa.$$.fragment,t),No=c(t),yt=i(t,"P",{});var Xr=p(yt);Wo=r(Xr,"We\u2019re training! No more "),Aa=i(Xr,"CODE",{});var Zr=p(Aa);Fo=r(Zr,"nan"),Zr.forEach(s),dr=r(Xr,"s, and our loss is declining\u2026 sort of. If you watch it for a while, you might start to get a bit impatient, because the loss value stays stubbornly high. Let\u2019s stop training here and try to think about what could be causing this problem. At this point, we\u2019re pretty sure both the data and the model are okay, but our model isn\u2019t learning well. What else is left? It\u2019s time to\u2026"),Xr.forEach(s),Uo=c(t),Qs=i(t,"H3",{class:!0});var wi=p(Qs);Ds=i(wi,"A",{id:!0,class:!0,href:!0});var ji=p(Ds);Dl=i(ji,"SPAN",{});var bh=p(Dl);j(gt.$$.fragment,bh),bh.forEach(s),ji.forEach(s),xa=c(wi),ea=i(wi,"SPAN",{});var Np=p(ea);zl=r(Np,"Check your hyperparameters"),Np.forEach(s),wi.forEach(s),wt=c(t),sa=i(t,"P",{});var Ra=p(sa);Ol=r(Ra,"If you look back at the code above, you might not be able to see any hyperparameters at all, except perhaps the "),Il=i(Ra,"CODE",{});var yh=p(Il);Lo=r(yh,"batch_size"),yh.forEach(s),fr=r(Ra,", and that doesn\u2019t seem like a likely culprit. Don\u2019t be fooled, though; there are always hyperparameters, and if you can\u2019t see them, it just means that you don\u2019t know what they\u2019re set to. In particular, remember a critical thing about Keras: if you set a loss, optimizer, or activation function with a string, "),Sl=i(Ra,"EM",{});var Wp=p(Sl);Mo=r(Wp,"all of its arguments will be set to their default values"),Wp.forEach(s),_n=r(Ra,". This means that even though using strings for this is very convenient, you should be very careful when doing so, as it can easily hide critical things from you. (Anyone trying the optional challenge above should take careful note of this fact.)"),Ra.forEach(s),Nl=c(t),He=i(t,"P",{});var Ka=p(He);Vi=r(Ka,"In this case, where have we set an argument with a string? We were setting the loss with a string initially, but we\u2019re not doing that anymore. We are, however, setting the optimizer with a string. Could that be hiding anything from us? Let\u2019s take a look at "),Da=i(Ka,"A",{href:!0,rel:!0});var gh=p(Da);Ji=r(gh,"its arguments"),gh.forEach(s),Xi=r(Ka,"."),Ka.forEach(s),Wl=c(t),Ye=i(t,"P",{});var Yt=p(Ye);Zi=r(Yt,"Does anything stand out here? That\u2019s right \u2014 the learning rate! When we just use the string "),br=i(Yt,"CODE",{});var wh=p(br);yr=r(wh,"'adam'"),wh.forEach(s),Qi=r(Yt,", we\u2019re going to get the default learning rate, which is 0.001, or 1e-3. This is way too high for a Transformer model! In general, we recommend trying learning rates between 1e-5 and 1e-4 for your models; that\u2019s somewhere between 10X and 100X smaller than the value we\u2019re actually using here. That sounds like it might be a major problem, so let\u2019s try reducing it. To do that, we need to import the actual "),gr=i(Yt,"CODE",{});var jh=p(gr);jt=r(jh,"optimizer"),jh.forEach(s),ep=r(Yt," object. While we\u2019re at it, let\u2019s reinitialize the model from the checkpoint, in case training with the high learning rate damaged its weights:"),Yt.forEach(s),wr=c(t),j(vn.$$.fragment,t),Go=c(t),j(_t.$$.fragment,t),vt=c(t),kn=i(t,"P",{});var Fp=p(kn);$n=r(Fp,"Now, we can try fitting the model with the new, improved learning rate:"),Fp.forEach(s),Fl=c(t),j(bs.$$.fragment,t),Bo=c(t),j(za.$$.fragment,t),Ho=c(t),jr=i(t,"P",{});var Qr=p(jr);Yo=r(Qr,"Now our loss is really going somewhere! Training finally looks like it\u2019s working. There\u2019s a lesson here: when your model is running but loss isn\u2019t declining, and you\u2019re sure your data is okay, it\u2019s a good idea to check hyperparameters like the learning rate and weight decay. Setting either of those too high is very likely to cause training to \u201Cstall\u201D at a high loss value."),Qr.forEach(s),kt=c(t),aa=i(t,"H2",{class:!0});var co=p(aa);ys=i(co,"A",{id:!0,class:!0,href:!0});var el=p(ys);_r=i(el,"SPAN",{});var Up=p(_r);j(ta.$$.fragment,Up),Up.forEach(s),el.forEach(s),sp=c(co),vr=i(co,"SPAN",{});var Yn=p(vr);En=r(Yn,"Other potential issues"),Yn.forEach(s),co.forEach(s),Ul=c(t),zs=i(t,"P",{});var _h=p(zs);ap=r(_h,"We\u2019ve covered the issues in the script above, but there are several other common errors you might face. Let\u2019s take a look at a (very incomplete) list."),_h.forEach(s),kr=c(t),Oa=i(t,"H3",{class:!0});var sl=p(Oa);$t=i(sl,"A",{id:!0,class:!0,href:!0});var vh=p($t);$r=i(vh,"SPAN",{});var kh=p($r);j(Os.$$.fragment,kh),kh.forEach(s),vh.forEach(s),Ro=c(sl),Et=i(sl,"SPAN",{});var Lp=p(Et);tp=r(Lp,"Dealing with out-of-memory errors"),Lp.forEach(s),sl.forEach(s),Ll=c(t),Oe=i(t,"P",{});var da=p(Oe);Ko=r(da,"The telltale sign of running out of memory is an error like \u201COOM when allocating tensor\u201D \u2014 OOM is short for \u201Cout of memory.\u201D This is a very common hazard when dealing with large language models. If you encounter this, a good strategy is to halve your batch size and try again. Bear in mind, though, that some models are "),Tt=i(da,"EM",{});var Mp=p(Tt);np=r(Mp,"very"),Mp.forEach(s),Vo=r(da," large. For example, the full-size GPT-2 has 1.5B parameters, which means you\u2019ll need 6 GB of memory just to store the model, and another 6 GB for its gradients! Training the full GPT-2 model will usually require over 20 GB of VRAM no matter what batch size you use, which only a few GPUs have. More lightweight models like "),Is=i(da,"CODE",{});var al=p(Is);Ct=r(al,"distilbert-base-cased"),al.forEach(s),Ml=r(da," are much easier to run, and train much more quickly too."),da.forEach(s),qt=c(t),j(Tn.$$.fragment,t),Er=c(t),Ia=i(t,"H3",{class:!0});var mo=p(Ia);Sa=i(mo,"A",{id:!0,class:!0,href:!0});var fo=p(Sa);Pt=i(fo,"SPAN",{});var $h=p(Pt);j(Cn.$$.fragment,$h),$h.forEach(s),fo.forEach(s),Jo=c(mo),na=i(mo,"SPAN",{});var Gp=p(na);rp=r(Gp,"Hungry Hungry TensorFlow \u{1F99B}"),Gp.forEach(s),mo.forEach(s),Tr=c(t),ra=i(t,"P",{});var Va=p(ra);lp=r(Va,"One particular quirk of TensorFlow that you should be aware of is that it allocates "),Cr=i(Va,"EM",{});var Eh=p(Cr);qn=r(Eh,"all"),Eh.forEach(s),Xo=r(Va," of your GPU memory to itself as soon as you load a model or do any training, and then it divides up that memory as required. This is different from the behavior of other frameworks, like PyTorch, which allocate memory as required with CUDA rather than doing it internally. One advantage of the TensorFlow approach is that it can often give useful errors when you run out of memory, and it can recover from that state without crashing the whole CUDA kernel. But there\u2019s also an important downside: if you run two TensorFlow processes at once, then "),At=i(Va,"STRONG",{});var Bp=p(At);op=r(Bp,"you\u2019re going to have a bad time"),Bp.forEach(s),Zo=r(Va,"."),Va.forEach(s),Ss=c(t),qr=i(t,"P",{});var tl=p(qr);Gl=r(tl,"If you\u2019re running on Colab you don\u2019t need to worry about this, but if you\u2019re running locally this is definitely something you should be careful about. In particular, be aware that closing a notebook tab does not necessarily shut that notebook down! You may need to select running notebooks (the ones with a green icon) and manually shut them down in the directory listing. Any running notebook that was using TensorFlow could still be holding on to a bunch of your GPU memory, and that means any new notebook you start may encounter some very odd issues."),tl.forEach(s),Qo=c(t),xt=i(t,"P",{});var bo=p(xt);Bl=r(bo,"If you start getting errors about CUDA, BLAS, or cuBLAS in code that worked before, this is very often the culprit. You can use a command like "),Hl=i(bo,"CODE",{});var e=p(Hl);ip=r(e,"nvidia-smi"),e.forEach(s),ei=r(bo," to check \u2014 when you shut down or restart your current notebook, is most of your memory free, or is it still in use? If it\u2019s still in use, something else is holding on to it!"),bo.forEach(s),Dt=c(t),la=i(t,"H3",{class:!0});var h=p(la);Re=i(h,"A",{id:!0,class:!0,href:!0});var Hp=p(Re);Yl=i(Hp,"SPAN",{});var xh=p(Yl);j(Na.$$.fragment,xh),xh.forEach(s),Hp.forEach(s),pp=c(h),Rl=i(h,"SPAN",{});var Dh=p(Rl);si=r(Dh,"Check your data (again!)"),Dh.forEach(s),h.forEach(s),oa=c(t),te=i(t,"P",{});var Ja=p(te);Kl=r(Ja,"Your model will only learn something if it\u2019s actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are attributed randomly, it\u2019s very likely you won\u2019t get any model training on your dataset. One helpful tool here is "),Wa=i(Ja,"CODE",{});var Yp=p(Wa);hp=r(Yp,"tokenizer.decode()"),Yp.forEach(s),Vl=r(Ja,". This will turn "),Jl=i(Ja,"CODE",{});var zh=p(Jl);ai=r(zh,"input_ids"),zh.forEach(s),zt=r(Ja," back into strings, so you can view the data and see if your training data is teaching what you want it to teach. For example, after you get a "),Xl=i(Ja,"CODE",{});var Rp=p(Xl);Zl=r(Rp,"batch"),Rp.forEach(s),up=r(Ja," from your "),Ql=i(Ja,"CODE",{});var Oh=p(Ql);ti=r(Oh,"tf.data.Dataset"),Oh.forEach(s),Pr=r(Ja," like we did above, you can decode the first element like so:"),Ja.forEach(s),ni=c(t),j(Ot.$$.fragment,t),It=c(t),Pn=i(t,"P",{});var yo=p(Pn);Fa=r(yo,"Then you can compare it with the first label, like so:"),yo.forEach(s),Ua=c(t),j(La.$$.fragment,t),St=c(t),Ar=i(t,"P",{});var Ih=p(Ar);eo=r(Ih,"Once you can view your data like this, you can ask yourself the following questions:"),Ih.forEach(s),ri=c(t),Ke=i(t,"UL",{});var Rn=p(Ke);ia=i(Rn,"LI",{});var Xa=p(ia);cp=r(Xa,"Is the decoded data understandable?"),Xa.forEach(s),so=c(Rn),ao=i(Rn,"LI",{});var Sh=p(ao);mp=r(Sh,"Do you agree with the labels?"),Sh.forEach(s),li=c(Rn),Ma=i(Rn,"LI",{});var Nh=p(Ma);oi=r(Nh,"Is there one label that\u2019s more common than the others?"),Nh.forEach(s),An=c(Rn),xr=i(Rn,"LI",{});var Wh=p(xr);Dr=r(Wh,"What should the loss/metric be if the model predicted a random answer/always the same answer?"),Wh.forEach(s),Rn.forEach(s),ii=c(t),xn=i(t,"P",{});var Fh=p(xn);Nt=r(Fh,"After looking at your data, go through a few of the model\u2019s predictions \u2014 if your model outputs tokens, try decoding them too! If the model is always predicting the same thing it might be because your dataset is biased toward one category (for classification problems), so techniques like oversampling rare classes might help. Alternatively, this can also be caused by training issues like bad hyperparameter settings."),Fh.forEach(s),pi=c(t),Wt=i(t,"P",{});var Uh=p(Wt);dp=r(Uh,"If the loss/metric you get on your initial model before any training is very different from the loss/metric you would expect for random predictions, double-check the way your loss or metric is computed, as there is probably a bug there. If you are using several losses that you add at the end, make sure they are of the same scale."),Uh.forEach(s),hi=c(t),Dn=i(t,"P",{});var Lh=p(Dn);zn=r(Lh,"When you are sure your data is perfect, you can see if the model is capable of training on it with one simple test."),Lh.forEach(s),to=c(t),Ve=i(t,"H3",{class:!0});var Kp=p(Ve);Ga=i(Kp,"A",{id:!0,class:!0,href:!0});var Mh=p(Ga);Ba=i(Mh,"SPAN",{});var Gh=p(Ba);j(Ft.$$.fragment,Gh),Gh.forEach(s),Mh.forEach(s),zr=c(Kp),no=i(Kp,"SPAN",{});var Za=p(no);ui=r(Za,"Overfit your model on one batch"),Za.forEach(s),Kp.forEach(s),Ut=c(t),On=i(t,"P",{});var Bh=p(On);Or=r(Bh,"Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high."),Bh.forEach(s),ci=c(t),Ns=i(t,"P",{});var go=p(Ns);In=r(go,"Doing this once you have defined your "),Ir=i(go,"CODE",{});var Hh=p(Ir);pa=r(Hh,"model"),Hh.forEach(s),fp=r(go," is really easy; just grab a batch of training data, then treat that "),Sn=i(go,"CODE",{});var Yh=p(Sn);bp=r(Yh,"batch"),Yh.forEach(s),yp=r(go," as your entire dataset, fitting on it for a large number of epochs:"),go.forEach(s),Sr=c(t),j(Nn.$$.fragment,t),mi=c(t),j(Lt.$$.fragment,t),Mt=c(t),Ha=i(t,"P",{});var Kn=p(Ha);Wn=r(Kn,"The resulting model should have close-to-perfect results on the "),Nr=i(Kn,"CODE",{});var Rh=p(Nr);gs=r(Rh,"batch"),Rh.forEach(s),gp=r(Kn,", with a loss declining quickly toward 0 (or the minimum value for the loss you\u2019re using)."),Kn.forEach(s),Wr=c(t),Fr=i(t,"P",{});var Kh=p(Fr);wp=r(Kh,"If you don\u2019t manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something."),Kh.forEach(s),Ur=c(t),j(Fn.$$.fragment,t),this.h()},h(){$(Y,"id","check-your-data"),$(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Y,"href","#check-your-data"),$(R,"class","relative group"),$(Le,"id","check-your-model"),$(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Le,"href","#check-your-model"),$(ms,"class","relative group"),$(Ds,"id","check-your-hyperparameters"),$(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ds,"href","#check-your-hyperparameters"),$(Qs,"class","relative group"),$(Da,"href","https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"),$(Da,"rel","nofollow"),$(ys,"id","other-potential-issues"),$(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ys,"href","#other-potential-issues"),$(aa,"class","relative group"),$($t,"id","dealing-with-outofmemory-errors"),$($t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$($t,"href","#dealing-with-outofmemory-errors"),$(Oa,"class","relative group"),$(Sa,"id","hungry-hungry-tensorflow"),$(Sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Sa,"href","#hungry-hungry-tensorflow"),$(Ia,"class","relative group"),$(Re,"id","check-your-data-again"),$(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Re,"href","#check-your-data-again"),$(la,"class","relative group"),$(Ga,"id","overfit-your-model-on-one-batch"),$(Ga,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ga,"href","#overfit-your-model-on-one-batch"),$(Ve,"class","relative group")},m(t,f){_(m,t,f),l(t,k,f),l(t,d,f),a(d,E),a(d,z),a(z,C),a(d,q),l(t,O,f),l(t,D,f),a(D,A),l(t,W,f),_(T,t,f),l(t,x,f),l(t,L,f),a(L,F),l(t,I,f),l(t,R,f),a(R,Y),a(Y,Q),_(J,Q,null),a(R,Ie),a(R,V),a(V,ws),l(t,H,f),l(t,M,f),a(M,ne),l(t,ge,f),l(t,B,f),a(B,me),a(B,re),a(re,we),a(B,oe),a(B,fa),a(fa,Us),a(B,je),a(B,ie),a(ie,Je),a(B,Se),a(B,qe),a(qe,Ls),a(B,X),a(B,ba),a(ba,js),a(B,Kt),a(B,Ms),a(Ms,Xe),a(B,Ne),l(t,We,f),_(_e,t,f),l(t,Ze,f),l(t,ve,f),a(ve,Fe),a(Fe,Vt),a(ve,Qa),a(ve,ke),a(ke,_s),a(ve,Qe),a(ve,es),a(es,Jt),a(ve,ya),l(t,$e,f),_(ss,t,f),l(t,as,f),l(t,K,f),a(K,vs),a(K,pe),a(pe,et),a(K,Gs),a(K,ga),a(ga,ks),a(K,Pe),a(K,wa),a(wa,st),a(K,$s),l(t,ts,f),l(t,ee,f),a(ee,at),a(ee,ns),a(ns,Xt),a(ee,Es),l(t,de,f),l(t,Ee,f),a(Ee,Ts),a(Ee,Cs),a(Cs,Z),a(Ee,Zt),a(Ee,rs),a(rs,Vn),a(Ee,qs),l(t,ja,f),_(Bs,t,f),l(t,_a,f),l(t,le,f),a(le,Jn),l(t,ls,f),_(os,t,f),l(t,Qt,f),l(t,Ue,f),a(Ue,Ae),a(Ue,is),a(is,ps),a(Ue,Hs),l(t,tt,f),_(ae,t,f),l(t,xe,f),l(t,b,f),a(b,N),l(t,hs,f),l(t,fe,f),a(fe,us),a(us,Ys),a(fe,va),l(t,cs,f),l(t,ms,f),a(ms,Le),a(Le,Rs),_(Ks,Rs,null),a(ms,ka),a(ms,ds),a(ds,en),l(t,Ps,f),l(t,he,f),a(he,As),a(As,Xn),a(he,sn),a(he,De),a(De,an),a(he,Vs),a(he,Te),a(Te,Js),a(he,Zn),l(t,$a,f),l(t,ue,f),a(ue,nt),a(ue,fs),a(fs,rt),a(ue,Me),a(ue,lt),a(lt,ll),a(ue,Ei),l(t,wo,f),_(Ea,t,f),l(t,jo,f),_(tn,t,f),l(t,ol,f),l(t,U,f),a(U,_o),a(U,Ta),a(Ta,vo),a(U,Xs),a(U,il),a(il,pl),a(U,Ti),a(U,hl),a(hl,ul),a(U,Ci),a(U,cl),a(cl,ko),a(U,Qn),a(U,ml),a(ml,$o),a(U,nn),a(U,er),a(er,sr),a(U,qi),a(U,ar),a(ar,tr),a(U,Pi),a(U,nr),a(nr,Ca),a(U,ot),a(U,rn),a(rn,ln),a(U,Ai),a(U,on),a(on,xi),a(U,Eo),a(U,be),a(be,Di),a(U,dl),a(U,fl),a(fl,zi),a(U,bl),a(U,yl),a(yl,Oi),a(U,gl),a(U,wl),a(wl,Ii),a(U,jl),a(U,_l),a(_l,Si),a(U,To),l(t,it,f),l(t,Ge,f),a(Ge,rr),a(Ge,vl),a(vl,Co),a(Ge,pn),a(Ge,lr),a(lr,Zs),a(Ge,Ni),a(Ge,hn),a(hn,Wi),a(Ge,Fi),l(t,or,f),_(un,t,f),l(t,qo,f),l(t,cn,f),a(cn,mn),l(t,kl,f),_(xs,t,f),l(t,$l,f),l(t,se,f),a(se,El),a(El,Tl),a(se,Ui),a(se,Cl),a(Cl,ql),a(se,Li),a(se,Pl),a(Pl,Al),a(se,Mi),l(t,Po,f),_(pt,t,f),l(t,Be,f),_(dn,t,f),l(t,ir,f),l(t,pr,f),a(pr,Gi),l(t,hr,f),_(fn,t,f),l(t,Ao,f),_(qa,t,f),l(t,xo,f),l(t,ur,f),a(ur,Do),l(t,ht,f),_(ut,t,f),l(t,bn,f),_(yn,t,f),l(t,xl,f),l(t,ze,f),a(ze,zo),a(ze,ct),a(ct,Bi),a(ze,Oo),a(ze,mt),a(mt,Hi),a(ze,Io),l(t,dt,f),_(ft,t,f),l(t,Ce,f),_(gn,t,f),l(t,cr,f),l(t,bt,f),a(bt,Yi),a(bt,wn),a(wn,Ri),a(bt,Ki),l(t,mr,f),_(jn,t,f),l(t,So,f),_(Pa,t,f),l(t,No,f),l(t,yt,f),a(yt,Wo),a(yt,Aa),a(Aa,Fo),a(yt,dr),l(t,Uo,f),l(t,Qs,f),a(Qs,Ds),a(Ds,Dl),_(gt,Dl,null),a(Qs,xa),a(Qs,ea),a(ea,zl),l(t,wt,f),l(t,sa,f),a(sa,Ol),a(sa,Il),a(Il,Lo),a(sa,fr),a(sa,Sl),a(Sl,Mo),a(sa,_n),l(t,Nl,f),l(t,He,f),a(He,Vi),a(He,Da),a(Da,Ji),a(He,Xi),l(t,Wl,f),l(t,Ye,f),a(Ye,Zi),a(Ye,br),a(br,yr),a(Ye,Qi),a(Ye,gr),a(gr,jt),a(Ye,ep),l(t,wr,f),_(vn,t,f),l(t,Go,f),_(_t,t,f),l(t,vt,f),l(t,kn,f),a(kn,$n),l(t,Fl,f),_(bs,t,f),l(t,Bo,f),_(za,t,f),l(t,Ho,f),l(t,jr,f),a(jr,Yo),l(t,kt,f),l(t,aa,f),a(aa,ys),a(ys,_r),_(ta,_r,null),a(aa,sp),a(aa,vr),a(vr,En),l(t,Ul,f),l(t,zs,f),a(zs,ap),l(t,kr,f),l(t,Oa,f),a(Oa,$t),a($t,$r),_(Os,$r,null),a(Oa,Ro),a(Oa,Et),a(Et,tp),l(t,Ll,f),l(t,Oe,f),a(Oe,Ko),a(Oe,Tt),a(Tt,np),a(Oe,Vo),a(Oe,Is),a(Is,Ct),a(Oe,Ml),l(t,qt,f),_(Tn,t,f),l(t,Er,f),l(t,Ia,f),a(Ia,Sa),a(Sa,Pt),_(Cn,Pt,null),a(Ia,Jo),a(Ia,na),a(na,rp),l(t,Tr,f),l(t,ra,f),a(ra,lp),a(ra,Cr),a(Cr,qn),a(ra,Xo),a(ra,At),a(At,op),a(ra,Zo),l(t,Ss,f),l(t,qr,f),a(qr,Gl),l(t,Qo,f),l(t,xt,f),a(xt,Bl),a(xt,Hl),a(Hl,ip),a(xt,ei),l(t,Dt,f),l(t,la,f),a(la,Re),a(Re,Yl),_(Na,Yl,null),a(la,pp),a(la,Rl),a(Rl,si),l(t,oa,f),l(t,te,f),a(te,Kl),a(te,Wa),a(Wa,hp),a(te,Vl),a(te,Jl),a(Jl,ai),a(te,zt),a(te,Xl),a(Xl,Zl),a(te,up),a(te,Ql),a(Ql,ti),a(te,Pr),l(t,ni,f),_(Ot,t,f),l(t,It,f),l(t,Pn,f),a(Pn,Fa),l(t,Ua,f),_(La,t,f),l(t,St,f),l(t,Ar,f),a(Ar,eo),l(t,ri,f),l(t,Ke,f),a(Ke,ia),a(ia,cp),a(Ke,so),a(Ke,ao),a(ao,mp),a(Ke,li),a(Ke,Ma),a(Ma,oi),a(Ke,An),a(Ke,xr),a(xr,Dr),l(t,ii,f),l(t,xn,f),a(xn,Nt),l(t,pi,f),l(t,Wt,f),a(Wt,dp),l(t,hi,f),l(t,Dn,f),a(Dn,zn),l(t,to,f),l(t,Ve,f),a(Ve,Ga),a(Ga,Ba),_(Ft,Ba,null),a(Ve,zr),a(Ve,no),a(no,ui),l(t,Ut,f),l(t,On,f),a(On,Or),l(t,ci,f),l(t,Ns,f),a(Ns,In),a(Ns,Ir),a(Ir,pa),a(Ns,fp),a(Ns,Sn),a(Sn,bp),a(Ns,yp),l(t,Sr,f),_(Nn,t,f),l(t,mi,f),_(Lt,t,f),l(t,Mt,f),l(t,Ha,f),a(Ha,Wn),a(Ha,Nr),a(Nr,gs),a(Ha,gp),l(t,Wr,f),l(t,Fr,f),a(Fr,wp),l(t,Ur,f),_(Fn,t,f),di=!0},i(t){di||(g(m.$$.fragment,t),g(T.$$.fragment,t),g(J.$$.fragment,t),g(_e.$$.fragment,t),g(ss.$$.fragment,t),g(Bs.$$.fragment,t),g(os.$$.fragment,t),g(ae.$$.fragment,t),g(Ks.$$.fragment,t),g(Ea.$$.fragment,t),g(tn.$$.fragment,t),g(un.$$.fragment,t),g(xs.$$.fragment,t),g(pt.$$.fragment,t),g(dn.$$.fragment,t),g(fn.$$.fragment,t),g(qa.$$.fragment,t),g(ut.$$.fragment,t),g(yn.$$.fragment,t),g(ft.$$.fragment,t),g(gn.$$.fragment,t),g(jn.$$.fragment,t),g(Pa.$$.fragment,t),g(gt.$$.fragment,t),g(vn.$$.fragment,t),g(_t.$$.fragment,t),g(bs.$$.fragment,t),g(za.$$.fragment,t),g(ta.$$.fragment,t),g(Os.$$.fragment,t),g(Tn.$$.fragment,t),g(Cn.$$.fragment,t),g(Na.$$.fragment,t),g(Ot.$$.fragment,t),g(La.$$.fragment,t),g(Ft.$$.fragment,t),g(Nn.$$.fragment,t),g(Lt.$$.fragment,t),g(Fn.$$.fragment,t),di=!0)},o(t){y(m.$$.fragment,t),y(T.$$.fragment,t),y(J.$$.fragment,t),y(_e.$$.fragment,t),y(ss.$$.fragment,t),y(Bs.$$.fragment,t),y(os.$$.fragment,t),y(ae.$$.fragment,t),y(Ks.$$.fragment,t),y(Ea.$$.fragment,t),y(tn.$$.fragment,t),y(un.$$.fragment,t),y(xs.$$.fragment,t),y(pt.$$.fragment,t),y(dn.$$.fragment,t),y(fn.$$.fragment,t),y(qa.$$.fragment,t),y(ut.$$.fragment,t),y(yn.$$.fragment,t),y(ft.$$.fragment,t),y(gn.$$.fragment,t),y(jn.$$.fragment,t),y(Pa.$$.fragment,t),y(gt.$$.fragment,t),y(vn.$$.fragment,t),y(_t.$$.fragment,t),y(bs.$$.fragment,t),y(za.$$.fragment,t),y(ta.$$.fragment,t),y(Os.$$.fragment,t),y(Tn.$$.fragment,t),y(Cn.$$.fragment,t),y(Na.$$.fragment,t),y(Ot.$$.fragment,t),y(La.$$.fragment,t),y(Ft.$$.fragment,t),y(Nn.$$.fragment,t),y(Lt.$$.fragment,t),y(Fn.$$.fragment,t),di=!1},d(t){v(m,t),t&&s(k),t&&s(d),t&&s(O),t&&s(D),t&&s(W),v(T,t),t&&s(x),t&&s(L),t&&s(I),t&&s(R),v(J),t&&s(H),t&&s(M),t&&s(ge),t&&s(B),t&&s(We),v(_e,t),t&&s(Ze),t&&s(ve),t&&s($e),v(ss,t),t&&s(as),t&&s(K),t&&s(ts),t&&s(ee),t&&s(de),t&&s(Ee),t&&s(ja),v(Bs,t),t&&s(_a),t&&s(le),t&&s(ls),v(os,t),t&&s(Qt),t&&s(Ue),t&&s(tt),v(ae,t),t&&s(xe),t&&s(b),t&&s(hs),t&&s(fe),t&&s(cs),t&&s(ms),v(Ks),t&&s(Ps),t&&s(he),t&&s($a),t&&s(ue),t&&s(wo),v(Ea,t),t&&s(jo),v(tn,t),t&&s(ol),t&&s(U),t&&s(it),t&&s(Ge),t&&s(or),v(un,t),t&&s(qo),t&&s(cn),t&&s(kl),v(xs,t),t&&s($l),t&&s(se),t&&s(Po),v(pt,t),t&&s(Be),v(dn,t),t&&s(ir),t&&s(pr),t&&s(hr),v(fn,t),t&&s(Ao),v(qa,t),t&&s(xo),t&&s(ur),t&&s(ht),v(ut,t),t&&s(bn),v(yn,t),t&&s(xl),t&&s(ze),t&&s(dt),v(ft,t),t&&s(Ce),v(gn,t),t&&s(cr),t&&s(bt),t&&s(mr),v(jn,t),t&&s(So),v(Pa,t),t&&s(No),t&&s(yt),t&&s(Uo),t&&s(Qs),v(gt),t&&s(wt),t&&s(sa),t&&s(Nl),t&&s(He),t&&s(Wl),t&&s(Ye),t&&s(wr),v(vn,t),t&&s(Go),v(_t,t),t&&s(vt),t&&s(kn),t&&s(Fl),v(bs,t),t&&s(Bo),v(za,t),t&&s(Ho),t&&s(jr),t&&s(kt),t&&s(aa),v(ta),t&&s(Ul),t&&s(zs),t&&s(kr),t&&s(Oa),v(Os),t&&s(Ll),t&&s(Oe),t&&s(qt),v(Tn,t),t&&s(Er),t&&s(Ia),v(Cn),t&&s(Tr),t&&s(ra),t&&s(Ss),t&&s(qr),t&&s(Qo),t&&s(xt),t&&s(Dt),t&&s(la),v(Na),t&&s(oa),t&&s(te),t&&s(ni),v(Ot,t),t&&s(It),t&&s(Pn),t&&s(Ua),v(La,t),t&&s(St),t&&s(Ar),t&&s(ri),t&&s(Ke),t&&s(ii),t&&s(xn),t&&s(pi),t&&s(Wt),t&&s(hi),t&&s(Dn),t&&s(to),t&&s(Ve),v(Ft),t&&s(Ut),t&&s(On),t&&s(ci),t&&s(Ns),t&&s(Sr),v(Nn,t),t&&s(mi),v(Lt,t),t&&s(Mt),t&&s(Ha),t&&s(Wr),t&&s(Fr),t&&s(Ur),v(Fn,t)}}}function Pm(S){let m,k,d,E,z,C,q,O,D,A,W,T,x,L,F,I,R,Y,Q,J,Ie,V,ws,H,M,ne,ge,B,me,re,we,oe,fa,Us,je,ie,Je,Se,qe,Ls,X,ba,js,Kt,Ms,Xe,Ne,We,_e,Ze,ve,Fe,Vt,Qa,ke,_s,Qe,es,Jt,ya,$e,ss,as,K,vs,pe,et,Gs,ga,ks,Pe,wa,st,$s,ts,ee,at,ns,Xt,Es,de,Ee,Ts,Cs,Z,Zt,rs,Vn,qs,ja,Bs,_a,le,Jn,ls,os,Qt,Ue,Ae,is,ps,Hs,tt,ae,xe,b,N,hs,fe,us,Ys,va,cs,ms,Le,Rs,Ks,ka,ds,en,Ps,he,As,Xn,sn,De,an,Vs,Te,Js,Zn,$a,ue,nt,fs,rt,Me,lt,ll,Ei,wo,Ea,jo,tn,ol,U,_o,Ta,vo,Xs,il,pl,Ti,hl,ul,Ci,cl,ko,Qn,ml,$o,nn,er,sr,qi,ar,tr,Pi,nr,Ca,ot,rn,ln,Ai,on,xi,Eo,be,Di,dl,fl,zi,bl,yl,Oi,gl,wl,Ii,jl,_l,Si,To,it,Ge,rr,vl,Co,pn,lr,Zs,Ni,hn,Wi,Fi,or,un,qo,cn,mn,kl,xs,$l,se,El,Tl,Ui,Cl,ql,Li,Pl,Al,Mi,Po,pt,Be,dn,ir,pr,Gi,hr,fn,Ao,qa,xo,ur,Do,ht,ut,bn,yn,xl,ze,zo,ct,Bi,Oo,mt,Hi,Io,dt,ft,Ce,gn,cr,bt,Yi,wn,Ri,Ki,mr,jn,So,Pa,No,yt,Wo,Aa,Fo,dr,Uo,Qs,Ds,Dl,gt,xa,ea,zl,wt,sa,Ol,Il,Lo,fr,Sl,Mo,_n,Nl,He,Vi,Da,Ji,Xi,Wl,Ye,Zi,br,yr,Qi,gr,jt,ep,wr,vn,Go,_t,vt,kn,$n,Fl,bs,Bo,za,Ho,jr,Yo,kt,aa,ys,_r,ta,sp,vr,En,Ul,zs,ap,kr,Oa,$t,$r,Os,Ro,Et,tp,Ll,Oe,Ko,Tt,np,Vo,Is,Ct,Ml,qt,Tn,Er,Ia,Sa,Pt,Cn,Jo,na,rp,Tr,ra,lp,Cr,qn,Xo,At,op,Zo,Ss,qr,Gl,Qo,xt,Bl,Hl,ip,ei,Dt,la,Re,Yl,Na,pp,Rl,si,oa,te,Kl,Wa,hp,Vl,Jl,ai,zt,Xl,Zl,up,Ql,ti,Pr,ni,Ot,It,Pn,Fa,Ua,La,St,Ar,eo,ri,Ke,ia,cp,so,ao,mp,li,Ma,oi,An,xr,Dr,ii,xn,Nt,pi,Wt,dp,hi,Dn,zn,to,Ve,Ga,Ba,Ft,zr,no,ui,Ut,On,Or,ci,Ns,In,Ir,pa,fp,Sn,bp,yp,Sr,Nn,mi,Lt,Mt,Ha,Wn,Nr,gs,gp,Wr,Fr,wp,Ur,Fn,di,t,f,Vp,jp,Lr,ro,Mr,_p,Un,Jp,ye,Xp,Zp,vp,Gr,kp,Br,Gt,lo,Qp,$p,Ws,eh,Ep,Hr,oo,io,Yr,Tp,Rr,Cp,ha,Ln,fi,Mn,sh,Kr,ah,qp,po,Gn,Pp,Bt,Bn,Ht,Vr,th,G,nh,Ap,ho,rh,xp,Fs,bi,lh,oh,yi,ih,ph,gi,hh,uh,ua,ch,Dp,Jr,zp,ca,mh,Op,uo,dh,Ip,Ya,fh,Sp,ma,Hn,Xr,Zr,wi,ji,bh,Np,Ra,yh,Wp,Ka,gh,Yt,wh,jh,Fp,Qr,co,el,Up,Yn,_h,sl,vh,kh,Lp,da,Mp,al,mo,fo,$h,Gp,Va,Eh,Bp,tl,bo;return m=new P({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=raw_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),C=new P({props:{code:"'ValueError: You have to specify either input_ids or inputs_embeds'",highlighted:'<span class="hljs-string">&#x27;ValueError: You have to specify either input_ids or inputs_embeds&#x27;</span>'}}),W=new ce({}),B=new P({props:{code:"trainer.train_dataset[0]",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>]'}}),re=new P({props:{code:`{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}`,highlighted:`{<span class="hljs-string">&#x27;hypothesis&#x27;</span>: <span class="hljs-string">&#x27;Product and geography are what make cream skimming work. &#x27;</span>,
 <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;premise&#x27;</span>: <span class="hljs-string">&#x27;Conceptually cream skimming has two basic dimensions - product and geography.&#x27;</span>}`}}),_s=new P({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),$e=new P({props:{code:"'ValueError: expected sequence of length 43 at dim 1 (got 37)'",highlighted:'<span class="hljs-string">&#x27;ValueError: expected sequence of length 43 at dim 1 (got 37)&#x27;</span>'}}),pe=new P({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch`}}),$s=new P({props:{code:'tokenizer.decode(trainer.train_dataset[0]["input_ids"])',highlighted:'tokenizer.decode(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),ee=new P({props:{code:"'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'",highlighted:'<span class="hljs-string">&#x27;[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]&#x27;</span>'}}),de=new P({props:{code:"trainer.train_dataset[0].keys()",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>].keys()'}}),Ts=new P({props:{code:"dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;hypothesis&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;premise&#x27;</span>])'}}),is=new P({props:{code:"type(trainer.model)",highlighted:'<span class="hljs-built_in">type</span>(trainer.model)'}}),Hs=new P({props:{code:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification",highlighted:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"}}),ds=new P({props:{code:'tokenizer.decode(trainer.train_dataset[0]["attention_mask"])',highlighted:'tokenizer.decode(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>])'}}),Ps=new P({props:{code:"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]",highlighted:'[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),De=new P({props:{code:`len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)`,highlighted:`<span class="hljs-built_in">len</span>(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>]) == <span class="hljs-built_in">len</span>(
    trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>]
)`}}),Vs=new P({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),ue=new P({props:{code:'trainer.train_dataset[0]["label"]',highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;label&quot;</span>]'}}),fs=new P({props:{code:"1",highlighted:'<span class="hljs-number">1</span>'}}),U=new P({props:{code:'trainer.train_dataset.features["label"].names',highlighted:'trainer.train_dataset.features[<span class="hljs-string">&quot;label&quot;</span>].names'}}),Ta=new P({props:{code:"['entailment', 'neutral', 'contradiction']",highlighted:'[<span class="hljs-string">&#x27;entailment&#x27;</span>, <span class="hljs-string">&#x27;neutral&#x27;</span>, <span class="hljs-string">&#x27;contradiction&#x27;</span>]'}}),nn=new Rt({props:{$$slots:{default:[Im]},$$scope:{ctx:S}}}),ln=new ce({}),it=new P({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),pn=new P({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch

ValueError: expected sequence of length <span class="hljs-number">45</span> at dim <span class="hljs-number">1</span> (got <span class="hljs-number">76</span>)`}}),mn=new P({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`}}),xs=new P({props:{code:"<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>",highlighted:'&lt;function transformers.data.data_collator.default_data_collator(features: <span class="hljs-type">List</span>[InputDataClass], return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]&gt;'}}),ht=new P({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),ze=new P({props:{code:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",highlighted:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"}}),dt=new P({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),Aa=new P({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),wt=new ce({}),_n=new P({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),vt=new P({props:{code:"outputs = trainer.model.cpu()(**batch)",highlighted:"outputs = trainer.model.cpu()(**batch)"}}),$n=new P({props:{code:`~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.`,highlighted:`~/.pyenv/versions/<span class="hljs-number">3.7</span><span class="hljs-number">.9</span>/envs/base/lib/python3<span class="hljs-number">.7</span>/site-packages/torch/nn/functional.py <span class="hljs-keyword">in</span> nll_loss(<span class="hljs-built_in">input</span>, target, weight, size_average, ignore_index, reduce, reduction)
   <span class="hljs-number">2386</span>         )
   <span class="hljs-number">2387</span>     <span class="hljs-keyword">if</span> dim == <span class="hljs-number">2</span>:
-&gt; <span class="hljs-number">2388</span>         ret = torch._C._nn.nll_loss(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)
   <span class="hljs-number">2389</span>     <span class="hljs-keyword">elif</span> dim == <span class="hljs-number">4</span>:
   <span class="hljs-number">2390</span>         ret = torch._C._nn.nll_loss2d(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target <span class="hljs-number">2</span> <span class="hljs-keyword">is</span> out of bounds.`}}),kt=new P({props:{code:"trainer.model.config.num_labels",highlighted:"trainer.model.config.num_labels"}}),ys=new P({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),En=new P({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),Os=new P({props:{code:`for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

outputs = trainer.model.cpu()(**batch)`}}),Oe=new P({props:{code:`import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

outputs = trainer.model.to(device)(**batch)`}}),qt=new ce({}),qn=new P({props:{code:`loss = outputs.loss
loss.backward()`,highlighted:`loss = outputs.loss
loss.backward()`}}),Dt=new P({props:{code:`trainer.create_optimizer()
trainer.optimizer.step()`,highlighted:`trainer.create_optimizer()
trainer.optimizer.step()`}}),Wa=new ce({}),It=new Rt({props:{$$slots:{default:[Sm]},$$scope:{ctx:S}}}),St=new ce({}),Ma=new P({props:{code:`# This will take a long time and error out, so you shouldn't run this cell
trainer.train()`,highlighted:`<span class="hljs-comment"># This will take a long time and error out, so you shouldn&#x27;t run this cell</span>
trainer.train()`}}),An=new P({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),zn=new P({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),Ve=new P({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),Ba=new Rt({props:{$$slots:{default:[Nm]},$$scope:{ctx:S}}}),Ut=new P({props:{code:`for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_eval_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)`}}),In=new P({props:{code:`~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()`,highlighted:`~/git/datasets/src/datasets/metric.py <span class="hljs-keyword">in</span> add_batch(self, predictions, references)
    <span class="hljs-number">431</span>         <span class="hljs-string">&quot;&quot;&quot;
    432         batch = {&quot;predictions&quot;: predictions, &quot;references&quot;: references}
--&gt; 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()</span>`}}),Mt=new P({props:{code:`predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))`,highlighted:`predictions = outputs.logits.cpu().numpy()
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].cpu().numpy()

compute_metrics((predictions, labels))`}}),Wn=new P({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),Lr=new P({props:{code:"predictions.shape, labels.shape",highlighted:"predictions.shape, labels.shape"}}),Mr=new P({props:{code:"((8, 3), (8,))",highlighted:'((<span class="hljs-number">8</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">8</span>,))'}}),Gr=new P({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`}}),Br=new P({props:{code:"{'accuracy': 0.625}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.625</span>}'}}),Hr=new P({props:{code:`import numpy as np
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),Rr=new Rt({props:{$$slots:{default:[Wm]},$$scope:{ctx:S}}}),Mn=new ce({}),Vr=new ce({}),Jr=new Rt({props:{warning:!0,$$slots:{default:[Fm]},$$scope:{ctx:S}}}),Zr=new ce({}),Qr=new P({props:{code:`for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
trainer.create_optimizer()

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`}}),el=new Rt({props:{$$slots:{default:[Um]},$$scope:{ctx:S}}}),da=new P({props:{code:`with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`,highlighted:`<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`}}),al=new P({props:{code:"{'accuracy': 1.0}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">1.0</span>}'}}),tl=new Rt({props:{warning:!0,$$slots:{default:[Lm]},$$scope:{ctx:S}}}),{c(){w(m.$$.fragment),k=u(),d=o("p"),E=n("If you try to execute it, you will be met with a rather cryptic error:"),z=u(),w(C.$$.fragment),q=u(),O=o("h3"),D=o("a"),A=o("span"),w(W.$$.fragment),T=u(),x=o("span"),L=n("Check your data"),F=u(),I=o("p"),R=n("This goes without saying, but if your data is corrupted, the "),Y=o("code"),Q=n("Trainer"),J=n(" is not going to be able to form batches, let alone train your model. So first things first, you need to have a look at what is inside your training set."),Ie=u(),V=o("p"),ws=n("To avoid countless hours spent trying to fix something that is not the source of the bug, we recommend you use "),H=o("code"),M=n("trainer.train_dataset"),ne=n(" for your checks and nothing else. So let\u2019s do that here:"),ge=u(),w(B.$$.fragment),me=u(),w(re.$$.fragment),we=u(),oe=o("p"),fa=n("Do you notice something wrong? This, in conjunction with the error message about "),Us=o("code"),je=n("input_ids"),ie=n(" missing, should make you realize those are texts, not numbers the model can make sense of. Here, the original error is very misleading because the "),Je=o("code"),Se=n("Trainer"),qe=n(" automatically removes the columns that don\u2019t match the model signature (that is, the arguments expected by the model). That means here, everything apart from the labels was discarded. There was thus no issue with creating batches and then sending them to the model, which in turn complained it didn\u2019t receive the proper input."),Ls=u(),X=o("p"),ba=n("Why wasn\u2019t the data processed? We did use the "),js=o("code"),Kt=n("Dataset.map()"),Ms=n(" method on the datasets to apply the tokenizer on each sample. But if you look closely at the code, you will see that we made a mistake when passing the training and evaluation sets to the "),Xe=o("code"),Ne=n("Trainer"),We=n(". Instead of using "),_e=o("code"),Ze=n("tokenized_datasets"),ve=n(" here, we used "),Fe=o("code"),Vt=n("raw_datasets"),Qa=n(" \u{1F926}. So let\u2019s fix this!"),ke=u(),w(_s.$$.fragment),Qe=u(),es=o("p"),Jt=n("This new code will now give a different error (progress!):"),ya=u(),w($e.$$.fragment),ss=u(),as=o("p"),K=n("Looking at the traceback, we can see the error happens in the data collation step:"),vs=u(),w(pe.$$.fragment),et=u(),Gs=o("p"),ga=n("So, we should move to that. Before we do, however, let\u2019s finish inspecting our data, just to be 100% sure it\u2019s correct."),ks=u(),Pe=o("p"),wa=n("One thing you should always do when debugging a training session is have a look at the decoded inputs of your model. We can\u2019t make sense of the numbers that we feed it directly, so we should look at what those numbers represent. In computer vision, for example, that means looking at the decoded pictures of the pixels you pass, in speech it means listening to the decoded audio samples, and for our NLP example here it means using our tokenizer to decode the inputs:"),st=u(),w($s.$$.fragment),ts=u(),w(ee.$$.fragment),at=u(),ns=o("p"),Xt=n("So that seems correct. You should do this for all the keys in the inputs:"),Es=u(),w(de.$$.fragment),Ee=u(),w(Ts.$$.fragment),Cs=u(),Z=o("p"),Zt=n("Note that the keys that don\u2019t correspond to inputs accepted by the model will be automatically discarded, so here we will only keep "),rs=o("code"),Vn=n("input_ids"),qs=n(", "),ja=o("code"),Bs=n("attention_mask"),_a=n(", and "),le=o("code"),Jn=n("label"),ls=n(" (which will be renamed "),os=o("code"),Qt=n("labels"),Ue=n("). To double-check the model signature, you can print the class of your model, then go check its documentation:"),Ae=u(),w(is.$$.fragment),ps=u(),w(Hs.$$.fragment),tt=u(),ae=o("p"),xe=n("So in our case, we can check the parameters accepted on "),b=o("a"),N=n("this page"),hs=n(". The "),fe=o("code"),us=n("Trainer"),Ys=n(" will also log the columns it\u2019s discarding."),va=u(),cs=o("p"),ms=n("We have checked that the input IDs are correct by decoding them. Next is the "),Le=o("code"),Rs=n("attention_mask"),Ks=n(":"),ka=u(),w(ds.$$.fragment),en=u(),w(Ps.$$.fragment),he=u(),As=o("p"),Xn=n("Since we didn\u2019t apply padding in our preprocessing, this seems perfectly natural. To be sure there is no issue with that attention mask, let\u2019s check it is the same length as our input IDs:"),sn=u(),w(De.$$.fragment),an=u(),w(Vs.$$.fragment),Te=u(),Js=o("p"),Zn=n("That\u2019s good! Lastly, let\u2019s check our label:"),$a=u(),w(ue.$$.fragment),nt=u(),w(fs.$$.fragment),rt=u(),Me=o("p"),lt=n("Like the input IDs, this is a number that doesn\u2019t really make sense on its own. As we saw before, the map between integers and label names is stored inside the "),ll=o("code"),Ei=n("names"),wo=n(" attribute of the corresponding "),Ea=o("em"),jo=n("feature"),tn=n(" of the dataset:"),ol=u(),w(U.$$.fragment),_o=u(),w(Ta.$$.fragment),vo=u(),Xs=o("p"),il=n("So "),pl=o("code"),Ti=n("1"),hl=n(" means "),ul=o("code"),Ci=n("neutral"),cl=n(", which means the two sentences we saw above are not in contradiction, and the first one does not imply the second one. That seems correct!"),ko=u(),Qn=o("p"),ml=n("We don\u2019t have token type IDs here, since DistilBERT does not expect them; if you have some in your model, you should also make sure that they properly match where the first and second sentences are in the input."),$o=u(),w(nn.$$.fragment),er=u(),sr=o("p"),qi=n("We are only doing the check on the training set here, but you should of course double-check the validation and test sets the same way."),ar=u(),tr=o("p"),Pi=n("Now that we know our datasets look good, it\u2019s time to check the next step of the training pipeline."),nr=u(),Ca=o("h3"),ot=o("a"),rn=o("span"),w(ln.$$.fragment),Ai=u(),on=o("span"),xi=n("From datasets to dataloaders"),Eo=u(),be=o("p"),Di=n("The next thing that can go wrong in the training pipeline is when the "),dl=o("code"),fl=n("Trainer"),zi=n(" tries to form batches from the training or validation set. Once you are sure the "),bl=o("code"),yl=n("Trainer"),Oi=n("\u2019s datasets are correct, you can try to manually form a batch by executing the following (replace "),gl=o("code"),wl=n("train"),Ii=n(" with "),jl=o("code"),_l=n("eval"),Si=n(" for the validation dataloader):"),To=u(),w(it.$$.fragment),Ge=u(),rr=o("p"),vl=n("This code creates the training dataloader, then iterates through it, stopping at the first iteration. If the code executes without error, you have the first training batch that you can inspect, and if the code errors out, you know for sure the problem is in the dataloader, as is the case here:"),Co=u(),w(pn.$$.fragment),lr=u(),Zs=o("p"),Ni=n("Inspecting the last frame of the traceback should be enough to give you a clue, but let\u2019s do a bit more digging. Most of the problems during batch creation arise because of the collation of examples into a single batch, so the first thing to check when in doubt is what "),hn=o("code"),Wi=n("collate_fn"),Fi=n(" your "),or=o("code"),un=n("DataLoader"),qo=n(" is using:"),cn=u(),w(mn.$$.fragment),kl=u(),w(xs.$$.fragment),$l=u(),se=o("p"),El=n("So this is the "),Tl=o("code"),Ui=n("default_data_collator"),Cl=n(", but that\u2019s not what we want in this case. We want to pad our examples to the longest sentence in the batch, which is done by the "),ql=o("code"),Li=n("DataCollatorWithPadding"),Pl=n(" collator. And this data collator is supposed to be used by default by the "),Al=o("code"),Mi=n("Trainer"),Po=n(", so why is it not used here?"),pt=u(),Be=o("p"),dn=n("The answer is because we did not pass the "),ir=o("code"),pr=n("tokenizer"),Gi=n(" to the "),hr=o("code"),fn=n("Trainer"),Ao=n(", so it couldn\u2019t create the "),qa=o("code"),xo=n("DataCollatorWithPadding"),ur=n(" we want. In practice, you should never hesitate to explicitly pass along the data collator you want to use, to make sure you avoid these kinds of errors. Let\u2019s adapt our code to do exactly that:"),Do=u(),w(ht.$$.fragment),ut=u(),bn=o("p"),yn=n("The good news? We don\u2019t get the same error as before, which is definitely progress. The bad news? We get an infamous CUDA error instead:"),xl=u(),w(ze.$$.fragment),zo=u(),ct=o("p"),Bi=n("This is bad because CUDA errors are extremely hard to debug in general. We will see in a minute how to solve this, but first let\u2019s finish our analysis of batch creation."),Oo=u(),mt=o("p"),Hi=n("If you are sure your data collator is the right one, you should try to apply it on a couple of samples of your dataset:"),Io=u(),w(dt.$$.fragment),ft=u(),Ce=o("p"),gn=n("This code will fail because the "),cr=o("code"),bt=n("train_dataset"),Yi=n(" contains string columns, which the "),wn=o("code"),Ri=n("Trainer"),Ki=n(" usually removes. You can remove them manually, or if you want to replicate exactly what the "),mr=o("code"),jn=n("Trainer"),So=n(" is doing behind the scenes, you can call the private "),Pa=o("code"),No=n("Trainer._remove_unused_columns()"),yt=n(" method that does that:"),Wo=u(),w(Aa.$$.fragment),Fo=u(),dr=o("p"),Uo=n("You should then be able to manually debug what happens inside the data collator if the error persists."),Qs=u(),Ds=o("p"),Dl=n("Now that we\u2019ve debugged the batch creation process, it\u2019s time to pass one through the model!"),gt=u(),xa=o("h3"),ea=o("a"),zl=o("span"),w(wt.$$.fragment),sa=u(),Ol=o("span"),Il=n("Going through the model"),Lo=u(),fr=o("p"),Sl=n("You should be able to get a batch by executing the following command:"),Mo=u(),w(_n.$$.fragment),Nl=u(),He=o("p"),Vi=n("If you\u2019re running this code in a notebook, you may get a CUDA error that\u2019s similar to the one we saw earlier, in which case you need to restart your notebook and reexecute the last snippet without the "),Da=o("code"),Ji=n("trainer.train()"),Xi=n(" line. That\u2019s the second most annoying thing about CUDA errors: they irremediably break your kernel. The most annoying thing about them is the fact that they are hard to debug."),Wl=u(),Ye=o("p"),Zi=n("Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don\u2019t know it instantly. It\u2019s only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass."),br=u(),yr=o("p"),Qi=n("So how do we debug those errors? The answer is easy: we don\u2019t. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it."),gr=u(),jt=o("p"),ep=n("To do this in our case, we just have to put the model back on the CPU and call it on our batch \u2014 the batch returned by the "),wr=o("code"),vn=n("DataLoader"),Go=n(" has not been moved to the GPU yet:"),_t=u(),w(vt.$$.fragment),kn=u(),w($n.$$.fragment),Fl=u(),bs=o("p"),Bo=n("So, the picture is getting clearer. Instead of having a CUDA error, we now have an "),za=o("code"),Ho=n("IndexError"),jr=n(" in the loss computation (so nothing to do with the backward pass, as we said earlier). More precisely, we can see that it\u2019s target 2 that creates the error, so this is a very good moment to check the number of labels of our model:"),Yo=u(),w(kt.$$.fragment),aa=u(),w(ys.$$.fragment),_r=u(),ta=o("p"),sp=n("With two labels, only 0s and 1s are allowed as targets, but according to the error message we got a 2. Getting a 2 is actually normal: if we remember the label names we extracted earlier, there were three, so we have indices 0, 1, and 2 in our dataset. The problem is that we didn\u2019t tell that to our model, which should have been created with three labels. So let\u2019s fix that!"),vr=u(),w(En.$$.fragment),Ul=u(),zs=o("p"),ap=n("We aren\u2019t including the "),kr=o("code"),Oa=n("trainer.train()"),$t=n(" line yet, to take the time to check that everything looks good. If we request a batch and pass it to our model, it now works without error!"),$r=u(),w(Os.$$.fragment),Ro=u(),Et=o("p"),tp=n("The next step is then to move back to the GPU and check that everything still works:"),Ll=u(),w(Oe.$$.fragment),Ko=u(),Tt=o("p"),np=n("If you still get an error, make sure you restart your notebook and only execute the last version of the script."),Vo=u(),Is=o("h3"),Ct=o("a"),Ml=o("span"),w(qt.$$.fragment),Tn=u(),Er=o("span"),Ia=n("Performing one optimization step"),Sa=u(),Pt=o("p"),Cn=n("Now that we know that we can build batches that actually go through the model, we are ready for the next step of the training pipeline: computing the gradients and performing an optimization step."),Jo=u(),na=o("p"),rp=n("The first part is just a matter of calling the "),Tr=o("code"),ra=n("backward()"),lp=n(" method on the loss:"),Cr=u(),w(qn.$$.fragment),Xo=u(),At=o("p"),op=n("It\u2019s pretty rare to get an error at this stage, but if you do get one, make sure to go back to the CPU to get a helpful error message."),Zo=u(),Ss=o("p"),qr=n("To perform the optimization step, we just need to create the "),Gl=o("code"),Qo=n("optimizer"),xt=n(" and call its "),Bl=o("code"),Hl=n("step()"),ip=n(" method:"),ei=u(),w(Dt.$$.fragment),la=u(),Re=o("p"),Yl=n("Again, if you\u2019re using the default optimizer in the "),Na=o("code"),pp=n("Trainer"),Rl=n(", you shouldn\u2019t get an error at this stage, but if you have a custom optimizer, there might be some problems to debug here. Don\u2019t forget to go back to the CPU if you get a weird CUDA error at this stage. Speaking of CUDA errors, earlier we mentioned a special case. Let\u2019s have a look at that now."),si=u(),oa=o("h3"),te=o("a"),Kl=o("span"),w(Wa.$$.fragment),hp=u(),Vl=o("span"),Jl=n("Dealing with CUDA out-of-memory errors"),ai=u(),zt=o("p"),Xl=n("Whenever you get an error message that starts with "),Zl=o("code"),up=n("RuntimeError: CUDA out of memory"),Ql=n(", this indicates that you are out of GPU memory. This is not directly linked to your code, and it can happen with a script that runs perfectly fine. This error means that you tried to put too many things in the internal memory of your GPU, and that resulted in an error. Like with other CUDA errors, you will need to restart your kernel to be in a spot where you can run your training again."),ti=u(),Pr=o("p"),ni=n("To solve this issue, you just need to use less GPU space \u2014 something that is often easier said than done. First, make sure you don\u2019t have two models on the GPU at the same time (unless that\u2019s required for your problem, of course). Then, you should probably reduce your batch size, as it directly affects the sizes of all the intermediate outputs of the model and their gradients. If the problem persists, consider using a smaller version of your model."),Ot=u(),w(It.$$.fragment),Pn=u(),Fa=o("h3"),Ua=o("a"),La=o("span"),w(St.$$.fragment),Ar=u(),eo=o("span"),ri=n("Evaluating the model"),Ke=u(),ia=o("p"),cp=n("Now that we\u2019ve solved all the issues with our code, everything is perfect and the training should run smoothly, right? Not so fast! If you run the "),so=o("code"),ao=n("trainer.train()"),mp=n(" command, everything will look good at first, but after a while you will get the following:"),li=u(),w(Ma.$$.fragment),oi=u(),w(An.$$.fragment),xr=u(),Dr=o("p"),ii=n("You will realize this error appears during the evaluation phase, so this is the last thing we will need to debug."),xn=u(),Nt=o("p"),pi=n("You can run the evaluation loop of the "),Wt=o("code"),dp=n("Trainer"),hi=n(" independently form the training like this:"),Dn=u(),w(zn.$$.fragment),to=u(),w(Ve.$$.fragment),Ga=u(),w(Ba.$$.fragment),Ft=u(),zr=o("p"),no=n("Before attempting to debug a problem in the evaluation loop, you should first make sure that you\u2019ve had a look at the data, are able to form a batch properly, and can run your model on it. We\u2019ve completed all of those steps, so the following code can be executed without error:"),ui=u(),w(Ut.$$.fragment),On=u(),Or=o("p"),ci=n("The error comes later, at the end of the evaluation phase, and if we look at the traceback we see this:"),Ns=u(),w(In.$$.fragment),Ir=u(),pa=o("p"),fp=n("This tells us that the error originates in the "),Sn=o("code"),bp=n("datasets/metric.py"),yp=n(" module \u2014 so this is a problem with our "),Sr=o("code"),Nn=n("compute_metrics()"),mi=n(" function. It takes a tuple with the logits and the labels as NumPy arrays, so let\u2019s try to feed it that:"),Lt=u(),w(Mt.$$.fragment),Ha=u(),w(Wn.$$.fragment),Nr=u(),gs=o("p"),gp=n("We get the same error, so the problem definitely lies with that function. If we look back at its code, we see it\u2019s just forwarding the "),Wr=o("code"),Fr=n("predictions"),wp=n(" and the "),Ur=o("code"),Fn=n("labels"),di=n(" to "),t=o("code"),f=n("metric.compute()"),Vp=n(". So is there a problem with that method? Not really. Let\u2019s have a quick look at the shapes:"),jp=u(),w(Lr.$$.fragment),ro=u(),w(Mr.$$.fragment),_p=u(),Un=o("p"),Jp=n("Our predictions are still logits, not the actual predictions, which is why the metric is returning this (somewhat obscure) error. The fix is pretty easy; we just have to add an argmax in the "),ye=o("code"),Xp=n("compute_metrics()"),Zp=n(" function:"),vp=u(),w(Gr.$$.fragment),kp=u(),w(Br.$$.fragment),Gt=u(),lo=o("p"),Qp=n("Now our error is fixed! This was the last one, so our script will now train a model properly."),$p=u(),Ws=o("p"),eh=n("For reference, here is the completely fixed script:"),Ep=u(),w(Hr.$$.fragment),oo=u(),io=o("p"),Yr=n("In this instance, there are no more problems, and our script will fine-tune a model that should give reasonable results. But what can we do when the training proceeds without any error, and the model trained does not perform well at all? That\u2019s the hardest part of machine learning, and we\u2019ll show you a few techniques that can help."),Tp=u(),w(Rr.$$.fragment),Cp=u(),ha=o("h2"),Ln=o("a"),fi=o("span"),w(Mn.$$.fragment),sh=u(),Kr=o("span"),ah=n("Debugging silent errors during training"),qp=u(),po=o("p"),Gn=n("What can we do to debug a training that completes without error but doesn\u2019t get good results? We\u2019ll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer."),Pp=u(),Bt=o("h3"),Bn=o("a"),Ht=o("span"),w(Vr.$$.fragment),th=u(),G=o("span"),nh=n("Check your data (again!)"),Ap=u(),ho=o("p"),rh=n("Your model will only learn something if it\u2019s actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are attributed randomly, it\u2019s very likely you won\u2019t get any model training on your dataset. So always start by double-checking your decoded inputs and labels, and ask yourself the following questions:"),xp=u(),Fs=o("ul"),bi=o("li"),lh=n("Is the decoded data understandable?"),oh=u(),yi=o("li"),ih=n("Do you agree with the labels?"),ph=u(),gi=o("li"),hh=n("Is there one label that\u2019s more common than the others?"),uh=u(),ua=o("li"),ch=n("What should the loss/metric be if the model predicted a random answer/always the same answer?"),Dp=u(),w(Jr.$$.fragment),zp=u(),ca=o("p"),mh=n("After looking at your data, go through a few of the model\u2019s predictions and decode them too. If the model is always predicting the same thing, it might be because your dataset is biased toward one category (for classification problems); techniques like oversampling rare classes might help."),Op=u(),uo=o("p"),dh=n("If the loss/metric you get on your initial model is very different from the loss/metric you would expect for random predictions, double-check the way your loss or metric is computed, as there is probably a bug there. If you are using several losses that you add at the end, make sure they are of the same scale."),Ip=u(),Ya=o("p"),fh=n("When you are sure your data is perfect, you can see if the model is capable of training on it with one simple test."),Sp=u(),ma=o("h3"),Hn=o("a"),Xr=o("span"),w(Zr.$$.fragment),wi=u(),ji=o("span"),bh=n("Overfit your model on one batch"),Np=u(),Ra=o("p"),yh=n("Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high."),Wp=u(),Ka=o("p"),gh=n("Doing this once you have defined your "),Yt=o("code"),wh=n("Trainer"),jh=n(" is really easy; just grab a batch of training data, then run a small manual training loop only using that batch for something like 20 steps:"),Fp=u(),w(Qr.$$.fragment),co=u(),w(el.$$.fragment),Up=u(),Yn=o("p"),_h=n("The resulting model should have close-to-perfect results on the same "),sl=o("code"),vh=n("batch"),kh=n(". Let\u2019s compute the metric on the resulting predictions:"),Lp=u(),w(da.$$.fragment),Mp=u(),w(al.$$.fragment),mo=u(),fo=o("p"),$h=n("100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on any other sentence, it will very likely give you a wrong answer)!"),Gp=u(),Va=o("p"),Eh=n("If you don\u2019t manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something."),Bp=u(),w(tl.$$.fragment),this.h()},l(e){j(m.$$.fragment,e),k=c(e),d=i(e,"P",{});var h=p(d);E=r(h,"If you try to execute it, you will be met with a rather cryptic error:"),h.forEach(s),z=c(e),j(C.$$.fragment,e),q=c(e),O=i(e,"H3",{class:!0});var Hp=p(O);D=i(Hp,"A",{id:!0,class:!0,href:!0});var xh=p(D);A=i(xh,"SPAN",{});var Dh=p(A);j(W.$$.fragment,Dh),Dh.forEach(s),xh.forEach(s),T=c(Hp),x=i(Hp,"SPAN",{});var Ja=p(x);L=r(Ja,"Check your data"),Ja.forEach(s),Hp.forEach(s),F=c(e),I=i(e,"P",{});var Yp=p(I);R=r(Yp,"This goes without saying, but if your data is corrupted, the "),Y=i(Yp,"CODE",{});var zh=p(Y);Q=r(zh,"Trainer"),zh.forEach(s),J=r(Yp," is not going to be able to form batches, let alone train your model. So first things first, you need to have a look at what is inside your training set."),Yp.forEach(s),Ie=c(e),V=i(e,"P",{});var Rp=p(V);ws=r(Rp,"To avoid countless hours spent trying to fix something that is not the source of the bug, we recommend you use "),H=i(Rp,"CODE",{});var Oh=p(H);M=r(Oh,"trainer.train_dataset"),Oh.forEach(s),ne=r(Rp," for your checks and nothing else. So let\u2019s do that here:"),Rp.forEach(s),ge=c(e),j(B.$$.fragment,e),me=c(e),j(re.$$.fragment,e),we=c(e),oe=i(e,"P",{});var yo=p(oe);fa=r(yo,"Do you notice something wrong? This, in conjunction with the error message about "),Us=i(yo,"CODE",{});var Ih=p(Us);je=r(Ih,"input_ids"),Ih.forEach(s),ie=r(yo," missing, should make you realize those are texts, not numbers the model can make sense of. Here, the original error is very misleading because the "),Je=i(yo,"CODE",{});var Rn=p(Je);Se=r(Rn,"Trainer"),Rn.forEach(s),qe=r(yo," automatically removes the columns that don\u2019t match the model signature (that is, the arguments expected by the model). That means here, everything apart from the labels was discarded. There was thus no issue with creating batches and then sending them to the model, which in turn complained it didn\u2019t receive the proper input."),yo.forEach(s),Ls=c(e),X=i(e,"P",{});var Xa=p(X);ba=r(Xa,"Why wasn\u2019t the data processed? We did use the "),js=i(Xa,"CODE",{});var Sh=p(js);Kt=r(Sh,"Dataset.map()"),Sh.forEach(s),Ms=r(Xa," method on the datasets to apply the tokenizer on each sample. But if you look closely at the code, you will see that we made a mistake when passing the training and evaluation sets to the "),Xe=i(Xa,"CODE",{});var Nh=p(Xe);Ne=r(Nh,"Trainer"),Nh.forEach(s),We=r(Xa,". Instead of using "),_e=i(Xa,"CODE",{});var Wh=p(_e);Ze=r(Wh,"tokenized_datasets"),Wh.forEach(s),ve=r(Xa," here, we used "),Fe=i(Xa,"CODE",{});var Fh=p(Fe);Vt=r(Fh,"raw_datasets"),Fh.forEach(s),Qa=r(Xa," \u{1F926}. So let\u2019s fix this!"),Xa.forEach(s),ke=c(e),j(_s.$$.fragment,e),Qe=c(e),es=i(e,"P",{});var Uh=p(es);Jt=r(Uh,"This new code will now give a different error (progress!):"),Uh.forEach(s),ya=c(e),j($e.$$.fragment,e),ss=c(e),as=i(e,"P",{});var Lh=p(as);K=r(Lh,"Looking at the traceback, we can see the error happens in the data collation step:"),Lh.forEach(s),vs=c(e),j(pe.$$.fragment,e),et=c(e),Gs=i(e,"P",{});var Kp=p(Gs);ga=r(Kp,"So, we should move to that. Before we do, however, let\u2019s finish inspecting our data, just to be 100% sure it\u2019s correct."),Kp.forEach(s),ks=c(e),Pe=i(e,"P",{});var Mh=p(Pe);wa=r(Mh,"One thing you should always do when debugging a training session is have a look at the decoded inputs of your model. We can\u2019t make sense of the numbers that we feed it directly, so we should look at what those numbers represent. In computer vision, for example, that means looking at the decoded pictures of the pixels you pass, in speech it means listening to the decoded audio samples, and for our NLP example here it means using our tokenizer to decode the inputs:"),Mh.forEach(s),st=c(e),j($s.$$.fragment,e),ts=c(e),j(ee.$$.fragment,e),at=c(e),ns=i(e,"P",{});var Gh=p(ns);Xt=r(Gh,"So that seems correct. You should do this for all the keys in the inputs:"),Gh.forEach(s),Es=c(e),j(de.$$.fragment,e),Ee=c(e),j(Ts.$$.fragment,e),Cs=c(e),Z=i(e,"P",{});var Za=p(Z);Zt=r(Za,"Note that the keys that don\u2019t correspond to inputs accepted by the model will be automatically discarded, so here we will only keep "),rs=i(Za,"CODE",{});var Bh=p(rs);Vn=r(Bh,"input_ids"),Bh.forEach(s),qs=r(Za,", "),ja=i(Za,"CODE",{});var go=p(ja);Bs=r(go,"attention_mask"),go.forEach(s),_a=r(Za,", and "),le=i(Za,"CODE",{});var Hh=p(le);Jn=r(Hh,"label"),Hh.forEach(s),ls=r(Za," (which will be renamed "),os=i(Za,"CODE",{});var Yh=p(os);Qt=r(Yh,"labels"),Yh.forEach(s),Ue=r(Za,"). To double-check the model signature, you can print the class of your model, then go check its documentation:"),Za.forEach(s),Ae=c(e),j(is.$$.fragment,e),ps=c(e),j(Hs.$$.fragment,e),tt=c(e),ae=i(e,"P",{});var Kn=p(ae);xe=r(Kn,"So in our case, we can check the parameters accepted on "),b=i(Kn,"A",{href:!0,rel:!0});var Rh=p(b);N=r(Rh,"this page"),Rh.forEach(s),hs=r(Kn,". The "),fe=i(Kn,"CODE",{});var Kh=p(fe);us=r(Kh,"Trainer"),Kh.forEach(s),Ys=r(Kn," will also log the columns it\u2019s discarding."),Kn.forEach(s),va=c(e),cs=i(e,"P",{});var Vh=p(cs);ms=r(Vh,"We have checked that the input IDs are correct by decoding them. Next is the "),Le=i(Vh,"CODE",{});var gu=p(Le);Rs=r(gu,"attention_mask"),gu.forEach(s),Ks=r(Vh,":"),Vh.forEach(s),ka=c(e),j(ds.$$.fragment,e),en=c(e),j(Ps.$$.fragment,e),he=c(e),As=i(e,"P",{});var wu=p(As);Xn=r(wu,"Since we didn\u2019t apply padding in our preprocessing, this seems perfectly natural. To be sure there is no issue with that attention mask, let\u2019s check it is the same length as our input IDs:"),wu.forEach(s),sn=c(e),j(De.$$.fragment,e),an=c(e),j(Vs.$$.fragment,e),Te=c(e),Js=i(e,"P",{});var ju=p(Js);Zn=r(ju,"That\u2019s good! Lastly, let\u2019s check our label:"),ju.forEach(s),$a=c(e),j(ue.$$.fragment,e),nt=c(e),j(fs.$$.fragment,e),rt=c(e),Me=i(e,"P",{});var Th=p(Me);lt=r(Th,"Like the input IDs, this is a number that doesn\u2019t really make sense on its own. As we saw before, the map between integers and label names is stored inside the "),ll=i(Th,"CODE",{});var _u=p(ll);Ei=r(_u,"names"),_u.forEach(s),wo=r(Th," attribute of the corresponding "),Ea=i(Th,"EM",{});var vu=p(Ea);jo=r(vu,"feature"),vu.forEach(s),tn=r(Th," of the dataset:"),Th.forEach(s),ol=c(e),j(U.$$.fragment,e),_o=c(e),j(Ta.$$.fragment,e),vo=c(e),Xs=i(e,"P",{});var Ch=p(Xs);il=r(Ch,"So "),pl=i(Ch,"CODE",{});var ku=p(pl);Ti=r(ku,"1"),ku.forEach(s),hl=r(Ch," means "),ul=i(Ch,"CODE",{});var $u=p(ul);Ci=r($u,"neutral"),$u.forEach(s),cl=r(Ch,", which means the two sentences we saw above are not in contradiction, and the first one does not imply the second one. That seems correct!"),Ch.forEach(s),ko=c(e),Qn=i(e,"P",{});var Eu=p(Qn);ml=r(Eu,"We don\u2019t have token type IDs here, since DistilBERT does not expect them; if you have some in your model, you should also make sure that they properly match where the first and second sentences are in the input."),Eu.forEach(s),$o=c(e),j(nn.$$.fragment,e),er=c(e),sr=i(e,"P",{});var Tu=p(sr);qi=r(Tu,"We are only doing the check on the training set here, but you should of course double-check the validation and test sets the same way."),Tu.forEach(s),ar=c(e),tr=i(e,"P",{});var Cu=p(tr);Pi=r(Cu,"Now that we know our datasets look good, it\u2019s time to check the next step of the training pipeline."),Cu.forEach(s),nr=c(e),Ca=i(e,"H3",{class:!0});var Jh=p(Ca);ot=i(Jh,"A",{id:!0,class:!0,href:!0});var qu=p(ot);rn=i(qu,"SPAN",{});var Pu=p(rn);j(ln.$$.fragment,Pu),Pu.forEach(s),qu.forEach(s),Ai=c(Jh),on=i(Jh,"SPAN",{});var Au=p(on);xi=r(Au,"From datasets to dataloaders"),Au.forEach(s),Jh.forEach(s),Eo=c(e),be=i(e,"P",{});var nl=p(be);Di=r(nl,"The next thing that can go wrong in the training pipeline is when the "),dl=i(nl,"CODE",{});var xu=p(dl);fl=r(xu,"Trainer"),xu.forEach(s),zi=r(nl," tries to form batches from the training or validation set. Once you are sure the "),bl=i(nl,"CODE",{});var Du=p(bl);yl=r(Du,"Trainer"),Du.forEach(s),Oi=r(nl,"\u2019s datasets are correct, you can try to manually form a batch by executing the following (replace "),gl=i(nl,"CODE",{});var zu=p(gl);wl=r(zu,"train"),zu.forEach(s),Ii=r(nl," with "),jl=i(nl,"CODE",{});var Ou=p(jl);_l=r(Ou,"eval"),Ou.forEach(s),Si=r(nl," for the validation dataloader):"),nl.forEach(s),To=c(e),j(it.$$.fragment,e),Ge=c(e),rr=i(e,"P",{});var Iu=p(rr);vl=r(Iu,"This code creates the training dataloader, then iterates through it, stopping at the first iteration. If the code executes without error, you have the first training batch that you can inspect, and if the code errors out, you know for sure the problem is in the dataloader, as is the case here:"),Iu.forEach(s),Co=c(e),j(pn.$$.fragment,e),lr=c(e),Zs=i(e,"P",{});var qh=p(Zs);Ni=r(qh,"Inspecting the last frame of the traceback should be enough to give you a clue, but let\u2019s do a bit more digging. Most of the problems during batch creation arise because of the collation of examples into a single batch, so the first thing to check when in doubt is what "),hn=i(qh,"CODE",{});var Su=p(hn);Wi=r(Su,"collate_fn"),Su.forEach(s),Fi=r(qh," your "),or=i(qh,"CODE",{});var Nu=p(or);un=r(Nu,"DataLoader"),Nu.forEach(s),qo=r(qh," is using:"),qh.forEach(s),cn=c(e),j(mn.$$.fragment,e),kl=c(e),j(xs.$$.fragment,e),$l=c(e),se=i(e,"P",{});var _i=p(se);El=r(_i,"So this is the "),Tl=i(_i,"CODE",{});var Wu=p(Tl);Ui=r(Wu,"default_data_collator"),Wu.forEach(s),Cl=r(_i,", but that\u2019s not what we want in this case. We want to pad our examples to the longest sentence in the batch, which is done by the "),ql=i(_i,"CODE",{});var Fu=p(ql);Li=r(Fu,"DataCollatorWithPadding"),Fu.forEach(s),Pl=r(_i," collator. And this data collator is supposed to be used by default by the "),Al=i(_i,"CODE",{});var Uu=p(Al);Mi=r(Uu,"Trainer"),Uu.forEach(s),Po=r(_i,", so why is it not used here?"),_i.forEach(s),pt=c(e),Be=i(e,"P",{});var vi=p(Be);dn=r(vi,"The answer is because we did not pass the "),ir=i(vi,"CODE",{});var Lu=p(ir);pr=r(Lu,"tokenizer"),Lu.forEach(s),Gi=r(vi," to the "),hr=i(vi,"CODE",{});var Mu=p(hr);fn=r(Mu,"Trainer"),Mu.forEach(s),Ao=r(vi,", so it couldn\u2019t create the "),qa=i(vi,"CODE",{});var Gu=p(qa);xo=r(Gu,"DataCollatorWithPadding"),Gu.forEach(s),ur=r(vi," we want. In practice, you should never hesitate to explicitly pass along the data collator you want to use, to make sure you avoid these kinds of errors. Let\u2019s adapt our code to do exactly that:"),vi.forEach(s),Do=c(e),j(ht.$$.fragment,e),ut=c(e),bn=i(e,"P",{});var Bu=p(bn);yn=r(Bu,"The good news? We don\u2019t get the same error as before, which is definitely progress. The bad news? We get an infamous CUDA error instead:"),Bu.forEach(s),xl=c(e),j(ze.$$.fragment,e),zo=c(e),ct=i(e,"P",{});var Hu=p(ct);Bi=r(Hu,"This is bad because CUDA errors are extremely hard to debug in general. We will see in a minute how to solve this, but first let\u2019s finish our analysis of batch creation."),Hu.forEach(s),Oo=c(e),mt=i(e,"P",{});var Yu=p(mt);Hi=r(Yu,"If you are sure your data collator is the right one, you should try to apply it on a couple of samples of your dataset:"),Yu.forEach(s),Io=c(e),j(dt.$$.fragment,e),ft=c(e),Ce=i(e,"P",{});var rl=p(Ce);gn=r(rl,"This code will fail because the "),cr=i(rl,"CODE",{});var Ru=p(cr);bt=r(Ru,"train_dataset"),Ru.forEach(s),Yi=r(rl," contains string columns, which the "),wn=i(rl,"CODE",{});var Ku=p(wn);Ri=r(Ku,"Trainer"),Ku.forEach(s),Ki=r(rl," usually removes. You can remove them manually, or if you want to replicate exactly what the "),mr=i(rl,"CODE",{});var Vu=p(mr);jn=r(Vu,"Trainer"),Vu.forEach(s),So=r(rl," is doing behind the scenes, you can call the private "),Pa=i(rl,"CODE",{});var Ju=p(Pa);No=r(Ju,"Trainer._remove_unused_columns()"),Ju.forEach(s),yt=r(rl," method that does that:"),rl.forEach(s),Wo=c(e),j(Aa.$$.fragment,e),Fo=c(e),dr=i(e,"P",{});var Xu=p(dr);Uo=r(Xu,"You should then be able to manually debug what happens inside the data collator if the error persists."),Xu.forEach(s),Qs=c(e),Ds=i(e,"P",{});var Zu=p(Ds);Dl=r(Zu,"Now that we\u2019ve debugged the batch creation process, it\u2019s time to pass one through the model!"),Zu.forEach(s),gt=c(e),xa=i(e,"H3",{class:!0});var Xh=p(xa);ea=i(Xh,"A",{id:!0,class:!0,href:!0});var Qu=p(ea);zl=i(Qu,"SPAN",{});var ec=p(zl);j(wt.$$.fragment,ec),ec.forEach(s),Qu.forEach(s),sa=c(Xh),Ol=i(Xh,"SPAN",{});var sc=p(Ol);Il=r(sc,"Going through the model"),sc.forEach(s),Xh.forEach(s),Lo=c(e),fr=i(e,"P",{});var ac=p(fr);Sl=r(ac,"You should be able to get a batch by executing the following command:"),ac.forEach(s),Mo=c(e),j(_n.$$.fragment,e),Nl=c(e),He=i(e,"P",{});var Zh=p(He);Vi=r(Zh,"If you\u2019re running this code in a notebook, you may get a CUDA error that\u2019s similar to the one we saw earlier, in which case you need to restart your notebook and reexecute the last snippet without the "),Da=i(Zh,"CODE",{});var tc=p(Da);Ji=r(tc,"trainer.train()"),tc.forEach(s),Xi=r(Zh," line. That\u2019s the second most annoying thing about CUDA errors: they irremediably break your kernel. The most annoying thing about them is the fact that they are hard to debug."),Zh.forEach(s),Wl=c(e),Ye=i(e,"P",{});var nc=p(Ye);Zi=r(nc,"Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don\u2019t know it instantly. It\u2019s only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass."),nc.forEach(s),br=c(e),yr=i(e,"P",{});var rc=p(yr);Qi=r(rc,"So how do we debug those errors? The answer is easy: we don\u2019t. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it."),rc.forEach(s),gr=c(e),jt=i(e,"P",{});var Qh=p(jt);ep=r(Qh,"To do this in our case, we just have to put the model back on the CPU and call it on our batch \u2014 the batch returned by the "),wr=i(Qh,"CODE",{});var lc=p(wr);vn=r(lc,"DataLoader"),lc.forEach(s),Go=r(Qh," has not been moved to the GPU yet:"),Qh.forEach(s),_t=c(e),j(vt.$$.fragment,e),kn=c(e),j($n.$$.fragment,e),Fl=c(e),bs=i(e,"P",{});var eu=p(bs);Bo=r(eu,"So, the picture is getting clearer. Instead of having a CUDA error, we now have an "),za=i(eu,"CODE",{});var oc=p(za);Ho=r(oc,"IndexError"),oc.forEach(s),jr=r(eu," in the loss computation (so nothing to do with the backward pass, as we said earlier). More precisely, we can see that it\u2019s target 2 that creates the error, so this is a very good moment to check the number of labels of our model:"),eu.forEach(s),Yo=c(e),j(kt.$$.fragment,e),aa=c(e),j(ys.$$.fragment,e),_r=c(e),ta=i(e,"P",{});var ic=p(ta);sp=r(ic,"With two labels, only 0s and 1s are allowed as targets, but according to the error message we got a 2. Getting a 2 is actually normal: if we remember the label names we extracted earlier, there were three, so we have indices 0, 1, and 2 in our dataset. The problem is that we didn\u2019t tell that to our model, which should have been created with three labels. So let\u2019s fix that!"),ic.forEach(s),vr=c(e),j(En.$$.fragment,e),Ul=c(e),zs=i(e,"P",{});var su=p(zs);ap=r(su,"We aren\u2019t including the "),kr=i(su,"CODE",{});var pc=p(kr);Oa=r(pc,"trainer.train()"),pc.forEach(s),$t=r(su," line yet, to take the time to check that everything looks good. If we request a batch and pass it to our model, it now works without error!"),su.forEach(s),$r=c(e),j(Os.$$.fragment,e),Ro=c(e),Et=i(e,"P",{});var hc=p(Et);tp=r(hc,"The next step is then to move back to the GPU and check that everything still works:"),hc.forEach(s),Ll=c(e),j(Oe.$$.fragment,e),Ko=c(e),Tt=i(e,"P",{});var uc=p(Tt);np=r(uc,"If you still get an error, make sure you restart your notebook and only execute the last version of the script."),uc.forEach(s),Vo=c(e),Is=i(e,"H3",{class:!0});var au=p(Is);Ct=i(au,"A",{id:!0,class:!0,href:!0});var cc=p(Ct);Ml=i(cc,"SPAN",{});var mc=p(Ml);j(qt.$$.fragment,mc),mc.forEach(s),cc.forEach(s),Tn=c(au),Er=i(au,"SPAN",{});var dc=p(Er);Ia=r(dc,"Performing one optimization step"),dc.forEach(s),au.forEach(s),Sa=c(e),Pt=i(e,"P",{});var fc=p(Pt);Cn=r(fc,"Now that we know that we can build batches that actually go through the model, we are ready for the next step of the training pipeline: computing the gradients and performing an optimization step."),fc.forEach(s),Jo=c(e),na=i(e,"P",{});var tu=p(na);rp=r(tu,"The first part is just a matter of calling the "),Tr=i(tu,"CODE",{});var bc=p(Tr);ra=r(bc,"backward()"),bc.forEach(s),lp=r(tu," method on the loss:"),tu.forEach(s),Cr=c(e),j(qn.$$.fragment,e),Xo=c(e),At=i(e,"P",{});var yc=p(At);op=r(yc,"It\u2019s pretty rare to get an error at this stage, but if you do get one, make sure to go back to the CPU to get a helpful error message."),yc.forEach(s),Zo=c(e),Ss=i(e,"P",{});var Ph=p(Ss);qr=r(Ph,"To perform the optimization step, we just need to create the "),Gl=i(Ph,"CODE",{});var gc=p(Gl);Qo=r(gc,"optimizer"),gc.forEach(s),xt=r(Ph," and call its "),Bl=i(Ph,"CODE",{});var wc=p(Bl);Hl=r(wc,"step()"),wc.forEach(s),ip=r(Ph," method:"),Ph.forEach(s),ei=c(e),j(Dt.$$.fragment,e),la=c(e),Re=i(e,"P",{});var nu=p(Re);Yl=r(nu,"Again, if you\u2019re using the default optimizer in the "),Na=i(nu,"CODE",{});var jc=p(Na);pp=r(jc,"Trainer"),jc.forEach(s),Rl=r(nu,", you shouldn\u2019t get an error at this stage, but if you have a custom optimizer, there might be some problems to debug here. Don\u2019t forget to go back to the CPU if you get a weird CUDA error at this stage. Speaking of CUDA errors, earlier we mentioned a special case. Let\u2019s have a look at that now."),nu.forEach(s),si=c(e),oa=i(e,"H3",{class:!0});var ru=p(oa);te=i(ru,"A",{id:!0,class:!0,href:!0});var _c=p(te);Kl=i(_c,"SPAN",{});var vc=p(Kl);j(Wa.$$.fragment,vc),vc.forEach(s),_c.forEach(s),hp=c(ru),Vl=i(ru,"SPAN",{});var kc=p(Vl);Jl=r(kc,"Dealing with CUDA out-of-memory errors"),kc.forEach(s),ru.forEach(s),ai=c(e),zt=i(e,"P",{});var lu=p(zt);Xl=r(lu,"Whenever you get an error message that starts with "),Zl=i(lu,"CODE",{});var $c=p(Zl);up=r($c,"RuntimeError: CUDA out of memory"),$c.forEach(s),Ql=r(lu,", this indicates that you are out of GPU memory. This is not directly linked to your code, and it can happen with a script that runs perfectly fine. This error means that you tried to put too many things in the internal memory of your GPU, and that resulted in an error. Like with other CUDA errors, you will need to restart your kernel to be in a spot where you can run your training again."),lu.forEach(s),ti=c(e),Pr=i(e,"P",{});var Ec=p(Pr);ni=r(Ec,"To solve this issue, you just need to use less GPU space \u2014 something that is often easier said than done. First, make sure you don\u2019t have two models on the GPU at the same time (unless that\u2019s required for your problem, of course). Then, you should probably reduce your batch size, as it directly affects the sizes of all the intermediate outputs of the model and their gradients. If the problem persists, consider using a smaller version of your model."),Ec.forEach(s),Ot=c(e),j(It.$$.fragment,e),Pn=c(e),Fa=i(e,"H3",{class:!0});var ou=p(Fa);Ua=i(ou,"A",{id:!0,class:!0,href:!0});var Tc=p(Ua);La=i(Tc,"SPAN",{});var Cc=p(La);j(St.$$.fragment,Cc),Cc.forEach(s),Tc.forEach(s),Ar=c(ou),eo=i(ou,"SPAN",{});var qc=p(eo);ri=r(qc,"Evaluating the model"),qc.forEach(s),ou.forEach(s),Ke=c(e),ia=i(e,"P",{});var iu=p(ia);cp=r(iu,"Now that we\u2019ve solved all the issues with our code, everything is perfect and the training should run smoothly, right? Not so fast! If you run the "),so=i(iu,"CODE",{});var Pc=p(so);ao=r(Pc,"trainer.train()"),Pc.forEach(s),mp=r(iu," command, everything will look good at first, but after a while you will get the following:"),iu.forEach(s),li=c(e),j(Ma.$$.fragment,e),oi=c(e),j(An.$$.fragment,e),xr=c(e),Dr=i(e,"P",{});var Ac=p(Dr);ii=r(Ac,"You will realize this error appears during the evaluation phase, so this is the last thing we will need to debug."),Ac.forEach(s),xn=c(e),Nt=i(e,"P",{});var pu=p(Nt);pi=r(pu,"You can run the evaluation loop of the "),Wt=i(pu,"CODE",{});var xc=p(Wt);dp=r(xc,"Trainer"),xc.forEach(s),hi=r(pu," independently form the training like this:"),pu.forEach(s),Dn=c(e),j(zn.$$.fragment,e),to=c(e),j(Ve.$$.fragment,e),Ga=c(e),j(Ba.$$.fragment,e),Ft=c(e),zr=i(e,"P",{});var Dc=p(zr);no=r(Dc,"Before attempting to debug a problem in the evaluation loop, you should first make sure that you\u2019ve had a look at the data, are able to form a batch properly, and can run your model on it. We\u2019ve completed all of those steps, so the following code can be executed without error:"),Dc.forEach(s),ui=c(e),j(Ut.$$.fragment,e),On=c(e),Or=i(e,"P",{});var zc=p(Or);ci=r(zc,"The error comes later, at the end of the evaluation phase, and if we look at the traceback we see this:"),zc.forEach(s),Ns=c(e),j(In.$$.fragment,e),Ir=c(e),pa=i(e,"P",{});var Ah=p(pa);fp=r(Ah,"This tells us that the error originates in the "),Sn=i(Ah,"CODE",{});var Oc=p(Sn);bp=r(Oc,"datasets/metric.py"),Oc.forEach(s),yp=r(Ah," module \u2014 so this is a problem with our "),Sr=i(Ah,"CODE",{});var Ic=p(Sr);Nn=r(Ic,"compute_metrics()"),Ic.forEach(s),mi=r(Ah," function. It takes a tuple with the logits and the labels as NumPy arrays, so let\u2019s try to feed it that:"),Ah.forEach(s),Lt=c(e),j(Mt.$$.fragment,e),Ha=c(e),j(Wn.$$.fragment,e),Nr=c(e),gs=i(e,"P",{});var ki=p(gs);gp=r(ki,"We get the same error, so the problem definitely lies with that function. If we look back at its code, we see it\u2019s just forwarding the "),Wr=i(ki,"CODE",{});var Sc=p(Wr);Fr=r(Sc,"predictions"),Sc.forEach(s),wp=r(ki," and the "),Ur=i(ki,"CODE",{});var Nc=p(Ur);Fn=r(Nc,"labels"),Nc.forEach(s),di=r(ki," to "),t=i(ki,"CODE",{});var Wc=p(t);f=r(Wc,"metric.compute()"),Wc.forEach(s),Vp=r(ki,". So is there a problem with that method? Not really. Let\u2019s have a quick look at the shapes:"),ki.forEach(s),jp=c(e),j(Lr.$$.fragment,e),ro=c(e),j(Mr.$$.fragment,e),_p=c(e),Un=i(e,"P",{});var hu=p(Un);Jp=r(hu,"Our predictions are still logits, not the actual predictions, which is why the metric is returning this (somewhat obscure) error. The fix is pretty easy; we just have to add an argmax in the "),ye=i(hu,"CODE",{});var Fc=p(ye);Xp=r(Fc,"compute_metrics()"),Fc.forEach(s),Zp=r(hu," function:"),hu.forEach(s),vp=c(e),j(Gr.$$.fragment,e),kp=c(e),j(Br.$$.fragment,e),Gt=c(e),lo=i(e,"P",{});var Uc=p(lo);Qp=r(Uc,"Now our error is fixed! This was the last one, so our script will now train a model properly."),Uc.forEach(s),$p=c(e),Ws=i(e,"P",{});var Lc=p(Ws);eh=r(Lc,"For reference, here is the completely fixed script:"),Lc.forEach(s),Ep=c(e),j(Hr.$$.fragment,e),oo=c(e),io=i(e,"P",{});var Mc=p(io);Yr=r(Mc,"In this instance, there are no more problems, and our script will fine-tune a model that should give reasonable results. But what can we do when the training proceeds without any error, and the model trained does not perform well at all? That\u2019s the hardest part of machine learning, and we\u2019ll show you a few techniques that can help."),Mc.forEach(s),Tp=c(e),j(Rr.$$.fragment,e),Cp=c(e),ha=i(e,"H2",{class:!0});var uu=p(ha);Ln=i(uu,"A",{id:!0,class:!0,href:!0});var Gc=p(Ln);fi=i(Gc,"SPAN",{});var Bc=p(fi);j(Mn.$$.fragment,Bc),Bc.forEach(s),Gc.forEach(s),sh=c(uu),Kr=i(uu,"SPAN",{});var Hc=p(Kr);ah=r(Hc,"Debugging silent errors during training"),Hc.forEach(s),uu.forEach(s),qp=c(e),po=i(e,"P",{});var Yc=p(po);Gn=r(Yc,"What can we do to debug a training that completes without error but doesn\u2019t get good results? We\u2019ll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer."),Yc.forEach(s),Pp=c(e),Bt=i(e,"H3",{class:!0});var cu=p(Bt);Bn=i(cu,"A",{id:!0,class:!0,href:!0});var Rc=p(Bn);Ht=i(Rc,"SPAN",{});var Kc=p(Ht);j(Vr.$$.fragment,Kc),Kc.forEach(s),Rc.forEach(s),th=c(cu),G=i(cu,"SPAN",{});var Vc=p(G);nh=r(Vc,"Check your data (again!)"),Vc.forEach(s),cu.forEach(s),Ap=c(e),ho=i(e,"P",{});var Jc=p(ho);rh=r(Jc,"Your model will only learn something if it\u2019s actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are attributed randomly, it\u2019s very likely you won\u2019t get any model training on your dataset. So always start by double-checking your decoded inputs and labels, and ask yourself the following questions:"),Jc.forEach(s),xp=c(e),Fs=i(e,"UL",{});var $i=p(Fs);bi=i($i,"LI",{});var Xc=p(bi);lh=r(Xc,"Is the decoded data understandable?"),Xc.forEach(s),oh=c($i),yi=i($i,"LI",{});var Zc=p(yi);ih=r(Zc,"Do you agree with the labels?"),Zc.forEach(s),ph=c($i),gi=i($i,"LI",{});var Qc=p(gi);hh=r(Qc,"Is there one label that\u2019s more common than the others?"),Qc.forEach(s),uh=c($i),ua=i($i,"LI",{});var em=p(ua);ch=r(em,"What should the loss/metric be if the model predicted a random answer/always the same answer?"),em.forEach(s),$i.forEach(s),Dp=c(e),j(Jr.$$.fragment,e),zp=c(e),ca=i(e,"P",{});var sm=p(ca);mh=r(sm,"After looking at your data, go through a few of the model\u2019s predictions and decode them too. If the model is always predicting the same thing, it might be because your dataset is biased toward one category (for classification problems); techniques like oversampling rare classes might help."),sm.forEach(s),Op=c(e),uo=i(e,"P",{});var am=p(uo);dh=r(am,"If the loss/metric you get on your initial model is very different from the loss/metric you would expect for random predictions, double-check the way your loss or metric is computed, as there is probably a bug there. If you are using several losses that you add at the end, make sure they are of the same scale."),am.forEach(s),Ip=c(e),Ya=i(e,"P",{});var tm=p(Ya);fh=r(tm,"When you are sure your data is perfect, you can see if the model is capable of training on it with one simple test."),tm.forEach(s),Sp=c(e),ma=i(e,"H3",{class:!0});var mu=p(ma);Hn=i(mu,"A",{id:!0,class:!0,href:!0});var nm=p(Hn);Xr=i(nm,"SPAN",{});var rm=p(Xr);j(Zr.$$.fragment,rm),rm.forEach(s),nm.forEach(s),wi=c(mu),ji=i(mu,"SPAN",{});var lm=p(ji);bh=r(lm,"Overfit your model on one batch"),lm.forEach(s),mu.forEach(s),Np=c(e),Ra=i(e,"P",{});var om=p(Ra);yh=r(om,"Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high."),om.forEach(s),Wp=c(e),Ka=i(e,"P",{});var du=p(Ka);gh=r(du,"Doing this once you have defined your "),Yt=i(du,"CODE",{});var im=p(Yt);wh=r(im,"Trainer"),im.forEach(s),jh=r(du," is really easy; just grab a batch of training data, then run a small manual training loop only using that batch for something like 20 steps:"),du.forEach(s),Fp=c(e),j(Qr.$$.fragment,e),co=c(e),j(el.$$.fragment,e),Up=c(e),Yn=i(e,"P",{});var fu=p(Yn);_h=r(fu,"The resulting model should have close-to-perfect results on the same "),sl=i(fu,"CODE",{});var pm=p(sl);vh=r(pm,"batch"),pm.forEach(s),kh=r(fu,". Let\u2019s compute the metric on the resulting predictions:"),fu.forEach(s),Lp=c(e),j(da.$$.fragment,e),Mp=c(e),j(al.$$.fragment,e),mo=c(e),fo=i(e,"P",{});var hm=p(fo);$h=r(hm,"100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on any other sentence, it will very likely give you a wrong answer)!"),hm.forEach(s),Gp=c(e),Va=i(e,"P",{});var um=p(Va);Eh=r(um,"If you don\u2019t manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something."),um.forEach(s),Bp=c(e),j(tl.$$.fragment,e),this.h()},h(){$(D,"id","check-your-data"),$(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(D,"href","#check-your-data"),$(O,"class","relative group"),$(b,"href","https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification"),$(b,"rel","nofollow"),$(ot,"id","from-datasets-to-dataloaders"),$(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ot,"href","#from-datasets-to-dataloaders"),$(Ca,"class","relative group"),$(ea,"id","going-through-the-model"),$(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ea,"href","#going-through-the-model"),$(xa,"class","relative group"),$(Ct,"id","performing-one-optimization-step"),$(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ct,"href","#performing-one-optimization-step"),$(Is,"class","relative group"),$(te,"id","dealing-with-cuda-outofmemory-errors"),$(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(te,"href","#dealing-with-cuda-outofmemory-errors"),$(oa,"class","relative group"),$(Ua,"id","evaluating-the-model"),$(Ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ua,"href","#evaluating-the-model"),$(Fa,"class","relative group"),$(Ln,"id","debugging-silent-errors-during-training"),$(Ln,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ln,"href","#debugging-silent-errors-during-training"),$(ha,"class","relative group"),$(Bn,"id","check-your-data-again"),$(Bn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Bn,"href","#check-your-data-again"),$(Bt,"class","relative group"),$(Hn,"id","overfit-your-model-on-one-batch"),$(Hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Hn,"href","#overfit-your-model-on-one-batch"),$(ma,"class","relative group")},m(e,h){_(m,e,h),l(e,k,h),l(e,d,h),a(d,E),l(e,z,h),_(C,e,h),l(e,q,h),l(e,O,h),a(O,D),a(D,A),_(W,A,null),a(O,T),a(O,x),a(x,L),l(e,F,h),l(e,I,h),a(I,R),a(I,Y),a(Y,Q),a(I,J),l(e,Ie,h),l(e,V,h),a(V,ws),a(V,H),a(H,M),a(V,ne),l(e,ge,h),_(B,e,h),l(e,me,h),_(re,e,h),l(e,we,h),l(e,oe,h),a(oe,fa),a(oe,Us),a(Us,je),a(oe,ie),a(oe,Je),a(Je,Se),a(oe,qe),l(e,Ls,h),l(e,X,h),a(X,ba),a(X,js),a(js,Kt),a(X,Ms),a(X,Xe),a(Xe,Ne),a(X,We),a(X,_e),a(_e,Ze),a(X,ve),a(X,Fe),a(Fe,Vt),a(X,Qa),l(e,ke,h),_(_s,e,h),l(e,Qe,h),l(e,es,h),a(es,Jt),l(e,ya,h),_($e,e,h),l(e,ss,h),l(e,as,h),a(as,K),l(e,vs,h),_(pe,e,h),l(e,et,h),l(e,Gs,h),a(Gs,ga),l(e,ks,h),l(e,Pe,h),a(Pe,wa),l(e,st,h),_($s,e,h),l(e,ts,h),_(ee,e,h),l(e,at,h),l(e,ns,h),a(ns,Xt),l(e,Es,h),_(de,e,h),l(e,Ee,h),_(Ts,e,h),l(e,Cs,h),l(e,Z,h),a(Z,Zt),a(Z,rs),a(rs,Vn),a(Z,qs),a(Z,ja),a(ja,Bs),a(Z,_a),a(Z,le),a(le,Jn),a(Z,ls),a(Z,os),a(os,Qt),a(Z,Ue),l(e,Ae,h),_(is,e,h),l(e,ps,h),_(Hs,e,h),l(e,tt,h),l(e,ae,h),a(ae,xe),a(ae,b),a(b,N),a(ae,hs),a(ae,fe),a(fe,us),a(ae,Ys),l(e,va,h),l(e,cs,h),a(cs,ms),a(cs,Le),a(Le,Rs),a(cs,Ks),l(e,ka,h),_(ds,e,h),l(e,en,h),_(Ps,e,h),l(e,he,h),l(e,As,h),a(As,Xn),l(e,sn,h),_(De,e,h),l(e,an,h),_(Vs,e,h),l(e,Te,h),l(e,Js,h),a(Js,Zn),l(e,$a,h),_(ue,e,h),l(e,nt,h),_(fs,e,h),l(e,rt,h),l(e,Me,h),a(Me,lt),a(Me,ll),a(ll,Ei),a(Me,wo),a(Me,Ea),a(Ea,jo),a(Me,tn),l(e,ol,h),_(U,e,h),l(e,_o,h),_(Ta,e,h),l(e,vo,h),l(e,Xs,h),a(Xs,il),a(Xs,pl),a(pl,Ti),a(Xs,hl),a(Xs,ul),a(ul,Ci),a(Xs,cl),l(e,ko,h),l(e,Qn,h),a(Qn,ml),l(e,$o,h),_(nn,e,h),l(e,er,h),l(e,sr,h),a(sr,qi),l(e,ar,h),l(e,tr,h),a(tr,Pi),l(e,nr,h),l(e,Ca,h),a(Ca,ot),a(ot,rn),_(ln,rn,null),a(Ca,Ai),a(Ca,on),a(on,xi),l(e,Eo,h),l(e,be,h),a(be,Di),a(be,dl),a(dl,fl),a(be,zi),a(be,bl),a(bl,yl),a(be,Oi),a(be,gl),a(gl,wl),a(be,Ii),a(be,jl),a(jl,_l),a(be,Si),l(e,To,h),_(it,e,h),l(e,Ge,h),l(e,rr,h),a(rr,vl),l(e,Co,h),_(pn,e,h),l(e,lr,h),l(e,Zs,h),a(Zs,Ni),a(Zs,hn),a(hn,Wi),a(Zs,Fi),a(Zs,or),a(or,un),a(Zs,qo),l(e,cn,h),_(mn,e,h),l(e,kl,h),_(xs,e,h),l(e,$l,h),l(e,se,h),a(se,El),a(se,Tl),a(Tl,Ui),a(se,Cl),a(se,ql),a(ql,Li),a(se,Pl),a(se,Al),a(Al,Mi),a(se,Po),l(e,pt,h),l(e,Be,h),a(Be,dn),a(Be,ir),a(ir,pr),a(Be,Gi),a(Be,hr),a(hr,fn),a(Be,Ao),a(Be,qa),a(qa,xo),a(Be,ur),l(e,Do,h),_(ht,e,h),l(e,ut,h),l(e,bn,h),a(bn,yn),l(e,xl,h),_(ze,e,h),l(e,zo,h),l(e,ct,h),a(ct,Bi),l(e,Oo,h),l(e,mt,h),a(mt,Hi),l(e,Io,h),_(dt,e,h),l(e,ft,h),l(e,Ce,h),a(Ce,gn),a(Ce,cr),a(cr,bt),a(Ce,Yi),a(Ce,wn),a(wn,Ri),a(Ce,Ki),a(Ce,mr),a(mr,jn),a(Ce,So),a(Ce,Pa),a(Pa,No),a(Ce,yt),l(e,Wo,h),_(Aa,e,h),l(e,Fo,h),l(e,dr,h),a(dr,Uo),l(e,Qs,h),l(e,Ds,h),a(Ds,Dl),l(e,gt,h),l(e,xa,h),a(xa,ea),a(ea,zl),_(wt,zl,null),a(xa,sa),a(xa,Ol),a(Ol,Il),l(e,Lo,h),l(e,fr,h),a(fr,Sl),l(e,Mo,h),_(_n,e,h),l(e,Nl,h),l(e,He,h),a(He,Vi),a(He,Da),a(Da,Ji),a(He,Xi),l(e,Wl,h),l(e,Ye,h),a(Ye,Zi),l(e,br,h),l(e,yr,h),a(yr,Qi),l(e,gr,h),l(e,jt,h),a(jt,ep),a(jt,wr),a(wr,vn),a(jt,Go),l(e,_t,h),_(vt,e,h),l(e,kn,h),_($n,e,h),l(e,Fl,h),l(e,bs,h),a(bs,Bo),a(bs,za),a(za,Ho),a(bs,jr),l(e,Yo,h),_(kt,e,h),l(e,aa,h),_(ys,e,h),l(e,_r,h),l(e,ta,h),a(ta,sp),l(e,vr,h),_(En,e,h),l(e,Ul,h),l(e,zs,h),a(zs,ap),a(zs,kr),a(kr,Oa),a(zs,$t),l(e,$r,h),_(Os,e,h),l(e,Ro,h),l(e,Et,h),a(Et,tp),l(e,Ll,h),_(Oe,e,h),l(e,Ko,h),l(e,Tt,h),a(Tt,np),l(e,Vo,h),l(e,Is,h),a(Is,Ct),a(Ct,Ml),_(qt,Ml,null),a(Is,Tn),a(Is,Er),a(Er,Ia),l(e,Sa,h),l(e,Pt,h),a(Pt,Cn),l(e,Jo,h),l(e,na,h),a(na,rp),a(na,Tr),a(Tr,ra),a(na,lp),l(e,Cr,h),_(qn,e,h),l(e,Xo,h),l(e,At,h),a(At,op),l(e,Zo,h),l(e,Ss,h),a(Ss,qr),a(Ss,Gl),a(Gl,Qo),a(Ss,xt),a(Ss,Bl),a(Bl,Hl),a(Ss,ip),l(e,ei,h),_(Dt,e,h),l(e,la,h),l(e,Re,h),a(Re,Yl),a(Re,Na),a(Na,pp),a(Re,Rl),l(e,si,h),l(e,oa,h),a(oa,te),a(te,Kl),_(Wa,Kl,null),a(oa,hp),a(oa,Vl),a(Vl,Jl),l(e,ai,h),l(e,zt,h),a(zt,Xl),a(zt,Zl),a(Zl,up),a(zt,Ql),l(e,ti,h),l(e,Pr,h),a(Pr,ni),l(e,Ot,h),_(It,e,h),l(e,Pn,h),l(e,Fa,h),a(Fa,Ua),a(Ua,La),_(St,La,null),a(Fa,Ar),a(Fa,eo),a(eo,ri),l(e,Ke,h),l(e,ia,h),a(ia,cp),a(ia,so),a(so,ao),a(ia,mp),l(e,li,h),_(Ma,e,h),l(e,oi,h),_(An,e,h),l(e,xr,h),l(e,Dr,h),a(Dr,ii),l(e,xn,h),l(e,Nt,h),a(Nt,pi),a(Nt,Wt),a(Wt,dp),a(Nt,hi),l(e,Dn,h),_(zn,e,h),l(e,to,h),_(Ve,e,h),l(e,Ga,h),_(Ba,e,h),l(e,Ft,h),l(e,zr,h),a(zr,no),l(e,ui,h),_(Ut,e,h),l(e,On,h),l(e,Or,h),a(Or,ci),l(e,Ns,h),_(In,e,h),l(e,Ir,h),l(e,pa,h),a(pa,fp),a(pa,Sn),a(Sn,bp),a(pa,yp),a(pa,Sr),a(Sr,Nn),a(pa,mi),l(e,Lt,h),_(Mt,e,h),l(e,Ha,h),_(Wn,e,h),l(e,Nr,h),l(e,gs,h),a(gs,gp),a(gs,Wr),a(Wr,Fr),a(gs,wp),a(gs,Ur),a(Ur,Fn),a(gs,di),a(gs,t),a(t,f),a(gs,Vp),l(e,jp,h),_(Lr,e,h),l(e,ro,h),_(Mr,e,h),l(e,_p,h),l(e,Un,h),a(Un,Jp),a(Un,ye),a(ye,Xp),a(Un,Zp),l(e,vp,h),_(Gr,e,h),l(e,kp,h),_(Br,e,h),l(e,Gt,h),l(e,lo,h),a(lo,Qp),l(e,$p,h),l(e,Ws,h),a(Ws,eh),l(e,Ep,h),_(Hr,e,h),l(e,oo,h),l(e,io,h),a(io,Yr),l(e,Tp,h),_(Rr,e,h),l(e,Cp,h),l(e,ha,h),a(ha,Ln),a(Ln,fi),_(Mn,fi,null),a(ha,sh),a(ha,Kr),a(Kr,ah),l(e,qp,h),l(e,po,h),a(po,Gn),l(e,Pp,h),l(e,Bt,h),a(Bt,Bn),a(Bn,Ht),_(Vr,Ht,null),a(Bt,th),a(Bt,G),a(G,nh),l(e,Ap,h),l(e,ho,h),a(ho,rh),l(e,xp,h),l(e,Fs,h),a(Fs,bi),a(bi,lh),a(Fs,oh),a(Fs,yi),a(yi,ih),a(Fs,ph),a(Fs,gi),a(gi,hh),a(Fs,uh),a(Fs,ua),a(ua,ch),l(e,Dp,h),_(Jr,e,h),l(e,zp,h),l(e,ca,h),a(ca,mh),l(e,Op,h),l(e,uo,h),a(uo,dh),l(e,Ip,h),l(e,Ya,h),a(Ya,fh),l(e,Sp,h),l(e,ma,h),a(ma,Hn),a(Hn,Xr),_(Zr,Xr,null),a(ma,wi),a(ma,ji),a(ji,bh),l(e,Np,h),l(e,Ra,h),a(Ra,yh),l(e,Wp,h),l(e,Ka,h),a(Ka,gh),a(Ka,Yt),a(Yt,wh),a(Ka,jh),l(e,Fp,h),_(Qr,e,h),l(e,co,h),_(el,e,h),l(e,Up,h),l(e,Yn,h),a(Yn,_h),a(Yn,sl),a(sl,vh),a(Yn,kh),l(e,Lp,h),_(da,e,h),l(e,Mp,h),_(al,e,h),l(e,mo,h),l(e,fo,h),a(fo,$h),l(e,Gp,h),l(e,Va,h),a(Va,Eh),l(e,Bp,h),_(tl,e,h),bo=!0},i(e){bo||(g(m.$$.fragment,e),g(C.$$.fragment,e),g(W.$$.fragment,e),g(B.$$.fragment,e),g(re.$$.fragment,e),g(_s.$$.fragment,e),g($e.$$.fragment,e),g(pe.$$.fragment,e),g($s.$$.fragment,e),g(ee.$$.fragment,e),g(de.$$.fragment,e),g(Ts.$$.fragment,e),g(is.$$.fragment,e),g(Hs.$$.fragment,e),g(ds.$$.fragment,e),g(Ps.$$.fragment,e),g(De.$$.fragment,e),g(Vs.$$.fragment,e),g(ue.$$.fragment,e),g(fs.$$.fragment,e),g(U.$$.fragment,e),g(Ta.$$.fragment,e),g(nn.$$.fragment,e),g(ln.$$.fragment,e),g(it.$$.fragment,e),g(pn.$$.fragment,e),g(mn.$$.fragment,e),g(xs.$$.fragment,e),g(ht.$$.fragment,e),g(ze.$$.fragment,e),g(dt.$$.fragment,e),g(Aa.$$.fragment,e),g(wt.$$.fragment,e),g(_n.$$.fragment,e),g(vt.$$.fragment,e),g($n.$$.fragment,e),g(kt.$$.fragment,e),g(ys.$$.fragment,e),g(En.$$.fragment,e),g(Os.$$.fragment,e),g(Oe.$$.fragment,e),g(qt.$$.fragment,e),g(qn.$$.fragment,e),g(Dt.$$.fragment,e),g(Wa.$$.fragment,e),g(It.$$.fragment,e),g(St.$$.fragment,e),g(Ma.$$.fragment,e),g(An.$$.fragment,e),g(zn.$$.fragment,e),g(Ve.$$.fragment,e),g(Ba.$$.fragment,e),g(Ut.$$.fragment,e),g(In.$$.fragment,e),g(Mt.$$.fragment,e),g(Wn.$$.fragment,e),g(Lr.$$.fragment,e),g(Mr.$$.fragment,e),g(Gr.$$.fragment,e),g(Br.$$.fragment,e),g(Hr.$$.fragment,e),g(Rr.$$.fragment,e),g(Mn.$$.fragment,e),g(Vr.$$.fragment,e),g(Jr.$$.fragment,e),g(Zr.$$.fragment,e),g(Qr.$$.fragment,e),g(el.$$.fragment,e),g(da.$$.fragment,e),g(al.$$.fragment,e),g(tl.$$.fragment,e),bo=!0)},o(e){y(m.$$.fragment,e),y(C.$$.fragment,e),y(W.$$.fragment,e),y(B.$$.fragment,e),y(re.$$.fragment,e),y(_s.$$.fragment,e),y($e.$$.fragment,e),y(pe.$$.fragment,e),y($s.$$.fragment,e),y(ee.$$.fragment,e),y(de.$$.fragment,e),y(Ts.$$.fragment,e),y(is.$$.fragment,e),y(Hs.$$.fragment,e),y(ds.$$.fragment,e),y(Ps.$$.fragment,e),y(De.$$.fragment,e),y(Vs.$$.fragment,e),y(ue.$$.fragment,e),y(fs.$$.fragment,e),y(U.$$.fragment,e),y(Ta.$$.fragment,e),y(nn.$$.fragment,e),y(ln.$$.fragment,e),y(it.$$.fragment,e),y(pn.$$.fragment,e),y(mn.$$.fragment,e),y(xs.$$.fragment,e),y(ht.$$.fragment,e),y(ze.$$.fragment,e),y(dt.$$.fragment,e),y(Aa.$$.fragment,e),y(wt.$$.fragment,e),y(_n.$$.fragment,e),y(vt.$$.fragment,e),y($n.$$.fragment,e),y(kt.$$.fragment,e),y(ys.$$.fragment,e),y(En.$$.fragment,e),y(Os.$$.fragment,e),y(Oe.$$.fragment,e),y(qt.$$.fragment,e),y(qn.$$.fragment,e),y(Dt.$$.fragment,e),y(Wa.$$.fragment,e),y(It.$$.fragment,e),y(St.$$.fragment,e),y(Ma.$$.fragment,e),y(An.$$.fragment,e),y(zn.$$.fragment,e),y(Ve.$$.fragment,e),y(Ba.$$.fragment,e),y(Ut.$$.fragment,e),y(In.$$.fragment,e),y(Mt.$$.fragment,e),y(Wn.$$.fragment,e),y(Lr.$$.fragment,e),y(Mr.$$.fragment,e),y(Gr.$$.fragment,e),y(Br.$$.fragment,e),y(Hr.$$.fragment,e),y(Rr.$$.fragment,e),y(Mn.$$.fragment,e),y(Vr.$$.fragment,e),y(Jr.$$.fragment,e),y(Zr.$$.fragment,e),y(Qr.$$.fragment,e),y(el.$$.fragment,e),y(da.$$.fragment,e),y(al.$$.fragment,e),y(tl.$$.fragment,e),bo=!1},d(e){v(m,e),e&&s(k),e&&s(d),e&&s(z),v(C,e),e&&s(q),e&&s(O),v(W),e&&s(F),e&&s(I),e&&s(Ie),e&&s(V),e&&s(ge),v(B,e),e&&s(me),v(re,e),e&&s(we),e&&s(oe),e&&s(Ls),e&&s(X),e&&s(ke),v(_s,e),e&&s(Qe),e&&s(es),e&&s(ya),v($e,e),e&&s(ss),e&&s(as),e&&s(vs),v(pe,e),e&&s(et),e&&s(Gs),e&&s(ks),e&&s(Pe),e&&s(st),v($s,e),e&&s(ts),v(ee,e),e&&s(at),e&&s(ns),e&&s(Es),v(de,e),e&&s(Ee),v(Ts,e),e&&s(Cs),e&&s(Z),e&&s(Ae),v(is,e),e&&s(ps),v(Hs,e),e&&s(tt),e&&s(ae),e&&s(va),e&&s(cs),e&&s(ka),v(ds,e),e&&s(en),v(Ps,e),e&&s(he),e&&s(As),e&&s(sn),v(De,e),e&&s(an),v(Vs,e),e&&s(Te),e&&s(Js),e&&s($a),v(ue,e),e&&s(nt),v(fs,e),e&&s(rt),e&&s(Me),e&&s(ol),v(U,e),e&&s(_o),v(Ta,e),e&&s(vo),e&&s(Xs),e&&s(ko),e&&s(Qn),e&&s($o),v(nn,e),e&&s(er),e&&s(sr),e&&s(ar),e&&s(tr),e&&s(nr),e&&s(Ca),v(ln),e&&s(Eo),e&&s(be),e&&s(To),v(it,e),e&&s(Ge),e&&s(rr),e&&s(Co),v(pn,e),e&&s(lr),e&&s(Zs),e&&s(cn),v(mn,e),e&&s(kl),v(xs,e),e&&s($l),e&&s(se),e&&s(pt),e&&s(Be),e&&s(Do),v(ht,e),e&&s(ut),e&&s(bn),e&&s(xl),v(ze,e),e&&s(zo),e&&s(ct),e&&s(Oo),e&&s(mt),e&&s(Io),v(dt,e),e&&s(ft),e&&s(Ce),e&&s(Wo),v(Aa,e),e&&s(Fo),e&&s(dr),e&&s(Qs),e&&s(Ds),e&&s(gt),e&&s(xa),v(wt),e&&s(Lo),e&&s(fr),e&&s(Mo),v(_n,e),e&&s(Nl),e&&s(He),e&&s(Wl),e&&s(Ye),e&&s(br),e&&s(yr),e&&s(gr),e&&s(jt),e&&s(_t),v(vt,e),e&&s(kn),v($n,e),e&&s(Fl),e&&s(bs),e&&s(Yo),v(kt,e),e&&s(aa),v(ys,e),e&&s(_r),e&&s(ta),e&&s(vr),v(En,e),e&&s(Ul),e&&s(zs),e&&s($r),v(Os,e),e&&s(Ro),e&&s(Et),e&&s(Ll),v(Oe,e),e&&s(Ko),e&&s(Tt),e&&s(Vo),e&&s(Is),v(qt),e&&s(Sa),e&&s(Pt),e&&s(Jo),e&&s(na),e&&s(Cr),v(qn,e),e&&s(Xo),e&&s(At),e&&s(Zo),e&&s(Ss),e&&s(ei),v(Dt,e),e&&s(la),e&&s(Re),e&&s(si),e&&s(oa),v(Wa),e&&s(ai),e&&s(zt),e&&s(ti),e&&s(Pr),e&&s(Ot),v(It,e),e&&s(Pn),e&&s(Fa),v(St),e&&s(Ke),e&&s(ia),e&&s(li),v(Ma,e),e&&s(oi),v(An,e),e&&s(xr),e&&s(Dr),e&&s(xn),e&&s(Nt),e&&s(Dn),v(zn,e),e&&s(to),v(Ve,e),e&&s(Ga),v(Ba,e),e&&s(Ft),e&&s(zr),e&&s(ui),v(Ut,e),e&&s(On),e&&s(Or),e&&s(Ns),v(In,e),e&&s(Ir),e&&s(pa),e&&s(Lt),v(Mt,e),e&&s(Ha),v(Wn,e),e&&s(Nr),e&&s(gs),e&&s(jp),v(Lr,e),e&&s(ro),v(Mr,e),e&&s(_p),e&&s(Un),e&&s(vp),v(Gr,e),e&&s(kp),v(Br,e),e&&s(Gt),e&&s(lo),e&&s($p),e&&s(Ws),e&&s(Ep),v(Hr,e),e&&s(oo),e&&s(io),e&&s(Tp),v(Rr,e),e&&s(Cp),e&&s(ha),v(Mn),e&&s(qp),e&&s(po),e&&s(Pp),e&&s(Bt),v(Vr),e&&s(Ap),e&&s(ho),e&&s(xp),e&&s(Fs),e&&s(Dp),v(Jr,e),e&&s(zp),e&&s(ca),e&&s(Op),e&&s(uo),e&&s(Ip),e&&s(Ya),e&&s(Sp),e&&s(ma),v(Zr),e&&s(Np),e&&s(Ra),e&&s(Wp),e&&s(Ka),e&&s(Fp),v(Qr,e),e&&s(co),v(el,e),e&&s(Up),e&&s(Yn),e&&s(Lp),v(da,e),e&&s(Mp),v(al,e),e&&s(mo),e&&s(fo),e&&s(Gp),e&&s(Va),e&&s(Bp),v(tl,e)}}}function Am(S){let m,k,d,E,z,C,q,O,D,A,W,T,x,L,F,I,R,Y,Q,J,Ie,V,ws;return{c(){m=o("p"),k=n("\u270F\uFE0F "),d=o("strong"),E=n("Your turn!"),z=n(" As an optional challenge after we\u2019ve resolved the other issues, you can try coming back to this step and getting the model to work with the original Keras-computed loss instead of the internal loss. You\u2019ll need to add "),C=o("code"),q=n('"labels"'),O=n(" to the "),D=o("code"),A=n("label_cols"),W=n(" argument of "),T=o("code"),x=n("to_tf_dataset()"),L=n(" to ensure that the labels are correctly outputted, which will get you gradients \u2014 but there\u2019s one more problem with the loss that we specified. Training will still run with this problem, but learning will be very slow and will plateau at a high training loss. Can you figure out what it is?"),F=u(),I=o("p"),R=n("A ROT13-encoded hint, if you\u2019re stuck: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),Y=o("code"),Q=n("ybtvgf"),J=n(". Jung ner ybtvgf?"),Ie=u(),V=o("p"),ws=n("And a second hint: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?")},l(H){m=i(H,"P",{});var M=p(m);k=r(M,"\u270F\uFE0F "),d=i(M,"STRONG",{});var ne=p(d);E=r(ne,"Your turn!"),ne.forEach(s),z=r(M," As an optional challenge after we\u2019ve resolved the other issues, you can try coming back to this step and getting the model to work with the original Keras-computed loss instead of the internal loss. You\u2019ll need to add "),C=i(M,"CODE",{});var ge=p(C);q=r(ge,'"labels"'),ge.forEach(s),O=r(M," to the "),D=i(M,"CODE",{});var B=p(D);A=r(B,"label_cols"),B.forEach(s),W=r(M," argument of "),T=i(M,"CODE",{});var me=p(T);x=r(me,"to_tf_dataset()"),me.forEach(s),L=r(M," to ensure that the labels are correctly outputted, which will get you gradients \u2014 but there\u2019s one more problem with the loss that we specified. Training will still run with this problem, but learning will be very slow and will plateau at a high training loss. Can you figure out what it is?"),M.forEach(s),F=c(H),I=i(H,"P",{});var re=p(I);R=r(re,"A ROT13-encoded hint, if you\u2019re stuck: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),Y=i(re,"CODE",{});var we=p(Y);Q=r(we,"ybtvgf"),we.forEach(s),J=r(re,". Jung ner ybtvgf?"),re.forEach(s),Ie=c(H),V=i(H,"P",{});var oe=p(V);ws=r(oe,"And a second hint: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?"),oe.forEach(s)},m(H,M){l(H,m,M),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O),a(m,D),a(D,A),a(m,W),a(m,T),a(T,x),a(m,L),l(H,F,M),l(H,I,M),a(I,R),a(I,Y),a(Y,Q),a(I,J),l(H,Ie,M),l(H,V,M),a(V,ws)},d(H){H&&s(m),H&&s(F),H&&s(I),H&&s(Ie),H&&s(V)}}}function xm(S){let m,k,d,E,z;return{c(){m=o("p"),k=n("\u{1F4A1} You can also import the "),d=o("code"),E=n("create_optimizer()"),z=n(" function from \u{1F917} Transformers, which will give you an AdamW optimizer with correct weight decay as well as learning rate warmup and decay. This optimizer will often produce slightly better results than the ones you get with the default Adam optimizer.")},l(C){m=i(C,"P",{});var q=p(m);k=r(q,"\u{1F4A1} You can also import the "),d=i(q,"CODE",{});var O=p(d);E=r(O,"create_optimizer()"),O.forEach(s),z=r(q," function from \u{1F917} Transformers, which will give you an AdamW optimizer with correct weight decay as well as learning rate warmup and decay. This optimizer will often produce slightly better results than the ones you get with the default Adam optimizer."),q.forEach(s)},m(C,q){l(C,m,q),a(m,k),a(m,d),a(d,E),a(m,z)},d(C){C&&s(m)}}}function Dm(S){let m,k;return{c(){m=o("p"),k=n("In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function zm(S){let m,k;return{c(){m=o("p"),k=n("\u{1F4A1} If your training data is unbalanced, make sure to build a batch of training data containing all the labels.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"\u{1F4A1} If your training data is unbalanced, make sure to build a batch of training data containing all the labels."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function Om(S){let m,k;return{c(){m=o("p"),k=n("\u26A0\uFE0F You will have to recreate your model and recompile after this overfitting test, as the model obtained probably won\u2019t be able to recover and learn something useful on your full dataset.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"\u26A0\uFE0F You will have to recreate your model and recompile after this overfitting test, as the model obtained probably won\u2019t be able to recover and learn something useful on your full dataset."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function Im(S){let m,k,d,E,z;return{c(){m=o("p"),k=n("\u270F\uFE0F "),d=o("strong"),E=n("Your turn!"),z=n(" Check that everything seems correct with the second element of the training dataset.")},l(C){m=i(C,"P",{});var q=p(m);k=r(q,"\u270F\uFE0F "),d=i(q,"STRONG",{});var O=p(d);E=r(O,"Your turn!"),O.forEach(s),z=r(q," Check that everything seems correct with the second element of the training dataset."),q.forEach(s)},m(C,q){l(C,m,q),a(m,k),a(m,d),a(d,E),a(m,z)},d(C){C&&s(m)}}}function Sm(S){let m,k;return{c(){m=o("p"),k=n("In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"In the next part of the course, we\u2019ll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function Nm(S){let m,k,d,E,z,C,q,O;return{c(){m=o("p"),k=n("\u{1F4A1} You should always make sure you can run "),d=o("code"),E=n("trainer.evaluate()"),z=n(" before launching "),C=o("code"),q=n("trainer.train()"),O=n(", to avoid wasting lots of compute resources before hitting an error.")},l(D){m=i(D,"P",{});var A=p(m);k=r(A,"\u{1F4A1} You should always make sure you can run "),d=i(A,"CODE",{});var W=p(d);E=r(W,"trainer.evaluate()"),W.forEach(s),z=r(A," before launching "),C=i(A,"CODE",{});var T=p(C);q=r(T,"trainer.train()"),T.forEach(s),O=r(A,", to avoid wasting lots of compute resources before hitting an error."),A.forEach(s)},m(D,A){l(D,m,A),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O)},d(D){D&&s(m)}}}function Wm(S){let m,k,d,E,z,C,q,O,D,A,W;return{c(){m=o("p"),k=n("\u{1F4A1} If you\u2019re using a manual training loop, the same steps apply to debug your training pipeline, but it\u2019s easier to separate them. Make sure you have not forgotten the "),d=o("code"),E=n("model.eval()"),z=n(" or "),C=o("code"),q=n("model.train()"),O=n(" at the right places, or the "),D=o("code"),A=n("zero_grad()"),W=n(" at each step, however!")},l(T){m=i(T,"P",{});var x=p(m);k=r(x,"\u{1F4A1} If you\u2019re using a manual training loop, the same steps apply to debug your training pipeline, but it\u2019s easier to separate them. Make sure you have not forgotten the "),d=i(x,"CODE",{});var L=p(d);E=r(L,"model.eval()"),L.forEach(s),z=r(x," or "),C=i(x,"CODE",{});var F=p(C);q=r(F,"model.train()"),F.forEach(s),O=r(x," at the right places, or the "),D=i(x,"CODE",{});var I=p(D);A=r(I,"zero_grad()"),I.forEach(s),W=r(x," at each step, however!"),x.forEach(s)},m(T,x){l(T,m,x),a(m,k),a(m,d),a(d,E),a(m,z),a(m,C),a(C,q),a(m,O),a(m,D),a(D,A),a(m,W)},d(T){T&&s(m)}}}function Fm(S){let m,k;return{c(){m=o("p"),k=n("\u26A0\uFE0F If you are doing distributed training, print samples of your dataset in each process and triple-check that you get the same thing. One common bug is to have some source of randomness in the data creation that makes each process have a different version of the dataset.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"\u26A0\uFE0F If you are doing distributed training, print samples of your dataset in each process and triple-check that you get the same thing. One common bug is to have some source of randomness in the data creation that makes each process have a different version of the dataset."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function Um(S){let m,k;return{c(){m=o("p"),k=n("\u{1F4A1} If your training data is unbalanced, make sure to build a batch of training data containing all the labels.")},l(d){m=i(d,"P",{});var E=p(m);k=r(E,"\u{1F4A1} If your training data is unbalanced, make sure to build a batch of training data containing all the labels."),E.forEach(s)},m(d,E){l(d,m,E),a(m,k)},d(d){d&&s(m)}}}function Lm(S){let m,k,d,E,z;return{c(){m=o("p"),k=n("\u26A0\uFE0F You will have to recreate your model and your "),d=o("code"),E=n("Trainer"),z=n(" after this test, as the model obtained probably won\u2019t be able to recover and learn something useful on your full dataset.")},l(C){m=i(C,"P",{});var q=p(m);k=r(q,"\u26A0\uFE0F You will have to recreate your model and your "),d=i(q,"CODE",{});var O=p(d);E=r(O,"Trainer"),O.forEach(s),z=r(q," after this test, as the model obtained probably won\u2019t be able to recover and learn something useful on your full dataset."),q.forEach(s)},m(C,q){l(C,m,q),a(m,k),a(m,d),a(d,E),a(m,z)},d(C){C&&s(m)}}}function Mm(S){let m,k,d,E,z,C,q,O,D,A,W;return{c(){m=o("p"),k=n("Intense hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it\u2019s just the last step to help you gain a little bit on the metric. "),d=o("em"),E=n("Very"),z=n(" bad values for your hyperparameters, like using the default Adam learning rate of 1e-3 with a Transformer model, will make learning proceed very slowly or completely stall, of course, but most of the time \u201Creasonable\u201D hyperparameters, like a learning rate from 1e-5 to 5e-5, will work just fine to give you good results. So, don\u2019t launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset."),C=u(),q=o("p"),O=n("Once you have a good enough model, you can start tweaking a bit. Don\u2019t try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact."),D=u(),A=o("p"),W=n("If you are tweaking the model itself, keep it simple and don\u2019t try anything you can\u2019t reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn\u2019t had any unintended consequences.")},l(T){m=i(T,"P",{});var x=p(m);k=r(x,"Intense hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it\u2019s just the last step to help you gain a little bit on the metric. "),d=i(x,"EM",{});var L=p(d);E=r(L,"Very"),L.forEach(s),z=r(x," bad values for your hyperparameters, like using the default Adam learning rate of 1e-3 with a Transformer model, will make learning proceed very slowly or completely stall, of course, but most of the time \u201Creasonable\u201D hyperparameters, like a learning rate from 1e-5 to 5e-5, will work just fine to give you good results. So, don\u2019t launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset."),x.forEach(s),C=c(T),q=i(T,"P",{});var F=p(q);O=r(F,"Once you have a good enough model, you can start tweaking a bit. Don\u2019t try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact."),F.forEach(s),D=c(T),A=i(T,"P",{});var I=p(A);W=r(I,"If you are tweaking the model itself, keep it simple and don\u2019t try anything you can\u2019t reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn\u2019t had any unintended consequences."),I.forEach(s)},m(T,x){l(T,m,x),a(m,k),a(m,d),a(d,E),a(m,z),l(T,C,x),l(T,q,x),a(q,O),l(T,D,x),l(T,A,x),a(A,W)},d(T){T&&s(m),T&&s(C),T&&s(q),T&&s(D),T&&s(A)}}}function Gm(S){let m,k,d,E,z,C,q,O,D,A,W;return{c(){m=o("p"),k=n("Hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it\u2019s just the last step to help you gain a little bit on the metric. Most of the time, the default hyperparameters of the "),d=o("code"),E=n("Trainer"),z=n(" will work just fine to give you good results, so don\u2019t launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset."),C=u(),q=o("p"),O=n("Once you have a good enough model, you can start tweaking a bit. Don\u2019t try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact."),D=u(),A=o("p"),W=n("If you are tweaking the model itself, keep it simple and don\u2019t try anything you can\u2019t reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn\u2019t had any unintended consequences.")},l(T){m=i(T,"P",{});var x=p(m);k=r(x,"Hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it\u2019s just the last step to help you gain a little bit on the metric. Most of the time, the default hyperparameters of the "),d=i(x,"CODE",{});var L=p(d);E=r(L,"Trainer"),L.forEach(s),z=r(x," will work just fine to give you good results, so don\u2019t launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset."),x.forEach(s),C=c(T),q=i(T,"P",{});var F=p(q);O=r(F,"Once you have a good enough model, you can start tweaking a bit. Don\u2019t try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact."),F.forEach(s),D=c(T),A=i(T,"P",{});var I=p(A);W=r(I,"If you are tweaking the model itself, keep it simple and don\u2019t try anything you can\u2019t reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn\u2019t had any unintended consequences."),I.forEach(s)},m(T,x){l(T,m,x),a(m,k),a(m,d),a(d,E),a(m,z),l(T,C,x),l(T,q,x),a(q,O),l(T,D,x),l(T,A,x),a(A,W)},d(T){T&&s(m),T&&s(C),T&&s(q),T&&s(D),T&&s(A)}}}function Bm(S){let m,k,d,E,z,C,q,O,D,A,W,T,x,L,F,I,R,Y,Q,J,Ie,V,ws,H,M,ne,ge,B,me,re,we,oe,fa,Us,je,ie,Je,Se,qe,Ls,X,ba,js,Kt,Ms,Xe,Ne,We,_e,Ze,ve,Fe,Vt,Qa,ke,_s,Qe,es,Jt,ya,$e,ss,as,K,vs,pe,et,Gs,ga,ks,Pe,wa,st,$s,ts,ee,at,ns,Xt,Es,de,Ee,Ts,Cs,Z,Zt,rs;d=new wm({props:{fw:S[0]}}),O=new ce({});const Vn=[_m,jm],qs=[];function ja(b,N){return b[0]==="pt"?0:1}x=ja(S),L=qs[x]=Vn[x](S);function Bs(b,N){return b[0]==="pt"?km:vm}let _a=Bs(S),le=_a(S);J=new ce({});const Jn=[Em,$m],ls=[];function os(b,N){return b[0]==="pt"?0:1}M=os(S),ne=ls[M]=Jn[M](S);function Qt(b,N){return b[0]==="pt"?Cm:Tm}let Ue=Qt(S),Ae=Ue(S);const is=[Pm,qm],ps=[];function Hs(b,N){return b[0]==="pt"?0:1}je=Hs(S),ie=ps[je]=is[je](S),X=new ce({});function tt(b,N){return b[0]==="pt"?Gm:Mm}let ae=tt(S),xe=ae(S);return Ze=new ce({}),{c(){m=o("meta"),k=u(),w(d.$$.fragment),E=u(),z=o("h1"),C=o("a"),q=o("span"),w(O.$$.fragment),D=u(),A=o("span"),W=n("Debugging the training pipeline"),T=u(),L.c(),F=u(),le.c(),I=u(),R=o("h2"),Y=o("a"),Q=o("span"),w(J.$$.fragment),Ie=u(),V=o("span"),ws=n("Debugging the training pipeline"),H=u(),ne.c(),ge=u(),Ae.c(),B=u(),me=o("p"),re=n("To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the "),we=o("a"),oe=n("MNLI dataset"),fa=n(":"),Us=u(),ie.c(),Je=u(),Se=o("h3"),qe=o("a"),Ls=o("span"),w(X.$$.fragment),ba=u(),js=o("span"),Kt=n("Don't tune anything until you have a first baseline"),Ms=u(),xe.c(),Xe=u(),Ne=o("h3"),We=o("a"),_e=o("span"),w(Ze.$$.fragment),ve=u(),Fe=o("span"),Vt=n("Ask for help"),Qa=u(),ke=o("p"),_s=n("Hopefully you will have found some advice in this section that helped you solve your issue, but if that\u2019s not the case, remember you can always ask the community on the "),Qe=o("a"),es=n("forums"),Jt=n("."),ya=u(),$e=o("p"),ss=n("Here are some additional resources that may prove helpful:"),as=u(),K=o("ul"),vs=o("li"),pe=o("a"),et=n("\u201CReproducibility as a vehicle for engineering best practices\u201D"),Gs=n(" by Joel Grus"),ga=u(),ks=o("li"),Pe=o("a"),wa=n("\u201CChecklist for debugging neural networks\u201D"),st=n(" by Cecelia Shao"),$s=u(),ts=o("li"),ee=o("a"),at=n("\u201CHow to unit test machine learning code\u201D"),ns=n(" by Chase Roberts"),Xt=u(),Es=o("li"),de=o("a"),Ee=n("\u201CA Recipe for Training Neural Networks\u201D"),Ts=n(" by Andrej Karpathy"),Cs=u(),Z=o("p"),Zt=n("Of course, not every problem you encounter when training neural nets is your own fault! If you encounter something in the \u{1F917} Transformers or \u{1F917} Datasets library that does not seem right, you may have encountered a bug. You should definitely tell us all about it, and in the next section we\u2019ll explain exactly how to do that."),this.h()},l(b){const N=ym('[data-svelte="svelte-1phssyn"]',document.head);m=i(N,"META",{name:!0,content:!0}),N.forEach(s),k=c(b),j(d.$$.fragment,b),E=c(b),z=i(b,"H1",{class:!0});var hs=p(z);C=i(hs,"A",{id:!0,class:!0,href:!0});var fe=p(C);q=i(fe,"SPAN",{});var us=p(q);j(O.$$.fragment,us),us.forEach(s),fe.forEach(s),D=c(hs),A=i(hs,"SPAN",{});var Ys=p(A);W=r(Ys,"Debugging the training pipeline"),Ys.forEach(s),hs.forEach(s),T=c(b),L.l(b),F=c(b),le.l(b),I=c(b),R=i(b,"H2",{class:!0});var va=p(R);Y=i(va,"A",{id:!0,class:!0,href:!0});var cs=p(Y);Q=i(cs,"SPAN",{});var ms=p(Q);j(J.$$.fragment,ms),ms.forEach(s),cs.forEach(s),Ie=c(va),V=i(va,"SPAN",{});var Le=p(V);ws=r(Le,"Debugging the training pipeline"),Le.forEach(s),va.forEach(s),H=c(b),ne.l(b),ge=c(b),Ae.l(b),B=c(b),me=i(b,"P",{});var Rs=p(me);re=r(Rs,"To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the "),we=i(Rs,"A",{href:!0,rel:!0});var Ks=p(we);oe=r(Ks,"MNLI dataset"),Ks.forEach(s),fa=r(Rs,":"),Rs.forEach(s),Us=c(b),ie.l(b),Je=c(b),Se=i(b,"H3",{class:!0});var ka=p(Se);qe=i(ka,"A",{id:!0,class:!0,href:!0});var ds=p(qe);Ls=i(ds,"SPAN",{});var en=p(Ls);j(X.$$.fragment,en),en.forEach(s),ds.forEach(s),ba=c(ka),js=i(ka,"SPAN",{});var Ps=p(js);Kt=r(Ps,"Don't tune anything until you have a first baseline"),Ps.forEach(s),ka.forEach(s),Ms=c(b),xe.l(b),Xe=c(b),Ne=i(b,"H3",{class:!0});var he=p(Ne);We=i(he,"A",{id:!0,class:!0,href:!0});var As=p(We);_e=i(As,"SPAN",{});var Xn=p(_e);j(Ze.$$.fragment,Xn),Xn.forEach(s),As.forEach(s),ve=c(he),Fe=i(he,"SPAN",{});var sn=p(Fe);Vt=r(sn,"Ask for help"),sn.forEach(s),he.forEach(s),Qa=c(b),ke=i(b,"P",{});var De=p(ke);_s=r(De,"Hopefully you will have found some advice in this section that helped you solve your issue, but if that\u2019s not the case, remember you can always ask the community on the "),Qe=i(De,"A",{href:!0,rel:!0});var an=p(Qe);es=r(an,"forums"),an.forEach(s),Jt=r(De,"."),De.forEach(s),ya=c(b),$e=i(b,"P",{});var Vs=p($e);ss=r(Vs,"Here are some additional resources that may prove helpful:"),Vs.forEach(s),as=c(b),K=i(b,"UL",{});var Te=p(K);vs=i(Te,"LI",{});var Js=p(vs);pe=i(Js,"A",{href:!0,rel:!0});var Zn=p(pe);et=r(Zn,"\u201CReproducibility as a vehicle for engineering best practices\u201D"),Zn.forEach(s),Gs=r(Js," by Joel Grus"),Js.forEach(s),ga=c(Te),ks=i(Te,"LI",{});var $a=p(ks);Pe=i($a,"A",{href:!0,rel:!0});var ue=p(Pe);wa=r(ue,"\u201CChecklist for debugging neural networks\u201D"),ue.forEach(s),st=r($a," by Cecelia Shao"),$a.forEach(s),$s=c(Te),ts=i(Te,"LI",{});var nt=p(ts);ee=i(nt,"A",{href:!0,rel:!0});var fs=p(ee);at=r(fs,"\u201CHow to unit test machine learning code\u201D"),fs.forEach(s),ns=r(nt," by Chase Roberts"),nt.forEach(s),Xt=c(Te),Es=i(Te,"LI",{});var rt=p(Es);de=i(rt,"A",{href:!0,rel:!0});var Me=p(de);Ee=r(Me,"\u201CA Recipe for Training Neural Networks\u201D"),Me.forEach(s),Ts=r(rt," by Andrej Karpathy"),rt.forEach(s),Te.forEach(s),Cs=c(b),Z=i(b,"P",{});var lt=p(Z);Zt=r(lt,"Of course, not every problem you encounter when training neural nets is your own fault! If you encounter something in the \u{1F917} Transformers or \u{1F917} Datasets library that does not seem right, you may have encountered a bug. You should definitely tell us all about it, and in the next section we\u2019ll explain exactly how to do that."),lt.forEach(s),this.h()},h(){$(m,"name","hf:doc:metadata"),$(m,"content",JSON.stringify(Hm)),$(C,"id","debugging-the-training-pipeline"),$(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(C,"href","#debugging-the-training-pipeline"),$(z,"class","relative group"),$(Y,"id","debugging-the-training-pipeline"),$(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Y,"href","#debugging-the-training-pipeline"),$(R,"class","relative group"),$(we,"href","https://huggingface.co/datasets/glue"),$(we,"rel","nofollow"),$(qe,"id","dont-tune-anything-until-you-have-a-first-baseline"),$(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(qe,"href","#dont-tune-anything-until-you-have-a-first-baseline"),$(Se,"class","relative group"),$(We,"id","ask-for-help"),$(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(We,"href","#ask-for-help"),$(Ne,"class","relative group"),$(Qe,"href","https://discuss.huggingface.co/"),$(Qe,"rel","nofollow"),$(pe,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),$(pe,"rel","nofollow"),$(Pe,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),$(Pe,"rel","nofollow"),$(ee,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),$(ee,"rel","nofollow"),$(de,"href","http://karpathy.github.io/2019/04/25/recipe/"),$(de,"rel","nofollow")},m(b,N){a(document.head,m),l(b,k,N),_(d,b,N),l(b,E,N),l(b,z,N),a(z,C),a(C,q),_(O,q,null),a(z,D),a(z,A),a(A,W),l(b,T,N),qs[x].m(b,N),l(b,F,N),le.m(b,N),l(b,I,N),l(b,R,N),a(R,Y),a(Y,Q),_(J,Q,null),a(R,Ie),a(R,V),a(V,ws),l(b,H,N),ls[M].m(b,N),l(b,ge,N),Ae.m(b,N),l(b,B,N),l(b,me,N),a(me,re),a(me,we),a(we,oe),a(me,fa),l(b,Us,N),ps[je].m(b,N),l(b,Je,N),l(b,Se,N),a(Se,qe),a(qe,Ls),_(X,Ls,null),a(Se,ba),a(Se,js),a(js,Kt),l(b,Ms,N),xe.m(b,N),l(b,Xe,N),l(b,Ne,N),a(Ne,We),a(We,_e),_(Ze,_e,null),a(Ne,ve),a(Ne,Fe),a(Fe,Vt),l(b,Qa,N),l(b,ke,N),a(ke,_s),a(ke,Qe),a(Qe,es),a(ke,Jt),l(b,ya,N),l(b,$e,N),a($e,ss),l(b,as,N),l(b,K,N),a(K,vs),a(vs,pe),a(pe,et),a(vs,Gs),a(K,ga),a(K,ks),a(ks,Pe),a(Pe,wa),a(ks,st),a(K,$s),a(K,ts),a(ts,ee),a(ee,at),a(ts,ns),a(K,Xt),a(K,Es),a(Es,de),a(de,Ee),a(Es,Ts),l(b,Cs,N),l(b,Z,N),a(Z,Zt),rs=!0},p(b,[N]){const hs={};N&1&&(hs.fw=b[0]),d.$set(hs);let fe=x;x=ja(b),x!==fe&&(yu(),y(qs[fe],1,1,()=>{qs[fe]=null}),bu(),L=qs[x],L||(L=qs[x]=Vn[x](b),L.c()),g(L,1),L.m(F.parentNode,F)),_a!==(_a=Bs(b))&&(le.d(1),le=_a(b),le&&(le.c(),le.m(I.parentNode,I)));let us=M;M=os(b),M!==us&&(yu(),y(ls[us],1,1,()=>{ls[us]=null}),bu(),ne=ls[M],ne||(ne=ls[M]=Jn[M](b),ne.c()),g(ne,1),ne.m(ge.parentNode,ge)),Ue!==(Ue=Qt(b))&&(Ae.d(1),Ae=Ue(b),Ae&&(Ae.c(),Ae.m(B.parentNode,B)));let Ys=je;je=Hs(b),je!==Ys&&(yu(),y(ps[Ys],1,1,()=>{ps[Ys]=null}),bu(),ie=ps[je],ie||(ie=ps[je]=is[je](b),ie.c()),g(ie,1),ie.m(Je.parentNode,Je)),ae!==(ae=tt(b))&&(xe.d(1),xe=ae(b),xe&&(xe.c(),xe.m(Xe.parentNode,Xe)))},i(b){rs||(g(d.$$.fragment,b),g(O.$$.fragment,b),g(L),g(J.$$.fragment,b),g(ne),g(ie),g(X.$$.fragment,b),g(Ze.$$.fragment,b),rs=!0)},o(b){y(d.$$.fragment,b),y(O.$$.fragment,b),y(L),y(J.$$.fragment,b),y(ne),y(ie),y(X.$$.fragment,b),y(Ze.$$.fragment,b),rs=!1},d(b){s(m),b&&s(k),v(d,b),b&&s(E),b&&s(z),v(O),b&&s(T),qs[x].d(b),b&&s(F),le.d(b),b&&s(I),b&&s(R),v(J),b&&s(H),ls[M].d(b),b&&s(ge),Ae.d(b),b&&s(B),b&&s(me),b&&s(Us),ps[je].d(b),b&&s(Je),b&&s(Se),v(X),b&&s(Ms),xe.d(b),b&&s(Xe),b&&s(Ne),v(Ze),b&&s(Qa),b&&s(ke),b&&s(ya),b&&s($e),b&&s(as),b&&s(K),b&&s(Cs),b&&s(Z)}}}const Hm={local:"debugging-the-training-pipeline",sections:[{local:"debugging-the-training-pipeline",sections:[{local:"check-your-data",title:"Check your data"},{local:"from-datasets-to-dataloaders",title:"From datasets to dataloaders"},{local:"going-through-the-model",title:"Going through the model"},{local:"performing-one-optimization-step",title:"Performing one optimization step"},{local:"dealing-with-cuda-outofmemory-errors",title:"Dealing with CUDA out-of-memory errors"},{local:"evaluating-the-model",title:"Evaluating the model"}],title:"Debugging the training pipeline"},{local:"debugging-silent-errors-during-training",sections:[{local:"check-your-data-again",title:"Check your data (again!)"},{local:"overfit-your-model-on-one-batch",title:"Overfit your model on one batch"},{local:"check-your-data",title:"Check your data"},{local:"check-your-model",title:"Check your model"},{local:"check-your-hyperparameters",title:"Check your hyperparameters"}],title:"Debugging silent errors during training"},{local:"other-potential-issues",sections:[{local:"dealing-with-outofmemory-errors",title:"Dealing with out-of-memory errors"},{local:"hungry-hungry-tensorflow",title:"Hungry Hungry TensorFlow \u{1F99B}"},{local:"check-your-data-again",title:"Check your data (again!)"},{local:"overfit-your-model-on-one-batch",title:"Overfit your model on one batch"},{local:"dont-tune-anything-until-you-have-a-first-baseline",title:"Don't tune anything until you have a first baseline"},{local:"ask-for-help",title:"Ask for help"}],title:"Other potential issues "}],title:"Debugging the training pipeline"};function Ym(S,m,k){let d="pt";return gm(()=>{const E=new URLSearchParams(window.location.search);k(0,d=E.get("fw")||"pt")}),[d]}class ed extends dm{constructor(m){super();fm(this,m,Ym,Bm,bm,{})}}export{ed as default,Hm as metadata};
