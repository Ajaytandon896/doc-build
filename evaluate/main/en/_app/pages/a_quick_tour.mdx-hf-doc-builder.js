import{S as Kd,i as Qd,s as Xd,e as s,k as p,w as d,t as n,M as Zd,c as l,d as a,m as u,a as o,x as m,h as r,b as h,N as em,G as t,g as c,y as f,q as v,o as y,B as g,v as tm}from"../chunks/vendor-hf-doc-builder.js";import{T as am}from"../chunks/Tip-hf-doc-builder.js";import{I as C}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";function sm(wl){let E,ce,T,O,J;return{c(){E=s("p"),ce=n("Note that features always describe the type of a single input element. In general we will add lists of elements so you can always think of a list around the types in "),T=s("code"),O=n("features"),J=n(". Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation.")},l(D){E=l(D,"P",{});var H=o(E);ce=r(H,"Note that features always describe the type of a single input element. In general we will add lists of elements so you can always think of a list around the types in "),T=l(H,"CODE",{});var pe=o(T);O=r(pe,"features"),pe.forEach(a),J=r(H,". Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation."),H.forEach(a)},m(D,H){c(D,E,H),t(E,ce),t(E,T),t(T,O),t(E,J)},d(D){D&&a(E)}}}function lm(wl){let E,ce,T,O,J,D,H,pe,Cn,El,Gt,Dn,jl,K,ue,_a,Re,On,ba,Sn,$l,Rt,In,kl,F,he,wa,Nn,Mn,Ue,Hn,Fn,Bn,de,Ea,Ln,Wn,Ve,Gn,Rn,Un,me,ja,Vn,Yn,Ye,zn,Jn,xl,fe,Kn,ze,Qn,Xn,Tl,Je,Ut,ou,ql,ve,Zn,Vt,er,tr,Al,Q,ye,$a,Ke,ar,ka,sr,Pl,ge,lr,xa,or,nr,Cl,Qe,Dl,Yt,rr,Ol,Xe,Sl,X,_e,Ta,Ze,ir,qa,cr,Il,zt,pr,Nl,et,Ml,be,ur,Jt,hr,dr,Hl,Z,we,Aa,tt,mr,Pa,fr,Fl,Ee,vr,Kt,yr,gr,Bl,at,Ll,ee,je,Ca,st,_r,Da,br,Wl,$e,wr,Qt,Er,jr,Gl,ke,Oa,lt,Sa,$r,kr,Ia,xr,Tr,b,ot,Na,Ma,qr,Ar,Ha,Pr,Cr,nt,Fa,Ba,Dr,Or,La,Sr,Ir,rt,Wa,Ga,Nr,Mr,it,Hr,Ra,Fr,Br,Lr,ct,Ua,Va,Wr,Gr,pt,Rr,Ya,Ur,Vr,Yr,ut,za,Ja,zr,Jr,Ka,Kr,Qr,ht,Qa,Xa,Xr,Zr,Za,ei,ti,dt,es,ts,ai,si,as,li,oi,mt,ss,ls,ni,ri,os,ii,ci,ft,ns,rs,pi,ui,is,hi,Rl,xe,di,cs,mi,fi,Ul,vt,Vl,Te,vi,ps,yi,gi,Yl,yt,zl,Xt,_i,Jl,gt,Kl,qe,Ql,te,Ae,us,_t,bi,hs,wi,Xl,Zt,Ei,Zl,Pe,ds,ji,$i,ms,ki,eo,q,xi,ea,Ti,qi,ta,Ai,Pi,aa,Ci,Di,fs,Oi,Si,to,ae,Ce,vs,bt,Ii,ys,Ni,ao,S,Mi,gs,Hi,Fi,_s,Bi,Li,bs,Wi,Gi,so,wt,lo,B,Ri,ws,Ui,Vi,Es,Yi,zi,oo,se,De,js,Et,Ji,$s,Ki,no,A,Qi,ks,Xi,Zi,xs,ec,tc,Ts,ac,sc,qs,lc,oc,ro,jt,io,I,nc,As,rc,ic,Ps,cc,pc,Cs,uc,hc,co,$t,po,sa,dc,uo,kt,ho,le,Oe,Ds,xt,mc,Os,fc,mo,P,vc,Ss,yc,gc,Is,_c,bc,Tt,wc,Ec,Ns,jc,$c,fo,la,kc,vo,L,xc,Ms,Tc,qc,Hs,Ac,Pc,yo,oa,Cc,go,oe,Se,Fs,qt,Dc,Bs,Oc,_o,_,Sc,Ls,Ic,Nc,Ws,Mc,Hc,Gs,Fc,Bc,Rs,Lc,Wc,Us,Gc,Rc,Vs,Uc,Vc,Ys,Yc,zc,zs,Jc,Kc,bo,At,wo,Ie,Qc,Js,Xc,Zc,Eo,ne,Ne,Ks,Pt,ep,Qs,tp,jo,Me,ap,na,sp,lp,$o,Ct,ko,W,op,Xs,np,rp,Zs,ip,cp,xo,Dt,To,re,He,el,Ot,pp,tl,up,qo,Fe,hp,ra,dp,mp,Ao,St,Po,ia,fp,Co,It,Do,ca,vp,Oo,Be,yp,pa,gp,_p,So,Nt,Io,ie,Le,al,Mt,bp,sl,wp,No,k,Ep,ua,jp,$p,ll,kp,xp,Ht,Tp,qp,ol,Ap,Pp,nl,Cp,Dp,Mo,G,Op,rl,Sp,Ip,il,Np,Mp,Ho,Ft,Fo,R,Hp,cl,Fp,Bp,pl,Lp,Wp,Bo,Bt,Lo,U,Gp,ul,Rp,Up,hl,Vp,Yp,Wo,Lt,Go,x,zp,dl,Jp,Kp,ml,Qp,Xp,fl,Zp,eu,vl,tu,au,yl,su,lu,Ro;return D=new C({}),Re=new C({}),Ke=new C({}),Qe=new w({props:{code:`import evaluate
accuracy = evaluate.load("accuracy")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),Xe=new w({props:{code:'word_length = evaluate.load("word_length", module_type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>word_length = evaluate.load(<span class="hljs-string">&quot;word_length&quot;</span>, module_type=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),Ze=new C({}),et=new w({props:{code:'element_count = evaluate.load("lvwerra/element_count", module_type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>element_count = evaluate.load(<span class="hljs-string">&quot;lvwerra/element_count&quot;</span>, module_type=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),tt=new C({}),at=new w({props:{code:`evaluate.list_evaluation_modules(
  module_type="comparison",
  include_community=False, 
  with_details=True)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>evaluate.list_evaluation_modules(
<span class="hljs-meta">... </span>  module_type=<span class="hljs-string">&quot;comparison&quot;</span>,
<span class="hljs-meta">... </span>  include_community=<span class="hljs-literal">False</span>, 
<span class="hljs-meta">... </span>  with_details=<span class="hljs-literal">True</span>)

[{<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;mcnemar&#x27;</span>, <span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;comparison&#x27;</span>, <span class="hljs-string">&#x27;community&#x27;</span>: <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;likes&#x27;</span>: <span class="hljs-number">1</span>},
 {<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;exact_match&#x27;</span>, <span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;comparison&#x27;</span>, <span class="hljs-string">&#x27;community&#x27;</span>: <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;likes&#x27;</span>: <span class="hljs-number">0</span>}]`}}),st=new C({}),vt=new w({props:{code:`accuracy = evaluate.load("accuracy")
accuracy.description`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.description
Accuracy <span class="hljs-keyword">is</span> the proportion of correct predictions among the total number of cases processed. It can be computed <span class="hljs-keyword">with</span>:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: <span class="hljs-literal">True</span> positive
TN: <span class="hljs-literal">True</span> negative
FP: <span class="hljs-literal">False</span> positive
FN: <span class="hljs-literal">False</span> negative`}}),yt=new w({props:{code:"accuracy.citation",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.citation
<span class="hljs-meta">@article{scikit-learn,</span>
  title={Scikit-learn: Machine Learning <span class="hljs-keyword">in</span> {P}ython},
  author={Pedregosa, F. <span class="hljs-keyword">and</span> Varoquaux, G. <span class="hljs-keyword">and</span> Gramfort, A. <span class="hljs-keyword">and</span> Michel, V.
         <span class="hljs-keyword">and</span> Thirion, B. <span class="hljs-keyword">and</span> Grisel, O. <span class="hljs-keyword">and</span> Blondel, M. <span class="hljs-keyword">and</span> Prettenhofer, P.
         <span class="hljs-keyword">and</span> Weiss, R. <span class="hljs-keyword">and</span> Dubourg, V. <span class="hljs-keyword">and</span> Vanderplas, J. <span class="hljs-keyword">and</span> Passos, A. <span class="hljs-keyword">and</span>
         Cournapeau, D. <span class="hljs-keyword">and</span> Brucher, M. <span class="hljs-keyword">and</span> Perrot, M. <span class="hljs-keyword">and</span> Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={<span class="hljs-number">12</span>},
  pages={<span class="hljs-number">2825</span>--<span class="hljs-number">2830</span>},
  year={<span class="hljs-number">2011</span>}
}`}}),gt=new w({props:{code:"accuracy.features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.features
{
    <span class="hljs-string">&#x27;predictions&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
    <span class="hljs-string">&#x27;references&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)
}`}}),qe=new am({props:{$$slots:{default:[sm]},$$scope:{ctx:wl}}}),_t=new C({}),bt=new C({}),wt=new w({props:{code:"accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),Et=new C({}),jt=new w({props:{code:`for ref, pred in zip([0,1,0,1], [1,0,0,1]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add(references=ref, predictions=pred)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),$t=new w({props:{code:`for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):
    accuracy.add_batch(references=refs, predictions=preds)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add_batch(references=refs, predictions=preds)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),kt=new w({props:{code:`for model_inputs, gold_standards in evaluation_dataset:
    predictions = model(model_inputs)
    metric.add_batch(references=gold_standards, predictions=predictions)
metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> model_inputs, gold_standards <span class="hljs-keyword">in</span> evaluation_dataset:
<span class="hljs-meta">&gt;&gt;&gt; </span>    predictions = model(model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric.add_batch(references=gold_standards, predictions=predictions)
<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()`}}),xt=new C({}),qt=new C({}),At=new w({props:{code:`
metric = evaluate.load("accuracy", normalize=False)
refs, preds = [1, 1], [1, 0]
acc_1 = metric.compute(references=refs, predictions=preds)["accuracy"]
acc_2 = metric.compute(references=refs, predictions=preds, normalize=True)["accuracy"]
acc_3 = metric.compute(references=refs, predictions=preds)["accuracy"]
print((acc_1, acc_2, acc_3))`,highlighted:`
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>, normalize=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>refs, preds = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>acc_1 = metric.compute(references=refs, predictions=preds)[<span class="hljs-string">&quot;accuracy&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>acc_2 = metric.compute(references=refs, predictions=preds, normalize=<span class="hljs-literal">True</span>)[<span class="hljs-string">&quot;accuracy&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>acc_3 = metric.compute(references=refs, predictions=preds)[<span class="hljs-string">&quot;accuracy&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>((acc_1, acc_2, acc_3))
(<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>)`}}),Pt=new C({}),Ct=new w({props:{code:'clf_metrics = evaluate.combine(["accuracy", "f1", "precision", "recall"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>clf_metrics = evaluate.combine([<span class="hljs-string">&quot;accuracy&quot;</span>, <span class="hljs-string">&quot;f1&quot;</span>, <span class="hljs-string">&quot;precision&quot;</span>, <span class="hljs-string">&quot;recall&quot;</span>])'}}),Dt=new w({props:{code:`clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>clf_metrics.compute(predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

{
  <span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">&#x27;precision&#x27;</span>: <span class="hljs-number">1.0</span>,
  <span class="hljs-string">&#x27;recall&#x27;</span>: <span class="hljs-number">0.5</span>
}`}}),Ot=new C({}),St=new w({props:{code:`result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])

hyperparams = {"model": "bert-base-uncased"}
evaluate.save("./results/"experiment="run 42", **result, **hyperparams)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>hyperparams = {<span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;bert-base-uncased&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluate.save(<span class="hljs-string">&quot;./results/&quot;</span>experiment=<span class="hljs-string">&quot;run 42&quot;</span>, **result, **hyperparams)
PosixPath(<span class="hljs-string">&#x27;results/result-2022_05_30-22_09_11.json&#x27;</span>)`}}),It=new w({props:{code:`{
    "experiment": "run 42",
    "accuracy": 0.5,
    "model": "bert-base-uncased",
    "_timestamp": "2022-05-30T22:09:11.959469",
    "_git_commit_hash": "123456789abcdefghijkl",
    "_evaluate_version": "0.1.0",
    "_python_version": "3.9.12 (main, Mar 26 2022, 15:51:15) \\n[Clang 13.1.6 (clang-1316.0.21.2)]",
    "_interpreter_path": "/Users/leandro/git/evaluate/env/bin/python"
}`,highlighted:`<span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;experiment&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;run 42&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;accuracy&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.5</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bert-base-uncased&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;_timestamp&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;2022-05-30T22:09:11.959469&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;_git_commit_hash&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;123456789abcdefghijkl&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;_evaluate_version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.1.0&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;_python_version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;3.9.12 (main, Mar 26 2022, 15:51:15) \\n[Clang 13.1.6 (clang-1316.0.21.2)]&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;_interpreter_path&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;/Users/leandro/git/evaluate/env/bin/python&quot;</span>
<span class="hljs-punctuation">}</span>`}}),Nt=new w({props:{code:`evaluate.push_to_hub(
  model_id="huggingface/gpt2-wikitext2",  # model repository on hub
  metric_value=0.5,                       # metric value
  metric_type="bleu",                     # metric name, e.g. accuracy.name
  metric_name="BLEU",                     # pretty name which is displayed
  dataset_type="wikitext",                # dataset name on the hub
  dataset_name="WikiText",                # pretty name
  dataset_split="test",                   # dataset split used
  task_type="text-generation",            # task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json
  task_name="Text Generation"             # pretty name for task
)`,highlighted:`evaluate.push_to_hub(
  model_id=<span class="hljs-string">&quot;huggingface/gpt2-wikitext2&quot;</span>,  <span class="hljs-comment"># model repository on hub</span>
  metric_value=<span class="hljs-number">0.5</span>,                       <span class="hljs-comment"># metric value</span>
  metric_type=<span class="hljs-string">&quot;bleu&quot;</span>,                     <span class="hljs-comment"># metric name, e.g. accuracy.name</span>
  metric_name=<span class="hljs-string">&quot;BLEU&quot;</span>,                     <span class="hljs-comment"># pretty name which is displayed</span>
  dataset_type=<span class="hljs-string">&quot;wikitext&quot;</span>,                <span class="hljs-comment"># dataset name on the hub</span>
  dataset_name=<span class="hljs-string">&quot;WikiText&quot;</span>,                <span class="hljs-comment"># pretty name</span>
  dataset_split=<span class="hljs-string">&quot;test&quot;</span>,                   <span class="hljs-comment"># dataset split used</span>
  task_type=<span class="hljs-string">&quot;text-generation&quot;</span>,            <span class="hljs-comment"># task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json</span>
  task_name=<span class="hljs-string">&quot;Text Generation&quot;</span>             <span class="hljs-comment"># pretty name for task</span>
)`}}),Mt=new C({}),Ft=new w({props:{code:`from transformers import pipeline
from datasets import load_dataset
from evaluate import evaluator
import evaluate

pipe = pipeline("text-classification", model="lvwerra/distilbert-imdb", device=0)
data = load_dataset("imdb", split="test").shuffle().select(range(1000))
metric = evaluate.load("accuracy")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-keyword">import</span> evaluate

pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=<span class="hljs-string">&quot;lvwerra/distilbert-imdb&quot;</span>, device=<span class="hljs-number">0</span>)
data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>).shuffle().select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),Bt=new w({props:{code:`eval = evaluator("text-classification")

results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={"NEGATIVE": 0, "POSITIVE": 1},)

print(results)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">eval</span> = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>results = <span class="hljs-built_in">eval</span>.compute(model_or_pipeline=pipe, data=data, metric=metric,
<span class="hljs-meta">... </span>                       label_mapping={<span class="hljs-string">&quot;NEGATIVE&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;POSITIVE&quot;</span>: <span class="hljs-number">1</span>},)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(results)
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.934</span>}`}}),Lt=new w({props:{code:`results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={"NEGATIVE": 0, "POSITIVE": 1},
                       strategy="bootstrap", n_resamples=200)

print(results)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = <span class="hljs-built_in">eval</span>.compute(model_or_pipeline=pipe, data=data, metric=metric,
<span class="hljs-meta">... </span>                       label_mapping={<span class="hljs-string">&quot;NEGATIVE&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;POSITIVE&quot;</span>: <span class="hljs-number">1</span>},
<span class="hljs-meta">... </span>                       strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>, n_resamples=<span class="hljs-number">200</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(results)
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: 
    {
      <span class="hljs-string">&#x27;confidence_interval&#x27;</span>: (<span class="hljs-number">0.906</span>, <span class="hljs-number">0.9406749892841922</span>),
      <span class="hljs-string">&#x27;standard_error&#x27;</span>: <span class="hljs-number">0.00865213251082787</span>,
      <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.923</span>
    }
}`}}),{c(){E=s("meta"),ce=p(),T=s("h1"),O=s("a"),J=s("span"),d(D.$$.fragment),H=p(),pe=s("span"),Cn=n("A quick tour"),El=p(),Gt=s("p"),Dn=n("\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),jl=p(),K=s("h2"),ue=s("a"),_a=s("span"),d(Re.$$.fragment),On=p(),ba=s("span"),Sn=n("Types of evaluations"),$l=p(),Rt=s("p"),In=n("There are different aspects of a typical machine learning pipeline that can be evaluated and for each aspect \u{1F917} Evaluate provides a tool:"),kl=p(),F=s("ul"),he=s("li"),wa=s("strong"),Nn=n("Metric"),Mn=n(": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Ue=s("a"),Hn=n("evaluate-metric"),Fn=n("."),Bn=p(),de=s("li"),Ea=s("strong"),Ln=n("Comparison"),Wn=n(": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at "),Ve=s("a"),Gn=n("evaluate-comparison"),Rn=n("."),Un=p(),me=s("li"),ja=s("strong"),Vn=n("Measurement"),Yn=n(": The dataset is as important as the model trained on it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),Ye=s("a"),zn=n("evaluate-measurement"),Jn=n("."),xl=p(),fe=s("p"),Kn=n("Each of these evaluation modules live on Hugging Face Hub as a Space. They come with an interactive widget and a documentation card documenting its use and limitations. For example "),ze=s("a"),Qn=n("accuracy"),Xn=n(":"),Tl=p(),Je=s("div"),Ut=s("img"),ql=p(),ve=s("p"),Zn=n("Each metric, comparison, and measurement is a separate Python module, but for using any of them, there is a single entry point: "),Vt=s("a"),er=n("evaluate.load()"),tr=n("!"),Al=p(),Q=s("h2"),ye=s("a"),$a=s("span"),d(Ke.$$.fragment),ar=p(),ka=s("span"),sr=n("Load"),Pl=p(),ge=s("p"),lr=n("Any metric, comparison, or measurement is loaded with the "),xa=s("code"),or=n("evaluate.load"),nr=n(" function:"),Cl=p(),d(Qe.$$.fragment),Dl=p(),Yt=s("p"),rr=n("If you want to make sure you are loading the right type of evaluation (especially if there are name clashes) you can explicitly pass the type:"),Ol=p(),d(Xe.$$.fragment),Sl=p(),X=s("h3"),_e=s("a"),Ta=s("span"),d(Ze.$$.fragment),ir=p(),qa=s("span"),cr=n("Community modules"),Il=p(),zt=s("p"),pr=n("Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by specifying the repository ID of the metric implementation:"),Nl=p(),d(et.$$.fragment),Ml=p(),be=s("p"),ur=n("See the "),Jt=s("a"),hr=n("Creating and Sharing Guide"),dr=n(" for information about uploading custom metrics."),Hl=p(),Z=s("h3"),we=s("a"),Aa=s("span"),d(tt.$$.fragment),mr=p(),Pa=s("span"),fr=n("List available modules"),Fl=p(),Ee=s("p"),vr=n("With "),Kt=s("a"),yr=n("list_evaluation_modules()"),gr=n(" you can check what modules are available on the hub. You can also filter for a specific modules and skip community metrics if you want. You can also see additional information such as likes:"),Bl=p(),d(at.$$.fragment),Ll=p(),ee=s("h2"),je=s("a"),Ca=s("span"),d(st.$$.fragment),_r=p(),Da=s("span"),br=n("Module attributes"),Wl=p(),$e=s("p"),wr=n("All evalution modules come with a range of useful attributes that help to use a module stored in a "),Qt=s("a"),Er=n("EvaluationModuleInfo"),jr=n(" object."),Gl=p(),ke=s("table"),Oa=s("thead"),lt=s("tr"),Sa=s("th"),$r=n("Attribute"),kr=p(),Ia=s("th"),xr=n("Description"),Tr=p(),b=s("tbody"),ot=s("tr"),Na=s("td"),Ma=s("code"),qr=n("description"),Ar=p(),Ha=s("td"),Pr=n("A short description of the evaluation module."),Cr=p(),nt=s("tr"),Fa=s("td"),Ba=s("code"),Dr=n("citation"),Or=p(),La=s("td"),Sr=n("A BibTex string for citation when available."),Ir=p(),rt=s("tr"),Wa=s("td"),Ga=s("code"),Nr=n("config"),Mr=p(),it=s("td"),Hr=n("A "),Ra=s("code"),Fr=n("dataclass"),Br=n(" containing the settings of the module."),Lr=p(),ct=s("tr"),Ua=s("td"),Va=s("code"),Wr=n("features"),Gr=p(),pt=s("td"),Rr=n("A "),Ya=s("code"),Ur=n("Features"),Vr=n(" object defining the input format."),Yr=p(),ut=s("tr"),za=s("td"),Ja=s("code"),zr=n("inputs_description"),Jr=p(),Ka=s("td"),Kr=n("This is equivalent to the modules docstring."),Qr=p(),ht=s("tr"),Qa=s("td"),Xa=s("code"),Xr=n("homepage"),Zr=p(),Za=s("td"),ei=n("The homepage of the module."),ti=p(),dt=s("tr"),es=s("td"),ts=s("code"),ai=n("license"),si=p(),as=s("td"),li=n("The license of the module."),oi=p(),mt=s("tr"),ss=s("td"),ls=s("code"),ni=n("codebase_urls"),ri=p(),os=s("td"),ii=n("Link to the code behind the module."),ci=p(),ft=s("tr"),ns=s("td"),rs=s("code"),pi=n("reference_urls"),ui=p(),is=s("td"),hi=n("Additional reference URLs."),Rl=p(),xe=s("p"),di=n("Let\u2019s have a look at a few examples. First, let\u2019s look at the "),cs=s("code"),mi=n("description"),fi=n(" attribute of the accuracy metric:"),Ul=p(),d(vt.$$.fragment),Vl=p(),Te=s("p"),vi=n("You can see that it describes how the metric works in theory. If you use this metric for your work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),ps=s("code"),yi=n("citation"),gi=n(" attribute:"),Yl=p(),d(yt.$$.fragment),zl=p(),Xt=s("p"),_i=n("Before we can apply a metric or other evaluation module to a use-case, we need to know what the input format of the metric is:"),Jl=p(),d(gt.$$.fragment),Kl=p(),d(qe.$$.fragment),Ql=p(),te=s("h2"),Ae=s("a"),us=s("span"),d(_t.$$.fragment),bi=p(),hs=s("span"),wi=n("Compute"),Xl=p(),Zt=s("p"),Ei=n("Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),Zl=p(),Pe=s("ol"),ds=s("li"),ji=n("All-in-one"),$i=p(),ms=s("li"),ki=n("Incremental"),eo=p(),q=s("p"),xi=n("In the incremental approach the necessary inputs are added to the module with "),ea=s("a"),Ti=n("EvaluationModule.add()"),qi=n(" or "),ta=s("a"),Ai=n("EvaluationModule.add_batch()"),Pi=n(" and the score is calculated at the end with "),aa=s("a"),Ci=n("EvaluationModule.compute()"),Di=n(". Alternatively, one can pass all the inputs at once to "),fs=s("code"),Oi=n("compute()"),Si=n(". Let\u2019s have a look at the two approaches."),to=p(),ae=s("h3"),Ce=s("a"),vs=s("span"),d(bt.$$.fragment),Ii=p(),ys=s("span"),Ni=n("How to compute"),ao=p(),S=s("p"),Mi=n("The simplest way to calculate the score of an evaluation module is by calling "),gs=s("code"),Hi=n("compute()"),Fi=n(" directly with the necessary inputs. Simply pass the inputs as seen in "),_s=s("code"),Bi=n("features"),Li=n(" to the "),bs=s("code"),Wi=n("compute()"),Gi=n(" method."),so=p(),d(wt.$$.fragment),lo=p(),B=s("p"),Ri=n("Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),ws=s("code"),Ui=n("add()"),Vi=n(" or "),Es=s("code"),Yi=n("add_batch()"),zi=n(" are useful."),oo=p(),se=s("h3"),De=s("a"),js=s("span"),d(Et.$$.fragment),Ji=p(),$s=s("span"),Ki=n("Calculate a single metric or a batch of metrics"),no=p(),A=s("p"),Qi=n("In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ks=s("code"),Xi=n("compute()"),Zi=n(". With "),xs=s("code"),ec=n("add()"),tc=n(" and "),Ts=s("code"),ac=n("add_batch()"),sc=n(" you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),qs=s("code"),lc=n("add()"),oc=n(":"),ro=p(),d(jt.$$.fragment),io=p(),I=s("p"),nc=n("Once you have gathered all predictions you can call "),As=s("code"),rc=n("compute()"),ic=n(" to compute the score based on all stored values. When getting predictions and references in batches you can use "),Ps=s("code"),cc=n("add_batch()"),pc=n(" which adds a list elements for later processing. The rest works as with "),Cs=s("code"),uc=n("add()"),hc=n(":"),co=p(),d($t.$$.fragment),po=p(),sa=s("p"),dc=n("This is especially useful when you need to get the predictions from your model in batches:"),uo=p(),d(kt.$$.fragment),ho=p(),le=s("h3"),Oe=s("a"),Ds=s("span"),d(xt.$$.fragment),mc=p(),Os=s("span"),fc=n("Distributed evaluation"),mo=p(),P=s("p"),vc=n("Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Ss=s("code"),yc=n("f(AuB) = f(A) + f(B)"),gc=n("), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Is=s("code"),_c=n("f(AuB) \u2260 f(A) + f(B)"),bc=n("), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),Tt=s("a"),wc=n("F1"),Ec=n(" scores of each data subset as your "),Ns=s("strong"),jc=n("final metric"),$c=n("."),fo=p(),la=s("p"),kc=n("A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),vo=p(),L=s("p"),xc=n("\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Ms=s("code"),Tc=n("compute()"),qc=n(" the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Hs=s("code"),Ac=n("compute()"),Pc=n(" will perform the final metric evaluation."),yo=p(),oa=s("p"),Cc=n("This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),go=p(),oe=s("h2"),Se=s("a"),Fs=s("span"),d(qt.$$.fragment),Dc=p(),Bs=s("span"),Oc=n("Configuration"),_o=p(),_=s("p"),Sc=n("Some metrics can be configured with additional settings. For example, "),Ls=s("code"),Ic=n("accuracy"),Nc=n(" has an extra "),Ws=s("code"),Mc=n("normalize"),Hc=n(" setting which returns the fraction of correctly classified samples and is set to "),Gs=s("code"),Fc=n("True"),Bc=n(" by default. To change it you have two options: pass it as a keyword argument with "),Rs=s("code"),Lc=n("load()"),Wc=n(" or during "),Us=s("code"),Gc=n("compute()"),Rc=n(". With "),Vs=s("code"),Uc=n("load()"),Vc=n(", the setting is changed permanently for the module, while passing it to "),Ys=s("code"),Yc=n("compute()"),zc=n(" only changes it for the duration of the "),zs=s("code"),Jc=n("compute()"),Kc=n(" call."),bo=p(),d(At.$$.fragment),wo=p(),Ie=s("p"),Qc=n("This is also useful for the following "),Js=s("code"),Xc=n("combine()"),Zc=n(" method since it allows to load modules with specific settings before combining them."),Eo=p(),ne=s("h2"),Ne=s("a"),Ks=s("span"),d(Pt.$$.fragment),ep=p(),Qs=s("span"),tp=n("Combining several evaluations"),jo=p(),Me=s("p"),ap=n("Often one wants to not only evaluate a single metric but a range of different metrics capturing different aspects of a model. E.g. for classification it is usually a good idea to compute F1-score, recall, and precision in addition to accuracy to get a better picture of model performance. Naturally, you can load a bunch of metrics and call them sequentially. However, a more convenient way is to use the "),na=s("a"),sp=n("combine()"),lp=n(" function to bundle them together:"),$o=p(),d(Ct.$$.fragment),ko=p(),W=s("p"),op=n("The "),Xs=s("code"),np=n("combine"),rp=n(" function accepts both the list of names of the metrics as well as an instantiated modules. The "),Zs=s("code"),ip=n("compute"),cp=n(" call then computes each metric:"),xo=p(),d(Dt.$$.fragment),To=p(),re=s("h2"),He=s("a"),el=s("span"),d(Ot.$$.fragment),pp=p(),tl=s("span"),up=n("Save and push to the Hub"),qo=p(),Fe=s("p"),hp=n("Saving and sharing evaluation results is an important step. We provide the "),ra=s("a"),dp=n("evaluate.save()"),mp=n(" function to easily save metrics results. You can either pass a specific filename or a directory. In the latter case, the results are saved in a file with an automatically created file name. Besides the directory or file name, the function takes any key-value pairs as inputs and stores them in a JSON file."),Ao=p(),d(St.$$.fragment),Po=p(),ia=s("p"),fp=n("The content of the JSON file look like the following:"),Co=p(),d(It.$$.fragment),Do=p(),ca=s("p"),vp=n("In addition to the specified fields, it also contains useful system information for reproducing the results."),Oo=p(),Be=s("p"),yp=n("Besides storing the results locally, you should report them on the model\u2019s repository on the Hub. With the "),pa=s("a"),gp=n("evaluate.push_to_hub()"),_p=n(" function, you can easily report evaluation results to the model\u2019s repository:"),So=p(),d(Nt.$$.fragment),Io=p(),ie=s("h2"),Le=s("a"),al=s("span"),d(Mt.$$.fragment),bp=p(),sl=s("span"),wp=n("Evaluator"),No=p(),k=s("p"),Ep=n("The "),ua=s("a"),jp=n("evaluate.evaluator()"),$p=n(" provides automated evaluation and only requires a model, dataset, metric in contrast to the metrics in "),ll=s("code"),kp=n("EvaluationModule"),xp=n("s that require the model\u2019s predictions. As such it is easier to evaluate a model on a dataset with a given metric as the inference is handled internally. To make that possible it uses the "),Ht=s("a"),Tp=n("pipeline"),qp=n(" abstraction from "),ol=s("code"),Ap=n("transformers"),Pp=n(". However, you can use your own framework as long as it follows the "),nl=s("code"),Cp=n("pipeline"),Dp=n(" interface."),Mo=p(),G=s("p"),Op=n("To make an evaluation with the "),rl=s("code"),Sp=n("evaluator"),Ip=n(" let\u2019s load a "),il=s("code"),Np=n("transformers"),Mp=n(" pipeline (but you can pass your own custom inference class for any framework as long as it follows the pipeline call API) with an model trained on IMDb, the IMDb test split and the accuracy metric."),Ho=p(),d(Ft.$$.fragment),Fo=p(),R=s("p"),Hp=n("Then you can create an evaluator for text classification and pass the three objects to the "),cl=s("code"),Fp=n("compute()"),Bp=n(" method. With the label mapping "),pl=s("code"),Lp=n("evaluate"),Wp=n(" provides a method to align the pipeline outputs with the label column in the dataset:"),Bo=p(),d(Bt.$$.fragment),Lo=p(),U=s("p"),Gp=n("Calculating the value of the metric alone is often not enough to know if a model performs significantly better than another one. With "),ul=s("em"),Rp=n("bootstrapping"),Up=p(),hl=s("code"),Vp=n("evaluate"),Yp=n(" computes confidence intervals and the standard error which helps estimate how stable a score is:"),Wo=p(),d(Lt.$$.fragment),Go=p(),x=s("p"),zp=n("The evaluator expects a "),dl=s("code"),Jp=n('"text"'),Kp=n(" and "),ml=s("code"),Qp=n('"label"'),Xp=n(" column for the data input. If your dataset differs you can provide the columns with the keywords "),fl=s("code"),Zp=n('input_column="text"'),eu=n(" and "),vl=s("code"),tu=n('label_column="label"'),au=n(". Currently only "),yl=s("code"),su=n('"text-classification"'),lu=n(" is supported with more tasks being added in the future."),this.h()},l(e){const i=Zd('[data-svelte="svelte-1phssyn"]',document.head);E=l(i,"META",{name:!0,content:!0}),i.forEach(a),ce=u(e),T=l(e,"H1",{class:!0});var Wt=o(T);O=l(Wt,"A",{id:!0,class:!0,href:!0});var nu=o(O);J=l(nu,"SPAN",{});var ru=o(J);m(D.$$.fragment,ru),ru.forEach(a),nu.forEach(a),H=u(Wt),pe=l(Wt,"SPAN",{});var iu=o(pe);Cn=r(iu,"A quick tour"),iu.forEach(a),Wt.forEach(a),El=u(e),Gt=l(e,"P",{});var cu=o(Gt);Dn=r(cu,"\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),cu.forEach(a),jl=u(e),K=l(e,"H2",{class:!0});var Uo=o(K);ue=l(Uo,"A",{id:!0,class:!0,href:!0});var pu=o(ue);_a=l(pu,"SPAN",{});var uu=o(_a);m(Re.$$.fragment,uu),uu.forEach(a),pu.forEach(a),On=u(Uo),ba=l(Uo,"SPAN",{});var hu=o(ba);Sn=r(hu,"Types of evaluations"),hu.forEach(a),Uo.forEach(a),$l=u(e),Rt=l(e,"P",{});var du=o(Rt);In=r(du,"There are different aspects of a typical machine learning pipeline that can be evaluated and for each aspect \u{1F917} Evaluate provides a tool:"),du.forEach(a),kl=u(e),F=l(e,"UL",{});var ha=o(F);he=l(ha,"LI",{});var gl=o(he);wa=l(gl,"STRONG",{});var mu=o(wa);Nn=r(mu,"Metric"),mu.forEach(a),Mn=r(gl,": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Ue=l(gl,"A",{href:!0,rel:!0});var fu=o(Ue);Hn=r(fu,"evaluate-metric"),fu.forEach(a),Fn=r(gl,"."),gl.forEach(a),Bn=u(ha),de=l(ha,"LI",{});var _l=o(de);Ea=l(_l,"STRONG",{});var vu=o(Ea);Ln=r(vu,"Comparison"),vu.forEach(a),Wn=r(_l,": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at "),Ve=l(_l,"A",{href:!0,rel:!0});var yu=o(Ve);Gn=r(yu,"evaluate-comparison"),yu.forEach(a),Rn=r(_l,"."),_l.forEach(a),Un=u(ha),me=l(ha,"LI",{});var bl=o(me);ja=l(bl,"STRONG",{});var gu=o(ja);Vn=r(gu,"Measurement"),gu.forEach(a),Yn=r(bl,": The dataset is as important as the model trained on it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),Ye=l(bl,"A",{href:!0,rel:!0});var _u=o(Ye);zn=r(_u,"evaluate-measurement"),_u.forEach(a),Jn=r(bl,"."),bl.forEach(a),ha.forEach(a),xl=u(e),fe=l(e,"P",{});var Vo=o(fe);Kn=r(Vo,"Each of these evaluation modules live on Hugging Face Hub as a Space. They come with an interactive widget and a documentation card documenting its use and limitations. For example "),ze=l(Vo,"A",{href:!0,rel:!0});var bu=o(ze);Qn=r(bu,"accuracy"),bu.forEach(a),Xn=r(Vo,":"),Vo.forEach(a),Tl=u(e),Je=l(e,"DIV",{class:!0});var wu=o(Je);Ut=l(wu,"IMG",{src:!0,width:!0}),wu.forEach(a),ql=u(e),ve=l(e,"P",{});var Yo=o(ve);Zn=r(Yo,"Each metric, comparison, and measurement is a separate Python module, but for using any of them, there is a single entry point: "),Vt=l(Yo,"A",{href:!0});var Eu=o(Vt);er=r(Eu,"evaluate.load()"),Eu.forEach(a),tr=r(Yo,"!"),Yo.forEach(a),Al=u(e),Q=l(e,"H2",{class:!0});var zo=o(Q);ye=l(zo,"A",{id:!0,class:!0,href:!0});var ju=o(ye);$a=l(ju,"SPAN",{});var $u=o($a);m(Ke.$$.fragment,$u),$u.forEach(a),ju.forEach(a),ar=u(zo),ka=l(zo,"SPAN",{});var ku=o(ka);sr=r(ku,"Load"),ku.forEach(a),zo.forEach(a),Pl=u(e),ge=l(e,"P",{});var Jo=o(ge);lr=r(Jo,"Any metric, comparison, or measurement is loaded with the "),xa=l(Jo,"CODE",{});var xu=o(xa);or=r(xu,"evaluate.load"),xu.forEach(a),nr=r(Jo," function:"),Jo.forEach(a),Cl=u(e),m(Qe.$$.fragment,e),Dl=u(e),Yt=l(e,"P",{});var Tu=o(Yt);rr=r(Tu,"If you want to make sure you are loading the right type of evaluation (especially if there are name clashes) you can explicitly pass the type:"),Tu.forEach(a),Ol=u(e),m(Xe.$$.fragment,e),Sl=u(e),X=l(e,"H3",{class:!0});var Ko=o(X);_e=l(Ko,"A",{id:!0,class:!0,href:!0});var qu=o(_e);Ta=l(qu,"SPAN",{});var Au=o(Ta);m(Ze.$$.fragment,Au),Au.forEach(a),qu.forEach(a),ir=u(Ko),qa=l(Ko,"SPAN",{});var Pu=o(qa);cr=r(Pu,"Community modules"),Pu.forEach(a),Ko.forEach(a),Il=u(e),zt=l(e,"P",{});var Cu=o(zt);pr=r(Cu,"Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by specifying the repository ID of the metric implementation:"),Cu.forEach(a),Nl=u(e),m(et.$$.fragment,e),Ml=u(e),be=l(e,"P",{});var Qo=o(be);ur=r(Qo,"See the "),Jt=l(Qo,"A",{href:!0});var Du=o(Jt);hr=r(Du,"Creating and Sharing Guide"),Du.forEach(a),dr=r(Qo," for information about uploading custom metrics."),Qo.forEach(a),Hl=u(e),Z=l(e,"H3",{class:!0});var Xo=o(Z);we=l(Xo,"A",{id:!0,class:!0,href:!0});var Ou=o(we);Aa=l(Ou,"SPAN",{});var Su=o(Aa);m(tt.$$.fragment,Su),Su.forEach(a),Ou.forEach(a),mr=u(Xo),Pa=l(Xo,"SPAN",{});var Iu=o(Pa);fr=r(Iu,"List available modules"),Iu.forEach(a),Xo.forEach(a),Fl=u(e),Ee=l(e,"P",{});var Zo=o(Ee);vr=r(Zo,"With "),Kt=l(Zo,"A",{href:!0});var Nu=o(Kt);yr=r(Nu,"list_evaluation_modules()"),Nu.forEach(a),gr=r(Zo," you can check what modules are available on the hub. You can also filter for a specific modules and skip community metrics if you want. You can also see additional information such as likes:"),Zo.forEach(a),Bl=u(e),m(at.$$.fragment,e),Ll=u(e),ee=l(e,"H2",{class:!0});var en=o(ee);je=l(en,"A",{id:!0,class:!0,href:!0});var Mu=o(je);Ca=l(Mu,"SPAN",{});var Hu=o(Ca);m(st.$$.fragment,Hu),Hu.forEach(a),Mu.forEach(a),_r=u(en),Da=l(en,"SPAN",{});var Fu=o(Da);br=r(Fu,"Module attributes"),Fu.forEach(a),en.forEach(a),Wl=u(e),$e=l(e,"P",{});var tn=o($e);wr=r(tn,"All evalution modules come with a range of useful attributes that help to use a module stored in a "),Qt=l(tn,"A",{href:!0});var Bu=o(Qt);Er=r(Bu,"EvaluationModuleInfo"),Bu.forEach(a),jr=r(tn," object."),tn.forEach(a),Gl=u(e),ke=l(e,"TABLE",{});var an=o(ke);Oa=l(an,"THEAD",{});var Lu=o(Oa);lt=l(Lu,"TR",{});var sn=o(lt);Sa=l(sn,"TH",{});var Wu=o(Sa);$r=r(Wu,"Attribute"),Wu.forEach(a),kr=u(sn),Ia=l(sn,"TH",{});var Gu=o(Ia);xr=r(Gu,"Description"),Gu.forEach(a),sn.forEach(a),Lu.forEach(a),Tr=u(an),b=l(an,"TBODY",{});var j=o(b);ot=l(j,"TR",{});var ln=o(ot);Na=l(ln,"TD",{});var Ru=o(Na);Ma=l(Ru,"CODE",{});var Uu=o(Ma);qr=r(Uu,"description"),Uu.forEach(a),Ru.forEach(a),Ar=u(ln),Ha=l(ln,"TD",{});var Vu=o(Ha);Pr=r(Vu,"A short description of the evaluation module."),Vu.forEach(a),ln.forEach(a),Cr=u(j),nt=l(j,"TR",{});var on=o(nt);Fa=l(on,"TD",{});var Yu=o(Fa);Ba=l(Yu,"CODE",{});var zu=o(Ba);Dr=r(zu,"citation"),zu.forEach(a),Yu.forEach(a),Or=u(on),La=l(on,"TD",{});var Ju=o(La);Sr=r(Ju,"A BibTex string for citation when available."),Ju.forEach(a),on.forEach(a),Ir=u(j),rt=l(j,"TR",{});var nn=o(rt);Wa=l(nn,"TD",{});var Ku=o(Wa);Ga=l(Ku,"CODE",{});var Qu=o(Ga);Nr=r(Qu,"config"),Qu.forEach(a),Ku.forEach(a),Mr=u(nn),it=l(nn,"TD",{});var rn=o(it);Hr=r(rn,"A "),Ra=l(rn,"CODE",{});var Xu=o(Ra);Fr=r(Xu,"dataclass"),Xu.forEach(a),Br=r(rn," containing the settings of the module."),rn.forEach(a),nn.forEach(a),Lr=u(j),ct=l(j,"TR",{});var cn=o(ct);Ua=l(cn,"TD",{});var Zu=o(Ua);Va=l(Zu,"CODE",{});var eh=o(Va);Wr=r(eh,"features"),eh.forEach(a),Zu.forEach(a),Gr=u(cn),pt=l(cn,"TD",{});var pn=o(pt);Rr=r(pn,"A "),Ya=l(pn,"CODE",{});var th=o(Ya);Ur=r(th,"Features"),th.forEach(a),Vr=r(pn," object defining the input format."),pn.forEach(a),cn.forEach(a),Yr=u(j),ut=l(j,"TR",{});var un=o(ut);za=l(un,"TD",{});var ah=o(za);Ja=l(ah,"CODE",{});var sh=o(Ja);zr=r(sh,"inputs_description"),sh.forEach(a),ah.forEach(a),Jr=u(un),Ka=l(un,"TD",{});var lh=o(Ka);Kr=r(lh,"This is equivalent to the modules docstring."),lh.forEach(a),un.forEach(a),Qr=u(j),ht=l(j,"TR",{});var hn=o(ht);Qa=l(hn,"TD",{});var oh=o(Qa);Xa=l(oh,"CODE",{});var nh=o(Xa);Xr=r(nh,"homepage"),nh.forEach(a),oh.forEach(a),Zr=u(hn),Za=l(hn,"TD",{});var rh=o(Za);ei=r(rh,"The homepage of the module."),rh.forEach(a),hn.forEach(a),ti=u(j),dt=l(j,"TR",{});var dn=o(dt);es=l(dn,"TD",{});var ih=o(es);ts=l(ih,"CODE",{});var ch=o(ts);ai=r(ch,"license"),ch.forEach(a),ih.forEach(a),si=u(dn),as=l(dn,"TD",{});var ph=o(as);li=r(ph,"The license of the module."),ph.forEach(a),dn.forEach(a),oi=u(j),mt=l(j,"TR",{});var mn=o(mt);ss=l(mn,"TD",{});var uh=o(ss);ls=l(uh,"CODE",{});var hh=o(ls);ni=r(hh,"codebase_urls"),hh.forEach(a),uh.forEach(a),ri=u(mn),os=l(mn,"TD",{});var dh=o(os);ii=r(dh,"Link to the code behind the module."),dh.forEach(a),mn.forEach(a),ci=u(j),ft=l(j,"TR",{});var fn=o(ft);ns=l(fn,"TD",{});var mh=o(ns);rs=l(mh,"CODE",{});var fh=o(rs);pi=r(fh,"reference_urls"),fh.forEach(a),mh.forEach(a),ui=u(fn),is=l(fn,"TD",{});var vh=o(is);hi=r(vh,"Additional reference URLs."),vh.forEach(a),fn.forEach(a),j.forEach(a),an.forEach(a),Rl=u(e),xe=l(e,"P",{});var vn=o(xe);di=r(vn,"Let\u2019s have a look at a few examples. First, let\u2019s look at the "),cs=l(vn,"CODE",{});var yh=o(cs);mi=r(yh,"description"),yh.forEach(a),fi=r(vn," attribute of the accuracy metric:"),vn.forEach(a),Ul=u(e),m(vt.$$.fragment,e),Vl=u(e),Te=l(e,"P",{});var yn=o(Te);vi=r(yn,"You can see that it describes how the metric works in theory. If you use this metric for your work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),ps=l(yn,"CODE",{});var gh=o(ps);yi=r(gh,"citation"),gh.forEach(a),gi=r(yn," attribute:"),yn.forEach(a),Yl=u(e),m(yt.$$.fragment,e),zl=u(e),Xt=l(e,"P",{});var _h=o(Xt);_i=r(_h,"Before we can apply a metric or other evaluation module to a use-case, we need to know what the input format of the metric is:"),_h.forEach(a),Jl=u(e),m(gt.$$.fragment,e),Kl=u(e),m(qe.$$.fragment,e),Ql=u(e),te=l(e,"H2",{class:!0});var gn=o(te);Ae=l(gn,"A",{id:!0,class:!0,href:!0});var bh=o(Ae);us=l(bh,"SPAN",{});var wh=o(us);m(_t.$$.fragment,wh),wh.forEach(a),bh.forEach(a),bi=u(gn),hs=l(gn,"SPAN",{});var Eh=o(hs);wi=r(Eh,"Compute"),Eh.forEach(a),gn.forEach(a),Xl=u(e),Zt=l(e,"P",{});var jh=o(Zt);Ei=r(jh,"Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),jh.forEach(a),Zl=u(e),Pe=l(e,"OL",{});var _n=o(Pe);ds=l(_n,"LI",{});var $h=o(ds);ji=r($h,"All-in-one"),$h.forEach(a),$i=u(_n),ms=l(_n,"LI",{});var kh=o(ms);ki=r(kh,"Incremental"),kh.forEach(a),_n.forEach(a),eo=u(e),q=l(e,"P",{});var V=o(q);xi=r(V,"In the incremental approach the necessary inputs are added to the module with "),ea=l(V,"A",{href:!0});var xh=o(ea);Ti=r(xh,"EvaluationModule.add()"),xh.forEach(a),qi=r(V," or "),ta=l(V,"A",{href:!0});var Th=o(ta);Ai=r(Th,"EvaluationModule.add_batch()"),Th.forEach(a),Pi=r(V," and the score is calculated at the end with "),aa=l(V,"A",{href:!0});var qh=o(aa);Ci=r(qh,"EvaluationModule.compute()"),qh.forEach(a),Di=r(V,". Alternatively, one can pass all the inputs at once to "),fs=l(V,"CODE",{});var Ah=o(fs);Oi=r(Ah,"compute()"),Ah.forEach(a),Si=r(V,". Let\u2019s have a look at the two approaches."),V.forEach(a),to=u(e),ae=l(e,"H3",{class:!0});var bn=o(ae);Ce=l(bn,"A",{id:!0,class:!0,href:!0});var Ph=o(Ce);vs=l(Ph,"SPAN",{});var Ch=o(vs);m(bt.$$.fragment,Ch),Ch.forEach(a),Ph.forEach(a),Ii=u(bn),ys=l(bn,"SPAN",{});var Dh=o(ys);Ni=r(Dh,"How to compute"),Dh.forEach(a),bn.forEach(a),ao=u(e),S=l(e,"P",{});var We=o(S);Mi=r(We,"The simplest way to calculate the score of an evaluation module is by calling "),gs=l(We,"CODE",{});var Oh=o(gs);Hi=r(Oh,"compute()"),Oh.forEach(a),Fi=r(We," directly with the necessary inputs. Simply pass the inputs as seen in "),_s=l(We,"CODE",{});var Sh=o(_s);Bi=r(Sh,"features"),Sh.forEach(a),Li=r(We," to the "),bs=l(We,"CODE",{});var Ih=o(bs);Wi=r(Ih,"compute()"),Ih.forEach(a),Gi=r(We," method."),We.forEach(a),so=u(e),m(wt.$$.fragment,e),lo=u(e),B=l(e,"P",{});var da=o(B);Ri=r(da,"Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),ws=l(da,"CODE",{});var Nh=o(ws);Ui=r(Nh,"add()"),Nh.forEach(a),Vi=r(da," or "),Es=l(da,"CODE",{});var Mh=o(Es);Yi=r(Mh,"add_batch()"),Mh.forEach(a),zi=r(da," are useful."),da.forEach(a),oo=u(e),se=l(e,"H3",{class:!0});var wn=o(se);De=l(wn,"A",{id:!0,class:!0,href:!0});var Hh=o(De);js=l(Hh,"SPAN",{});var Fh=o(js);m(Et.$$.fragment,Fh),Fh.forEach(a),Hh.forEach(a),Ji=u(wn),$s=l(wn,"SPAN",{});var Bh=o($s);Ki=r(Bh,"Calculate a single metric or a batch of metrics"),Bh.forEach(a),wn.forEach(a),no=u(e),A=l(e,"P",{});var Y=o(A);Qi=r(Y,"In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ks=l(Y,"CODE",{});var Lh=o(ks);Xi=r(Lh,"compute()"),Lh.forEach(a),Zi=r(Y,". With "),xs=l(Y,"CODE",{});var Wh=o(xs);ec=r(Wh,"add()"),Wh.forEach(a),tc=r(Y," and "),Ts=l(Y,"CODE",{});var Gh=o(Ts);ac=r(Gh,"add_batch()"),Gh.forEach(a),sc=r(Y," you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),qs=l(Y,"CODE",{});var Rh=o(qs);lc=r(Rh,"add()"),Rh.forEach(a),oc=r(Y,":"),Y.forEach(a),ro=u(e),m(jt.$$.fragment,e),io=u(e),I=l(e,"P",{});var Ge=o(I);nc=r(Ge,"Once you have gathered all predictions you can call "),As=l(Ge,"CODE",{});var Uh=o(As);rc=r(Uh,"compute()"),Uh.forEach(a),ic=r(Ge," to compute the score based on all stored values. When getting predictions and references in batches you can use "),Ps=l(Ge,"CODE",{});var Vh=o(Ps);cc=r(Vh,"add_batch()"),Vh.forEach(a),pc=r(Ge," which adds a list elements for later processing. The rest works as with "),Cs=l(Ge,"CODE",{});var Yh=o(Cs);uc=r(Yh,"add()"),Yh.forEach(a),hc=r(Ge,":"),Ge.forEach(a),co=u(e),m($t.$$.fragment,e),po=u(e),sa=l(e,"P",{});var zh=o(sa);dc=r(zh,"This is especially useful when you need to get the predictions from your model in batches:"),zh.forEach(a),uo=u(e),m(kt.$$.fragment,e),ho=u(e),le=l(e,"H3",{class:!0});var En=o(le);Oe=l(En,"A",{id:!0,class:!0,href:!0});var Jh=o(Oe);Ds=l(Jh,"SPAN",{});var Kh=o(Ds);m(xt.$$.fragment,Kh),Kh.forEach(a),Jh.forEach(a),mc=u(En),Os=l(En,"SPAN",{});var Qh=o(Os);fc=r(Qh,"Distributed evaluation"),Qh.forEach(a),En.forEach(a),mo=u(e),P=l(e,"P",{});var z=o(P);vc=r(z,"Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Ss=l(z,"CODE",{});var Xh=o(Ss);yc=r(Xh,"f(AuB) = f(A) + f(B)"),Xh.forEach(a),gc=r(z,"), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Is=l(z,"CODE",{});var Zh=o(Is);_c=r(Zh,"f(AuB) \u2260 f(A) + f(B)"),Zh.forEach(a),bc=r(z,"), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),Tt=l(z,"A",{href:!0,rel:!0});var ed=o(Tt);wc=r(ed,"F1"),ed.forEach(a),Ec=r(z," scores of each data subset as your "),Ns=l(z,"STRONG",{});var td=o(Ns);jc=r(td,"final metric"),td.forEach(a),$c=r(z,"."),z.forEach(a),fo=u(e),la=l(e,"P",{});var ad=o(la);kc=r(ad,"A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),ad.forEach(a),vo=u(e),L=l(e,"P",{});var ma=o(L);xc=r(ma,"\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Ms=l(ma,"CODE",{});var sd=o(Ms);Tc=r(sd,"compute()"),sd.forEach(a),qc=r(ma," the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Hs=l(ma,"CODE",{});var ld=o(Hs);Ac=r(ld,"compute()"),ld.forEach(a),Pc=r(ma," will perform the final metric evaluation."),ma.forEach(a),yo=u(e),oa=l(e,"P",{});var od=o(oa);Cc=r(od,"This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),od.forEach(a),go=u(e),oe=l(e,"H2",{class:!0});var jn=o(oe);Se=l(jn,"A",{id:!0,class:!0,href:!0});var nd=o(Se);Fs=l(nd,"SPAN",{});var rd=o(Fs);m(qt.$$.fragment,rd),rd.forEach(a),nd.forEach(a),Dc=u(jn),Bs=l(jn,"SPAN",{});var id=o(Bs);Oc=r(id,"Configuration"),id.forEach(a),jn.forEach(a),_o=u(e),_=l(e,"P",{});var $=o(_);Sc=r($,"Some metrics can be configured with additional settings. For example, "),Ls=l($,"CODE",{});var cd=o(Ls);Ic=r(cd,"accuracy"),cd.forEach(a),Nc=r($," has an extra "),Ws=l($,"CODE",{});var pd=o(Ws);Mc=r(pd,"normalize"),pd.forEach(a),Hc=r($," setting which returns the fraction of correctly classified samples and is set to "),Gs=l($,"CODE",{});var ud=o(Gs);Fc=r(ud,"True"),ud.forEach(a),Bc=r($," by default. To change it you have two options: pass it as a keyword argument with "),Rs=l($,"CODE",{});var hd=o(Rs);Lc=r(hd,"load()"),hd.forEach(a),Wc=r($," or during "),Us=l($,"CODE",{});var dd=o(Us);Gc=r(dd,"compute()"),dd.forEach(a),Rc=r($,". With "),Vs=l($,"CODE",{});var md=o(Vs);Uc=r(md,"load()"),md.forEach(a),Vc=r($,", the setting is changed permanently for the module, while passing it to "),Ys=l($,"CODE",{});var fd=o(Ys);Yc=r(fd,"compute()"),fd.forEach(a),zc=r($," only changes it for the duration of the "),zs=l($,"CODE",{});var vd=o(zs);Jc=r(vd,"compute()"),vd.forEach(a),Kc=r($," call."),$.forEach(a),bo=u(e),m(At.$$.fragment,e),wo=u(e),Ie=l(e,"P",{});var $n=o(Ie);Qc=r($n,"This is also useful for the following "),Js=l($n,"CODE",{});var yd=o(Js);Xc=r(yd,"combine()"),yd.forEach(a),Zc=r($n," method since it allows to load modules with specific settings before combining them."),$n.forEach(a),Eo=u(e),ne=l(e,"H2",{class:!0});var kn=o(ne);Ne=l(kn,"A",{id:!0,class:!0,href:!0});var gd=o(Ne);Ks=l(gd,"SPAN",{});var _d=o(Ks);m(Pt.$$.fragment,_d),_d.forEach(a),gd.forEach(a),ep=u(kn),Qs=l(kn,"SPAN",{});var bd=o(Qs);tp=r(bd,"Combining several evaluations"),bd.forEach(a),kn.forEach(a),jo=u(e),Me=l(e,"P",{});var xn=o(Me);ap=r(xn,"Often one wants to not only evaluate a single metric but a range of different metrics capturing different aspects of a model. E.g. for classification it is usually a good idea to compute F1-score, recall, and precision in addition to accuracy to get a better picture of model performance. Naturally, you can load a bunch of metrics and call them sequentially. However, a more convenient way is to use the "),na=l(xn,"A",{href:!0});var wd=o(na);sp=r(wd,"combine()"),wd.forEach(a),lp=r(xn," function to bundle them together:"),xn.forEach(a),$o=u(e),m(Ct.$$.fragment,e),ko=u(e),W=l(e,"P",{});var fa=o(W);op=r(fa,"The "),Xs=l(fa,"CODE",{});var Ed=o(Xs);np=r(Ed,"combine"),Ed.forEach(a),rp=r(fa," function accepts both the list of names of the metrics as well as an instantiated modules. The "),Zs=l(fa,"CODE",{});var jd=o(Zs);ip=r(jd,"compute"),jd.forEach(a),cp=r(fa," call then computes each metric:"),fa.forEach(a),xo=u(e),m(Dt.$$.fragment,e),To=u(e),re=l(e,"H2",{class:!0});var Tn=o(re);He=l(Tn,"A",{id:!0,class:!0,href:!0});var $d=o(He);el=l($d,"SPAN",{});var kd=o(el);m(Ot.$$.fragment,kd),kd.forEach(a),$d.forEach(a),pp=u(Tn),tl=l(Tn,"SPAN",{});var xd=o(tl);up=r(xd,"Save and push to the Hub"),xd.forEach(a),Tn.forEach(a),qo=u(e),Fe=l(e,"P",{});var qn=o(Fe);hp=r(qn,"Saving and sharing evaluation results is an important step. We provide the "),ra=l(qn,"A",{href:!0});var Td=o(ra);dp=r(Td,"evaluate.save()"),Td.forEach(a),mp=r(qn," function to easily save metrics results. You can either pass a specific filename or a directory. In the latter case, the results are saved in a file with an automatically created file name. Besides the directory or file name, the function takes any key-value pairs as inputs and stores them in a JSON file."),qn.forEach(a),Ao=u(e),m(St.$$.fragment,e),Po=u(e),ia=l(e,"P",{});var qd=o(ia);fp=r(qd,"The content of the JSON file look like the following:"),qd.forEach(a),Co=u(e),m(It.$$.fragment,e),Do=u(e),ca=l(e,"P",{});var Ad=o(ca);vp=r(Ad,"In addition to the specified fields, it also contains useful system information for reproducing the results."),Ad.forEach(a),Oo=u(e),Be=l(e,"P",{});var An=o(Be);yp=r(An,"Besides storing the results locally, you should report them on the model\u2019s repository on the Hub. With the "),pa=l(An,"A",{href:!0});var Pd=o(pa);gp=r(Pd,"evaluate.push_to_hub()"),Pd.forEach(a),_p=r(An," function, you can easily report evaluation results to the model\u2019s repository:"),An.forEach(a),So=u(e),m(Nt.$$.fragment,e),Io=u(e),ie=l(e,"H2",{class:!0});var Pn=o(ie);Le=l(Pn,"A",{id:!0,class:!0,href:!0});var Cd=o(Le);al=l(Cd,"SPAN",{});var Dd=o(al);m(Mt.$$.fragment,Dd),Dd.forEach(a),Cd.forEach(a),bp=u(Pn),sl=l(Pn,"SPAN",{});var Od=o(sl);wp=r(Od,"Evaluator"),Od.forEach(a),Pn.forEach(a),No=u(e),k=l(e,"P",{});var N=o(k);Ep=r(N,"The "),ua=l(N,"A",{href:!0});var Sd=o(ua);jp=r(Sd,"evaluate.evaluator()"),Sd.forEach(a),$p=r(N," provides automated evaluation and only requires a model, dataset, metric in contrast to the metrics in "),ll=l(N,"CODE",{});var Id=o(ll);kp=r(Id,"EvaluationModule"),Id.forEach(a),xp=r(N,"s that require the model\u2019s predictions. As such it is easier to evaluate a model on a dataset with a given metric as the inference is handled internally. To make that possible it uses the "),Ht=l(N,"A",{href:!0,rel:!0});var Nd=o(Ht);Tp=r(Nd,"pipeline"),Nd.forEach(a),qp=r(N," abstraction from "),ol=l(N,"CODE",{});var Md=o(ol);Ap=r(Md,"transformers"),Md.forEach(a),Pp=r(N,". However, you can use your own framework as long as it follows the "),nl=l(N,"CODE",{});var Hd=o(nl);Cp=r(Hd,"pipeline"),Hd.forEach(a),Dp=r(N," interface."),N.forEach(a),Mo=u(e),G=l(e,"P",{});var va=o(G);Op=r(va,"To make an evaluation with the "),rl=l(va,"CODE",{});var Fd=o(rl);Sp=r(Fd,"evaluator"),Fd.forEach(a),Ip=r(va," let\u2019s load a "),il=l(va,"CODE",{});var Bd=o(il);Np=r(Bd,"transformers"),Bd.forEach(a),Mp=r(va," pipeline (but you can pass your own custom inference class for any framework as long as it follows the pipeline call API) with an model trained on IMDb, the IMDb test split and the accuracy metric."),va.forEach(a),Ho=u(e),m(Ft.$$.fragment,e),Fo=u(e),R=l(e,"P",{});var ya=o(R);Hp=r(ya,"Then you can create an evaluator for text classification and pass the three objects to the "),cl=l(ya,"CODE",{});var Ld=o(cl);Fp=r(Ld,"compute()"),Ld.forEach(a),Bp=r(ya," method. With the label mapping "),pl=l(ya,"CODE",{});var Wd=o(pl);Lp=r(Wd,"evaluate"),Wd.forEach(a),Wp=r(ya," provides a method to align the pipeline outputs with the label column in the dataset:"),ya.forEach(a),Bo=u(e),m(Bt.$$.fragment,e),Lo=u(e),U=l(e,"P",{});var ga=o(U);Gp=r(ga,"Calculating the value of the metric alone is often not enough to know if a model performs significantly better than another one. With "),ul=l(ga,"EM",{});var Gd=o(ul);Rp=r(Gd,"bootstrapping"),Gd.forEach(a),Up=u(ga),hl=l(ga,"CODE",{});var Rd=o(hl);Vp=r(Rd,"evaluate"),Rd.forEach(a),Yp=r(ga," computes confidence intervals and the standard error which helps estimate how stable a score is:"),ga.forEach(a),Wo=u(e),m(Lt.$$.fragment,e),Go=u(e),x=l(e,"P",{});var M=o(x);zp=r(M,"The evaluator expects a "),dl=l(M,"CODE",{});var Ud=o(dl);Jp=r(Ud,'"text"'),Ud.forEach(a),Kp=r(M," and "),ml=l(M,"CODE",{});var Vd=o(ml);Qp=r(Vd,'"label"'),Vd.forEach(a),Xp=r(M," column for the data input. If your dataset differs you can provide the columns with the keywords "),fl=l(M,"CODE",{});var Yd=o(fl);Zp=r(Yd,'input_column="text"'),Yd.forEach(a),eu=r(M," and "),vl=l(M,"CODE",{});var zd=o(vl);tu=r(zd,'label_column="label"'),zd.forEach(a),au=r(M,". Currently only "),yl=l(M,"CODE",{});var Jd=o(yl);su=r(Jd,'"text-classification"'),Jd.forEach(a),lu=r(M," is supported with more tasks being added in the future."),M.forEach(a),this.h()},h(){h(E,"name","hf:doc:metadata"),h(E,"content",JSON.stringify(om)),h(O,"id","a-quick-tour"),h(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(O,"href","#a-quick-tour"),h(T,"class","relative group"),h(ue,"id","types-of-evaluations"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#types-of-evaluations"),h(K,"class","relative group"),h(Ue,"href","https://huggingface.co/evaluate-metric"),h(Ue,"rel","nofollow"),h(Ve,"href","https://huggingface.co/evaluate-comparison"),h(Ve,"rel","nofollow"),h(Ye,"href","https://huggingface.co/evaluate-measurement"),h(Ye,"rel","nofollow"),h(ze,"href","https://huggingface.co/spaces/evaluate-metric/accuracy"),h(ze,"rel","nofollow"),em(Ut.src,ou="https://huggingface.co/datasets/evaluate/media/resolve/main/metric-widget.png")||h(Ut,"src",ou),h(Ut,"width","400"),h(Je,"class","flex justify-center"),h(Vt,"href","/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load"),h(ye,"id","load"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#load"),h(Q,"class","relative group"),h(_e,"id","community-modules"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#community-modules"),h(X,"class","relative group"),h(Jt,"href","/docs/evaluate/main/en/creating_and_sharing"),h(we,"id","list-available-modules"),h(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(we,"href","#list-available-modules"),h(Z,"class","relative group"),h(Kt,"href","/docs/evaluate/main/en/package_reference/loading_methods#evaluate.list_evaluation_modules"),h(je,"id","module-attributes"),h(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(je,"href","#module-attributes"),h(ee,"class","relative group"),h(Qt,"href","/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModuleInfo"),h(Ae,"id","compute"),h(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ae,"href","#compute"),h(te,"class","relative group"),h(ea,"href","/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.add"),h(ta,"href","/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.add_batch"),h(aa,"href","/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute"),h(Ce,"id","how-to-compute"),h(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ce,"href","#how-to-compute"),h(ae,"class","relative group"),h(De,"id","calculate-a-single-metric-or-a-batch-of-metrics"),h(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(De,"href","#calculate-a-single-metric-or-a-batch-of-metrics"),h(se,"class","relative group"),h(Oe,"id","distributed-evaluation"),h(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Oe,"href","#distributed-evaluation"),h(le,"class","relative group"),h(Tt,"href","https://huggingface.co/spaces/evaluate-metric/f1"),h(Tt,"rel","nofollow"),h(Se,"id","configuration"),h(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Se,"href","#configuration"),h(oe,"class","relative group"),h(Ne,"id","combining-several-evaluations"),h(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ne,"href","#combining-several-evaluations"),h(ne,"class","relative group"),h(na,"href","/docs/evaluate/main/en/package_reference/main_classes#evaluate.combine"),h(He,"id","save-and-push-to-the-hub"),h(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(He,"href","#save-and-push-to-the-hub"),h(re,"class","relative group"),h(ra,"href","/docs/evaluate/main/en/package_reference/saving_methods#evaluate.save"),h(pa,"href","/docs/evaluate/main/en/package_reference/hub_methods#evaluate.push_to_hub"),h(Le,"id","evaluator"),h(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Le,"href","#evaluator"),h(ie,"class","relative group"),h(ua,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),h(Ht,"href","https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),h(Ht,"rel","nofollow")},m(e,i){t(document.head,E),c(e,ce,i),c(e,T,i),t(T,O),t(O,J),f(D,J,null),t(T,H),t(T,pe),t(pe,Cn),c(e,El,i),c(e,Gt,i),t(Gt,Dn),c(e,jl,i),c(e,K,i),t(K,ue),t(ue,_a),f(Re,_a,null),t(K,On),t(K,ba),t(ba,Sn),c(e,$l,i),c(e,Rt,i),t(Rt,In),c(e,kl,i),c(e,F,i),t(F,he),t(he,wa),t(wa,Nn),t(he,Mn),t(he,Ue),t(Ue,Hn),t(he,Fn),t(F,Bn),t(F,de),t(de,Ea),t(Ea,Ln),t(de,Wn),t(de,Ve),t(Ve,Gn),t(de,Rn),t(F,Un),t(F,me),t(me,ja),t(ja,Vn),t(me,Yn),t(me,Ye),t(Ye,zn),t(me,Jn),c(e,xl,i),c(e,fe,i),t(fe,Kn),t(fe,ze),t(ze,Qn),t(fe,Xn),c(e,Tl,i),c(e,Je,i),t(Je,Ut),c(e,ql,i),c(e,ve,i),t(ve,Zn),t(ve,Vt),t(Vt,er),t(ve,tr),c(e,Al,i),c(e,Q,i),t(Q,ye),t(ye,$a),f(Ke,$a,null),t(Q,ar),t(Q,ka),t(ka,sr),c(e,Pl,i),c(e,ge,i),t(ge,lr),t(ge,xa),t(xa,or),t(ge,nr),c(e,Cl,i),f(Qe,e,i),c(e,Dl,i),c(e,Yt,i),t(Yt,rr),c(e,Ol,i),f(Xe,e,i),c(e,Sl,i),c(e,X,i),t(X,_e),t(_e,Ta),f(Ze,Ta,null),t(X,ir),t(X,qa),t(qa,cr),c(e,Il,i),c(e,zt,i),t(zt,pr),c(e,Nl,i),f(et,e,i),c(e,Ml,i),c(e,be,i),t(be,ur),t(be,Jt),t(Jt,hr),t(be,dr),c(e,Hl,i),c(e,Z,i),t(Z,we),t(we,Aa),f(tt,Aa,null),t(Z,mr),t(Z,Pa),t(Pa,fr),c(e,Fl,i),c(e,Ee,i),t(Ee,vr),t(Ee,Kt),t(Kt,yr),t(Ee,gr),c(e,Bl,i),f(at,e,i),c(e,Ll,i),c(e,ee,i),t(ee,je),t(je,Ca),f(st,Ca,null),t(ee,_r),t(ee,Da),t(Da,br),c(e,Wl,i),c(e,$e,i),t($e,wr),t($e,Qt),t(Qt,Er),t($e,jr),c(e,Gl,i),c(e,ke,i),t(ke,Oa),t(Oa,lt),t(lt,Sa),t(Sa,$r),t(lt,kr),t(lt,Ia),t(Ia,xr),t(ke,Tr),t(ke,b),t(b,ot),t(ot,Na),t(Na,Ma),t(Ma,qr),t(ot,Ar),t(ot,Ha),t(Ha,Pr),t(b,Cr),t(b,nt),t(nt,Fa),t(Fa,Ba),t(Ba,Dr),t(nt,Or),t(nt,La),t(La,Sr),t(b,Ir),t(b,rt),t(rt,Wa),t(Wa,Ga),t(Ga,Nr),t(rt,Mr),t(rt,it),t(it,Hr),t(it,Ra),t(Ra,Fr),t(it,Br),t(b,Lr),t(b,ct),t(ct,Ua),t(Ua,Va),t(Va,Wr),t(ct,Gr),t(ct,pt),t(pt,Rr),t(pt,Ya),t(Ya,Ur),t(pt,Vr),t(b,Yr),t(b,ut),t(ut,za),t(za,Ja),t(Ja,zr),t(ut,Jr),t(ut,Ka),t(Ka,Kr),t(b,Qr),t(b,ht),t(ht,Qa),t(Qa,Xa),t(Xa,Xr),t(ht,Zr),t(ht,Za),t(Za,ei),t(b,ti),t(b,dt),t(dt,es),t(es,ts),t(ts,ai),t(dt,si),t(dt,as),t(as,li),t(b,oi),t(b,mt),t(mt,ss),t(ss,ls),t(ls,ni),t(mt,ri),t(mt,os),t(os,ii),t(b,ci),t(b,ft),t(ft,ns),t(ns,rs),t(rs,pi),t(ft,ui),t(ft,is),t(is,hi),c(e,Rl,i),c(e,xe,i),t(xe,di),t(xe,cs),t(cs,mi),t(xe,fi),c(e,Ul,i),f(vt,e,i),c(e,Vl,i),c(e,Te,i),t(Te,vi),t(Te,ps),t(ps,yi),t(Te,gi),c(e,Yl,i),f(yt,e,i),c(e,zl,i),c(e,Xt,i),t(Xt,_i),c(e,Jl,i),f(gt,e,i),c(e,Kl,i),f(qe,e,i),c(e,Ql,i),c(e,te,i),t(te,Ae),t(Ae,us),f(_t,us,null),t(te,bi),t(te,hs),t(hs,wi),c(e,Xl,i),c(e,Zt,i),t(Zt,Ei),c(e,Zl,i),c(e,Pe,i),t(Pe,ds),t(ds,ji),t(Pe,$i),t(Pe,ms),t(ms,ki),c(e,eo,i),c(e,q,i),t(q,xi),t(q,ea),t(ea,Ti),t(q,qi),t(q,ta),t(ta,Ai),t(q,Pi),t(q,aa),t(aa,Ci),t(q,Di),t(q,fs),t(fs,Oi),t(q,Si),c(e,to,i),c(e,ae,i),t(ae,Ce),t(Ce,vs),f(bt,vs,null),t(ae,Ii),t(ae,ys),t(ys,Ni),c(e,ao,i),c(e,S,i),t(S,Mi),t(S,gs),t(gs,Hi),t(S,Fi),t(S,_s),t(_s,Bi),t(S,Li),t(S,bs),t(bs,Wi),t(S,Gi),c(e,so,i),f(wt,e,i),c(e,lo,i),c(e,B,i),t(B,Ri),t(B,ws),t(ws,Ui),t(B,Vi),t(B,Es),t(Es,Yi),t(B,zi),c(e,oo,i),c(e,se,i),t(se,De),t(De,js),f(Et,js,null),t(se,Ji),t(se,$s),t($s,Ki),c(e,no,i),c(e,A,i),t(A,Qi),t(A,ks),t(ks,Xi),t(A,Zi),t(A,xs),t(xs,ec),t(A,tc),t(A,Ts),t(Ts,ac),t(A,sc),t(A,qs),t(qs,lc),t(A,oc),c(e,ro,i),f(jt,e,i),c(e,io,i),c(e,I,i),t(I,nc),t(I,As),t(As,rc),t(I,ic),t(I,Ps),t(Ps,cc),t(I,pc),t(I,Cs),t(Cs,uc),t(I,hc),c(e,co,i),f($t,e,i),c(e,po,i),c(e,sa,i),t(sa,dc),c(e,uo,i),f(kt,e,i),c(e,ho,i),c(e,le,i),t(le,Oe),t(Oe,Ds),f(xt,Ds,null),t(le,mc),t(le,Os),t(Os,fc),c(e,mo,i),c(e,P,i),t(P,vc),t(P,Ss),t(Ss,yc),t(P,gc),t(P,Is),t(Is,_c),t(P,bc),t(P,Tt),t(Tt,wc),t(P,Ec),t(P,Ns),t(Ns,jc),t(P,$c),c(e,fo,i),c(e,la,i),t(la,kc),c(e,vo,i),c(e,L,i),t(L,xc),t(L,Ms),t(Ms,Tc),t(L,qc),t(L,Hs),t(Hs,Ac),t(L,Pc),c(e,yo,i),c(e,oa,i),t(oa,Cc),c(e,go,i),c(e,oe,i),t(oe,Se),t(Se,Fs),f(qt,Fs,null),t(oe,Dc),t(oe,Bs),t(Bs,Oc),c(e,_o,i),c(e,_,i),t(_,Sc),t(_,Ls),t(Ls,Ic),t(_,Nc),t(_,Ws),t(Ws,Mc),t(_,Hc),t(_,Gs),t(Gs,Fc),t(_,Bc),t(_,Rs),t(Rs,Lc),t(_,Wc),t(_,Us),t(Us,Gc),t(_,Rc),t(_,Vs),t(Vs,Uc),t(_,Vc),t(_,Ys),t(Ys,Yc),t(_,zc),t(_,zs),t(zs,Jc),t(_,Kc),c(e,bo,i),f(At,e,i),c(e,wo,i),c(e,Ie,i),t(Ie,Qc),t(Ie,Js),t(Js,Xc),t(Ie,Zc),c(e,Eo,i),c(e,ne,i),t(ne,Ne),t(Ne,Ks),f(Pt,Ks,null),t(ne,ep),t(ne,Qs),t(Qs,tp),c(e,jo,i),c(e,Me,i),t(Me,ap),t(Me,na),t(na,sp),t(Me,lp),c(e,$o,i),f(Ct,e,i),c(e,ko,i),c(e,W,i),t(W,op),t(W,Xs),t(Xs,np),t(W,rp),t(W,Zs),t(Zs,ip),t(W,cp),c(e,xo,i),f(Dt,e,i),c(e,To,i),c(e,re,i),t(re,He),t(He,el),f(Ot,el,null),t(re,pp),t(re,tl),t(tl,up),c(e,qo,i),c(e,Fe,i),t(Fe,hp),t(Fe,ra),t(ra,dp),t(Fe,mp),c(e,Ao,i),f(St,e,i),c(e,Po,i),c(e,ia,i),t(ia,fp),c(e,Co,i),f(It,e,i),c(e,Do,i),c(e,ca,i),t(ca,vp),c(e,Oo,i),c(e,Be,i),t(Be,yp),t(Be,pa),t(pa,gp),t(Be,_p),c(e,So,i),f(Nt,e,i),c(e,Io,i),c(e,ie,i),t(ie,Le),t(Le,al),f(Mt,al,null),t(ie,bp),t(ie,sl),t(sl,wp),c(e,No,i),c(e,k,i),t(k,Ep),t(k,ua),t(ua,jp),t(k,$p),t(k,ll),t(ll,kp),t(k,xp),t(k,Ht),t(Ht,Tp),t(k,qp),t(k,ol),t(ol,Ap),t(k,Pp),t(k,nl),t(nl,Cp),t(k,Dp),c(e,Mo,i),c(e,G,i),t(G,Op),t(G,rl),t(rl,Sp),t(G,Ip),t(G,il),t(il,Np),t(G,Mp),c(e,Ho,i),f(Ft,e,i),c(e,Fo,i),c(e,R,i),t(R,Hp),t(R,cl),t(cl,Fp),t(R,Bp),t(R,pl),t(pl,Lp),t(R,Wp),c(e,Bo,i),f(Bt,e,i),c(e,Lo,i),c(e,U,i),t(U,Gp),t(U,ul),t(ul,Rp),t(U,Up),t(U,hl),t(hl,Vp),t(U,Yp),c(e,Wo,i),f(Lt,e,i),c(e,Go,i),c(e,x,i),t(x,zp),t(x,dl),t(dl,Jp),t(x,Kp),t(x,ml),t(ml,Qp),t(x,Xp),t(x,fl),t(fl,Zp),t(x,eu),t(x,vl),t(vl,tu),t(x,au),t(x,yl),t(yl,su),t(x,lu),Ro=!0},p(e,[i]){const Wt={};i&2&&(Wt.$$scope={dirty:i,ctx:e}),qe.$set(Wt)},i(e){Ro||(v(D.$$.fragment,e),v(Re.$$.fragment,e),v(Ke.$$.fragment,e),v(Qe.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),v(et.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(st.$$.fragment,e),v(vt.$$.fragment,e),v(yt.$$.fragment,e),v(gt.$$.fragment,e),v(qe.$$.fragment,e),v(_t.$$.fragment,e),v(bt.$$.fragment,e),v(wt.$$.fragment,e),v(Et.$$.fragment,e),v(jt.$$.fragment,e),v($t.$$.fragment,e),v(kt.$$.fragment,e),v(xt.$$.fragment,e),v(qt.$$.fragment,e),v(At.$$.fragment,e),v(Pt.$$.fragment,e),v(Ct.$$.fragment,e),v(Dt.$$.fragment,e),v(Ot.$$.fragment,e),v(St.$$.fragment,e),v(It.$$.fragment,e),v(Nt.$$.fragment,e),v(Mt.$$.fragment,e),v(Ft.$$.fragment,e),v(Bt.$$.fragment,e),v(Lt.$$.fragment,e),Ro=!0)},o(e){y(D.$$.fragment,e),y(Re.$$.fragment,e),y(Ke.$$.fragment,e),y(Qe.$$.fragment,e),y(Xe.$$.fragment,e),y(Ze.$$.fragment,e),y(et.$$.fragment,e),y(tt.$$.fragment,e),y(at.$$.fragment,e),y(st.$$.fragment,e),y(vt.$$.fragment,e),y(yt.$$.fragment,e),y(gt.$$.fragment,e),y(qe.$$.fragment,e),y(_t.$$.fragment,e),y(bt.$$.fragment,e),y(wt.$$.fragment,e),y(Et.$$.fragment,e),y(jt.$$.fragment,e),y($t.$$.fragment,e),y(kt.$$.fragment,e),y(xt.$$.fragment,e),y(qt.$$.fragment,e),y(At.$$.fragment,e),y(Pt.$$.fragment,e),y(Ct.$$.fragment,e),y(Dt.$$.fragment,e),y(Ot.$$.fragment,e),y(St.$$.fragment,e),y(It.$$.fragment,e),y(Nt.$$.fragment,e),y(Mt.$$.fragment,e),y(Ft.$$.fragment,e),y(Bt.$$.fragment,e),y(Lt.$$.fragment,e),Ro=!1},d(e){a(E),e&&a(ce),e&&a(T),g(D),e&&a(El),e&&a(Gt),e&&a(jl),e&&a(K),g(Re),e&&a($l),e&&a(Rt),e&&a(kl),e&&a(F),e&&a(xl),e&&a(fe),e&&a(Tl),e&&a(Je),e&&a(ql),e&&a(ve),e&&a(Al),e&&a(Q),g(Ke),e&&a(Pl),e&&a(ge),e&&a(Cl),g(Qe,e),e&&a(Dl),e&&a(Yt),e&&a(Ol),g(Xe,e),e&&a(Sl),e&&a(X),g(Ze),e&&a(Il),e&&a(zt),e&&a(Nl),g(et,e),e&&a(Ml),e&&a(be),e&&a(Hl),e&&a(Z),g(tt),e&&a(Fl),e&&a(Ee),e&&a(Bl),g(at,e),e&&a(Ll),e&&a(ee),g(st),e&&a(Wl),e&&a($e),e&&a(Gl),e&&a(ke),e&&a(Rl),e&&a(xe),e&&a(Ul),g(vt,e),e&&a(Vl),e&&a(Te),e&&a(Yl),g(yt,e),e&&a(zl),e&&a(Xt),e&&a(Jl),g(gt,e),e&&a(Kl),g(qe,e),e&&a(Ql),e&&a(te),g(_t),e&&a(Xl),e&&a(Zt),e&&a(Zl),e&&a(Pe),e&&a(eo),e&&a(q),e&&a(to),e&&a(ae),g(bt),e&&a(ao),e&&a(S),e&&a(so),g(wt,e),e&&a(lo),e&&a(B),e&&a(oo),e&&a(se),g(Et),e&&a(no),e&&a(A),e&&a(ro),g(jt,e),e&&a(io),e&&a(I),e&&a(co),g($t,e),e&&a(po),e&&a(sa),e&&a(uo),g(kt,e),e&&a(ho),e&&a(le),g(xt),e&&a(mo),e&&a(P),e&&a(fo),e&&a(la),e&&a(vo),e&&a(L),e&&a(yo),e&&a(oa),e&&a(go),e&&a(oe),g(qt),e&&a(_o),e&&a(_),e&&a(bo),g(At,e),e&&a(wo),e&&a(Ie),e&&a(Eo),e&&a(ne),g(Pt),e&&a(jo),e&&a(Me),e&&a($o),g(Ct,e),e&&a(ko),e&&a(W),e&&a(xo),g(Dt,e),e&&a(To),e&&a(re),g(Ot),e&&a(qo),e&&a(Fe),e&&a(Ao),g(St,e),e&&a(Po),e&&a(ia),e&&a(Co),g(It,e),e&&a(Do),e&&a(ca),e&&a(Oo),e&&a(Be),e&&a(So),g(Nt,e),e&&a(Io),e&&a(ie),g(Mt),e&&a(No),e&&a(k),e&&a(Mo),e&&a(G),e&&a(Ho),g(Ft,e),e&&a(Fo),e&&a(R),e&&a(Bo),g(Bt,e),e&&a(Lo),e&&a(U),e&&a(Wo),g(Lt,e),e&&a(Go),e&&a(x)}}}const om={local:"a-quick-tour",sections:[{local:"types-of-evaluations",title:"Types of evaluations"},{local:"load",sections:[{local:"community-modules",title:"Community modules"},{local:"list-available-modules",title:"List available modules"}],title:"Load"},{local:"module-attributes",title:"Module attributes"},{local:"compute",sections:[{local:"how-to-compute",title:"How to compute"},{local:"calculate-a-single-metric-or-a-batch-of-metrics",title:"Calculate a single metric or a batch of metrics"},{local:"distributed-evaluation",title:"Distributed evaluation"}],title:"Compute"},{local:"configuration",title:"Configuration"},{local:"combining-several-evaluations",title:"Combining several evaluations"},{local:"save-and-push-to-the-hub",title:"Save and push to the Hub"},{local:"evaluator",title:"Evaluator"}],title:"A quick tour"};function nm(wl){return tm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class um extends Kd{constructor(E){super();Qd(this,E,nm,lm,Xd,{})}}export{um as default,om as metadata};
