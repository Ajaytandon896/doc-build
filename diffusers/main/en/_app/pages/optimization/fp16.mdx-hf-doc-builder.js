import{S as At,i as Tt,s as Ct,e as s,k as f,w as j,t as r,M as Ut,c as i,d as t,m as h,a as n,x,h as l,b as c,G as o,g as p,y as A,q as T,o as C,B as U,v as Mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as Nt}from"../../chunks/Tip-hf-doc-builder.js";import{I as Ae}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as pt}from"../../chunks/CodeBlock-hf-doc-builder.js";function Ot(pe){let u;return{c(){u=r("Attention slicing is useful even if a batch size of just 1 is used - as long as the model uses more than one attention head. If there is more than one attention head the *QK^T* attention matrix can be computed sequentially for each head which can save a significant amount of memory.")},l(d){u=l(d,"Attention slicing is useful even if a batch size of just 1 is used - as long as the model uses more than one attention head. If there is more than one attention head the *QK^T* attention matrix can be computed sequentially for each head which can save a significant amount of memory.")},m(d,m){p(d,u,m)},d(d){d&&t(u)}}}function Vt(pe){let u,d,m,$,Q,M,Te,W,Ce,fe,b,Ue,J,Me,Ne,he,_,k,X,N,Oe,z,Ve,Y,Ge,ce,v,He,Z,Ie,Fe,ee,ze,Be,ue,O,me,E,Re,te,Ke,Le,de,w,P,oe,V,Qe,ae,We,ve,y,Je,se,Xe,Ye,ie,Ze,et,ye,G,_e,g,q,ne,H,tt,re,ot,we,B,at,ge,D,$e,S,st,le,it,nt,be,I,ke,R,rt,Ee;return M=new Ae({}),N=new Ae({}),O=new pt({props:{code:`from torch import autocast
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", use_auth_token=True)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
with autocast("cuda"):
    image = pipe(prompt).images[0]  `,highlighted:`<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> autocast
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, use_auth_token=<span class="hljs-literal">True</span>)
pipe = pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)

prompt = <span class="hljs-string">&quot;a photo of an astronaut riding a horse on mars&quot;</span>
<span class="hljs-keyword">with</span> autocast(<span class="hljs-string">&quot;cuda&quot;</span>):
    image = pipe(prompt).images[<span class="hljs-number">0</span>]  `}}),V=new Ae({}),G=new pt({props:{code:`pipe = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    revision="fp16",
    torch_dtype=torch.float16,
    use_auth_token=True
)`,highlighted:`pipe = StableDiffusionPipeline.from_pretrained(
    <span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>,
    revision=<span class="hljs-string">&quot;fp16&quot;</span>,
    torch_dtype=torch.float16,
    use_auth_token=<span class="hljs-literal">True</span>
)`}}),H=new Ae({}),D=new Nt({props:{$$slots:{default:[Ot]},$$scope:{ctx:pe}}}),I=new pt({props:{code:`import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    revision="fp16",
    torch_dtype=torch.float16,
    use_auth_token=True
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_attention_slicing()
with torch.autocast("cuda"):
    image = pipe(prompt).images[0]  `,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    <span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>,
    revision=<span class="hljs-string">&quot;fp16&quot;</span>,
    torch_dtype=torch.float16,
    use_auth_token=<span class="hljs-literal">True</span>
)
pipe = pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)

prompt = <span class="hljs-string">&quot;a photo of an astronaut riding a horse on mars&quot;</span>
pipe.enable_attention_slicing()
<span class="hljs-keyword">with</span> torch.autocast(<span class="hljs-string">&quot;cuda&quot;</span>):
    image = pipe(prompt).images[<span class="hljs-number">0</span>]  `}}),{c(){u=s("meta"),d=f(),m=s("h1"),$=s("a"),Q=s("span"),j(M.$$.fragment),Te=f(),W=s("span"),Ce=r("Memory and speed"),fe=f(),b=s("p"),Ue=r("We present some techniques and ideas to optimize \u{1F917} Diffusers "),J=s("em"),Me=r("inference"),Ne=r(" for memory or speed."),he=f(),_=s("h2"),k=s("a"),X=s("span"),j(N.$$.fragment),Oe=f(),z=s("span"),Ve=r("CUDA "),Y=s("code"),Ge=r("autocast"),ce=f(),v=s("p"),He=r("If you use a CUDA GPU, you can take advantage of "),Z=s("code"),Ie=r("torch.autocast"),Fe=r(" to perform inference roughly twice as fast at the cost of slightly lower precision. All you need to do is put your inference call inside an "),ee=s("code"),ze=r("autocast"),Be=r(" context manager. The following example shows how to do it using Stable Diffusion text-to-image generation as an example:"),ue=f(),j(O.$$.fragment),me=f(),E=s("p"),Re=r("Despite the precision loss, in our experience the final image results look the same as the "),te=s("code"),Ke=r("float32"),Le=r(" versions. Feel free to experiment and report back!"),de=f(),w=s("h2"),P=s("a"),oe=s("span"),j(V.$$.fragment),Qe=f(),ae=s("span"),We=r("Half precision weights"),ve=f(),y=s("p"),Je=r("To save more GPU memory, you can load the model weights directly in half precision. This involves loading the float16 version of the weights, which was saved to a branch named "),se=s("code"),Xe=r("fp16"),Ye=r(", and telling PyTorch to use the "),ie=s("code"),Ze=r("float16"),et=r(" type when loading them:"),ye=f(),j(G.$$.fragment),_e=f(),g=s("h2"),q=s("a"),ne=s("span"),j(H.$$.fragment),tt=f(),re=s("span"),ot=r("Sliced attention for additional memory savings"),we=f(),B=s("p"),at=r("For even additional memory savings, you can use a sliced version of attention that performs the computation in steps instead of all at once."),ge=f(),j(D.$$.fragment),$e=f(),S=s("p"),st=r("To perform the attention computation sequentially over each head, you only need to invoke "),le=s("code"),it=r("enable_attention_slicing()"),nt=r(" in your pipeline before inference, like here:"),be=f(),j(I.$$.fragment),ke=f(),R=s("p"),rt=r("There\u2019s a small performance penalty of about 10% slower inference times, but this method allows you to use Stable Diffusion in as little as 3.2 GB of VRAM!"),this.h()},l(e){const a=Ut('[data-svelte="svelte-1phssyn"]',document.head);u=i(a,"META",{name:!0,content:!0}),a.forEach(t),d=h(e),m=i(e,"H1",{class:!0});var F=n(m);$=i(F,"A",{id:!0,class:!0,href:!0});var ft=n($);Q=i(ft,"SPAN",{});var ht=n(Q);x(M.$$.fragment,ht),ht.forEach(t),ft.forEach(t),Te=h(F),W=i(F,"SPAN",{});var ct=n(W);Ce=l(ct,"Memory and speed"),ct.forEach(t),F.forEach(t),fe=h(e),b=i(e,"P",{});var Pe=n(b);Ue=l(Pe,"We present some techniques and ideas to optimize \u{1F917} Diffusers "),J=i(Pe,"EM",{});var ut=n(J);Me=l(ut,"inference"),ut.forEach(t),Ne=l(Pe," for memory or speed."),Pe.forEach(t),he=h(e),_=i(e,"H2",{class:!0});var qe=n(_);k=i(qe,"A",{id:!0,class:!0,href:!0});var mt=n(k);X=i(mt,"SPAN",{});var dt=n(X);x(N.$$.fragment,dt),dt.forEach(t),mt.forEach(t),Oe=h(qe),z=i(qe,"SPAN",{});var lt=n(z);Ve=l(lt,"CUDA "),Y=i(lt,"CODE",{});var vt=n(Y);Ge=l(vt,"autocast"),vt.forEach(t),lt.forEach(t),qe.forEach(t),ce=h(e),v=i(e,"P",{});var K=n(v);He=l(K,"If you use a CUDA GPU, you can take advantage of "),Z=i(K,"CODE",{});var yt=n(Z);Ie=l(yt,"torch.autocast"),yt.forEach(t),Fe=l(K," to perform inference roughly twice as fast at the cost of slightly lower precision. All you need to do is put your inference call inside an "),ee=i(K,"CODE",{});var _t=n(ee);ze=l(_t,"autocast"),_t.forEach(t),Be=l(K," context manager. The following example shows how to do it using Stable Diffusion text-to-image generation as an example:"),K.forEach(t),ue=h(e),x(O.$$.fragment,e),me=h(e),E=i(e,"P",{});var De=n(E);Re=l(De,"Despite the precision loss, in our experience the final image results look the same as the "),te=i(De,"CODE",{});var wt=n(te);Ke=l(wt,"float32"),wt.forEach(t),Le=l(De," versions. Feel free to experiment and report back!"),De.forEach(t),de=h(e),w=i(e,"H2",{class:!0});var Se=n(w);P=i(Se,"A",{id:!0,class:!0,href:!0});var gt=n(P);oe=i(gt,"SPAN",{});var $t=n(oe);x(V.$$.fragment,$t),$t.forEach(t),gt.forEach(t),Qe=h(Se),ae=i(Se,"SPAN",{});var bt=n(ae);We=l(bt,"Half precision weights"),bt.forEach(t),Se.forEach(t),ve=h(e),y=i(e,"P",{});var L=n(y);Je=l(L,"To save more GPU memory, you can load the model weights directly in half precision. This involves loading the float16 version of the weights, which was saved to a branch named "),se=i(L,"CODE",{});var kt=n(se);Xe=l(kt,"fp16"),kt.forEach(t),Ye=l(L,", and telling PyTorch to use the "),ie=i(L,"CODE",{});var Et=n(ie);Ze=l(Et,"float16"),Et.forEach(t),et=l(L," type when loading them:"),L.forEach(t),ye=h(e),x(G.$$.fragment,e),_e=h(e),g=i(e,"H2",{class:!0});var je=n(g);q=i(je,"A",{id:!0,class:!0,href:!0});var Pt=n(q);ne=i(Pt,"SPAN",{});var qt=n(ne);x(H.$$.fragment,qt),qt.forEach(t),Pt.forEach(t),tt=h(je),re=i(je,"SPAN",{});var Dt=n(re);ot=l(Dt,"Sliced attention for additional memory savings"),Dt.forEach(t),je.forEach(t),we=h(e),B=i(e,"P",{});var St=n(B);at=l(St,"For even additional memory savings, you can use a sliced version of attention that performs the computation in steps instead of all at once."),St.forEach(t),ge=h(e),x(D.$$.fragment,e),$e=h(e),S=i(e,"P",{});var xe=n(S);st=l(xe,"To perform the attention computation sequentially over each head, you only need to invoke "),le=i(xe,"CODE",{});var jt=n(le);it=l(jt,"enable_attention_slicing()"),jt.forEach(t),nt=l(xe," in your pipeline before inference, like here:"),xe.forEach(t),be=h(e),x(I.$$.fragment,e),ke=h(e),R=i(e,"P",{});var xt=n(R);rt=l(xt,"There\u2019s a small performance penalty of about 10% slower inference times, but this method allows you to use Stable Diffusion in as little as 3.2 GB of VRAM!"),xt.forEach(t),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(Gt)),c($,"id","memory-and-speed"),c($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($,"href","#memory-and-speed"),c(m,"class","relative group"),c(k,"id","cuda-autocast"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#cuda-autocast"),c(_,"class","relative group"),c(P,"id","half-precision-weights"),c(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P,"href","#half-precision-weights"),c(w,"class","relative group"),c(q,"id","sliced-attention-for-additional-memory-savings"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#sliced-attention-for-additional-memory-savings"),c(g,"class","relative group")},m(e,a){o(document.head,u),p(e,d,a),p(e,m,a),o(m,$),o($,Q),A(M,Q,null),o(m,Te),o(m,W),o(W,Ce),p(e,fe,a),p(e,b,a),o(b,Ue),o(b,J),o(J,Me),o(b,Ne),p(e,he,a),p(e,_,a),o(_,k),o(k,X),A(N,X,null),o(_,Oe),o(_,z),o(z,Ve),o(z,Y),o(Y,Ge),p(e,ce,a),p(e,v,a),o(v,He),o(v,Z),o(Z,Ie),o(v,Fe),o(v,ee),o(ee,ze),o(v,Be),p(e,ue,a),A(O,e,a),p(e,me,a),p(e,E,a),o(E,Re),o(E,te),o(te,Ke),o(E,Le),p(e,de,a),p(e,w,a),o(w,P),o(P,oe),A(V,oe,null),o(w,Qe),o(w,ae),o(ae,We),p(e,ve,a),p(e,y,a),o(y,Je),o(y,se),o(se,Xe),o(y,Ye),o(y,ie),o(ie,Ze),o(y,et),p(e,ye,a),A(G,e,a),p(e,_e,a),p(e,g,a),o(g,q),o(q,ne),A(H,ne,null),o(g,tt),o(g,re),o(re,ot),p(e,we,a),p(e,B,a),o(B,at),p(e,ge,a),A(D,e,a),p(e,$e,a),p(e,S,a),o(S,st),o(S,le),o(le,it),o(S,nt),p(e,be,a),A(I,e,a),p(e,ke,a),p(e,R,a),o(R,rt),Ee=!0},p(e,[a]){const F={};a&2&&(F.$$scope={dirty:a,ctx:e}),D.$set(F)},i(e){Ee||(T(M.$$.fragment,e),T(N.$$.fragment,e),T(O.$$.fragment,e),T(V.$$.fragment,e),T(G.$$.fragment,e),T(H.$$.fragment,e),T(D.$$.fragment,e),T(I.$$.fragment,e),Ee=!0)},o(e){C(M.$$.fragment,e),C(N.$$.fragment,e),C(O.$$.fragment,e),C(V.$$.fragment,e),C(G.$$.fragment,e),C(H.$$.fragment,e),C(D.$$.fragment,e),C(I.$$.fragment,e),Ee=!1},d(e){t(u),e&&t(d),e&&t(m),U(M),e&&t(fe),e&&t(b),e&&t(he),e&&t(_),U(N),e&&t(ce),e&&t(v),e&&t(ue),U(O,e),e&&t(me),e&&t(E),e&&t(de),e&&t(w),U(V),e&&t(ve),e&&t(y),e&&t(ye),U(G,e),e&&t(_e),e&&t(g),U(H),e&&t(we),e&&t(B),e&&t(ge),U(D,e),e&&t($e),e&&t(S),e&&t(be),U(I,e),e&&t(ke),e&&t(R)}}}const Gt={local:"memory-and-speed",sections:[{local:"cuda-autocast",title:"CUDA `autocast`"},{local:"half-precision-weights",title:"Half precision weights"},{local:"sliced-attention-for-additional-memory-savings",title:"Sliced attention for additional memory savings"}],title:"Memory and speed"};function Ht(pe){return Mt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rt extends At{constructor(u){super();Tt(this,u,Ht,Vt,Ct,{})}}export{Rt as default,Gt as metadata};
