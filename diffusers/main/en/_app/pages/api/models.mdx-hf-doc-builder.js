import{S as Ma,i as xa,s as Ea,e as n,k as l,w as p,t as i,M as Na,c as r,d as t,m as c,a as s,x as m,h as d,b as a,G as e,g as f,y as h,q as _,o as g,B as v,v as Ta}from"../../chunks/vendor-hf-doc-builder.js";import{T as ka}from"../../chunks/Tip-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{I as B}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Ua(lo){let b,P,$,x,T,w,E,L;return{c(){b=n("p"),P=i("It is required to be logged in ("),$=n("code"),x=i("huggingface-cli login"),T=i(") when you want to use private or "),w=n("a"),E=i(`gated
models`),L=i("."),this.h()},l(Q){b=r(Q,"P",{});var U=s(b);P=d(U,"It is required to be logged in ("),$=r(U,"CODE",{});var A=s($);x=d(A,"huggingface-cli login"),A.forEach(t),T=d(U,") when you want to use private or "),w=r(U,"A",{href:!0,rel:!0});var co=s(w);E=d(co,`gated
models`),co.forEach(t),L=d(U,"."),U.forEach(t),this.h()},h(){a(w,"href","https://huggingface.co/docs/hub/models-gated#gated-models"),a(w,"rel","nofollow")},m(Q,U){f(Q,b,U),e(b,P),e(b,$),e($,x),e(b,T),e(b,w),e(w,E),e(b,L)},d(Q){Q&&t(b)}}}function Aa(lo){let b,P,$,x,T;return{c(){b=n("p"),P=i("Activate the special "),$=n("a"),x=i("\u201Coffline-mode\u201D"),T=i(` to use
this method in a firewalled environment.`),this.h()},l(w){b=r(w,"P",{});var E=s(b);P=d(E,"Activate the special "),$=r(E,"A",{href:!0,rel:!0});var L=s($);x=d(L,"\u201Coffline-mode\u201D"),L.forEach(t),T=d(E,` to use
this method in a firewalled environment.`),E.forEach(t),this.h()},h(){a($,"href","https://huggingface.co/diffusers/installation.html#offline-mode"),a($,"rel","nofollow")},m(w,E){f(w,b,E),e(b,P),e(b,$),e($,x),e(b,T)},d(w){w&&t(b)}}}function Oa(lo){let b,P,$,x,T,w,E,L,Q,U,A,co,Mo,dn,ln,xo,cn,un,gt,z,ne,Eo,we,fn,No,pn,vt,y,De,mn,To,hn,_n,uo,fo,gn,vn,bn,Uo,F,Ao,$n,yn,Oo,wn,Dn,po,kn,Mn,xn,K,ke,En,Co,Nn,Tn,qo,Un,An,I,Me,On,Vo,Cn,qn,Po,Vn,Pn,k,xe,Ln,Lo,Bn,Fn,S,Kn,Bo,In,Qn,Fo,zn,Sn,Wn,Ee,Xn,Ko,Hn,Yn,jn,Ne,Gn,Io,Rn,Jn,Zn,re,er,se,or,ae,Te,tr,Qo,nr,rr,ie,Ue,sr,Ae,ar,zo,ir,dr,bt,W,de,So,Oe,lr,Wo,cr,$t,Ce,qe,yt,X,le,Xo,Ve,ur,Ho,fr,wt,O,Pe,pr,Yo,mr,hr,Le,_r,mo,gr,vr,br,ho,Be,Dt,H,ce,jo,Fe,$r,Go,yr,kt,Ke,Ie,Mt,Y,ue,Ro,Qe,wr,Jo,Dr,xt,C,ze,kr,Zo,Mr,xr,Se,Er,_o,Nr,Tr,Ur,fe,We,Ar,et,Or,Et,j,pe,ot,Xe,Cr,tt,qr,Nt,G,He,Vr,nt,Pr,Tt,R,me,rt,Ye,Lr,st,Br,Ut,J,je,Fr,at,Kr,At,Z,he,it,Ge,Ir,dt,Qr,Ot,q,Re,zr,lt,Sr,Wr,Je,Xr,go,Hr,Yr,jr,vo,Ze,Ct,ee,_e,ct,eo,Gr,ut,Rr,qt,oe,oo,Jr,ft,Zr,Vt,te,ge,pt,to,es,mt,os,Pt,V,no,ts,ht,ns,rs,ro,ss,bo,as,is,ds,$o,so,Lt;return w=new B({}),we=new B({}),De=new D({props:{name:"class diffusers.ModelMixin",anchor:"diffusers.ModelMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L114"}}),ke=new D({props:{name:"disable_gradient_checkpointing",anchor:"diffusers.ModelMixin.disable_gradient_checkpointing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L152"}}),Me=new D({props:{name:"enable_gradient_checkpointing",anchor:"diffusers.ModelMixin.enable_gradient_checkpointing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L141"}}),xe=new D({props:{name:"from_pretrained",anchor:"diffusers.ModelMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.ModelMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids should have an organization name, like <code>google/ddpm-celebahq-256</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using <code>save_config</code>, e.g.,
<code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"diffusers.ModelMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"diffusers.ModelMixin.from_pretrained.torch_dtype",description:`<strong>torch_dtype</strong> (<code>str</code> or <code>torch.dtype</code>, <em>optional</em>) &#x2014;
Override the default <code>torch.dtype</code> and load the model under this dtype. If <code>&quot;auto&quot;</code> is passed the dtype
will be automatically derived from the model&#x2019;s weights.`,name:"torch_dtype"},{anchor:"diffusers.ModelMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"diffusers.ModelMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"diffusers.ModelMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"diffusers.ModelMixin.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"diffusers.ModelMixin.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (i.e., do not try to download the model).`,name:"local_files_only(bool,"},{anchor:"diffusers.ModelMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"diffusers.ModelMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"diffusers.ModelMixin.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&quot;</code>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo (either remote in
huggingface.co or downloaded locally), you can specify the folder name here.`,name:"subfolder"},{anchor:"diffusers.ModelMixin.from_pretrained.mirror",description:`<strong>mirror</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.`,name:"mirror"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L212"}}),re=new ka({props:{$$slots:{default:[Ua]},$$scope:{ctx:lo}}}),se=new ka({props:{$$slots:{default:[Aa]},$$scope:{ctx:lo}}}),Te=new D({props:{name:"num_parameters",anchor:"diffusers.ModelMixin.num_parameters",parameters:[{name:"only_trainable",val:": bool = False"},{name:"exclude_embeddings",val:": bool = False"}],parametersDescription:[{anchor:"diffusers.ModelMixin.num_parameters.only_trainable",description:`<strong>only_trainable</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return only the number of trainable parameters`,name:"only_trainable"},{anchor:"diffusers.ModelMixin.num_parameters.exclude_embeddings",description:`<strong>exclude_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return only the number of non-embeddings parameters`,name:"exclude_embeddings"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L574",returnDescription:`
<p>The number of parameters.</p>
`,returnType:`
<p><code>int</code></p>
`}}),Ue=new D({props:{name:"save_pretrained",anchor:"diffusers.ModelMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"is_main_process",val:": bool = True"},{name:"save_function",val:": typing.Callable = <function save at 0x7f027c3c04c0>"}],parametersDescription:[{anchor:"diffusers.ModelMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory to which to save. Will be created if it doesn&#x2019;t exist.`,name:"save_directory"},{anchor:"diffusers.ModelMixin.save_pretrained.is_main_process",description:`<strong>is_main_process</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on
the main process to avoid race conditions.`,name:"is_main_process"},{anchor:"diffusers.ModelMixin.save_pretrained.save_function",description:`<strong>save_function</strong> (<code>Callable</code>) &#x2014;
The function to use to save the state dictionary. Useful on distributed training like TPUs when one
need to replace <code>torch.save</code> by another method.`,name:"save_function"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L162"}}),Oe=new B({}),qe=new D({props:{name:"class diffusers.models.unet_2d.UNet2DOutput",anchor:"diffusers.models.unet_2d.UNet2DOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.unet_2d.UNet2DOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Hidden states output. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L15"}}),Ve=new B({}),Pe=new D({props:{name:"class diffusers.UNet2DModel",anchor:"diffusers.UNet2DModel",parameters:[{name:"sample_size",val:": typing.Optional[int] = None"},{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"center_input_sample",val:": bool = False"},{name:"time_embedding_type",val:": str = 'positional'"},{name:"freq_shift",val:": int = 0"},{name:"flip_sin_to_cos",val:": bool = True"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('AttnUpBlock2D', 'AttnUpBlock2D', 'AttnUpBlock2D', 'UpBlock2D')"},{name:"block_out_channels",val:": typing.Tuple[int] = (224, 448, 672, 896)"},{name:"layers_per_block",val:": int = 2"},{name:"mid_block_scale_factor",val:": float = 1"},{name:"downsample_padding",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"attention_head_dim",val:": int = 8"},{name:"norm_num_groups",val:": int = 32"},{name:"norm_eps",val:": float = 1e-05"}],parametersDescription:[{anchor:"diffusers.UNet2DModel.sample_size",description:`<strong>sample_size</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
Input sample size.`,name:"sample_size"},{anchor:"diffusers.UNet2DModel.in_channels",description:"<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.UNet2DModel.out_channels",description:"<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.UNet2DModel.center_input_sample",description:"<strong>center_input_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014; Whether to center the input sample.",name:"center_input_sample"},{anchor:"diffusers.UNet2DModel.time_embedding_type",description:"<strong>time_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;positional&quot;</code>) &#x2014; Type of time embedding to use.",name:"time_embedding_type"},{anchor:"diffusers.UNet2DModel.freq_shift",description:"<strong>freq_shift</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; Frequency shift for fourier time embedding.",name:"freq_shift"},{anchor:"diffusers.UNet2DModel.flip_sin_to_cos",description:`<strong>flip_sin_to_cos</strong> (<code>bool</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>False</code>): Whether to flip sin to cos for fourier time embedding.`,name:"flip_sin_to_cos"},{anchor:"diffusers.UNet2DModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownBlock2D&quot;, &quot;AttnDownBlock2D&quot;, &quot;AttnDownBlock2D&quot;, &quot;AttnDownBlock2D&quot;)</code>): Tuple of downsample block
types.`,name:"down_block_types"},{anchor:"diffusers.UNet2DModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;AttnUpBlock2D&quot;, &quot;AttnUpBlock2D&quot;, &quot;AttnUpBlock2D&quot;, &quot;UpBlock2D&quot;)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.UNet2DModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(224, 448, 672, 896)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.UNet2DModel.layers_per_block",description:"<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to <code>2</code>) &#x2014; The number of layers per block.",name:"layers_per_block"},{anchor:"diffusers.UNet2DModel.mid_block_scale_factor",description:"<strong>mid_block_scale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>1</code>) &#x2014; The scale factor for the mid block.",name:"mid_block_scale_factor"},{anchor:"diffusers.UNet2DModel.downsample_padding",description:"<strong>downsample_padding</strong> (<code>int</code>, <em>optional</em>, defaults to <code>1</code>) &#x2014; The padding for the downsample convolution.",name:"downsample_padding"},{anchor:"diffusers.UNet2DModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.UNet2DModel.attention_head_dim",description:"<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to <code>8</code>) &#x2014; The attention head dimension.",name:"attention_head_dim"},{anchor:"diffusers.UNet2DModel.norm_num_groups",description:"<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; The number of groups for the normalization.",name:"norm_num_groups"},{anchor:"diffusers.UNet2DModel.norm_eps",description:"<strong>norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to <code>1e-5</code>) &#x2014; The epsilon for the normalization.",name:"norm_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L25"}}),Be=new D({props:{name:"forward",anchor:"diffusers.UNet2DModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[torch.Tensor, float, int]"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.UNet2DModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) noisy inputs tensor",name:"sample"},{anchor:"diffusers.UNet2DModel.forward.timestep",description:"<strong>timestep</strong> (<code>torch.FloatTensor</code> or <code>float</code> or `int) &#x2014; (batch) timesteps",name:"timestep"},{anchor:"diffusers.UNet2DModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput">UNet2DOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L167",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput"
>UNet2DOutput</a> if <code>return_dict</code> is True,
otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput"
>UNet2DOutput</a> or <code>tuple</code></p>
`}}),Fe=new B({}),Ie=new D({props:{name:"class diffusers.models.unet_2d_condition.UNet2DConditionOutput",anchor:"diffusers.models.unet_2d_condition.UNet2DConditionOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.unet_2d_condition.UNet2DConditionOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Hidden states conditioned on <code>encoder_hidden_states</code> input. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L27"}}),Qe=new B({}),ze=new D({props:{name:"class diffusers.UNet2DConditionModel",anchor:"diffusers.UNet2DConditionModel",parameters:[{name:"sample_size",val:": typing.Optional[int] = None"},{name:"in_channels",val:": int = 4"},{name:"out_channels",val:": int = 4"},{name:"center_input_sample",val:": bool = False"},{name:"flip_sin_to_cos",val:": bool = True"},{name:"freq_shift",val:": int = 0"},{name:"down_block_types",val:": typing.Tuple[str] = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D')"},{name:"block_out_channels",val:": typing.Tuple[int] = (320, 640, 1280, 1280)"},{name:"layers_per_block",val:": int = 2"},{name:"downsample_padding",val:": int = 1"},{name:"mid_block_scale_factor",val:": float = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"norm_num_groups",val:": int = 32"},{name:"norm_eps",val:": float = 1e-05"},{name:"cross_attention_dim",val:": int = 1280"},{name:"attention_head_dim",val:": int = 8"}],parametersDescription:[{anchor:"diffusers.UNet2DConditionModel.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>) &#x2014; The size of the input sample.",name:"sample_size"},{anchor:"diffusers.UNet2DConditionModel.in_channels",description:"<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; The number of channels in the input sample.",name:"in_channels"},{anchor:"diffusers.UNet2DConditionModel.out_channels",description:"<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; The number of channels in the output.",name:"out_channels"},{anchor:"diffusers.UNet2DConditionModel.center_input_sample",description:"<strong>center_input_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014; Whether to center the input sample.",name:"center_input_sample"},{anchor:"diffusers.UNet2DConditionModel.flip_sin_to_cos",description:`<strong>flip_sin_to_cos</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to flip the sin to cos in the time embedding.`,name:"flip_sin_to_cos"},{anchor:"diffusers.UNet2DConditionModel.freq_shift",description:"<strong>freq_shift</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The frequency shift to apply to the time embedding.",name:"freq_shift"},{anchor:"diffusers.UNet2DConditionModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;DownBlock2D&quot;)</code>) &#x2014;
The tuple of downsample blocks to use.`,name:"down_block_types"},{anchor:"diffusers.UNet2DConditionModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;,)</code>) &#x2014;
The tuple of upsample blocks to use.`,name:"up_block_types"},{anchor:"diffusers.UNet2DConditionModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(320, 640, 1280, 1280)</code>) &#x2014;
The tuple of output channels for each block.`,name:"block_out_channels"},{anchor:"diffusers.UNet2DConditionModel.layers_per_block",description:"<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; The number of layers per block.",name:"layers_per_block"},{anchor:"diffusers.UNet2DConditionModel.downsample_padding",description:"<strong>downsample_padding</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The padding to use for the downsampling convolution.",name:"downsample_padding"},{anchor:"diffusers.UNet2DConditionModel.mid_block_scale_factor",description:"<strong>mid_block_scale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014; The scale factor to use for the mid block.",name:"mid_block_scale_factor"},{anchor:"diffusers.UNet2DConditionModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.UNet2DConditionModel.norm_num_groups",description:"<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014; The number of groups to use for the normalization.",name:"norm_num_groups"},{anchor:"diffusers.UNet2DConditionModel.norm_eps",description:"<strong>norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014; The epsilon to use for the normalization.",name:"norm_eps"},{anchor:"diffusers.UNet2DConditionModel.cross_attention_dim",description:"<strong>cross_attention_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014; The dimension of the cross attention features.",name:"cross_attention_dim"},{anchor:"diffusers.UNet2DConditionModel.attention_head_dim",description:"<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014; The dimension of the attention heads.",name:"attention_head_dim"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L37"}}),We=new D({props:{name:"forward",anchor:"diffusers.UNet2DConditionModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[torch.Tensor, float, int]"},{name:"encoder_hidden_states",val:": Tensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.UNet2DConditionModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) noisy inputs tensor",name:"sample"},{anchor:"diffusers.UNet2DConditionModel.forward.timestep",description:"<strong>timestep</strong> (<code>torch.FloatTensor</code> or <code>float</code> or <code>int</code>) &#x2014; (batch) timesteps",name:"timestep"},{anchor:"diffusers.UNet2DConditionModel.forward.encoder_hidden_states",description:"<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) encoder hidden states",name:"encoder_hidden_states"},{anchor:"diffusers.UNet2DConditionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput">models.unet_2d_condition.UNet2DConditionOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L219",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput"
>UNet2DConditionOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput"
>UNet2DConditionOutput</a> or <code>tuple</code></p>
`}}),Xe=new B({}),He=new D({props:{name:"class diffusers.models.vae.DecoderOutput",anchor:"diffusers.models.vae.DecoderOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.vae.DecoderOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Decoded output sample of the model. Output of the last layer of the model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L15"}}),Ye=new B({}),je=new D({props:{name:"class diffusers.models.vae.VQEncoderOutput",anchor:"diffusers.models.vae.VQEncoderOutput",parameters:[{name:"latents",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.vae.VQEncoderOutput.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Encoded output sample of the model. Output of the last layer of the model.`,name:"latents"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L28"}}),Ge=new B({}),Re=new D({props:{name:"class diffusers.VQModel",anchor:"diffusers.VQModel",parameters:[{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownEncoderBlock2D',)"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpDecoderBlock2D',)"},{name:"block_out_channels",val:": typing.Tuple[int] = (64,)"},{name:"layers_per_block",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"latent_channels",val:": int = 3"},{name:"sample_size",val:": int = 32"},{name:"num_vq_embeddings",val:": int = 256"},{name:"norm_num_groups",val:": int = 32"}],parametersDescription:[{anchor:"diffusers.VQModel.in_channels",description:"<strong>in_channels</strong> (int, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.VQModel.out_channels",description:"<strong>out_channels</strong> (int,  <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.VQModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownEncoderBlock2D&quot;,)</code>): Tuple of downsample block types.`,name:"down_block_types"},{anchor:"diffusers.VQModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;UpDecoderBlock2D&quot;,)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.VQModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(64,)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.VQModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.VQModel.latent_channels",description:"<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014; Number of channels in the latent space.",name:"latent_channels"},{anchor:"diffusers.VQModel.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; TODO",name:"sample_size"},{anchor:"diffusers.VQModel.num_vq_embeddings",description:"<strong>num_vq_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to <code>256</code>) &#x2014; Number of codebook vectors in the VQ-VAE.",name:"num_vq_embeddings"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L375"}}),Ze=new D({props:{name:"forward",anchor:"diffusers.VQModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; Input sample.",name:"sample"},{anchor:"diffusers.VQModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>DecoderOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L468"}}),eo=new B({}),oo=new D({props:{name:"class diffusers.models.vae.AutoencoderKLOutput",anchor:"diffusers.models.vae.AutoencoderKLOutput",parameters:[{name:"latent_dist",val:": DiagonalGaussianDistribution"}],parametersDescription:[{anchor:"diffusers.models.vae.AutoencoderKLOutput.latent_dist",description:`<strong>latent_dist</strong> (<code>DiagonalGaussianDistribution</code>) &#x2014;
Encoded outputs of <code>Encoder</code> represented as the mean and logvar of <code>DiagonalGaussianDistribution</code>.
<code>DiagonalGaussianDistribution</code> allows for sampling latents from the distribution.`,name:"latent_dist"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L41"}}),to=new B({}),no=new D({props:{name:"class diffusers.AutoencoderKL",anchor:"diffusers.AutoencoderKL",parameters:[{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownEncoderBlock2D',)"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpDecoderBlock2D',)"},{name:"block_out_channels",val:": typing.Tuple[int] = (64,)"},{name:"layers_per_block",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"latent_channels",val:": int = 4"},{name:"norm_num_groups",val:": int = 32"},{name:"sample_size",val:": int = 32"}],parametersDescription:[{anchor:"diffusers.AutoencoderKL.in_channels",description:"<strong>in_channels</strong> (int, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.AutoencoderKL.out_channels",description:"<strong>out_channels</strong> (int,  <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.AutoencoderKL.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownEncoderBlock2D&quot;,)</code>): Tuple of downsample block types.`,name:"down_block_types"},{anchor:"diffusers.AutoencoderKL.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;UpDecoderBlock2D&quot;,)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.AutoencoderKL.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(64,)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.AutoencoderKL.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.AutoencoderKL.latent_channels",description:"<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>4</code>) &#x2014; Number of channels in the latent space.",name:"latent_channels"},{anchor:"diffusers.AutoencoderKL.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; TODO",name:"sample_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L485"}}),so=new D({props:{name:"forward",anchor:"diffusers.AutoencoderKL.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"sample_posterior",val:": bool = False"},{name:"return_dict",val:": bool = True"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],parametersDescription:[{anchor:"diffusers.AutoencoderKL.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; Input sample.",name:"sample"},{anchor:"diffusers.AutoencoderKL.forward.sample_posterior",description:`<strong>sample_posterior</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to sample from the posterior.`,name:"sample_posterior"},{anchor:"diffusers.AutoencoderKL.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>DecoderOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L567"}}),{c(){b=n("meta"),P=l(),$=n("h1"),x=n("a"),T=n("span"),p(w.$$.fragment),E=l(),L=n("span"),Q=i("Models"),U=l(),A=n("p"),co=i(`Diffusers contains pretrained models for popular algorithms and modules for creating the next set of diffusion models.
The primary function of these models is to denoise an input sample, by modeling the distribution $p`),Mo=n("em"),dn=i("\\theta(\\mathbf{x}"),ln=i(`{t-1}|\\mathbf{x}_t)$.
The models are built on the base class [\u2018ModelMixin\u2019] that is a `),xo=n("code"),cn=i("torch.nn.module"),un=i(" with basic functionality for saving and loading models both locally and from the HuggingFace hub."),gt=l(),z=n("h2"),ne=n("a"),Eo=n("span"),p(we.$$.fragment),fn=l(),No=n("span"),pn=i("ModelMixin"),vt=l(),y=n("div"),p(De.$$.fragment),mn=l(),To=n("p"),hn=i("Base class for all models."),_n=l(),uo=n("p"),fo=n("a"),gn=i("ModelMixin"),vn=i(` takes care of storing the configuration of the models and handles methods for loading, downloading
and saving models.`),bn=l(),Uo=n("ul"),F=n("li"),Ao=n("strong"),$n=i("config_name"),yn=i(" ("),Oo=n("code"),wn=i("str"),Dn=i(`) \u2014 A filename under which the model should be stored when calling
`),po=n("a"),kn=i("save_pretrained()"),Mn=i("."),xn=l(),K=n("div"),p(ke.$$.fragment),En=l(),Co=n("p"),Nn=i("Deactivates gradient checkpointing for the current model."),Tn=l(),qo=n("p"),Un=i(`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),An=l(),I=n("div"),p(Me.$$.fragment),On=l(),Vo=n("p"),Cn=i("Activates gradient checkpointing for the current model."),qn=l(),Po=n("p"),Vn=i(`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),Pn=l(),k=n("div"),p(xe.$$.fragment),Ln=l(),Lo=n("p"),Bn=i("Instantiate a pretrained pytorch model from a pre-trained model configuration."),Fn=l(),S=n("p"),Kn=i("The model is set in evaluation mode by default using "),Bo=n("code"),In=i("model.eval()"),Qn=i(` (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with `),Fo=n("code"),zn=i("model.train()"),Sn=i("."),Wn=l(),Ee=n("p"),Xn=i("The warning "),Ko=n("em"),Hn=i("Weights from XXX not initialized from pretrained model"),Yn=i(` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),jn=l(),Ne=n("p"),Gn=i("The warning "),Io=n("em"),Rn=i("Weights from XXX not used in YYY"),Jn=i(` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),Zn=l(),p(re.$$.fragment),er=l(),p(se.$$.fragment),or=l(),ae=n("div"),p(Te.$$.fragment),tr=l(),Qo=n("p"),nr=i("Get number of (optionally, trainable or non-embeddings) parameters in the module."),rr=l(),ie=n("div"),p(Ue.$$.fragment),sr=l(),Ae=n("p"),ar=i(`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),zo=n("code"),ir=i("[from_pretrained()](/docs/diffusers/main/en/api/models#diffusers.ModelMixin.from_pretrained)"),dr=i(" class method."),bt=l(),W=n("h2"),de=n("a"),So=n("span"),p(Oe.$$.fragment),lr=l(),Wo=n("span"),cr=i("UNet2DOutput"),$t=l(),Ce=n("div"),p(qe.$$.fragment),yt=l(),X=n("h2"),le=n("a"),Xo=n("span"),p(Ve.$$.fragment),ur=l(),Ho=n("span"),fr=i("UNet2DModel"),wt=l(),O=n("div"),p(Pe.$$.fragment),pr=l(),Yo=n("p"),mr=i("UNet2DModel is a 2D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),hr=l(),Le=n("p"),_r=i("This model inherits from "),mo=n("a"),gr=i("ModelMixin"),vr=i(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),br=l(),ho=n("div"),p(Be.$$.fragment),Dt=l(),H=n("h2"),ce=n("a"),jo=n("span"),p(Fe.$$.fragment),$r=l(),Go=n("span"),yr=i("UNet2DConditionOutput"),kt=l(),Ke=n("div"),p(Ie.$$.fragment),Mt=l(),Y=n("h2"),ue=n("a"),Ro=n("span"),p(Qe.$$.fragment),wr=l(),Jo=n("span"),Dr=i("UNet2DConditionModel"),xt=l(),C=n("div"),p(ze.$$.fragment),kr=l(),Zo=n("p"),Mr=i(`UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep
and returns sample shaped output.`),xr=l(),Se=n("p"),Er=i("This model inherits from "),_o=n("a"),Nr=i("ModelMixin"),Tr=i(`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),Ur=l(),fe=n("div"),p(We.$$.fragment),Ar=l(),et=n("p"),Or=i("r"),Et=l(),j=n("h2"),pe=n("a"),ot=n("span"),p(Xe.$$.fragment),Cr=l(),tt=n("span"),qr=i("DecoderOutput"),Nt=l(),G=n("div"),p(He.$$.fragment),Vr=l(),nt=n("p"),Pr=i("Output of decoding method."),Tt=l(),R=n("h2"),me=n("a"),rt=n("span"),p(Ye.$$.fragment),Lr=l(),st=n("span"),Br=i("VQEncoderOutput"),Ut=l(),J=n("div"),p(je.$$.fragment),Fr=l(),at=n("p"),Kr=i("Output of VQModel encoding method."),At=l(),Z=n("h2"),he=n("a"),it=n("span"),p(Ge.$$.fragment),Ir=l(),dt=n("span"),Qr=i("VQModel"),Ot=l(),q=n("div"),p(Re.$$.fragment),zr=l(),lt=n("p"),Sr=i(`VQ-VAE model from the paper Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray
Kavukcuoglu.`),Wr=l(),Je=n("p"),Xr=i("This model inherits from "),go=n("a"),Hr=i("ModelMixin"),Yr=i(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),jr=l(),vo=n("div"),p(Ze.$$.fragment),Ct=l(),ee=n("h2"),_e=n("a"),ct=n("span"),p(eo.$$.fragment),Gr=l(),ut=n("span"),Rr=i("AutoencoderKLOutput"),qt=l(),oe=n("div"),p(oo.$$.fragment),Jr=l(),ft=n("p"),Zr=i("Output of AutoencoderKL encoding method."),Vt=l(),te=n("h2"),ge=n("a"),pt=n("span"),p(to.$$.fragment),es=l(),mt=n("span"),os=i("AutoencoderKL"),Pt=l(),V=n("div"),p(no.$$.fragment),ts=l(),ht=n("p"),ns=i(`Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma
and Max Welling.`),rs=l(),ro=n("p"),ss=i("This model inherits from "),bo=n("a"),as=i("ModelMixin"),is=i(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),ds=l(),$o=n("div"),p(so.$$.fragment),this.h()},l(o){const u=Na('[data-svelte="svelte-1phssyn"]',document.head);b=r(u,"META",{name:!0,content:!0}),u.forEach(t),P=c(o),$=r(o,"H1",{class:!0});var ao=s($);x=r(ao,"A",{id:!0,class:!0,href:!0});var _t=s(x);T=r(_t,"SPAN",{});var cs=s(T);m(w.$$.fragment,cs),cs.forEach(t),_t.forEach(t),E=c(ao),L=r(ao,"SPAN",{});var us=s(L);Q=d(us,"Models"),us.forEach(t),ao.forEach(t),U=c(o),A=r(o,"P",{});var yo=s(A);co=d(yo,`Diffusers contains pretrained models for popular algorithms and modules for creating the next set of diffusion models.
The primary function of these models is to denoise an input sample, by modeling the distribution $p`),Mo=r(yo,"EM",{});var fs=s(Mo);dn=d(fs,"\\theta(\\mathbf{x}"),fs.forEach(t),ln=d(yo,`{t-1}|\\mathbf{x}_t)$.
The models are built on the base class [\u2018ModelMixin\u2019] that is a `),xo=r(yo,"CODE",{});var ps=s(xo);cn=d(ps,"torch.nn.module"),ps.forEach(t),un=d(yo," with basic functionality for saving and loading models both locally and from the HuggingFace hub."),yo.forEach(t),gt=c(o),z=r(o,"H2",{class:!0});var Bt=s(z);ne=r(Bt,"A",{id:!0,class:!0,href:!0});var ms=s(ne);Eo=r(ms,"SPAN",{});var hs=s(Eo);m(we.$$.fragment,hs),hs.forEach(t),ms.forEach(t),fn=c(Bt),No=r(Bt,"SPAN",{});var _s=s(No);pn=d(_s,"ModelMixin"),_s.forEach(t),Bt.forEach(t),vt=c(o),y=r(o,"DIV",{class:!0});var M=s(y);m(De.$$.fragment,M),mn=c(M),To=r(M,"P",{});var gs=s(To);hn=d(gs,"Base class for all models."),gs.forEach(t),_n=c(M),uo=r(M,"P",{});var ls=s(uo);fo=r(ls,"A",{href:!0});var vs=s(fo);gn=d(vs,"ModelMixin"),vs.forEach(t),vn=d(ls,` takes care of storing the configuration of the models and handles methods for loading, downloading
and saving models.`),ls.forEach(t),bn=c(M),Uo=r(M,"UL",{});var bs=s(Uo);F=r(bs,"LI",{});var io=s(F);Ao=r(io,"STRONG",{});var $s=s(Ao);$n=d($s,"config_name"),$s.forEach(t),yn=d(io," ("),Oo=r(io,"CODE",{});var ys=s(Oo);wn=d(ys,"str"),ys.forEach(t),Dn=d(io,`) \u2014 A filename under which the model should be stored when calling
`),po=r(io,"A",{href:!0});var ws=s(po);kn=d(ws,"save_pretrained()"),ws.forEach(t),Mn=d(io,"."),io.forEach(t),bs.forEach(t),xn=c(M),K=r(M,"DIV",{class:!0});var wo=s(K);m(ke.$$.fragment,wo),En=c(wo),Co=r(wo,"P",{});var Ds=s(Co);Nn=d(Ds,"Deactivates gradient checkpointing for the current model."),Ds.forEach(t),Tn=c(wo),qo=r(wo,"P",{});var ks=s(qo);Un=d(ks,`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),ks.forEach(t),wo.forEach(t),An=c(M),I=r(M,"DIV",{class:!0});var Do=s(I);m(Me.$$.fragment,Do),On=c(Do),Vo=r(Do,"P",{});var Ms=s(Vo);Cn=d(Ms,"Activates gradient checkpointing for the current model."),Ms.forEach(t),qn=c(Do),Po=r(Do,"P",{});var xs=s(Po);Vn=d(xs,`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),xs.forEach(t),Do.forEach(t),Pn=c(M),k=r(M,"DIV",{class:!0});var N=s(k);m(xe.$$.fragment,N),Ln=c(N),Lo=r(N,"P",{});var Es=s(Lo);Bn=d(Es,"Instantiate a pretrained pytorch model from a pre-trained model configuration."),Es.forEach(t),Fn=c(N),S=r(N,"P",{});var ko=s(S);Kn=d(ko,"The model is set in evaluation mode by default using "),Bo=r(ko,"CODE",{});var Ns=s(Bo);In=d(Ns,"model.eval()"),Ns.forEach(t),Qn=d(ko,` (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with `),Fo=r(ko,"CODE",{});var Ts=s(Fo);zn=d(Ts,"model.train()"),Ts.forEach(t),Sn=d(ko,"."),ko.forEach(t),Wn=c(N),Ee=r(N,"P",{});var Ft=s(Ee);Xn=d(Ft,"The warning "),Ko=r(Ft,"EM",{});var Us=s(Ko);Hn=d(Us,"Weights from XXX not initialized from pretrained model"),Us.forEach(t),Yn=d(Ft,` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),Ft.forEach(t),jn=c(N),Ne=r(N,"P",{});var Kt=s(Ne);Gn=d(Kt,"The warning "),Io=r(Kt,"EM",{});var As=s(Io);Rn=d(As,"Weights from XXX not used in YYY"),As.forEach(t),Jn=d(Kt,` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),Kt.forEach(t),Zn=c(N),m(re.$$.fragment,N),er=c(N),m(se.$$.fragment,N),N.forEach(t),or=c(M),ae=r(M,"DIV",{class:!0});var It=s(ae);m(Te.$$.fragment,It),tr=c(It),Qo=r(It,"P",{});var Os=s(Qo);nr=d(Os,"Get number of (optionally, trainable or non-embeddings) parameters in the module."),Os.forEach(t),It.forEach(t),rr=c(M),ie=r(M,"DIV",{class:!0});var Qt=s(ie);m(Ue.$$.fragment,Qt),sr=c(Qt),Ae=r(Qt,"P",{});var zt=s(Ae);ar=d(zt,`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),zo=r(zt,"CODE",{});var Cs=s(zo);ir=d(Cs,"[from_pretrained()](/docs/diffusers/main/en/api/models#diffusers.ModelMixin.from_pretrained)"),Cs.forEach(t),dr=d(zt," class method."),zt.forEach(t),Qt.forEach(t),M.forEach(t),bt=c(o),W=r(o,"H2",{class:!0});var St=s(W);de=r(St,"A",{id:!0,class:!0,href:!0});var qs=s(de);So=r(qs,"SPAN",{});var Vs=s(So);m(Oe.$$.fragment,Vs),Vs.forEach(t),qs.forEach(t),lr=c(St),Wo=r(St,"SPAN",{});var Ps=s(Wo);cr=d(Ps,"UNet2DOutput"),Ps.forEach(t),St.forEach(t),$t=c(o),Ce=r(o,"DIV",{class:!0});var Ls=s(Ce);m(qe.$$.fragment,Ls),Ls.forEach(t),yt=c(o),X=r(o,"H2",{class:!0});var Wt=s(X);le=r(Wt,"A",{id:!0,class:!0,href:!0});var Bs=s(le);Xo=r(Bs,"SPAN",{});var Fs=s(Xo);m(Ve.$$.fragment,Fs),Fs.forEach(t),Bs.forEach(t),ur=c(Wt),Ho=r(Wt,"SPAN",{});var Ks=s(Ho);fr=d(Ks,"UNet2DModel"),Ks.forEach(t),Wt.forEach(t),wt=c(o),O=r(o,"DIV",{class:!0});var ve=s(O);m(Pe.$$.fragment,ve),pr=c(ve),Yo=r(ve,"P",{});var Is=s(Yo);mr=d(Is,"UNet2DModel is a 2D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),Is.forEach(t),hr=c(ve),Le=r(ve,"P",{});var Xt=s(Le);_r=d(Xt,"This model inherits from "),mo=r(Xt,"A",{href:!0});var Qs=s(mo);gr=d(Qs,"ModelMixin"),Qs.forEach(t),vr=d(Xt,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Xt.forEach(t),br=c(ve),ho=r(ve,"DIV",{class:!0});var zs=s(ho);m(Be.$$.fragment,zs),zs.forEach(t),ve.forEach(t),Dt=c(o),H=r(o,"H2",{class:!0});var Ht=s(H);ce=r(Ht,"A",{id:!0,class:!0,href:!0});var Ss=s(ce);jo=r(Ss,"SPAN",{});var Ws=s(jo);m(Fe.$$.fragment,Ws),Ws.forEach(t),Ss.forEach(t),$r=c(Ht),Go=r(Ht,"SPAN",{});var Xs=s(Go);yr=d(Xs,"UNet2DConditionOutput"),Xs.forEach(t),Ht.forEach(t),kt=c(o),Ke=r(o,"DIV",{class:!0});var Hs=s(Ke);m(Ie.$$.fragment,Hs),Hs.forEach(t),Mt=c(o),Y=r(o,"H2",{class:!0});var Yt=s(Y);ue=r(Yt,"A",{id:!0,class:!0,href:!0});var Ys=s(ue);Ro=r(Ys,"SPAN",{});var js=s(Ro);m(Qe.$$.fragment,js),js.forEach(t),Ys.forEach(t),wr=c(Yt),Jo=r(Yt,"SPAN",{});var Gs=s(Jo);Dr=d(Gs,"UNet2DConditionModel"),Gs.forEach(t),Yt.forEach(t),xt=c(o),C=r(o,"DIV",{class:!0});var be=s(C);m(ze.$$.fragment,be),kr=c(be),Zo=r(be,"P",{});var Rs=s(Zo);Mr=d(Rs,`UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep
and returns sample shaped output.`),Rs.forEach(t),xr=c(be),Se=r(be,"P",{});var jt=s(Se);Er=d(jt,"This model inherits from "),_o=r(jt,"A",{href:!0});var Js=s(_o);Nr=d(Js,"ModelMixin"),Js.forEach(t),Tr=d(jt,`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),jt.forEach(t),Ur=c(be),fe=r(be,"DIV",{class:!0});var Gt=s(fe);m(We.$$.fragment,Gt),Ar=c(Gt),et=r(Gt,"P",{});var Zs=s(et);Or=d(Zs,"r"),Zs.forEach(t),Gt.forEach(t),be.forEach(t),Et=c(o),j=r(o,"H2",{class:!0});var Rt=s(j);pe=r(Rt,"A",{id:!0,class:!0,href:!0});var ea=s(pe);ot=r(ea,"SPAN",{});var oa=s(ot);m(Xe.$$.fragment,oa),oa.forEach(t),ea.forEach(t),Cr=c(Rt),tt=r(Rt,"SPAN",{});var ta=s(tt);qr=d(ta,"DecoderOutput"),ta.forEach(t),Rt.forEach(t),Nt=c(o),G=r(o,"DIV",{class:!0});var Jt=s(G);m(He.$$.fragment,Jt),Vr=c(Jt),nt=r(Jt,"P",{});var na=s(nt);Pr=d(na,"Output of decoding method."),na.forEach(t),Jt.forEach(t),Tt=c(o),R=r(o,"H2",{class:!0});var Zt=s(R);me=r(Zt,"A",{id:!0,class:!0,href:!0});var ra=s(me);rt=r(ra,"SPAN",{});var sa=s(rt);m(Ye.$$.fragment,sa),sa.forEach(t),ra.forEach(t),Lr=c(Zt),st=r(Zt,"SPAN",{});var aa=s(st);Br=d(aa,"VQEncoderOutput"),aa.forEach(t),Zt.forEach(t),Ut=c(o),J=r(o,"DIV",{class:!0});var en=s(J);m(je.$$.fragment,en),Fr=c(en),at=r(en,"P",{});var ia=s(at);Kr=d(ia,"Output of VQModel encoding method."),ia.forEach(t),en.forEach(t),At=c(o),Z=r(o,"H2",{class:!0});var on=s(Z);he=r(on,"A",{id:!0,class:!0,href:!0});var da=s(he);it=r(da,"SPAN",{});var la=s(it);m(Ge.$$.fragment,la),la.forEach(t),da.forEach(t),Ir=c(on),dt=r(on,"SPAN",{});var ca=s(dt);Qr=d(ca,"VQModel"),ca.forEach(t),on.forEach(t),Ot=c(o),q=r(o,"DIV",{class:!0});var $e=s(q);m(Re.$$.fragment,$e),zr=c($e),lt=r($e,"P",{});var ua=s(lt);Sr=d(ua,`VQ-VAE model from the paper Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray
Kavukcuoglu.`),ua.forEach(t),Wr=c($e),Je=r($e,"P",{});var tn=s(Je);Xr=d(tn,"This model inherits from "),go=r(tn,"A",{href:!0});var fa=s(go);Hr=d(fa,"ModelMixin"),fa.forEach(t),Yr=d(tn,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),tn.forEach(t),jr=c($e),vo=r($e,"DIV",{class:!0});var pa=s(vo);m(Ze.$$.fragment,pa),pa.forEach(t),$e.forEach(t),Ct=c(o),ee=r(o,"H2",{class:!0});var nn=s(ee);_e=r(nn,"A",{id:!0,class:!0,href:!0});var ma=s(_e);ct=r(ma,"SPAN",{});var ha=s(ct);m(eo.$$.fragment,ha),ha.forEach(t),ma.forEach(t),Gr=c(nn),ut=r(nn,"SPAN",{});var _a=s(ut);Rr=d(_a,"AutoencoderKLOutput"),_a.forEach(t),nn.forEach(t),qt=c(o),oe=r(o,"DIV",{class:!0});var rn=s(oe);m(oo.$$.fragment,rn),Jr=c(rn),ft=r(rn,"P",{});var ga=s(ft);Zr=d(ga,"Output of AutoencoderKL encoding method."),ga.forEach(t),rn.forEach(t),Vt=c(o),te=r(o,"H2",{class:!0});var sn=s(te);ge=r(sn,"A",{id:!0,class:!0,href:!0});var va=s(ge);pt=r(va,"SPAN",{});var ba=s(pt);m(to.$$.fragment,ba),ba.forEach(t),va.forEach(t),es=c(sn),mt=r(sn,"SPAN",{});var $a=s(mt);os=d($a,"AutoencoderKL"),$a.forEach(t),sn.forEach(t),Pt=c(o),V=r(o,"DIV",{class:!0});var ye=s(V);m(no.$$.fragment,ye),ts=c(ye),ht=r(ye,"P",{});var ya=s(ht);ns=d(ya,`Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma
and Max Welling.`),ya.forEach(t),rs=c(ye),ro=r(ye,"P",{});var an=s(ro);ss=d(an,"This model inherits from "),bo=r(an,"A",{href:!0});var wa=s(bo);as=d(wa,"ModelMixin"),wa.forEach(t),is=d(an,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),an.forEach(t),ds=c(ye),$o=r(ye,"DIV",{class:!0});var Da=s($o);m(so.$$.fragment,Da),Da.forEach(t),ye.forEach(t),this.h()},h(){a(b,"name","hf:doc:metadata"),a(b,"content",JSON.stringify(Ca)),a(x,"id","models"),a(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(x,"href","#models"),a($,"class","relative group"),a(ne,"id","diffusers.ModelMixin"),a(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ne,"href","#diffusers.ModelMixin"),a(z,"class","relative group"),a(fo,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin"),a(po,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin.save_pretrained"),a(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(de,"id","diffusers.models.unet_2d.UNet2DOutput"),a(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(de,"href","#diffusers.models.unet_2d.UNet2DOutput"),a(W,"class","relative group"),a(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(le,"id","diffusers.UNet2DModel"),a(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(le,"href","#diffusers.UNet2DModel"),a(X,"class","relative group"),a(mo,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin"),a(ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ce,"id","diffusers.models.unet_2d_condition.UNet2DConditionOutput"),a(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ce,"href","#diffusers.models.unet_2d_condition.UNet2DConditionOutput"),a(H,"class","relative group"),a(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ue,"id","diffusers.UNet2DConditionModel"),a(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ue,"href","#diffusers.UNet2DConditionModel"),a(Y,"class","relative group"),a(_o,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin"),a(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(pe,"id","diffusers.models.vae.DecoderOutput"),a(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(pe,"href","#diffusers.models.vae.DecoderOutput"),a(j,"class","relative group"),a(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(me,"id","diffusers.models.vae.VQEncoderOutput"),a(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(me,"href","#diffusers.models.vae.VQEncoderOutput"),a(R,"class","relative group"),a(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(he,"id","diffusers.VQModel"),a(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(he,"href","#diffusers.VQModel"),a(Z,"class","relative group"),a(go,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin"),a(vo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(_e,"id","diffusers.models.vae.AutoencoderKLOutput"),a(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(_e,"href","#diffusers.models.vae.AutoencoderKLOutput"),a(ee,"class","relative group"),a(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ge,"id","diffusers.AutoencoderKL"),a(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ge,"href","#diffusers.AutoencoderKL"),a(te,"class","relative group"),a(bo,"href","/docs/diffusers/main/en/api/models#diffusers.ModelMixin"),a($o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,u){e(document.head,b),f(o,P,u),f(o,$,u),e($,x),e(x,T),h(w,T,null),e($,E),e($,L),e(L,Q),f(o,U,u),f(o,A,u),e(A,co),e(A,Mo),e(Mo,dn),e(A,ln),e(A,xo),e(xo,cn),e(A,un),f(o,gt,u),f(o,z,u),e(z,ne),e(ne,Eo),h(we,Eo,null),e(z,fn),e(z,No),e(No,pn),f(o,vt,u),f(o,y,u),h(De,y,null),e(y,mn),e(y,To),e(To,hn),e(y,_n),e(y,uo),e(uo,fo),e(fo,gn),e(uo,vn),e(y,bn),e(y,Uo),e(Uo,F),e(F,Ao),e(Ao,$n),e(F,yn),e(F,Oo),e(Oo,wn),e(F,Dn),e(F,po),e(po,kn),e(F,Mn),e(y,xn),e(y,K),h(ke,K,null),e(K,En),e(K,Co),e(Co,Nn),e(K,Tn),e(K,qo),e(qo,Un),e(y,An),e(y,I),h(Me,I,null),e(I,On),e(I,Vo),e(Vo,Cn),e(I,qn),e(I,Po),e(Po,Vn),e(y,Pn),e(y,k),h(xe,k,null),e(k,Ln),e(k,Lo),e(Lo,Bn),e(k,Fn),e(k,S),e(S,Kn),e(S,Bo),e(Bo,In),e(S,Qn),e(S,Fo),e(Fo,zn),e(S,Sn),e(k,Wn),e(k,Ee),e(Ee,Xn),e(Ee,Ko),e(Ko,Hn),e(Ee,Yn),e(k,jn),e(k,Ne),e(Ne,Gn),e(Ne,Io),e(Io,Rn),e(Ne,Jn),e(k,Zn),h(re,k,null),e(k,er),h(se,k,null),e(y,or),e(y,ae),h(Te,ae,null),e(ae,tr),e(ae,Qo),e(Qo,nr),e(y,rr),e(y,ie),h(Ue,ie,null),e(ie,sr),e(ie,Ae),e(Ae,ar),e(Ae,zo),e(zo,ir),e(Ae,dr),f(o,bt,u),f(o,W,u),e(W,de),e(de,So),h(Oe,So,null),e(W,lr),e(W,Wo),e(Wo,cr),f(o,$t,u),f(o,Ce,u),h(qe,Ce,null),f(o,yt,u),f(o,X,u),e(X,le),e(le,Xo),h(Ve,Xo,null),e(X,ur),e(X,Ho),e(Ho,fr),f(o,wt,u),f(o,O,u),h(Pe,O,null),e(O,pr),e(O,Yo),e(Yo,mr),e(O,hr),e(O,Le),e(Le,_r),e(Le,mo),e(mo,gr),e(Le,vr),e(O,br),e(O,ho),h(Be,ho,null),f(o,Dt,u),f(o,H,u),e(H,ce),e(ce,jo),h(Fe,jo,null),e(H,$r),e(H,Go),e(Go,yr),f(o,kt,u),f(o,Ke,u),h(Ie,Ke,null),f(o,Mt,u),f(o,Y,u),e(Y,ue),e(ue,Ro),h(Qe,Ro,null),e(Y,wr),e(Y,Jo),e(Jo,Dr),f(o,xt,u),f(o,C,u),h(ze,C,null),e(C,kr),e(C,Zo),e(Zo,Mr),e(C,xr),e(C,Se),e(Se,Er),e(Se,_o),e(_o,Nr),e(Se,Tr),e(C,Ur),e(C,fe),h(We,fe,null),e(fe,Ar),e(fe,et),e(et,Or),f(o,Et,u),f(o,j,u),e(j,pe),e(pe,ot),h(Xe,ot,null),e(j,Cr),e(j,tt),e(tt,qr),f(o,Nt,u),f(o,G,u),h(He,G,null),e(G,Vr),e(G,nt),e(nt,Pr),f(o,Tt,u),f(o,R,u),e(R,me),e(me,rt),h(Ye,rt,null),e(R,Lr),e(R,st),e(st,Br),f(o,Ut,u),f(o,J,u),h(je,J,null),e(J,Fr),e(J,at),e(at,Kr),f(o,At,u),f(o,Z,u),e(Z,he),e(he,it),h(Ge,it,null),e(Z,Ir),e(Z,dt),e(dt,Qr),f(o,Ot,u),f(o,q,u),h(Re,q,null),e(q,zr),e(q,lt),e(lt,Sr),e(q,Wr),e(q,Je),e(Je,Xr),e(Je,go),e(go,Hr),e(Je,Yr),e(q,jr),e(q,vo),h(Ze,vo,null),f(o,Ct,u),f(o,ee,u),e(ee,_e),e(_e,ct),h(eo,ct,null),e(ee,Gr),e(ee,ut),e(ut,Rr),f(o,qt,u),f(o,oe,u),h(oo,oe,null),e(oe,Jr),e(oe,ft),e(ft,Zr),f(o,Vt,u),f(o,te,u),e(te,ge),e(ge,pt),h(to,pt,null),e(te,es),e(te,mt),e(mt,os),f(o,Pt,u),f(o,V,u),h(no,V,null),e(V,ts),e(V,ht),e(ht,ns),e(V,rs),e(V,ro),e(ro,ss),e(ro,bo),e(bo,as),e(ro,is),e(V,ds),e(V,$o),h(so,$o,null),Lt=!0},p(o,[u]){const ao={};u&2&&(ao.$$scope={dirty:u,ctx:o}),re.$set(ao);const _t={};u&2&&(_t.$$scope={dirty:u,ctx:o}),se.$set(_t)},i(o){Lt||(_(w.$$.fragment,o),_(we.$$.fragment,o),_(De.$$.fragment,o),_(ke.$$.fragment,o),_(Me.$$.fragment,o),_(xe.$$.fragment,o),_(re.$$.fragment,o),_(se.$$.fragment,o),_(Te.$$.fragment,o),_(Ue.$$.fragment,o),_(Oe.$$.fragment,o),_(qe.$$.fragment,o),_(Ve.$$.fragment,o),_(Pe.$$.fragment,o),_(Be.$$.fragment,o),_(Fe.$$.fragment,o),_(Ie.$$.fragment,o),_(Qe.$$.fragment,o),_(ze.$$.fragment,o),_(We.$$.fragment,o),_(Xe.$$.fragment,o),_(He.$$.fragment,o),_(Ye.$$.fragment,o),_(je.$$.fragment,o),_(Ge.$$.fragment,o),_(Re.$$.fragment,o),_(Ze.$$.fragment,o),_(eo.$$.fragment,o),_(oo.$$.fragment,o),_(to.$$.fragment,o),_(no.$$.fragment,o),_(so.$$.fragment,o),Lt=!0)},o(o){g(w.$$.fragment,o),g(we.$$.fragment,o),g(De.$$.fragment,o),g(ke.$$.fragment,o),g(Me.$$.fragment,o),g(xe.$$.fragment,o),g(re.$$.fragment,o),g(se.$$.fragment,o),g(Te.$$.fragment,o),g(Ue.$$.fragment,o),g(Oe.$$.fragment,o),g(qe.$$.fragment,o),g(Ve.$$.fragment,o),g(Pe.$$.fragment,o),g(Be.$$.fragment,o),g(Fe.$$.fragment,o),g(Ie.$$.fragment,o),g(Qe.$$.fragment,o),g(ze.$$.fragment,o),g(We.$$.fragment,o),g(Xe.$$.fragment,o),g(He.$$.fragment,o),g(Ye.$$.fragment,o),g(je.$$.fragment,o),g(Ge.$$.fragment,o),g(Re.$$.fragment,o),g(Ze.$$.fragment,o),g(eo.$$.fragment,o),g(oo.$$.fragment,o),g(to.$$.fragment,o),g(no.$$.fragment,o),g(so.$$.fragment,o),Lt=!1},d(o){t(b),o&&t(P),o&&t($),v(w),o&&t(U),o&&t(A),o&&t(gt),o&&t(z),v(we),o&&t(vt),o&&t(y),v(De),v(ke),v(Me),v(xe),v(re),v(se),v(Te),v(Ue),o&&t(bt),o&&t(W),v(Oe),o&&t($t),o&&t(Ce),v(qe),o&&t(yt),o&&t(X),v(Ve),o&&t(wt),o&&t(O),v(Pe),v(Be),o&&t(Dt),o&&t(H),v(Fe),o&&t(kt),o&&t(Ke),v(Ie),o&&t(Mt),o&&t(Y),v(Qe),o&&t(xt),o&&t(C),v(ze),v(We),o&&t(Et),o&&t(j),v(Xe),o&&t(Nt),o&&t(G),v(He),o&&t(Tt),o&&t(R),v(Ye),o&&t(Ut),o&&t(J),v(je),o&&t(At),o&&t(Z),v(Ge),o&&t(Ot),o&&t(q),v(Re),v(Ze),o&&t(Ct),o&&t(ee),v(eo),o&&t(qt),o&&t(oe),v(oo),o&&t(Vt),o&&t(te),v(to),o&&t(Pt),o&&t(V),v(no),v(so)}}}const Ca={local:"models",sections:[{local:"diffusers.ModelMixin",title:"ModelMixin"},{local:"diffusers.models.unet_2d.UNet2DOutput",title:"UNet2DOutput"},{local:"diffusers.UNet2DModel",title:"UNet2DModel"},{local:"diffusers.models.unet_2d_condition.UNet2DConditionOutput",title:"UNet2DConditionOutput"},{local:"diffusers.UNet2DConditionModel",title:"UNet2DConditionModel"},{local:"diffusers.models.vae.DecoderOutput",title:"DecoderOutput"},{local:"diffusers.models.vae.VQEncoderOutput",title:"VQEncoderOutput"},{local:"diffusers.VQModel",title:"VQModel"},{local:"diffusers.models.vae.AutoencoderKLOutput",title:"AutoencoderKLOutput"},{local:"diffusers.AutoencoderKL",title:"AutoencoderKL"}],title:"Models"};function qa(lo){return Ta(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fa extends Ma{constructor(b){super();xa(this,b,qa,Oa,Ea,{})}}export{Fa as default,Ca as metadata};
