import{S as to,i as io,s as ro,e as r,k as f,w as me,t as s,M as no,c as n,d as t,m as d,a as o,x as ge,h as a,b as l,N as Yt,G as e,g as u,y as _e,L as oo,q as be,o as ve,B as De,v as so}from"../../../chunks/vendor-hf-doc-builder.js";import{D as qe}from"../../../chunks/Docstring-hf-doc-builder.js";import{I as ao}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function lo(Vr){let L,yt,k,M,Ye,G,Wt,We,Jt,Lt,h,Qt,Je,Xt,Zt,R,ei,ti,F,ii,ri,B,ni,oi,K,si,ai,kt,I,li,H,fi,di,Se,ci,ui,Ct,x,hi,j,pi,mi,q,gi,_i,Mt,Y,Qe,bi,vi,Tt,T,Ie,Di,W,xe,Gr,Si,Xe,Ii,wt,w,Ze,b,et,xi,Pi,tt,Ei,yi,Pe,Li,ki,Ee,Ci,Mi,C,v,it,J,Ti,wi,rt,nt,$i,Ai,ye,Q,Le,Rr,zi,ke,X,Ni,Ui,D,ot,Z,Oi,Vi,st,at,Gi,Ri,Ce,ee,Me,Fr,Fi,Te,te,Bi,Ki,S,lt,ie,Hi,ji,re,ft,qi,Yi,dt,Wi,Ji,we,ne,$e,Br,Qi,Ae,Xi,$t,p,oe,Zi,ct,er,tr,se,ir,ze,rr,nr,or,Ne,ae,At,P,ut,ht,sr,ar,pt,lr,fr,mt,dr,zt,m,le,cr,gt,ur,hr,fe,pr,Ue,mr,gr,_r,Oe,de,Nt,E,_t,bt,br,vr,vt,Dr,Sr,Dt,Ir,Ut,g,ce,xr,ue,Pr,St,Er,yr,Lr,he,kr,Ve,Cr,Mr,Tr,Ge,pe,Ot,y,It,xt,wr,$r,Pt,Ar,zr,Et,Nr,Vt;return G=new ao({}),oe=new qe({props:{name:"class diffusers.StableDiffusionPipeline",anchor:"diffusers.StableDiffusionPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L16"}}),ae=new qe({props:{name:"__init__",anchor:"diffusers.StableDiffusionPipeline.__init__",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L44"}}),le=new qe({props:{name:"class diffusers.StableDiffusionImg2ImgPipeline",anchor:"diffusers.StableDiffusionImg2ImgPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L27"}}),de=new qe({props:{name:"__init__",anchor:"diffusers.StableDiffusionImg2ImgPipeline.__init__",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L55"}}),ce=new qe({props:{name:"class diffusers.StableDiffusionInpaintPipeline",anchor:"diffusers.StableDiffusionInpaintPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionInpaintPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionInpaintPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionInpaintPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionInpaintPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionInpaintPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionInpaintPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L41"}}),pe=new qe({props:{name:"__init__",anchor:"diffusers.StableDiffusionInpaintPipeline.__init__",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L69"}}),{c(){L=r("meta"),yt=f(),k=r("h1"),M=r("a"),Ye=r("span"),me(G.$$.fragment),Wt=f(),We=r("span"),Jt=s("Stable diffusion pipelines"),Lt=f(),h=r("p"),Qt=s("Stable Diffusion is a text-to-image "),Je=r("em"),Xt=s("latent diffusion"),Zt=s(" model created by the researchers and engineers from "),R=r("a"),ei=s("CompVis"),ti=s(", "),F=r("a"),ii=s("Stability AI"),ri=s(" and "),B=r("a"),ni=s("LAION"),oi=s(". It\u2019s trained on 512x512 images from a subset of the "),K=r("a"),si=s("LAION-5B"),ai=s(" dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),kt=f(),I=r("p"),li=s("Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),H=r("a"),fi=s("High-Resolution Image Synthesis with Latent Diffusion Models"),di=s(" by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),Se=r("a"),ci=s("specific pipeline for latent diffusion"),ui=s(" that is part of \u{1F917} Diffusers."),Ct=f(),x=r("p"),hi=s("For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),j=r("a"),pi=s("launch announcement post"),mi=s(" and "),q=r("a"),gi=s("this section of our own blog post"),_i=s("."),Mt=f(),Y=r("p"),Qe=r("em"),bi=s("Tips"),vi=s(":"),Tt=f(),T=r("ul"),Ie=r("li"),Di=s("To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),W=r("a"),xe=r("img"),Si=f(),Xe=r("li"),Ii=s("TODO: some interesting Tips"),wt=f(),w=r("table"),Ze=r("thead"),b=r("tr"),et=r("th"),xi=s("Pipeline"),Pi=f(),tt=r("th"),Ei=s("Tasks"),yi=f(),Pe=r("th"),Li=s("Colab"),ki=f(),Ee=r("th"),Ci=s("Demo"),Mi=f(),C=r("tbody"),v=r("tr"),it=r("td"),J=r("a"),Ti=s("pipeline_stable_diffusion.py"),wi=f(),rt=r("td"),nt=r("em"),$i=s("Text-to-Image Generation"),Ai=f(),ye=r("td"),Q=r("a"),Le=r("img"),zi=f(),ke=r("td"),X=r("a"),Ni=s("\u{1F917} Stable Diffusion"),Ui=f(),D=r("tr"),ot=r("td"),Z=r("a"),Oi=s("pipeline_stable_diffusion_img2img.py"),Vi=f(),st=r("td"),at=r("em"),Gi=s("Image-to-Image Text-Guided Generation"),Ri=f(),Ce=r("td"),ee=r("a"),Me=r("img"),Fi=f(),Te=r("td"),te=r("a"),Bi=s("\u{1F917} Diffuse the Rest"),Ki=f(),S=r("tr"),lt=r("td"),ie=r("a"),Hi=s("pipeline_stable_diffusion_inpaint.py"),ji=f(),re=r("td"),ft=r("strong"),qi=s("Experimental"),Yi=s(" \u2013 "),dt=r("em"),Wi=s("Text-Guided Image Inpainting"),Ji=f(),we=r("td"),ne=r("a"),$e=r("img"),Qi=f(),Ae=r("td"),Xi=s("Coming soon"),$t=f(),p=r("div"),me(oe.$$.fragment),Zi=f(),ct=r("p"),er=s("Pipeline for text-to-image generation using Stable Diffusion."),tr=f(),se=r("p"),ir=s("This model inherits from "),ze=r("a"),rr=s("DiffusionPipeline"),nr=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),or=f(),Ne=r("div"),me(ae.$$.fragment),At=f(),P=r("ul"),ut=r("li"),ht=r("strong"),sr=s("call"),ar=f(),pt=r("li"),lr=s("enable_attention_slicing"),fr=f(),mt=r("li"),dr=s("disable_attention_slicing"),zt=f(),m=r("div"),me(le.$$.fragment),cr=f(),gt=r("p"),ur=s("Pipeline for text-guided image to image generation using Stable Diffusion."),hr=f(),fe=r("p"),pr=s("This model inherits from "),Ue=r("a"),mr=s("DiffusionPipeline"),gr=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),_r=f(),Oe=r("div"),me(de.$$.fragment),Nt=f(),E=r("ul"),_t=r("li"),bt=r("strong"),br=s("call"),vr=f(),vt=r("li"),Dr=s("enable_attention_slicing"),Sr=f(),Dt=r("li"),Ir=s("disable_attention_slicing"),Ut=f(),g=r("div"),me(ce.$$.fragment),xr=f(),ue=r("p"),Pr=s("Pipeline for text-guided image inpainting using Stable Diffusion. "),St=r("em"),Er=s("This is an experimental feature"),yr=s("."),Lr=f(),he=r("p"),kr=s("This model inherits from "),Ve=r("a"),Cr=s("DiffusionPipeline"),Mr=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Tr=f(),Ge=r("div"),me(pe.$$.fragment),Ot=f(),y=r("ul"),It=r("li"),xt=r("strong"),wr=s("call"),$r=f(),Pt=r("li"),Ar=s("enable_attention_slicing"),zr=f(),Et=r("li"),Nr=s("disable_attention_slicing"),this.h()},l(i){const c=no('[data-svelte="svelte-1phssyn"]',document.head);L=n(c,"META",{name:!0,content:!0}),c.forEach(t),yt=d(i),k=n(i,"H1",{class:!0});var Gt=o(k);M=n(Gt,"A",{id:!0,class:!0,href:!0});var Kr=o(M);Ye=n(Kr,"SPAN",{});var Hr=o(Ye);ge(G.$$.fragment,Hr),Hr.forEach(t),Kr.forEach(t),Wt=d(Gt),We=n(Gt,"SPAN",{});var jr=o(We);Jt=a(jr,"Stable diffusion pipelines"),jr.forEach(t),Gt.forEach(t),Lt=d(i),h=n(i,"P",{});var _=o(h);Qt=a(_,"Stable Diffusion is a text-to-image "),Je=n(_,"EM",{});var qr=o(Je);Xt=a(qr,"latent diffusion"),qr.forEach(t),Zt=a(_," model created by the researchers and engineers from "),R=n(_,"A",{href:!0,rel:!0});var Yr=o(R);ei=a(Yr,"CompVis"),Yr.forEach(t),ti=a(_,", "),F=n(_,"A",{href:!0,rel:!0});var Wr=o(F);ii=a(Wr,"Stability AI"),Wr.forEach(t),ri=a(_," and "),B=n(_,"A",{href:!0,rel:!0});var Jr=o(B);ni=a(Jr,"LAION"),Jr.forEach(t),oi=a(_,". It\u2019s trained on 512x512 images from a subset of the "),K=n(_,"A",{href:!0,rel:!0});var Qr=o(K);si=a(Qr,"LAION-5B"),Qr.forEach(t),ai=a(_," dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),_.forEach(t),kt=d(i),I=n(i,"P",{});var Re=o(I);li=a(Re,"Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),H=n(Re,"A",{href:!0,rel:!0});var Xr=o(H);fi=a(Xr,"High-Resolution Image Synthesis with Latent Diffusion Models"),Xr.forEach(t),di=a(Re," by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),Se=n(Re,"A",{href:!0});var Zr=o(Se);ci=a(Zr,"specific pipeline for latent diffusion"),Zr.forEach(t),ui=a(Re," that is part of \u{1F917} Diffusers."),Re.forEach(t),Ct=d(i),x=n(i,"P",{});var Fe=o(x);hi=a(Fe,"For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),j=n(Fe,"A",{href:!0,rel:!0});var en=o(j);pi=a(en,"launch announcement post"),en.forEach(t),mi=a(Fe," and "),q=n(Fe,"A",{href:!0,rel:!0});var tn=o(q);gi=a(tn,"this section of our own blog post"),tn.forEach(t),_i=a(Fe,"."),Fe.forEach(t),Mt=d(i),Y=n(i,"P",{});var Ur=o(Y);Qe=n(Ur,"EM",{});var rn=o(Qe);bi=a(rn,"Tips"),rn.forEach(t),vi=a(Ur,":"),Ur.forEach(t),Tt=d(i),T=n(i,"UL",{});var Rt=o(T);Ie=n(Rt,"LI",{});var Or=o(Ie);Di=a(Or,"To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),W=n(Or,"A",{href:!0,rel:!0});var nn=o(W);xe=n(nn,"IMG",{src:!0,alt:!0}),nn.forEach(t),Or.forEach(t),Si=d(Rt),Xe=n(Rt,"LI",{});var on=o(Xe);Ii=a(on,"TODO: some interesting Tips"),on.forEach(t),Rt.forEach(t),wt=d(i),w=n(i,"TABLE",{});var Ft=o(w);Ze=n(Ft,"THEAD",{});var sn=o(Ze);b=n(sn,"TR",{});var $=o(b);et=n($,"TH",{});var an=o(et);xi=a(an,"Pipeline"),an.forEach(t),Pi=d($),tt=n($,"TH",{});var ln=o(tt);Ei=a(ln,"Tasks"),ln.forEach(t),yi=d($),Pe=n($,"TH",{align:!0});var fn=o(Pe);Li=a(fn,"Colab"),fn.forEach(t),ki=d($),Ee=n($,"TH",{align:!0});var dn=o(Ee);Ci=a(dn,"Demo"),dn.forEach(t),$.forEach(t),sn.forEach(t),Mi=d(Ft),C=n(Ft,"TBODY",{});var Be=o(C);v=n(Be,"TR",{});var A=o(v);it=n(A,"TD",{});var cn=o(it);J=n(cn,"A",{href:!0,rel:!0});var un=o(J);Ti=a(un,"pipeline_stable_diffusion.py"),un.forEach(t),cn.forEach(t),wi=d(A),rt=n(A,"TD",{});var hn=o(rt);nt=n(hn,"EM",{});var pn=o(nt);$i=a(pn,"Text-to-Image Generation"),pn.forEach(t),hn.forEach(t),Ai=d(A),ye=n(A,"TD",{align:!0});var mn=o(ye);Q=n(mn,"A",{href:!0,rel:!0});var gn=o(Q);Le=n(gn,"IMG",{src:!0,alt:!0}),gn.forEach(t),mn.forEach(t),zi=d(A),ke=n(A,"TD",{align:!0});var _n=o(ke);X=n(_n,"A",{href:!0,rel:!0});var bn=o(X);Ni=a(bn,"\u{1F917} Stable Diffusion"),bn.forEach(t),_n.forEach(t),A.forEach(t),Ui=d(Be),D=n(Be,"TR",{});var z=o(D);ot=n(z,"TD",{});var vn=o(ot);Z=n(vn,"A",{href:!0,rel:!0});var Dn=o(Z);Oi=a(Dn,"pipeline_stable_diffusion_img2img.py"),Dn.forEach(t),vn.forEach(t),Vi=d(z),st=n(z,"TD",{});var Sn=o(st);at=n(Sn,"EM",{});var In=o(at);Gi=a(In,"Image-to-Image Text-Guided Generation"),In.forEach(t),Sn.forEach(t),Ri=d(z),Ce=n(z,"TD",{align:!0});var xn=o(Ce);ee=n(xn,"A",{href:!0,rel:!0});var Pn=o(ee);Me=n(Pn,"IMG",{src:!0,alt:!0}),Pn.forEach(t),xn.forEach(t),Fi=d(z),Te=n(z,"TD",{align:!0});var En=o(Te);te=n(En,"A",{href:!0,rel:!0});var yn=o(te);Bi=a(yn,"\u{1F917} Diffuse the Rest"),yn.forEach(t),En.forEach(t),z.forEach(t),Ki=d(Be),S=n(Be,"TR",{});var N=o(S);lt=n(N,"TD",{});var Ln=o(lt);ie=n(Ln,"A",{href:!0,rel:!0});var kn=o(ie);Hi=a(kn,"pipeline_stable_diffusion_inpaint.py"),kn.forEach(t),Ln.forEach(t),ji=d(N),re=n(N,"TD",{});var Bt=o(re);ft=n(Bt,"STRONG",{});var Cn=o(ft);qi=a(Cn,"Experimental"),Cn.forEach(t),Yi=a(Bt," \u2013 "),dt=n(Bt,"EM",{});var Mn=o(dt);Wi=a(Mn,"Text-Guided Image Inpainting"),Mn.forEach(t),Bt.forEach(t),Ji=d(N),we=n(N,"TD",{align:!0});var Tn=o(we);ne=n(Tn,"A",{href:!0,rel:!0});var wn=o(ne);$e=n(wn,"IMG",{src:!0,alt:!0}),wn.forEach(t),Tn.forEach(t),Qi=d(N),Ae=n(N,"TD",{align:!0});var $n=o(Ae);Xi=a($n,"Coming soon"),$n.forEach(t),N.forEach(t),Be.forEach(t),Ft.forEach(t),$t=d(i),p=n(i,"DIV",{class:!0});var U=o(p);ge(oe.$$.fragment,U),Zi=d(U),ct=n(U,"P",{});var An=o(ct);er=a(An,"Pipeline for text-to-image generation using Stable Diffusion."),An.forEach(t),tr=d(U),se=n(U,"P",{});var Kt=o(se);ir=a(Kt,"This model inherits from "),ze=n(Kt,"A",{href:!0});var zn=o(ze);rr=a(zn,"DiffusionPipeline"),zn.forEach(t),nr=a(Kt,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Kt.forEach(t),or=d(U),Ne=n(U,"DIV",{class:!0});var Nn=o(Ne);ge(ae.$$.fragment,Nn),Nn.forEach(t),U.forEach(t),At=d(i),P=n(i,"UL",{});var Ke=o(P);ut=n(Ke,"LI",{});var Un=o(ut);ht=n(Un,"STRONG",{});var On=o(ht);sr=a(On,"call"),On.forEach(t),Un.forEach(t),ar=d(Ke),pt=n(Ke,"LI",{});var Vn=o(pt);lr=a(Vn,"enable_attention_slicing"),Vn.forEach(t),fr=d(Ke),mt=n(Ke,"LI",{});var Gn=o(mt);dr=a(Gn,"disable_attention_slicing"),Gn.forEach(t),Ke.forEach(t),zt=d(i),m=n(i,"DIV",{class:!0});var O=o(m);ge(le.$$.fragment,O),cr=d(O),gt=n(O,"P",{});var Rn=o(gt);ur=a(Rn,"Pipeline for text-guided image to image generation using Stable Diffusion."),Rn.forEach(t),hr=d(O),fe=n(O,"P",{});var Ht=o(fe);pr=a(Ht,"This model inherits from "),Ue=n(Ht,"A",{href:!0});var Fn=o(Ue);mr=a(Fn,"DiffusionPipeline"),Fn.forEach(t),gr=a(Ht,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Ht.forEach(t),_r=d(O),Oe=n(O,"DIV",{class:!0});var Bn=o(Oe);ge(de.$$.fragment,Bn),Bn.forEach(t),O.forEach(t),Nt=d(i),E=n(i,"UL",{});var He=o(E);_t=n(He,"LI",{});var Kn=o(_t);bt=n(Kn,"STRONG",{});var Hn=o(bt);br=a(Hn,"call"),Hn.forEach(t),Kn.forEach(t),vr=d(He),vt=n(He,"LI",{});var jn=o(vt);Dr=a(jn,"enable_attention_slicing"),jn.forEach(t),Sr=d(He),Dt=n(He,"LI",{});var qn=o(Dt);Ir=a(qn,"disable_attention_slicing"),qn.forEach(t),He.forEach(t),Ut=d(i),g=n(i,"DIV",{class:!0});var V=o(g);ge(ce.$$.fragment,V),xr=d(V),ue=n(V,"P",{});var jt=o(ue);Pr=a(jt,"Pipeline for text-guided image inpainting using Stable Diffusion. "),St=n(jt,"EM",{});var Yn=o(St);Er=a(Yn,"This is an experimental feature"),Yn.forEach(t),yr=a(jt,"."),jt.forEach(t),Lr=d(V),he=n(V,"P",{});var qt=o(he);kr=a(qt,"This model inherits from "),Ve=n(qt,"A",{href:!0});var Wn=o(Ve);Cr=a(Wn,"DiffusionPipeline"),Wn.forEach(t),Mr=a(qt,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),qt.forEach(t),Tr=d(V),Ge=n(V,"DIV",{class:!0});var Jn=o(Ge);ge(pe.$$.fragment,Jn),Jn.forEach(t),V.forEach(t),Ot=d(i),y=n(i,"UL",{});var je=o(y);It=n(je,"LI",{});var Qn=o(It);xt=n(Qn,"STRONG",{});var Xn=o(xt);wr=a(Xn,"call"),Xn.forEach(t),Qn.forEach(t),$r=d(je),Pt=n(je,"LI",{});var Zn=o(Pt);Ar=a(Zn,"enable_attention_slicing"),Zn.forEach(t),zr=d(je),Et=n(je,"LI",{});var eo=o(Et);Nr=a(eo,"disable_attention_slicing"),eo.forEach(t),je.forEach(t),this.h()},h(){l(L,"name","hf:doc:metadata"),l(L,"content",JSON.stringify(fo)),l(M,"id","diffusers.StableDiffusionPipeline"),l(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(M,"href","#diffusers.StableDiffusionPipeline"),l(k,"class","relative group"),l(R,"href","https://github.com/CompVis"),l(R,"rel","nofollow"),l(F,"href","https://stability.ai/"),l(F,"rel","nofollow"),l(B,"href","https://laion.ai/"),l(B,"rel","nofollow"),l(K,"href","https://laion.ai/blog/laion-5b/"),l(K,"rel","nofollow"),l(H,"href","https://arxiv.org/abs/2112.10752"),l(H,"rel","nofollow"),l(Se,"href","pipelines/latent_diffusion"),l(j,"href","https://stability.ai/blog/stable-diffusion-announcement"),l(j,"rel","nofollow"),l(q,"href","https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work"),l(q,"rel","nofollow"),Yt(xe.src,Gr="https://colab.research.google.com/assets/colab-badge.svg")||l(xe,"src",Gr),l(xe,"alt","Open In Colab"),l(W,"href","https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb"),l(W,"rel","nofollow"),l(Pe,"align","center"),l(Ee,"align","center"),l(J,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py"),l(J,"rel","nofollow"),Yt(Le.src,Rr="https://colab.research.google.com/assets/colab-badge.svg")||l(Le,"src",Rr),l(Le,"alt","Open In Colab"),l(Q,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"),l(Q,"rel","nofollow"),l(ye,"align","center"),l(X,"href","https://huggingface.co/spaces/stabilityai/stable-diffusion"),l(X,"rel","nofollow"),l(ke,"align","center"),l(Z,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py"),l(Z,"rel","nofollow"),Yt(Me.src,Fr="https://colab.research.google.com/assets/colab-badge.svg")||l(Me,"src",Fr),l(Me,"alt","Open In Colab"),l(ee,"href","https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb"),l(ee,"rel","nofollow"),l(Ce,"align","center"),l(te,"href","https://huggingface.co/spaces/huggingface/diffuse-the-rest"),l(te,"rel","nofollow"),l(Te,"align","center"),l(ie,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py"),l(ie,"rel","nofollow"),Yt($e.src,Br="https://colab.research.google.com/assets/colab-badge.svg")||l($e,"src",Br),l($e,"alt","Open In Colab"),l(ne,"href","https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/in_painting_with_stable_diffusion_using_diffusers.ipynb"),l(ne,"rel","nofollow"),l(we,"align","center"),l(Ae,"align","center"),l(ze,"href","/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline"),l(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ue,"href","/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline"),l(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ve,"href","/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline"),l(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,c){e(document.head,L),u(i,yt,c),u(i,k,c),e(k,M),e(M,Ye),_e(G,Ye,null),e(k,Wt),e(k,We),e(We,Jt),u(i,Lt,c),u(i,h,c),e(h,Qt),e(h,Je),e(Je,Xt),e(h,Zt),e(h,R),e(R,ei),e(h,ti),e(h,F),e(F,ii),e(h,ri),e(h,B),e(B,ni),e(h,oi),e(h,K),e(K,si),e(h,ai),u(i,kt,c),u(i,I,c),e(I,li),e(I,H),e(H,fi),e(I,di),e(I,Se),e(Se,ci),e(I,ui),u(i,Ct,c),u(i,x,c),e(x,hi),e(x,j),e(j,pi),e(x,mi),e(x,q),e(q,gi),e(x,_i),u(i,Mt,c),u(i,Y,c),e(Y,Qe),e(Qe,bi),e(Y,vi),u(i,Tt,c),u(i,T,c),e(T,Ie),e(Ie,Di),e(Ie,W),e(W,xe),e(T,Si),e(T,Xe),e(Xe,Ii),u(i,wt,c),u(i,w,c),e(w,Ze),e(Ze,b),e(b,et),e(et,xi),e(b,Pi),e(b,tt),e(tt,Ei),e(b,yi),e(b,Pe),e(Pe,Li),e(b,ki),e(b,Ee),e(Ee,Ci),e(w,Mi),e(w,C),e(C,v),e(v,it),e(it,J),e(J,Ti),e(v,wi),e(v,rt),e(rt,nt),e(nt,$i),e(v,Ai),e(v,ye),e(ye,Q),e(Q,Le),e(v,zi),e(v,ke),e(ke,X),e(X,Ni),e(C,Ui),e(C,D),e(D,ot),e(ot,Z),e(Z,Oi),e(D,Vi),e(D,st),e(st,at),e(at,Gi),e(D,Ri),e(D,Ce),e(Ce,ee),e(ee,Me),e(D,Fi),e(D,Te),e(Te,te),e(te,Bi),e(C,Ki),e(C,S),e(S,lt),e(lt,ie),e(ie,Hi),e(S,ji),e(S,re),e(re,ft),e(ft,qi),e(re,Yi),e(re,dt),e(dt,Wi),e(S,Ji),e(S,we),e(we,ne),e(ne,$e),e(S,Qi),e(S,Ae),e(Ae,Xi),u(i,$t,c),u(i,p,c),_e(oe,p,null),e(p,Zi),e(p,ct),e(ct,er),e(p,tr),e(p,se),e(se,ir),e(se,ze),e(ze,rr),e(se,nr),e(p,or),e(p,Ne),_e(ae,Ne,null),u(i,At,c),u(i,P,c),e(P,ut),e(ut,ht),e(ht,sr),e(P,ar),e(P,pt),e(pt,lr),e(P,fr),e(P,mt),e(mt,dr),u(i,zt,c),u(i,m,c),_e(le,m,null),e(m,cr),e(m,gt),e(gt,ur),e(m,hr),e(m,fe),e(fe,pr),e(fe,Ue),e(Ue,mr),e(fe,gr),e(m,_r),e(m,Oe),_e(de,Oe,null),u(i,Nt,c),u(i,E,c),e(E,_t),e(_t,bt),e(bt,br),e(E,vr),e(E,vt),e(vt,Dr),e(E,Sr),e(E,Dt),e(Dt,Ir),u(i,Ut,c),u(i,g,c),_e(ce,g,null),e(g,xr),e(g,ue),e(ue,Pr),e(ue,St),e(St,Er),e(ue,yr),e(g,Lr),e(g,he),e(he,kr),e(he,Ve),e(Ve,Cr),e(he,Mr),e(g,Tr),e(g,Ge),_e(pe,Ge,null),u(i,Ot,c),u(i,y,c),e(y,It),e(It,xt),e(xt,wr),e(y,$r),e(y,Pt),e(Pt,Ar),e(y,zr),e(y,Et),e(Et,Nr),Vt=!0},p:oo,i(i){Vt||(be(G.$$.fragment,i),be(oe.$$.fragment,i),be(ae.$$.fragment,i),be(le.$$.fragment,i),be(de.$$.fragment,i),be(ce.$$.fragment,i),be(pe.$$.fragment,i),Vt=!0)},o(i){ve(G.$$.fragment,i),ve(oe.$$.fragment,i),ve(ae.$$.fragment,i),ve(le.$$.fragment,i),ve(de.$$.fragment,i),ve(ce.$$.fragment,i),ve(pe.$$.fragment,i),Vt=!1},d(i){t(L),i&&t(yt),i&&t(k),De(G),i&&t(Lt),i&&t(h),i&&t(kt),i&&t(I),i&&t(Ct),i&&t(x),i&&t(Mt),i&&t(Y),i&&t(Tt),i&&t(T),i&&t(wt),i&&t(w),i&&t($t),i&&t(p),De(oe),De(ae),i&&t(At),i&&t(P),i&&t(zt),i&&t(m),De(le),De(de),i&&t(Nt),i&&t(E),i&&t(Ut),i&&t(g),De(ce),De(pe),i&&t(Ot),i&&t(y)}}}const fo={local:"diffusers.StableDiffusionPipeline",title:"Stable diffusion pipelines"};function co(Vr){return so(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mo extends to{constructor(L){super();io(this,L,co,lo,ro,{})}}export{mo as default,fo as metadata};
