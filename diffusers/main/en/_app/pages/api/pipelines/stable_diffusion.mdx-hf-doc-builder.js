import{S as Sr,i as Er,s as $r,e as o,k as d,w as g,t as s,M as kr,c as i,d as t,m as c,a,x as _,h as l,b as r,N as qn,G as e,g as p,y as b,L as Tr,q as v,o as w,B as y,v as Cr}from"../../../chunks/vendor-hf-doc-builder.js";import{D as E}from"../../../chunks/Docstring-hf-doc-builder.js";import{I as nn}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function Lr(oa){let N,on,G,H,ht,se,Fn,mt,Un,an,q,R,gt,le,Vn,_t,Wn,rn,D,Hn,bt,Rn,Bn,de,Kn,jn,ce,Yn,Jn,fe,Qn,Xn,pe,Zn,eo,sn,L,to,ue,no,oo,je,io,ao,ln,M,ro,he,so,lo,me,co,fo,dn,F,B,vt,ge,po,wt,uo,cn,K,Ye,ho,_e,Je,ia,mo,yt,go,fn,U,j,Dt,be,_o,It,bo,pn,Y,xt,$,Pt,vo,wo,St,yo,Do,Qe,Io,xo,Xe,Po,So,V,k,Et,ve,Eo,$o,$t,kt,ko,To,Ze,we,et,aa,Co,tt,ye,Lo,Mo,T,Tt,De,Ao,Oo,Ct,Lt,zo,No,nt,Ie,ot,ra,Go,it,xe,qo,Fo,C,Mt,Pe,Uo,Vo,Se,At,Wo,Ho,Ot,Ro,Bo,at,Ee,rt,sa,Ko,st,jo,un,W,J,zt,$e,Yo,Nt,Jo,hn,u,ke,Qo,Gt,Xo,Zo,Te,ei,qt,ti,ni,oi,Q,Ce,ii,Ft,ai,ri,A,Le,si,Ut,li,di,Vt,ci,fi,X,Me,pi,Ae,ui,Wt,hi,mi,mn,h,Oe,gi,Ht,_i,bi,ze,vi,Rt,wi,yi,Di,Z,Ne,Ii,Bt,xi,Pi,O,Ge,Si,Kt,Ei,$i,jt,ki,Ti,ee,qe,Ci,Fe,Li,Yt,Mi,Ai,gn,m,Ue,Oi,Ve,zi,Jt,Ni,Gi,qi,We,Fi,Qt,Ui,Vi,Wi,te,He,Hi,Xt,Ri,Bi,z,Re,Ki,Zt,ji,Yi,en,Ji,Qi,ne,Be,Xi,Ke,Zi,tn,ea,ta,_n;return se=new nn({}),le=new nn({}),ge=new nn({}),be=new nn({}),$e=new nn({}),ke=new E({props:{name:"class diffusers.StableDiffusionPipeline",anchor:"diffusers.StableDiffusionPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionPipeline.scheduler",description:`<strong>scheduler</strong> (<code>SchedulerMixin</code>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<code>DDIMScheduler</code>, <code>LMSDiscreteScheduler</code>, or <code>PNDMScheduler</code>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L16"}}),Ce=new E({props:{name:"__call__",anchor:"diffusers.StableDiffusionPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"height",val:": typing.Optional[int] = 512"},{name:"width",val:": typing.Optional[int] = 512"},{name:"num_inference_steps",val:": typing.Optional[int] = 50"},{name:"guidance_scale",val:": typing.Optional[float] = 7.5"},{name:"eta",val:": typing.Optional[float] = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"latents",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionPipeline.__call__.height",description:`<strong>height</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The height in pixels of the generated image.`,name:"height"},{anchor:"diffusers.StableDiffusionPipeline.__call__.width",description:`<strong>width</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The width in pixels of the generated image.`,name:"width"},{anchor:"diffusers.StableDiffusionPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<code>schedulers.DDIMScheduler</code>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionPipeline.__call__.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
tensor will ge generated by sampling using the supplied random <code>generator</code>.`,name:"latents"},{anchor:"diffusers.StableDiffusionPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>nd.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>StableDiffusionPipelineOutput</code> instead of a
plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L93",returnDescription:`
<p><code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code> if <code>return_dict</code> is True, otherwise a tuple.
When returning a tuple, the first element is a list with the generated images, and the second element is a
list of <code>bool</code>s denoting whether the corresponding generated image likely represents \u201Cnot-safe-for-work\u201D
(nsfw) content, according to the <code>safety_checker</code>.</p>
`}}),Le=new E({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input batch to the attention heads, so attention will be computed in two
steps. If a number is provided, use as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L66"}}),Me=new E({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L85"}}),Oe=new E({props:{name:"class diffusers.StableDiffusionImg2ImgPipeline",anchor:"diffusers.StableDiffusionImg2ImgPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.scheduler",description:`<strong>scheduler</strong> (<code>SchedulerMixin</code>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<code>DDIMScheduler</code>, <code>LMSDiscreteScheduler</code>, or <code>PNDMScheduler</code>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L27"}}),Ne=new E({props:{name:"__call__",anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"init_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"strength",val:": float = 0.8"},{name:"num_inference_steps",val:": typing.Optional[int] = 50"},{name:"guidance_scale",val:": typing.Optional[float] = 7.5"},{name:"eta",val:": typing.Optional[float] = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.init_image",description:`<strong>init_image</strong> (<code>torch.FloatTensor</code> or <code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, that will be used as the starting point for the
process.`,name:"init_image"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.strength",description:`<strong>strength</strong> (<code>float</code>, <em>optional</em>, defaults to 0.8) &#x2014;
Conceptually, indicates how much to transform the reference <code>init_image</code>. Must be between 0 and 1.
<code>init_image</code> will be used as a starting point, adding more noise to it the larger the <code>strength</code>. The
number of denoising steps depends on the amount of noise initially added. When <code>strength</code> is 1, added
noise will be maximum and the denoising process will run for the full number of iterations specified in
<code>num_inference_steps</code>. A value of 1, therefore, essentially ignores <code>init_image</code>.`,name:"strength"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference. This parameter will be modulated by <code>strength</code>.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<code>schedulers.DDIMScheduler</code>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>nd.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>StableDiffusionPipelineOutput</code> instead of a
plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L104",returnDescription:`
<p><code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code> if <code>return_dict</code> is True, otherwise a tuple.
When returning a tuple, the first element is a list with the generated images, and the second element is a
list of <code>bool</code>s denoting whether the corresponding generated image likely represents \u201Cnot-safe-for-work\u201D
(nsfw) content, according to the <code>safety_checker</code>.</p>
`}}),Ge=new E({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionImg2ImgPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input batch to the attention heads, so attention will be computed in two
steps. If a number is provided, use as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L77"}}),qe=new E({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionImg2ImgPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L96"}}),Ue=new E({props:{name:"class diffusers.StableDiffusionInpaintPipeline",anchor:"diffusers.StableDiffusionInpaintPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPFeatureExtractor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.vae",description:`<strong>vae</strong> (<code>AutoencoderKL</code>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionInpaintPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionInpaintPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionInpaintPipeline.unet",description:"<strong>unet</strong> (<code>UNet2DConditionModel</code>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.",name:"unet"},{anchor:"diffusers.StableDiffusionInpaintPipeline.scheduler",description:`<strong>scheduler</strong> (<code>SchedulerMixin</code>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latens. Can be one of
<code>DDIMScheduler</code>, <code>LMSDiscreteScheduler</code>, or <code>PNDMScheduler</code>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionInpaintPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offsensive or harmful.
Please, refer to the <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionInpaintPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L41"}}),He=new E({props:{name:"__call__",anchor:"diffusers.StableDiffusionInpaintPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"init_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"mask_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"strength",val:": float = 0.8"},{name:"num_inference_steps",val:": typing.Optional[int] = 50"},{name:"guidance_scale",val:": typing.Optional[float] = 7.5"},{name:"eta",val:": typing.Optional[float] = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.init_image",description:`<strong>init_image</strong> (<code>torch.FloatTensor</code> or <code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, that will be used as the starting point for the
process. This is the image whose masked region will be inpainted.`,name:"init_image"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.mask_image",description:`<strong>mask_image</strong> (<code>torch.FloatTensor</code> or <code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, to mask <code>init_image</code>. White pixels in the mask will be
replaced by noise and therefore repainted, while black pixels will be preserved. The mask image will be
converted to a single channel (luminance) before use.`,name:"mask_image"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.strength",description:`<strong>strength</strong> (<code>float</code>, <em>optional</em>, defaults to 0.8) &#x2014;
Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When <code>strength</code>
is 1, the denoising process will be run on the masked area for the full number of iterations specified
in <code>num_inference_steps</code>. <code>init_image</code> will be used as a reference for the masked area, adding more
noise to that region the larger the <code>strength</code>. If <code>strength</code> is 0, no inpainting will occur.`,name:"strength"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The reference number of denoising steps. More denoising steps usually lead to a higher quality image at
the expense of slower inference. This parameter will be modulated by <code>strength</code>, as explained above.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<code>schedulers.DDIMScheduler</code>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>nd.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>StableDiffusionPipelineOutput</code> instead of a
plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L118",returnDescription:`
<p><code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code> if <code>return_dict</code> is True, otherwise a tuple.
When returning a tuple, the first element is a list with the generated images, and the second element is a
list of <code>bool</code>s denoting whether the corresponding generated image likely represents \u201Cnot-safe-for-work\u201D
(nsfw) content, according to the <code>safety_checker</code>.</p>
`}}),Re=new E({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionInpaintPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input batch to the attention heads, so attention will be computed in two
steps. If a number is provided, use as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L91"}}),Be=new E({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionInpaintPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L110"}}),{c(){N=o("meta"),on=d(),G=o("h1"),H=o("a"),ht=o("span"),g(se.$$.fragment),Fn=d(),mt=o("span"),Un=s("Stable diffusion pipelines"),an=d(),q=o("h2"),R=o("a"),gt=o("span"),g(le.$$.fragment),Vn=d(),_t=o("span"),Wn=s("Overview"),rn=d(),D=o("p"),Hn=s("Stable Diffusion is a text-to-image "),bt=o("em"),Rn=s("latent diffusion"),Bn=s(" model created by the researchers and engineers from "),de=o("a"),Kn=s("CompVis"),jn=s(", "),ce=o("a"),Yn=s("Stability AI"),Jn=s(" and "),fe=o("a"),Qn=s("LAION"),Xn=s(". It\u2019s trained on 512x512 images from a subset of the "),pe=o("a"),Zn=s("LAION-5B"),eo=s(" dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),sn=d(),L=o("p"),to=s("Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),ue=o("a"),no=s("High-Resolution Image Synthesis with Latent Diffusion Models"),oo=s(" by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),je=o("a"),io=s("specific pipeline for latent diffusion"),ao=s(" that is part of \u{1F917} Diffusers."),ln=d(),M=o("p"),ro=s("For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),he=o("a"),so=s("launch announcement post"),lo=s(" and "),me=o("a"),co=s("this section of our own blog post"),fo=s("."),dn=d(),F=o("h2"),B=o("a"),vt=o("span"),g(ge.$$.fragment),po=d(),wt=o("span"),uo=s("Tips"),cn=d(),K=o("ul"),Ye=o("li"),ho=s("To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),_e=o("a"),Je=o("img"),mo=d(),yt=o("li"),go=s("TODO: some interesting Tips"),fn=d(),U=o("h2"),j=o("a"),Dt=o("span"),g(be.$$.fragment),_o=d(),It=o("span"),bo=s("Available pipelines"),pn=d(),Y=o("table"),xt=o("thead"),$=o("tr"),Pt=o("th"),vo=s("Pipeline"),wo=d(),St=o("th"),yo=s("Tasks"),Do=d(),Qe=o("th"),Io=s("Colab"),xo=d(),Xe=o("th"),Po=s("Demo"),So=d(),V=o("tbody"),k=o("tr"),Et=o("td"),ve=o("a"),Eo=s("pipeline_stable_diffusion.py"),$o=d(),$t=o("td"),kt=o("em"),ko=s("Text-to-Image Generation"),To=d(),Ze=o("td"),we=o("a"),et=o("img"),Co=d(),tt=o("td"),ye=o("a"),Lo=s("\u{1F917} Stable Diffusion"),Mo=d(),T=o("tr"),Tt=o("td"),De=o("a"),Ao=s("pipeline_stable_diffusion_img2img.py"),Oo=d(),Ct=o("td"),Lt=o("em"),zo=s("Image-to-Image Text-Guided Generation"),No=d(),nt=o("td"),Ie=o("a"),ot=o("img"),Go=d(),it=o("td"),xe=o("a"),qo=s("\u{1F917} Diffuse the Rest"),Fo=d(),C=o("tr"),Mt=o("td"),Pe=o("a"),Uo=s("pipeline_stable_diffusion_inpaint.py"),Vo=d(),Se=o("td"),At=o("strong"),Wo=s("Experimental"),Ho=s(" \u2013 "),Ot=o("em"),Ro=s("Text-Guided Image Inpainting"),Bo=d(),at=o("td"),Ee=o("a"),rt=o("img"),Ko=d(),st=o("td"),jo=s("Coming soon"),un=d(),W=o("h2"),J=o("a"),zt=o("span"),g($e.$$.fragment),Yo=d(),Nt=o("span"),Jo=s("API"),hn=d(),u=o("div"),g(ke.$$.fragment),Qo=d(),Gt=o("p"),Xo=s("Pipeline for text-to-image generation using Stable Diffusion."),Zo=d(),Te=o("p"),ei=s("This model inherits from "),qt=o("code"),ti=s("DiffusionPipeline"),ni=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),oi=d(),Q=o("div"),g(Ce.$$.fragment),ii=d(),Ft=o("p"),ai=s("Function invoked when calling the pipeline for generation."),ri=d(),A=o("div"),g(Le.$$.fragment),si=d(),Ut=o("p"),li=s("Enable sliced attention computation."),di=d(),Vt=o("p"),ci=s(`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),fi=d(),X=o("div"),g(Me.$$.fragment),pi=d(),Ae=o("p"),ui=s("Disable sliced attention computation. If "),Wt=o("code"),hi=s("enable_attention_slicing"),mi=s(` was previously invoked, this method will go
back to computing attention in one step.`),mn=d(),h=o("div"),g(Oe.$$.fragment),gi=d(),Ht=o("p"),_i=s("Pipeline for text-guided image to image generation using Stable Diffusion."),bi=d(),ze=o("p"),vi=s("This model inherits from "),Rt=o("code"),wi=s("DiffusionPipeline"),yi=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Di=d(),Z=o("div"),g(Ne.$$.fragment),Ii=d(),Bt=o("p"),xi=s("Function invoked when calling the pipeline for generation."),Pi=d(),O=o("div"),g(Ge.$$.fragment),Si=d(),Kt=o("p"),Ei=s("Enable sliced attention computation."),$i=d(),jt=o("p"),ki=s(`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),Ti=d(),ee=o("div"),g(qe.$$.fragment),Ci=d(),Fe=o("p"),Li=s("Disable sliced attention computation. If "),Yt=o("code"),Mi=s("enable_attention_slicing"),Ai=s(` was previously invoked, this method will go
back to computing attention in one step.`),gn=d(),m=o("div"),g(Ue.$$.fragment),Oi=d(),Ve=o("p"),zi=s("Pipeline for text-guided image inpainting using Stable Diffusion. "),Jt=o("em"),Ni=s("This is an experimental feature"),Gi=s("."),qi=d(),We=o("p"),Fi=s("This model inherits from "),Qt=o("code"),Ui=s("DiffusionPipeline"),Vi=s(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Wi=d(),te=o("div"),g(He.$$.fragment),Hi=d(),Xt=o("p"),Ri=s("Function invoked when calling the pipeline for generation."),Bi=d(),z=o("div"),g(Re.$$.fragment),Ki=d(),Zt=o("p"),ji=s("Enable sliced attention computation."),Yi=d(),en=o("p"),Ji=s(`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),Qi=d(),ne=o("div"),g(Be.$$.fragment),Xi=d(),Ke=o("p"),Zi=s("Disable sliced attention computation. If "),tn=o("code"),ea=s("enable_attention_slicing"),ta=s(` was previously invoked, this method will go
back to computing attention in one step.`),this.h()},l(n){const f=kr('[data-svelte="svelte-1phssyn"]',document.head);N=i(f,"META",{name:!0,content:!0}),f.forEach(t),on=c(n),G=i(n,"H1",{class:!0});var bn=a(G);H=i(bn,"A",{id:!0,class:!0,href:!0});var la=a(H);ht=i(la,"SPAN",{});var da=a(ht);_(se.$$.fragment,da),da.forEach(t),la.forEach(t),Fn=c(bn),mt=i(bn,"SPAN",{});var ca=a(mt);Un=l(ca,"Stable diffusion pipelines"),ca.forEach(t),bn.forEach(t),an=c(n),q=i(n,"H2",{class:!0});var vn=a(q);R=i(vn,"A",{id:!0,class:!0,href:!0});var fa=a(R);gt=i(fa,"SPAN",{});var pa=a(gt);_(le.$$.fragment,pa),pa.forEach(t),fa.forEach(t),Vn=c(vn),_t=i(vn,"SPAN",{});var ua=a(_t);Wn=l(ua,"Overview"),ua.forEach(t),vn.forEach(t),rn=c(n),D=i(n,"P",{});var I=a(D);Hn=l(I,"Stable Diffusion is a text-to-image "),bt=i(I,"EM",{});var ha=a(bt);Rn=l(ha,"latent diffusion"),ha.forEach(t),Bn=l(I," model created by the researchers and engineers from "),de=i(I,"A",{href:!0,rel:!0});var ma=a(de);Kn=l(ma,"CompVis"),ma.forEach(t),jn=l(I,", "),ce=i(I,"A",{href:!0,rel:!0});var ga=a(ce);Yn=l(ga,"Stability AI"),ga.forEach(t),Jn=l(I," and "),fe=i(I,"A",{href:!0,rel:!0});var _a=a(fe);Qn=l(_a,"LAION"),_a.forEach(t),Xn=l(I,". It\u2019s trained on 512x512 images from a subset of the "),pe=i(I,"A",{href:!0,rel:!0});var ba=a(pe);Zn=l(ba,"LAION-5B"),ba.forEach(t),eo=l(I," dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),I.forEach(t),sn=c(n),L=i(n,"P",{});var lt=a(L);to=l(lt,"Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),ue=i(lt,"A",{href:!0,rel:!0});var va=a(ue);no=l(va,"High-Resolution Image Synthesis with Latent Diffusion Models"),va.forEach(t),oo=l(lt," by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),je=i(lt,"A",{href:!0});var wa=a(je);io=l(wa,"specific pipeline for latent diffusion"),wa.forEach(t),ao=l(lt," that is part of \u{1F917} Diffusers."),lt.forEach(t),ln=c(n),M=i(n,"P",{});var dt=a(M);ro=l(dt,"For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),he=i(dt,"A",{href:!0,rel:!0});var ya=a(he);so=l(ya,"launch announcement post"),ya.forEach(t),lo=l(dt," and "),me=i(dt,"A",{href:!0,rel:!0});var Da=a(me);co=l(Da,"this section of our own blog post"),Da.forEach(t),fo=l(dt,"."),dt.forEach(t),dn=c(n),F=i(n,"H2",{class:!0});var wn=a(F);B=i(wn,"A",{id:!0,class:!0,href:!0});var Ia=a(B);vt=i(Ia,"SPAN",{});var xa=a(vt);_(ge.$$.fragment,xa),xa.forEach(t),Ia.forEach(t),po=c(wn),wt=i(wn,"SPAN",{});var Pa=a(wt);uo=l(Pa,"Tips"),Pa.forEach(t),wn.forEach(t),cn=c(n),K=i(n,"UL",{});var yn=a(K);Ye=i(yn,"LI",{});var na=a(Ye);ho=l(na,"To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),_e=i(na,"A",{href:!0,rel:!0});var Sa=a(_e);Je=i(Sa,"IMG",{src:!0,alt:!0}),Sa.forEach(t),na.forEach(t),mo=c(yn),yt=i(yn,"LI",{});var Ea=a(yt);go=l(Ea,"TODO: some interesting Tips"),Ea.forEach(t),yn.forEach(t),fn=c(n),U=i(n,"H2",{class:!0});var Dn=a(U);j=i(Dn,"A",{id:!0,class:!0,href:!0});var $a=a(j);Dt=i($a,"SPAN",{});var ka=a(Dt);_(be.$$.fragment,ka),ka.forEach(t),$a.forEach(t),_o=c(Dn),It=i(Dn,"SPAN",{});var Ta=a(It);bo=l(Ta,"Available pipelines"),Ta.forEach(t),Dn.forEach(t),pn=c(n),Y=i(n,"TABLE",{});var In=a(Y);xt=i(In,"THEAD",{});var Ca=a(xt);$=i(Ca,"TR",{});var oe=a($);Pt=i(oe,"TH",{});var La=a(Pt);vo=l(La,"Pipeline"),La.forEach(t),wo=c(oe),St=i(oe,"TH",{});var Ma=a(St);yo=l(Ma,"Tasks"),Ma.forEach(t),Do=c(oe),Qe=i(oe,"TH",{align:!0});var Aa=a(Qe);Io=l(Aa,"Colab"),Aa.forEach(t),xo=c(oe),Xe=i(oe,"TH",{align:!0});var Oa=a(Xe);Po=l(Oa,"Demo"),Oa.forEach(t),oe.forEach(t),Ca.forEach(t),So=c(In),V=i(In,"TBODY",{});var ct=a(V);k=i(ct,"TR",{});var ie=a(k);Et=i(ie,"TD",{});var za=a(Et);ve=i(za,"A",{href:!0,rel:!0});var Na=a(ve);Eo=l(Na,"pipeline_stable_diffusion.py"),Na.forEach(t),za.forEach(t),$o=c(ie),$t=i(ie,"TD",{});var Ga=a($t);kt=i(Ga,"EM",{});var qa=a(kt);ko=l(qa,"Text-to-Image Generation"),qa.forEach(t),Ga.forEach(t),To=c(ie),Ze=i(ie,"TD",{align:!0});var Fa=a(Ze);we=i(Fa,"A",{href:!0,rel:!0});var Ua=a(we);et=i(Ua,"IMG",{src:!0,alt:!0}),Ua.forEach(t),Fa.forEach(t),Co=c(ie),tt=i(ie,"TD",{align:!0});var Va=a(tt);ye=i(Va,"A",{href:!0,rel:!0});var Wa=a(ye);Lo=l(Wa,"\u{1F917} Stable Diffusion"),Wa.forEach(t),Va.forEach(t),ie.forEach(t),Mo=c(ct),T=i(ct,"TR",{});var ae=a(T);Tt=i(ae,"TD",{});var Ha=a(Tt);De=i(Ha,"A",{href:!0,rel:!0});var Ra=a(De);Ao=l(Ra,"pipeline_stable_diffusion_img2img.py"),Ra.forEach(t),Ha.forEach(t),Oo=c(ae),Ct=i(ae,"TD",{});var Ba=a(Ct);Lt=i(Ba,"EM",{});var Ka=a(Lt);zo=l(Ka,"Image-to-Image Text-Guided Generation"),Ka.forEach(t),Ba.forEach(t),No=c(ae),nt=i(ae,"TD",{align:!0});var ja=a(nt);Ie=i(ja,"A",{href:!0,rel:!0});var Ya=a(Ie);ot=i(Ya,"IMG",{src:!0,alt:!0}),Ya.forEach(t),ja.forEach(t),Go=c(ae),it=i(ae,"TD",{align:!0});var Ja=a(it);xe=i(Ja,"A",{href:!0,rel:!0});var Qa=a(xe);qo=l(Qa,"\u{1F917} Diffuse the Rest"),Qa.forEach(t),Ja.forEach(t),ae.forEach(t),Fo=c(ct),C=i(ct,"TR",{});var re=a(C);Mt=i(re,"TD",{});var Xa=a(Mt);Pe=i(Xa,"A",{href:!0,rel:!0});var Za=a(Pe);Uo=l(Za,"pipeline_stable_diffusion_inpaint.py"),Za.forEach(t),Xa.forEach(t),Vo=c(re),Se=i(re,"TD",{});var xn=a(Se);At=i(xn,"STRONG",{});var er=a(At);Wo=l(er,"Experimental"),er.forEach(t),Ho=l(xn," \u2013 "),Ot=i(xn,"EM",{});var tr=a(Ot);Ro=l(tr,"Text-Guided Image Inpainting"),tr.forEach(t),xn.forEach(t),Bo=c(re),at=i(re,"TD",{align:!0});var nr=a(at);Ee=i(nr,"A",{href:!0,rel:!0});var or=a(Ee);rt=i(or,"IMG",{src:!0,alt:!0}),or.forEach(t),nr.forEach(t),Ko=c(re),st=i(re,"TD",{align:!0});var ir=a(st);jo=l(ir,"Coming soon"),ir.forEach(t),re.forEach(t),ct.forEach(t),In.forEach(t),un=c(n),W=i(n,"H2",{class:!0});var Pn=a(W);J=i(Pn,"A",{id:!0,class:!0,href:!0});var ar=a(J);zt=i(ar,"SPAN",{});var rr=a(zt);_($e.$$.fragment,rr),rr.forEach(t),ar.forEach(t),Yo=c(Pn),Nt=i(Pn,"SPAN",{});var sr=a(Nt);Jo=l(sr,"API"),sr.forEach(t),Pn.forEach(t),hn=c(n),u=i(n,"DIV",{class:!0});var x=a(u);_(ke.$$.fragment,x),Qo=c(x),Gt=i(x,"P",{});var lr=a(Gt);Xo=l(lr,"Pipeline for text-to-image generation using Stable Diffusion."),lr.forEach(t),Zo=c(x),Te=i(x,"P",{});var Sn=a(Te);ei=l(Sn,"This model inherits from "),qt=i(Sn,"CODE",{});var dr=a(qt);ti=l(dr,"DiffusionPipeline"),dr.forEach(t),ni=l(Sn,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Sn.forEach(t),oi=c(x),Q=i(x,"DIV",{class:!0});var En=a(Q);_(Ce.$$.fragment,En),ii=c(En),Ft=i(En,"P",{});var cr=a(Ft);ai=l(cr,"Function invoked when calling the pipeline for generation."),cr.forEach(t),En.forEach(t),ri=c(x),A=i(x,"DIV",{class:!0});var ft=a(A);_(Le.$$.fragment,ft),si=c(ft),Ut=i(ft,"P",{});var fr=a(Ut);li=l(fr,"Enable sliced attention computation."),fr.forEach(t),di=c(ft),Vt=i(ft,"P",{});var pr=a(Vt);ci=l(pr,`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),pr.forEach(t),ft.forEach(t),fi=c(x),X=i(x,"DIV",{class:!0});var $n=a(X);_(Me.$$.fragment,$n),pi=c($n),Ae=i($n,"P",{});var kn=a(Ae);ui=l(kn,"Disable sliced attention computation. If "),Wt=i(kn,"CODE",{});var ur=a(Wt);hi=l(ur,"enable_attention_slicing"),ur.forEach(t),mi=l(kn,` was previously invoked, this method will go
back to computing attention in one step.`),kn.forEach(t),$n.forEach(t),x.forEach(t),mn=c(n),h=i(n,"DIV",{class:!0});var P=a(h);_(Oe.$$.fragment,P),gi=c(P),Ht=i(P,"P",{});var hr=a(Ht);_i=l(hr,"Pipeline for text-guided image to image generation using Stable Diffusion."),hr.forEach(t),bi=c(P),ze=i(P,"P",{});var Tn=a(ze);vi=l(Tn,"This model inherits from "),Rt=i(Tn,"CODE",{});var mr=a(Rt);wi=l(mr,"DiffusionPipeline"),mr.forEach(t),yi=l(Tn,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Tn.forEach(t),Di=c(P),Z=i(P,"DIV",{class:!0});var Cn=a(Z);_(Ne.$$.fragment,Cn),Ii=c(Cn),Bt=i(Cn,"P",{});var gr=a(Bt);xi=l(gr,"Function invoked when calling the pipeline for generation."),gr.forEach(t),Cn.forEach(t),Pi=c(P),O=i(P,"DIV",{class:!0});var pt=a(O);_(Ge.$$.fragment,pt),Si=c(pt),Kt=i(pt,"P",{});var _r=a(Kt);Ei=l(_r,"Enable sliced attention computation."),_r.forEach(t),$i=c(pt),jt=i(pt,"P",{});var br=a(jt);ki=l(br,`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),br.forEach(t),pt.forEach(t),Ti=c(P),ee=i(P,"DIV",{class:!0});var Ln=a(ee);_(qe.$$.fragment,Ln),Ci=c(Ln),Fe=i(Ln,"P",{});var Mn=a(Fe);Li=l(Mn,"Disable sliced attention computation. If "),Yt=i(Mn,"CODE",{});var vr=a(Yt);Mi=l(vr,"enable_attention_slicing"),vr.forEach(t),Ai=l(Mn,` was previously invoked, this method will go
back to computing attention in one step.`),Mn.forEach(t),Ln.forEach(t),P.forEach(t),gn=c(n),m=i(n,"DIV",{class:!0});var S=a(m);_(Ue.$$.fragment,S),Oi=c(S),Ve=i(S,"P",{});var An=a(Ve);zi=l(An,"Pipeline for text-guided image inpainting using Stable Diffusion. "),Jt=i(An,"EM",{});var wr=a(Jt);Ni=l(wr,"This is an experimental feature"),wr.forEach(t),Gi=l(An,"."),An.forEach(t),qi=c(S),We=i(S,"P",{});var On=a(We);Fi=l(On,"This model inherits from "),Qt=i(On,"CODE",{});var yr=a(Qt);Ui=l(yr,"DiffusionPipeline"),yr.forEach(t),Vi=l(On,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),On.forEach(t),Wi=c(S),te=i(S,"DIV",{class:!0});var zn=a(te);_(He.$$.fragment,zn),Hi=c(zn),Xt=i(zn,"P",{});var Dr=a(Xt);Ri=l(Dr,"Function invoked when calling the pipeline for generation."),Dr.forEach(t),zn.forEach(t),Bi=c(S),z=i(S,"DIV",{class:!0});var ut=a(z);_(Re.$$.fragment,ut),Ki=c(ut),Zt=i(ut,"P",{});var Ir=a(Zt);ji=l(Ir,"Enable sliced attention computation."),Ir.forEach(t),Yi=c(ut),en=i(ut,"P",{});var xr=a(en);Ji=l(xr,`When this option is enabled, the attention module will split the input batch in slices, to compute attention in
several steps. This is useful to save some memory in exchange for a small speed decrease.`),xr.forEach(t),ut.forEach(t),Qi=c(S),ne=i(S,"DIV",{class:!0});var Nn=a(ne);_(Be.$$.fragment,Nn),Xi=c(Nn),Ke=i(Nn,"P",{});var Gn=a(Ke);Zi=l(Gn,"Disable sliced attention computation. If "),tn=i(Gn,"CODE",{});var Pr=a(tn);ea=l(Pr,"enable_attention_slicing"),Pr.forEach(t),ta=l(Gn,` was previously invoked, this method will go
back to computing attention in one step.`),Gn.forEach(t),Nn.forEach(t),S.forEach(t),this.h()},h(){r(N,"name","hf:doc:metadata"),r(N,"content",JSON.stringify(Mr)),r(H,"id","stable-diffusion-pipelines"),r(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(H,"href","#stable-diffusion-pipelines"),r(G,"class","relative group"),r(R,"id","overview"),r(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(R,"href","#overview"),r(q,"class","relative group"),r(de,"href","https://github.com/CompVis"),r(de,"rel","nofollow"),r(ce,"href","https://stability.ai/"),r(ce,"rel","nofollow"),r(fe,"href","https://laion.ai/"),r(fe,"rel","nofollow"),r(pe,"href","https://laion.ai/blog/laion-5b/"),r(pe,"rel","nofollow"),r(ue,"href","https://arxiv.org/abs/2112.10752"),r(ue,"rel","nofollow"),r(je,"href","pipelines/latent_diffusion"),r(he,"href","https://stability.ai/blog/stable-diffusion-announcement"),r(he,"rel","nofollow"),r(me,"href","https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work"),r(me,"rel","nofollow"),r(B,"id","tips"),r(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(B,"href","#tips"),r(F,"class","relative group"),qn(Je.src,ia="https://colab.research.google.com/assets/colab-badge.svg")||r(Je,"src",ia),r(Je,"alt","Open In Colab"),r(_e,"href","https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb"),r(_e,"rel","nofollow"),r(j,"id","available-pipelines"),r(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(j,"href","#available-pipelines"),r(U,"class","relative group"),r(Qe,"align","center"),r(Xe,"align","center"),r(ve,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py"),r(ve,"rel","nofollow"),qn(et.src,aa="https://colab.research.google.com/assets/colab-badge.svg")||r(et,"src",aa),r(et,"alt","Open In Colab"),r(we,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"),r(we,"rel","nofollow"),r(Ze,"align","center"),r(ye,"href","https://huggingface.co/spaces/stabilityai/stable-diffusion"),r(ye,"rel","nofollow"),r(tt,"align","center"),r(De,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py"),r(De,"rel","nofollow"),qn(ot.src,ra="https://colab.research.google.com/assets/colab-badge.svg")||r(ot,"src",ra),r(ot,"alt","Open In Colab"),r(Ie,"href","https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb"),r(Ie,"rel","nofollow"),r(nt,"align","center"),r(xe,"href","https://huggingface.co/spaces/huggingface/diffuse-the-rest"),r(xe,"rel","nofollow"),r(it,"align","center"),r(Pe,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py"),r(Pe,"rel","nofollow"),qn(rt.src,sa="https://colab.research.google.com/assets/colab-badge.svg")||r(rt,"src",sa),r(rt,"alt","Open In Colab"),r(Ee,"href","https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/in_painting_with_stable_diffusion_using_diffusers.ipynb"),r(Ee,"rel","nofollow"),r(at,"align","center"),r(st,"align","center"),r(J,"id","diffusers.StableDiffusionPipeline"),r(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(J,"href","#diffusers.StableDiffusionPipeline"),r(W,"class","relative group"),r(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),r(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(n,f){e(document.head,N),p(n,on,f),p(n,G,f),e(G,H),e(H,ht),b(se,ht,null),e(G,Fn),e(G,mt),e(mt,Un),p(n,an,f),p(n,q,f),e(q,R),e(R,gt),b(le,gt,null),e(q,Vn),e(q,_t),e(_t,Wn),p(n,rn,f),p(n,D,f),e(D,Hn),e(D,bt),e(bt,Rn),e(D,Bn),e(D,de),e(de,Kn),e(D,jn),e(D,ce),e(ce,Yn),e(D,Jn),e(D,fe),e(fe,Qn),e(D,Xn),e(D,pe),e(pe,Zn),e(D,eo),p(n,sn,f),p(n,L,f),e(L,to),e(L,ue),e(ue,no),e(L,oo),e(L,je),e(je,io),e(L,ao),p(n,ln,f),p(n,M,f),e(M,ro),e(M,he),e(he,so),e(M,lo),e(M,me),e(me,co),e(M,fo),p(n,dn,f),p(n,F,f),e(F,B),e(B,vt),b(ge,vt,null),e(F,po),e(F,wt),e(wt,uo),p(n,cn,f),p(n,K,f),e(K,Ye),e(Ye,ho),e(Ye,_e),e(_e,Je),e(K,mo),e(K,yt),e(yt,go),p(n,fn,f),p(n,U,f),e(U,j),e(j,Dt),b(be,Dt,null),e(U,_o),e(U,It),e(It,bo),p(n,pn,f),p(n,Y,f),e(Y,xt),e(xt,$),e($,Pt),e(Pt,vo),e($,wo),e($,St),e(St,yo),e($,Do),e($,Qe),e(Qe,Io),e($,xo),e($,Xe),e(Xe,Po),e(Y,So),e(Y,V),e(V,k),e(k,Et),e(Et,ve),e(ve,Eo),e(k,$o),e(k,$t),e($t,kt),e(kt,ko),e(k,To),e(k,Ze),e(Ze,we),e(we,et),e(k,Co),e(k,tt),e(tt,ye),e(ye,Lo),e(V,Mo),e(V,T),e(T,Tt),e(Tt,De),e(De,Ao),e(T,Oo),e(T,Ct),e(Ct,Lt),e(Lt,zo),e(T,No),e(T,nt),e(nt,Ie),e(Ie,ot),e(T,Go),e(T,it),e(it,xe),e(xe,qo),e(V,Fo),e(V,C),e(C,Mt),e(Mt,Pe),e(Pe,Uo),e(C,Vo),e(C,Se),e(Se,At),e(At,Wo),e(Se,Ho),e(Se,Ot),e(Ot,Ro),e(C,Bo),e(C,at),e(at,Ee),e(Ee,rt),e(C,Ko),e(C,st),e(st,jo),p(n,un,f),p(n,W,f),e(W,J),e(J,zt),b($e,zt,null),e(W,Yo),e(W,Nt),e(Nt,Jo),p(n,hn,f),p(n,u,f),b(ke,u,null),e(u,Qo),e(u,Gt),e(Gt,Xo),e(u,Zo),e(u,Te),e(Te,ei),e(Te,qt),e(qt,ti),e(Te,ni),e(u,oi),e(u,Q),b(Ce,Q,null),e(Q,ii),e(Q,Ft),e(Ft,ai),e(u,ri),e(u,A),b(Le,A,null),e(A,si),e(A,Ut),e(Ut,li),e(A,di),e(A,Vt),e(Vt,ci),e(u,fi),e(u,X),b(Me,X,null),e(X,pi),e(X,Ae),e(Ae,ui),e(Ae,Wt),e(Wt,hi),e(Ae,mi),p(n,mn,f),p(n,h,f),b(Oe,h,null),e(h,gi),e(h,Ht),e(Ht,_i),e(h,bi),e(h,ze),e(ze,vi),e(ze,Rt),e(Rt,wi),e(ze,yi),e(h,Di),e(h,Z),b(Ne,Z,null),e(Z,Ii),e(Z,Bt),e(Bt,xi),e(h,Pi),e(h,O),b(Ge,O,null),e(O,Si),e(O,Kt),e(Kt,Ei),e(O,$i),e(O,jt),e(jt,ki),e(h,Ti),e(h,ee),b(qe,ee,null),e(ee,Ci),e(ee,Fe),e(Fe,Li),e(Fe,Yt),e(Yt,Mi),e(Fe,Ai),p(n,gn,f),p(n,m,f),b(Ue,m,null),e(m,Oi),e(m,Ve),e(Ve,zi),e(Ve,Jt),e(Jt,Ni),e(Ve,Gi),e(m,qi),e(m,We),e(We,Fi),e(We,Qt),e(Qt,Ui),e(We,Vi),e(m,Wi),e(m,te),b(He,te,null),e(te,Hi),e(te,Xt),e(Xt,Ri),e(m,Bi),e(m,z),b(Re,z,null),e(z,Ki),e(z,Zt),e(Zt,ji),e(z,Yi),e(z,en),e(en,Ji),e(m,Qi),e(m,ne),b(Be,ne,null),e(ne,Xi),e(ne,Ke),e(Ke,Zi),e(Ke,tn),e(tn,ea),e(Ke,ta),_n=!0},p:Tr,i(n){_n||(v(se.$$.fragment,n),v(le.$$.fragment,n),v(ge.$$.fragment,n),v(be.$$.fragment,n),v($e.$$.fragment,n),v(ke.$$.fragment,n),v(Ce.$$.fragment,n),v(Le.$$.fragment,n),v(Me.$$.fragment,n),v(Oe.$$.fragment,n),v(Ne.$$.fragment,n),v(Ge.$$.fragment,n),v(qe.$$.fragment,n),v(Ue.$$.fragment,n),v(He.$$.fragment,n),v(Re.$$.fragment,n),v(Be.$$.fragment,n),_n=!0)},o(n){w(se.$$.fragment,n),w(le.$$.fragment,n),w(ge.$$.fragment,n),w(be.$$.fragment,n),w($e.$$.fragment,n),w(ke.$$.fragment,n),w(Ce.$$.fragment,n),w(Le.$$.fragment,n),w(Me.$$.fragment,n),w(Oe.$$.fragment,n),w(Ne.$$.fragment,n),w(Ge.$$.fragment,n),w(qe.$$.fragment,n),w(Ue.$$.fragment,n),w(He.$$.fragment,n),w(Re.$$.fragment,n),w(Be.$$.fragment,n),_n=!1},d(n){t(N),n&&t(on),n&&t(G),y(se),n&&t(an),n&&t(q),y(le),n&&t(rn),n&&t(D),n&&t(sn),n&&t(L),n&&t(ln),n&&t(M),n&&t(dn),n&&t(F),y(ge),n&&t(cn),n&&t(K),n&&t(fn),n&&t(U),y(be),n&&t(pn),n&&t(Y),n&&t(un),n&&t(W),y($e),n&&t(hn),n&&t(u),y(ke),y(Ce),y(Le),y(Me),n&&t(mn),n&&t(h),y(Oe),y(Ne),y(Ge),y(qe),n&&t(gn),n&&t(m),y(Ue),y(He),y(Re),y(Be)}}}const Mr={local:"stable-diffusion-pipelines",sections:[{local:"overview",title:"Overview"},{local:"tips",title:"Tips"},{local:"available-pipelines",title:"Available pipelines"},{local:"diffusers.StableDiffusionPipeline",title:"API"}],title:"Stable diffusion pipelines"};function Ar(oa){return Cr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gr extends Sr{constructor(N){super();Er(this,N,Ar,Lr,$r,{})}}export{Gr as default,Mr as metadata};
